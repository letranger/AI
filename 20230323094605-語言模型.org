:PROPERTIES:
:ID:       23447394-65f3-4149-b26d-c3a88272c0c9
:END:
#+title: 語言模型

* 什麼是語言模型
- 語言模型是一種基於統計和機器學習的自然語言處理技術，它可以模擬人類語言的生成和理解過程。
- 語言模型是自然語言處理領域不可或缺的一部分，它通過統計和機器學習技術，模擬人類語言的生成和理解過程[fn:2]。
- 語言模型起源於語音識別(speech recognition)，輸入一段音頻數據，語音識別系統通常會生成多個句子作為候選，究竟哪個句子更合理？就需要用到語言模型對候選句子進行排序。如今語言模型的應用範圍早已擴展到機器翻譯、信息檢索、問答、文摘等眾多NLP領域[fn:1]。
- 語言模型是這樣一個模型：對於任意的詞序列，它能夠計算出這個序列是一句話的概率[fn:1]。

* 語言模型的類別
語言模型發展史上的幾個里程碑式工作: N-gram LM、FeedForward Neural Network LM、RNN LM和GPT系列
** N-gram[fn:1]
在統計學模型橫行NLP的時代，語言模型任務的扛把子是N-gram語言模型。為了簡化 $p(w_i|w-1,...,w_{i-1})$ 的計算，我們引入一階馬爾可夫假設(first-order Markov assumption)：每個詞只依賴前一個詞，
$p(w_i|w-1,...,w_{i-1})\approx(w_i|w{i-1})$
** 前饋神經網絡語言模型(FeedForward Neural Network Language Models)[fn:1]
N-gram語言模型將詞看作離散變量，使用one-hot進行表示，由此帶來兩大弊端：
- 詞與詞之間不存在語義關聯，簡單來講，從相似性的角度，{"貓", "狗"}要比{"貓", "電腦"}更相似，但是使用one-hot表示後，任意兩個詞之間的距離都相等，則距離度量失去意義，相似性關係蕩然無存，這就導致N-gram語言模型的泛化能力(generalization)受到極大限制，凡是訓練集中沒有出現過的就不是句子。
- N-gram模型的參數數量是指數級：$|V|^{N-1}$ ，為了減少參數量，只能減小V和N的取值， 減小要去除很多低頻詞導致模型效果有所下降，最致命的是N減小讓N-gram模型徹底在長距離依賴(long dependency)問題上敗下陣來，一般N的取值也就是3~5，面對「Yann LeCun|出生|在|法國|他|提出了|LeNet|他|會說|英語|和|？」這樣的問題，束手無策。
** 循環神經網絡語言模型(RNN Language Models)[fn:1]
#+begin_quote
You shall know a word by the company it keeps (Firth,J. R. 1957:11).
#+end_quote

上下文信息(context)決定了詞的語義，而前饋神經網絡語言模型（FFNNLM）同N-gram類似，每個$w_i$只依賴前n個詞，無疑限制了FFNNLM的性能，有沒有更適合語言模型任務的網絡結構？顯然，RNN是個相當不錯的選擇，序列的問題就該讓序列模型去解決。得嘞您內：讀PyTorch源碼學習RNN（1）

說起RNNLM，就不得不提到一個人Tomas Mikolov，博士期間專注於RNNLM的研究：包括RNNLM的訓練、在標準數據集上的對比評估、訓練加速技巧等各方面，也就不難理解為什麼是Mikolov開發了word2vec，因為詞向量在語言模型中太重要了。建議對語言模型領域感興趣的同學都去讀一下Mikolov的博士論文[6]。

從方法創新角度來看，從FFNNLM到RNNLM，似乎只是模型上的普通過渡：用RNN模型替換了FFNN，或者是再用LSTM替換RNN。然而，在這看似平平無奇的模型迭代更新之下，一場新的方法論革命正在悄悄地醞釀、發酵。
** GPT系列[fn:1]
#+begin_quote
GPT GPT，聽說你要開闢新的天地？
GPT GPT，聽說你要把微調都丟棄？
GPT GPT，是誰給你的勇氣？
因為我的參數有1700億！
#+end_quote

2017年是NLP領域的大年，這一年Transformer橫空出世[9]，強大的模型擬合能力迅速席捲了sequence-to-sequence的各項任務，Transformer Encoder主攻特徵表示，Transformer Decoder擅長文本生成，二者合則天下無敵，分則各自為王。

OpenAI的GPT系列就對Transformer Decoder作為語言模型的能力進行了探索，Alec Radford等人高舉無監督預訓練的大旗，首先登場的是GPT-1模型[10]，在規模約5GB的數據集上，使用Transformer Decoder替換RNN進行語言模型任務，然後在下游任務對預訓練好的模型進行微調，並且對下游任務也不挑剔，甭管你是分類還是問答，沒有語言模型解決不了的問題，如果有，那就把訓練數據序列化，梭哈干就完事了！和RNNLM預訓練+微調的套路相似，但是這一次，預訓練數據集更大了，模型更強了，效果也更顯著了，15個下游任務，有12個都超過了當時的SOTA，並且多個任務效果提升明顯。

* 語言模型的應用

語言模型在自然語言處理領域有著廣泛的應用，主要包括以下幾個方面[fn:2]：

1. 語音識別：語言模型可以幫助語音識別系統對語音信號進行解碼，進而生成對應的文本。
1. 機器翻譯：語言模型可以幫助機器翻譯系統生成更加自然流暢的翻譯結果，提高翻譯的質量和可讀性。
1. 文本生成：語言模型可以幫助生成自然語言文本，例如自動寫作、情感分析、文本摘要等。
1. 對話生成：語言模型可以幫助生成自然流暢的對話，例如智能客服、智能助手等。
1. 語言理解：語言模型可以幫助自然語言理解系統識別和分析文本中的語義和結構。

* 語言模型的未來發展
隨著人工智慧技術的不斷發展，語言模型在自然語言處理領域的應用將會越來越廣泛。未來語言模型的發展方向主要包括以下幾個方面[fn:2]：
1. 優化模型結構和參數：通過深度學習技術和大量的文本數據，優化語言模型的結構和參數，提高模型的精度和泛化能力。
1. 深入理解語言：通過語言模型的應用和優化，進一步深入理解語言的結構、語法和語義，提高自然語言處理的效率和準確性。
1. 發展多模態語言模型：通過融合圖像、聲音、文本等多種語言模式，發展多模態語言模型，進一步拓展自然語言處理的應用範圍。
1. 基於語言模型的智能應用：通過與人工智慧技術的結合，基於語言模型開發智能助手、智能客服、智能寫作等應用，為人們帶來更加便捷和高效的服務體驗。

* Footnotes
[fn:2] [[https://foncc.com/archives/30865][語言模型是什麼？它如何改變了自然語言處理的未來？]]

[fn:1] [[https://zhuanlan.zhihu.com/p/32292060][一起入門語言模型(Language Models)]]
