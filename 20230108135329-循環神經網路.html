<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2023-05-25 Thu 18:59 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>循環神經網路</title>
<meta name="author" content="Yung-Chin Yen" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="../css/muse.css" />
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">循環神經網路</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgd93d094">1. 遞迴類神經網路(Recurrent Neural Networks, RNNs)</a></li>
<li><a href="#org1648a02">2. RNN: Recurrent Neural Network，Recursive neural networks。</a>
<ul>
<li><a href="#org6bdb443">2.1. LSTM:</a></li>
<li><a href="#org7867e49">2.2. Bi-directional RNN:</a></li>
</ul>
</li>
<li><a href="#orge4e92fc">3. RNN</a>
<ul>
<li><a href="#orgb795444">3.1. RNN實作</a></li>
<li><a href="#orgebe9e99">3.2. LSTM</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgd93d094" class="outline-2">
<h2 id="orgd93d094"><span class="section-number-2">1.</span> 遞迴類神經網路(Recurrent Neural Networks, RNNs)</h2>
<div class="outline-text-2" id="text-1">
<p>
RNN 能夠處理「任意個數的輸入序列」，所以十分適合用在「語言塑模」或「語音辨識」。理論上，RNN 可以用來處理任何問題，因為它已被證明具有「圖靈完備性」(Turing-Complete)。以遞迴關係的函數表示 RNN 可將其視為 \(S_t=f(S_{t-1},X_t)\)，這裡的\(S_t\)表示第\(t\)步的狀態，它是由函數\(f\)對上一步(\(t-1\))的狀態(即\(S_{t-1}\))與這一步的輸入\(X_t\)所計算出來的結果，這裡的函數\(f\)可以是任何可微分的函數，如\(S_t=tang(S_{t-1}*W+X_t*U)\)。<br />
正因為每個狀態都會與之前所有的計算有關，其所代表的重要含義為：隨著時間的推移，RNNs 可以說是有記憶力的，因為狀態 S 包含了之前所有步驟的資訊。<br />
</p>

<p>
語言塑模的目標是計算「字的序列」的機率，這在「語音辨識」、OCR、「機器翻譯」、「拼字校正」上都非常重要。以「字」為基準的「語言模型」是由「字的序列」來定義機率分佈，給定一個長度為\(m\)的字序列，它會為整個字序列給定一個機率\(P(w_1,...,w_m)\)，其「聯合機率」(joint probability)可以由公式\eqref{org5476c79}中的連鎖規則(chain rule)計算出來：<br />
</p>
\begin{equation}
\label{org5476c79}
P(w_1,...,w_m)=P(w_1)P(w_2|w_1)P(w3|w_2,w_1)...P(w_m|w_1,...,w_{m-1})
\end{equation}

<p>
這個聯合機率一般是基於一個「獨立性假設」(independence assumption)，即，第 i 個字只會相依於它之前的 n-1 個字，如果我們的模型是連續 n 個字的聯合機率，就稱為「n元」(n-gram)。例：<br />
</p>
<ul class="org-ul">
<li>1-gram / unigram: &ldquo;The&rdquo;, &ldquo;quick&rdquo;, &ldquo;brown&rdquo; and &ldquo;fox&rdquo;<br /></li>
<li>2-grams / bigram: &ldquo;The quick&rdquo;, &ldquo;quick brown&rdquo; and &ldquo;brown fox&rdquo;<br /></li>
<li>3-grams / trigram: &ldquo;The quick brown&rdquo; and &ldquo;quick brown fox&rdquo;<br /></li>
<li>4-grams: &ldquo;The quick brown fox&rdquo;<br /></li>
</ul>

<p>
現在，如果我們有一個巨大的語料庫(corpus of text)，我們就可以用一個特定的 n(通常為 2-4)搜尋所有「n元」在「語料庫」中出現的次數，進而在「給定前 n-1 個字的前提下」，估計出每個 n 元中最後一個字出現的機率。<br />
</p>
</div>
</div>

<div id="outline-container-org1648a02" class="outline-2">
<h2 id="org1648a02"><span class="section-number-2">2.</span> RNN: Recurrent Neural Network，Recursive neural networks。</h2>
<div class="outline-text-2" id="text-2">
<p>
雖然很多時候我們把這兩種網絡都叫做RNN，但事實上這兩種網路的結構事實上是不同的。而我們常常把兩個網絡放在一起的原因是：它們都可以處理有序列的問題，比如時間序列等。<br />
  舉個最簡單的例子，我們預測股票走勢用RNN就比普通的DNN效果要好，原因是股票走勢和時間相關，今天的價格和昨天、上周、上個月都有關係。而RNN有「記憶」能力，可以「模擬」數據間的依賴關係(Dependency)。<br />
</p>
</div>

<div id="outline-container-org6bdb443" class="outline-3">
<h3 id="org6bdb443"><span class="section-number-3">2.1.</span> LSTM:</h3>
<div class="outline-text-3" id="text-2-1">
<p>
為了加強這種RNN的「記憶能力」，人們開發各種各樣的變形體，如非常著名的Long Short-term Memory(LSTM)，用於解決「長期及遠距離的依賴關係」。如下圖所示，左邊的小圖是最簡單版本的循環網絡，而右邊是人們為了增強記憶能力而開發的LSTM。<br />
</p>

<div id="org6ddd83d" class="figure">
<p><img src="images/3r5o0000r126proo7o8q.jpg" alt="3r5o0000r126proo7o8q.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 1: </span>LSTM</p>
</div>
</div>
</div>
<div id="outline-container-org7867e49" class="outline-3">
<h3 id="org7867e49"><span class="section-number-3">2.2.</span> Bi-directional RNN:</h3>
<div class="outline-text-3" id="text-2-2">
<p>
另一個循環網絡的變種 - 雙向循環網絡(Bi-directional RNN)也是現階段自然語言處理和語音分析中的重要模型。開發雙向循環網絡的原因是語言/語音的構成取決於上下文，即「現在」依託於「過去」和「未來」。單向的循環網絡僅著重於從「過去」推出「現在」，而無法對「未來」的依賴性有效的建模。<br />
</p>
</div>
</div>
</div>

<div id="outline-container-orge4e92fc" class="outline-2">
<h2 id="orge4e92fc"><span class="section-number-2">3.</span> RNN</h2>
<div class="outline-text-2" id="text-3">
<p>
RNN 是一種有「記憶力」的神經網路，其最為人所知的形式如下：<br />
</p>
<p width="500">
<img src="images/rnn-static.png" alt="rnn-static.png" width="500" /><br />
如同上圖等號左側所示，RNN 跟一般深度學習中常見的前饋神經網路（Feedforward Neural Network, 後簡稱 FFNN）最不一樣的地方在於它有一個迴圈（Loop）。<br />
要了解這個迴圈在 RNN 裏頭怎麼運作，現在讓我們想像有一個輸入序列 X（Input Sequence）其長相如下：<br />
\[ X = [ x_0, x_1, x_2, \dots x_t ]\]<br />
</p>
<ol class="org-ol">
<li>不同於 FFNN，RNN 在第一個時間點 \(t_0\) 並不會直接把整個序列 \(X\) 讀入。反之，在第一個時間點 \(t_0\)，它只將該序列中的第一個元素 \(x_0\) 讀入中間的細胞 A。細胞 A 則會針對 \(x_0\) 做些處理以後，更新自己的「狀態」並輸出第一個結果 \(h_0\) 。<br /></li>
<li>在下個時間點 \(t_1\)，RNN 如法炮製，讀入序列 \(X\) 中的下一個元素 \(x_1\)，並利用剛剛處理完 \(x_0\) 得到的細胞狀態，處理 \(x_1\) 並更新自己的狀態（也被稱為記憶），接著輸出另個結果 \(h_1\)。<br /></li>
<li>剩下的 \(x_t\) 都會被以同樣的方式處理。但不管輸入的序列 \(X\) 有多長，RNN 的本體從頭到尾都是等號左邊的樣子：迴圈代表細胞 A 利用「上」一個時間點（比方說 \(t_1\)）儲存的狀態，來處理當下的輸入（比方說 \(x_2\) ）。<br /></li>
</ol>

<p>
但如果你將不同時間點（\(t_0\), \(t_1\) &#x2026;）的 RNN 以及它的輸入一起截圖，並把所有截圖從左到右一字排開的話，就會長得像等號右邊的形式。將 RNN 以右邊的形式表示的話，你可以很清楚地了解，當輸入序列越長，向右展開的 RNN 也就越長。（模型也就需要訓練更久時間，這也是為何我們在資料前處理時設定了序列的最長長度）<br />
</p>

<p>
為了確保你 100 % 理解 RNN，讓我們假設剛剛的序列 X 實際上是一個內容如下的英文問句：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">X</span> = [ What, time, <span style="color: #51afef;">is</span>, it, ? ]
</pre>
</div>
<p>
而且 RNN 已經處理完前兩個元素 What 和 time 了。<br />
</p>

<p>
則接下來 RNN 會這樣處理剩下的句子：<br />
</p>
<p width="500">
<img src="images/rnn-animate.gif" alt="rnn-animate.gif" width="500" /><br />
就像你現在閱讀這段話一樣，你是由左到右逐字在大腦裡處理我現在寫的文字，同時不斷地更新你腦中的記憶狀態。<br />
</p>

<p>
每當下個詞彙映入眼中，你腦中的處理都會跟以下兩者相關：<br />
</p>
<ul class="org-ul">
<li>前面所有已讀的詞彙<br /></li>
<li>目前腦中的記憶狀態<br /></li>
</ul>

<p>
當然，實際人腦的閱讀機制更為複雜，但 RNN抓到這個處理精髓，利用內在迴圈以及細胞內的「記憶狀態」來處理序列資料。<br />
</p>
</div>

<div id="outline-container-orgb795444" class="outline-3">
<h3 id="orgb795444"><span class="section-number-3">3.1.</span> RNN實作</h3>
<div class="outline-text-3" id="text-3-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">state_t</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">2: </span><span style="color: #51afef;">for</span> input_t <span style="color: #51afef;">in</span> input_sequence:
<span class="linenr">3: </span>    <span style="color: #dcaeea;">output_t</span> = f(input_t, state_t)
<span class="linenr">4: </span>    <span style="color: #dcaeea;">state_t</span> = output_t
</pre>
</div>
<p>
在 RNN 每次讀入任何新的序列數據前，細胞 A 中的記憶狀態 state_t 都會被初始化為 0。<br />
</p>

<p>
接著在每個時間點 t，RNN 會重複以下步驟：<br />
</p>

<ul class="org-ul">
<li>讀入 input_sequence 序列中的一個新元素 input_t<br /></li>
<li>利用 f 函式將當前細胞的狀態 state_t 以及輸入 input_t 做些處理產生 output_t<br /></li>
<li>輸出 output_t 並同時更新自己的狀態 state_t<br /></li>
</ul>

<p>
在 Keras 裏頭只要 2 行就可以建立一個 RNN layer：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">2: </span><span style="color: #dcaeea;">rnn</span> = layers.SimpleRNN()
</pre>
</div>

<div id="org25227a6" class="figure">
<p><img src="images/nn-layers.jpg" alt="nn-layers.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 2: </span>RNN示例</p>
</div>
</div>
</div>
<div id="outline-container-orgebe9e99" class="outline-3">
<h3 id="orgebe9e99"><span class="section-number-3">3.2.</span> LSTM</h3>
<div class="outline-text-3" id="text-3-2">
<p>
如下的簡易RNN<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">state_t</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">2: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32048;&#32990; A &#26371;&#37325;&#35079;&#22519;&#34892;&#20197;&#19979;&#34389;&#29702;</span>
<span class="linenr">3: </span><span style="color: #51afef;">for</span> input_t <span style="color: #51afef;">in</span> input_sequence:
<span class="linenr">4: </span>    <span style="color: #dcaeea;">output_t</span> = f(input_t, state_t)
<span class="linenr">5: </span>    <span style="color: #dcaeea;">state_t</span> = output_t
</pre>
</div>
<p>
要如何將細胞 A 當下的記憶 state_t 與輸入 input_t 結合，才能產生最有意義的輸出 output_t 呢？<br />
</p>

<p>
在 SimpleRNN 的細胞 A 裡頭，這個 f 的實作很簡單。而這導致其記憶狀態 state_t 沒辦法很好地「記住」前面處理過的序列元素，造成 RNN 在處理後來的元素時，就已經把前面重要的資訊給忘記了。(只有短期記憶，沒有長期記憶)<br />
</p>

<p>
長短期記憶（Long Short-Term Memory, 後簡稱 LSTM）就是被設計來解決 RNN 的這個問題。如下圖所示，你可以把 LSTM 想成是 RNN 中用來實現細胞 A 內部處理邏輯的一個特定方法：<br />
</p>

<p width="500">
<img src="images/lstm-cell.png" alt="lstm-cell.png" width="500" /><br />
基本上一個 LSTM 細胞裡頭會有 3 個閘門（Gates）來控制細胞在不同時間點的記憶狀態：<br />
</p>

<ul class="org-ul">
<li>Forget Gate：決定細胞是否要遺忘目前的記憶狀態<br /></li>
<li>Input Gate：決定目前輸入有沒有重要到值得處理<br /></li>
<li>Output Gate：決定更新後的記憶狀態有多少要輸出<br /></li>
</ul>

<p>
透過這些閘門控管機制，LSTM 可以將很久以前的記憶狀態儲存下來，在需要的時候再次拿出來使用。值得一提的是，這些閘門的參數也都是神經網路自己訓練出來的。<br />
</p>


<div id="org24dfdba" class="figure">
<p><img src="images/lstm-cell-detailed.png" alt="lstm-cell-detailed.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 3: </span>LSTM 細胞頂端那條 cell state 正代表著細胞記憶的轉換過程</p>
</div>

<p>
想像 LSTM 細胞裡頭的記憶狀態是一個包裹，上面那條直線就代表著一個輸送帶。<br />
</p>

<p>
LSTM 可以把任意時間點的記憶狀態（包裹）放上該輸送帶，然後在未來的某個時間點將其原封不動地取下來使用。<br />
</p>

<div id="orgfcd024b" class="figure">
<p><img src="images/accumulation-conveyor-101.jpg" alt="accumulation-conveyor-101.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 4: </span>Caption</p>
</div>

<p>
因為這樣的機制，讓 LSTM 即使面對很長的序列數據也能有效處理，不遺忘以前的記憶。<br />
</p>

<p>
因為效果卓越，LSTM 非常廣泛地被使用。事實上，當有人跟你說他用 RNN 做了什麼 NLP 專案時，有 9 成機率他是使用 LSTM 或是 GRU（LSTM 的改良版，只使用 2 個閘門） 來實作，而不是使用最簡單的 SimpleRNN。<br />
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Yung-Chin Yen</p>
<p class="date">Created: 2023-05-25 Thu 18:59</p>
</div>
</body>
</html>
