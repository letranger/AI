<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-03-30 Sun 14:38 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>時間序列的預言者：如何通過 RNN、LSTM 和 GRU 預測未來</title>
<meta name="author" content="Yung-Chin Yen" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="../css/muse.css" />
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">時間序列的預言者：如何通過 RNN、LSTM 和 GRU 預測未來</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org7408775">1. CNN的限制</a></li>
<li><a href="#org4ee3e01">2. 遞迴神經網路(Recurrent Neural Networks, RNNs)</a></li>
<li><a href="#orgd98dd1e">3. RNN實作</a>
<ul>
<li><a href="#orgc44fd4a">3.1. RNN的程式運作示例</a></li>
<li><a href="#org9ce6595">3.2. RNN的程式模型架構</a></li>
<li><a href="#org4902ed4">3.3. 應用情境</a></li>
</ul>
</li>
<li><a href="#orged3d88c">4. LSTM</a>
<ul>
<li><a href="#org4360c79">4.1. LSTM的運作原理</a></li>
<li><a href="#orgded9922">4.2. LSTM的程式運作示例</a></li>
<li><a href="#org547c33a">4.3. LSTM的程式模型架構</a></li>
<li><a href="#org875d06d">4.4. 實作: 以AI預測股價-隔日漲跌</a></li>
</ul>
</li>
<li><a href="#orgb95a974">5. 貼圖</a>
<ul>
<li><a href="#orge39366d">5.1. 讀圖</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-org7408775" class="outline-2">
<h2 id="org7408775"><span class="section-number-2">1.</span> CNN的限制</h2>
<div class="outline-text-2" id="text-1">
<p>
在<a href="20221023101414-卷積神經網路.html#ID-20221023T101414.457264">卷積神經網路</a>中，我們提過CNN的想法源自於對人類大腦認知方式的模仿，當我們辨識一個圖像，會先注意到顏色鮮明的點、線、面，之後將它們構成一個個不同的形狀(眼睛、鼻子、嘴巴 &#x2026;)，這種抽象化的過程就是 CNN 演算法建立模型的方式。其過程如圖<a href="#org9c9670f">1</a><sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>。<br />
</p>


<div id="org9c9670f" class="figure">
<p><img src="images/CNN-1.png" alt="CNN-1.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 1: </span>CNN 概念</p>
</div>

<p>
至於圖片中的每一個特徵則是利用卷積核來取得(如圖<a href="#org6faf10c">2</a>)，換言之，CNN其實是在模擬人類的眼睛。<br />
</p>

<div id="org6faf10c" class="figure">
<p><img src="images/CNN_卷積神經網路/2024-04-17_09-18-53_2024-04-17_09-11-28.png" alt="2024-04-17_09-18-53_2024-04-17_09-11-28.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 2: </span>CNN原理</p>
</div>

<p>
萬一我們所要處理的資料並不是一張張的圖、而是一系列連續性、有時間順序的資料呢？例如：<br />
</p>
<ul class="org-ul">
<li>一篇文章: 也許我們想生成這篇文章的摘要<br /></li>
<li>一段時間內蒐集一的某地PM 2.5數值: 也許我們想預測該地下週的PM 2.5<br /></li>
<li>一段演講錄音: 也許我們想生成逐字稿<br /></li>
</ul>

<p>
你會發現，這類資料其實不太適合用眼睛，可能更適合用耳朵，所以拿CNN來分析這類資料大概是用錯了工具(相信經過<a href="20240326202851-理財達人競賽.html#ID-0d76c861-2338-4fff-942a-47b6e02e86e3">理財達人競賽</a>的你應該深有同感)。<br />
</p>

<p>
那麼，哪一種模型比較適合模擬出人類的耳朵功能？這是本節的討論重點。<br />
</p>
</div>
</div>
<div id="outline-container-org4ee3e01" class="outline-2">
<h2 id="org4ee3e01"><span class="section-number-2">2.</span> 遞迴神經網路(Recurrent Neural Networks, RNNs)</h2>
<div class="outline-text-2" id="text-2">
<p>
遞迴神經網絡（RNN）是一種專門設計來處理序列資料的人工神經網絡。序列資料指的是那些隨時間連續出現的資料，比如語言（單詞組成的句子）、影片（一連串的影像畫面），或者是音樂（一連串的音符）。<br />
</p>

<p>
想像你利用每天晚上睡前花30分鐘追劇，每當新的一集開始時，你通常還會記得上一集發生了什麼。RNN也是這樣工作的：它在處理資料（例如一句話中的每個單詞）時，會記得之前的資訊，並利用這些資訊來幫助理解或預測下一步會發生什麼。<br />
</p>

<p>
那RNN是如何做到這點的呢?這種“記憶”是通過網絡中的循環連接實現的。這些連接使得訊息可以在模型的一層之間前後流動，就像你在看連續劇時保持對劇情的記憶一樣。我們先來看圖<a href="#orgd8631e9">3</a>，右邊是我們熟悉的神經網路(例如<a href="20221023101414-卷積神經網路.html#ID-20221023T101414.457264">CNN</a>、DNN)，資料一律由模型的左側layer往右側傳送；而左邊的RNN則有點不同，每一層的神經元在將資料往右傳遞的同時，還偷偷留了一份給 <b><b>自己</b></b> ，這裡說的自己不是真正的自己，而是 <b><b>下一個回合的自己</b></b> 。<br />
</p>


<div id="orgd8631e9" class="figure">
<p><img src="images/遞迴神經網路(Recurrent_Neural_Networks,_RNNs)/2024-05-10_08-43-10_RNN-03.png" alt="2024-05-10_08-43-10_RNN-03.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 3: </span>CNN v.s. RNN</p>
</div>

<p>
聽起來好像有點抽象，沒關係，我們現在把圖<a href="#orgd8631e9">3</a>中左側RNN的某一個神經元單獨抽出來分解它的內部動作，我們把圖<a href="#orgd8631e9">3</a>中的那個循環的箭頭拆解成如圖<a href="#orgd60ed51">4</a>。<br />
</p>


<div id="orgd60ed51" class="figure">
<p><img src="images/遞迴神經網路(Recurrent_Neural_Networks,_RNNs)/2024-05-10_09-01-35_RNN-Cell-1.png" alt="2024-05-10_09-01-35_RNN-Cell-1.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 4: </span>RNN典型結構</p>
</div>

<p>
要看懂圖<a href="#orgd60ed51">4</a>，你只要搞清楚三件事:<br />
</p>
<ol class="org-ol">
<li>RNN不像<a href="20221023101414-卷積神經網路.html#ID-20221023T101414.457264">CNN</a>那樣每次讀入一整張圖，而是分批讀入序列資料，例如，第1次(也就是第1個時間點(\(t_0\))讀入\(X_0\)、第2次(也就是第2個時間點(\(t_1\))讀入\(X_1\)&#x2026;<br /></li>
<li>圖中右側「展開後」的三神經元其實是同一個，分別代表不同時間點的神經元，我們可以由 \(h_{t-1}, h_{t}, h_{t+1}\) 和 \(X_{t-1}, X_{t}, X_{t+1}\) 觀察出同樣的意思。<br /></li>
<li>原本常見的資料在模型中傳遞方向是由左而右，在圖<a href="#orgd60ed51">4</a>中則是由下而上，也就是輸入資料是底下的\(X_t\)、輸出為上面的\(h_t\)。<br /></li>
</ol>

<p>
圖<a href="#orgd60ed51">4</a>右側代表的意思是：<br />
</p>
<ol class="org-ol">
<li>在第1個時間點(\(t-1\))取得輸入(\(X_{t-1}\))後，神經元會針對 \(X_{t-1}\) 進行運算，更新自己的「狀態」(這個就是會影響「下一個自己」的關鍵)然後輸出結果\(h_{t-1}\)<br /></li>
<li>在第2個時間點(\(t\))取得輸入(\(X_{t}\))後，利用剛才(時間點{\(t-1\))更新的「狀態」來運算\(X_t\)(這就是神經元受到上一個自己影響的來源，也被稱為「記憶」)，然後再次更新自己的狀態並輸出結果\(h_t\)<br /></li>
<li>最後，在第3個時間點(\(t+1\))取得輸入(\(X_{t+1}\))後，利用剛才(時間點{\(t\))更新的「狀態」來運算\(X_{t+1}\)，然後再次更新自己的狀態並輸出結果\(h_{t+1}\)<br /></li>
</ol>

<p>
整個資料讀取、處理、傳遞的流程大致如下圖所示：<br />
</p>


<div id="orga243cb8" class="figure">
<p><img src="images/遞迴神經網路(Recurrent_Neural_Networks,_RNNs)/2024-05-10_13-00-27_Fully_connected_Recurrent_Neural_Network.webp" alt="2024-05-10_13-00-27_Fully_connected_Recurrent_Neural_Network.webp" width="500" /><br />
</p>
<p><span class="figure-number">Figure 5: </span>RNN的運作流程</p>
</div>

<p>
用個具體一點的例子，假設我們假設剛剛的序列 X 實際上是一個內容如下的英文問句：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">X</span> = [ What, time, <span style="color: #51afef;">is</span>, it, ? ]
</pre>
</div>
<p>
而且 RNN 已經處理完前兩個元素 What 和 time 了。<br />
</p>

<p>
則接下來 RNN 會這樣處理剩下的句子：<br />
</p>

<div id="org06fe057" class="figure">
<p><img src="images/rnn-animate.gif" alt="rnn-animate.gif" width="500" /><br />
</p>
<p><span class="figure-number">Figure 6: </span>RNN如何處理自然語言</p>
</div>

<p>
如同我們由左到右逐字閱讀這段文字同時不斷地更新你腦中的記憶狀態，RNN也是以相同的原理在做這件事。RNN的這種設計使它特別適合於像語言翻譯、語音識別或任何需要考慮過去資訊以更好地理解當前情境的任務。例如，在翻譯句子時，理解前面的詞可以幫助更準確地翻譯後面的詞。<br />
</p>

<p>
上面提及RNN的「記憶」能力是由神經元的「狀態」實作出來，這種狀態以一個隱藏向量(hidden vector)的形式存在於神經元中，如圖<a href="#orgc4c15d9">7</a>中的\(h_{t}\))。<br />
</p>


<div id="orgc4c15d9" class="figure">
<p><img src="images/遞迴神經網路(Recurrent_Neural_Networks,_RNNs)/2024-05-10_15-39-00_RNN-Cell-2.png" alt="2024-05-10_15-39-00_RNN-Cell-2.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 7: </span>RNN神經元</p>
</div>

<p>
這個神經元在時間點t的輸出 \(h_t\) 由以下公式計算:<br />
\[ h_t = f(W_x \cdot X_t + W_h \cdot h_{t-1} + b) \]<br />
其中，<br />
</p>
<ul class="org-ul">
<li>\(X_t\): 這是在每個時間點(\(t\))輸入給神經元給 RNN 的資料，例如句子中的單字、圖像中的像素或時間序列中的資料點。<br /></li>
<li>\(h_{t-1}\): 先前的隱藏狀態(\(h_{t-1}\))，可以把它看成前一個時間點(\(t-1\))的網路記憶，就是它封裝了重要的歷史訊息，以舊有的記憶協助 RNN 理解當前的資料。<br /></li>
<li>權重矩陣(\(W_t, W_h\) ): 這些矩陣是模型訓練的目的，可以將其視為模型的「知識」。它們決定了應該對當前輸入(\(X_t\)) 和過去記憶(\(h_{t-1}\) ) 的重視程度。<br /></li>
<li>偏移值(\(b\)): 偏差項可作為模型的微調器，確保激活函數與資料的固有特徵協調運作。<br /></li>
<li>激活函數(\(f\)): RNN常用的激活函數有tanh 或 ReLU，讓RNN具備非線性的特徵，以捕捉線性模型可能忽略的複雜資料模式。<br /></li>
</ul>

<p>
但是，RNN也有一些限制，比如它們很難處理很長的序列，因為過長時間的記憶會逐漸消失。這就像如果你試圖回憶幾個月前看的某集連續劇的細節，可能會比較困難。這個問題在後來被一種叫做LSTM的更進階版本的RNN解決。<br />
</p>

<p>
總之，RNN是一種強大的工具，專門用於處理和預測序列資料中的模式，就像我們用記憶來理解和預測日常生活中的事件一樣。<br />
</p>
</div>
</div>
<div id="outline-container-orgd98dd1e" class="outline-2">
<h2 id="orgd98dd1e"><span class="section-number-2">3.</span> RNN實作</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-orgc44fd4a" class="outline-3">
<h3 id="orgc44fd4a"><span class="section-number-3">3.1.</span> RNN的程式運作示例</h3>
<div class="outline-text-3" id="text-3-1">
<p>
RNN的運作概念非常簡單，就是在每個時間點 t，RNN 會讀入一個新的序列資料 input_t，並利用這個資料以及自己的記憶狀態 state_t 來產生一個輸出 output_t。這個過程可以用下面的程式碼來表示：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">f</span>(input_t, state_t): <span style="color: #5B6268;"># </span><span style="color: #5B6268;">f &#20989;&#24335;&#26159;&#31070;&#32147;&#20803;&#30340;&#36939;&#31639;</span>
<span class="linenr">2: </span>    <span style="color: #51afef;">return</span> input_t + state_t
<span class="linenr">3: </span><span style="color: #dcaeea;">state_t</span> = <span style="color: #da8548; font-weight: bold;">0</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21021;&#22987;&#21270;&#32048;&#32990;&#30340;&#29376;&#24907;</span>
<span class="linenr">4: </span><span style="color: #51afef;">for</span> input_t <span style="color: #51afef;">in</span> input_sequence:
<span class="linenr">5: </span>    <span style="color: #dcaeea;">output_t</span> = f(input_t, state_t) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">f &#20989;&#24335;&#26159;&#31070;&#32147;&#20803;&#30340;&#36939;&#31639;</span>
<span class="linenr">6: </span>    <span style="color: #dcaeea;">state_t</span> = output_t <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26356;&#26032;&#32048;&#32990;&#30340;&#29376;&#24907;</span>
</pre>
</div>
<p>
在 RNN 每次讀入任何新的序列資料前，細胞 A 中的記憶狀態 state_t 都會被初始化為 0。<br />
</p>

<p>
接著在每個時間點 t，RNN 會重複以下步驟：<br />
</p>
<ul class="org-ul">
<li>讀入 input_sequence 序列中的一個新元素 input_t<br /></li>
<li>利用 f 函式將當前細胞的狀態 state_t 以及輸入 input_t 做些處理產生 output_t<br /></li>
<li>輸出 output_t 並同時更新自己的狀態 state_t<br /></li>
</ul>

<p>
面對一個如下的簡易RNN，要如何將神經元當下的記憶 state_t 與輸入 input_t 結合，才能產生最有意義的輸出 output_t 呢？<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">state_t</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">2: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32048;&#32990; A &#26371;&#37325;&#35079;&#22519;&#34892;&#20197;&#19979;&#34389;&#29702;</span>
<span class="linenr">3: </span><span style="color: #51afef;">for</span> input_t <span style="color: #51afef;">in</span> input_sequence:
<span class="linenr">4: </span>    <span style="color: #dcaeea;">output_t</span> = f(input_t, state_t)
<span class="linenr">5: </span>    <span style="color: #dcaeea;">state_t</span> = output_t
</pre>
</div>

<p>
RNN神經元在時間點t的輸出 \(h_t\) 由以下公式計算:<br />
\[ h_t = f(W_x \cdot X_t + W_h \cdot h_{t-1} + b) \]<br />
</p>

<p>
在 SimpleRNN 的神經元中，這個函數 \(f\) 的實作很簡單，這也導致了其記憶狀態 state_t 沒辦法很好地「記住」前面處理過的序列元素，因而造成 RNN 在處理後來的元素時，就已經把前面重要的資訊給忘記了，也就是只有短期記憶，沒有長期記憶。長短期記憶（Long Short-Term Memory, 後簡稱 LSTM）就是被設計來解決 RNN 的這個問題。<br />
</p>
</div>
</div>
<div id="outline-container-org9ce6595" class="outline-3">
<h3 id="org9ce6595"><span class="section-number-3">3.2.</span> RNN的程式模型架構</h3>
<div class="outline-text-3" id="text-3-2">
<p>
RNN的模型架構非常簡單，只需要一個 RNN 層即可。以 Keras 為例，建立一個 RNN 層只需要建立一個 SimpleRNN 層即可。以下是一個簡單的 RNN 模型架構：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span class="linenr"> 2: </span><span style="color: #51afef;">from</span> tensorflow.keras <span style="color: #51afef;">import</span> layers, models
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27169;&#22411;&#36664;&#20837;&#23652; (4&#20491;&#31680;&#40670;)</span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">inputs</span> = layers.Input(shape=(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">4</span>))
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38577;&#34255;&#23652; (RNN&#23652;, 2&#20491;&#31680;&#40670;&#65292;&#23565;&#25033;h1, h2)</span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">hidden</span> = layers.SimpleRNN(units=<span style="color: #da8548; font-weight: bold;">2</span>, activation=<span style="color: #98be65;">'tanh'</span>, return_sequences=<span style="color: #a9a1e1;">False</span>)(inputs)
<span class="linenr"> 9: </span>
<span class="linenr">10: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27169;&#22411;&#36664;&#20986;&#23652; (2&#20491;&#31680;&#40670;)</span>
<span class="linenr">11: </span><span style="color: #dcaeea;">outputs</span> = layers.Dense(units=<span style="color: #da8548; font-weight: bold;">2</span>, activation=<span style="color: #98be65;">'linear'</span>)(hidden)
<span class="linenr">12: </span>
<span class="linenr">13: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24314;&#31435;&#20006;&#32232;&#35695;&#27169;&#22411;</span>
<span class="linenr">14: </span><span style="color: #dcaeea;">model</span> = models.Model(inputs=inputs, outputs=outputs)
<span class="linenr">15: </span>model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'adam'</span>, loss=<span style="color: #98be65;">'mse'</span>)
<span class="linenr">16: </span>
<span class="linenr">17: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#27169;&#22411;&#32080;&#27083;</span>
<span class="linenr">18: </span>model.summary()
</pre>
</div>

<pre class="example" id="org00679de">
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input_layer (InputLayer)        │ (None, 1, 4)           │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ simple_rnn (SimpleRNN)          │ (None, 2)              │            14 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense (Dense)                   │ (None, 2)              │             6 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 20 (80.00 B)
 Trainable params: 20 (80.00 B)
 Non-trainable params: 0 (0.00 B)
</pre>
<p>
上述模型架構的資料流向如下：<br />
</p>

<div id="org5a823e5" class="figure">
<p><img src="rnn_model.png" alt="rnn_model.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 8: </span>RNN架構</p>
</div>
</div>
</div>
<div id="outline-container-org4902ed4" class="outline-3">
<h3 id="org4902ed4"><span class="section-number-3">3.3.</span> 應用情境</h3>
<div class="outline-text-3" id="text-3-3">
<p>
針對上述模型，我們可以設想一個智慧工廠中的機器維護預測系統，這個工廠中的一台重要機器每分鐘會傳回 4 個感測器數據：溫度、壓力、振動幅度和電流。這些數據會被送進 RNN 模型中，並根據目前時間點的數值預測機器的異常風險和維修建議指數。<br />
</p>

<p>
機器感測器每分鐘會收到下列資料：<br />
</p>
<ul class="org-ul">
<li>溫度（Temperature）<br /></li>
<li>壓力（Pressure）<br /></li>
<li>振動幅度（Vibration）<br /></li>
<li>電流（Current）<br /></li>
</ul>

<p>
這些資料會被送進 RNN 模型中，並根據目前時間點的數值預測：<br />
</p>
<ul class="org-ul">
<li>預測下個時間點的異常風險（是否有可能故障）<br /></li>
<li>預測是否需維修（建議立即維護/可繼續運行）<br /></li>
</ul>

<p>
模型解讀：<br />
</p>
<ul class="org-ul">
<li>輸入層（4 個感測器值）<br /></li>
<li>RNN 隱藏層會考慮時間序列的歷史變化（例如溫度持續升高等趨勢）<br /></li>
<li>輸出層（2 個節點）代表兩個預測目標：<br />
<ul class="org-ul">
<li>y₁: 機器異常風險機率<br /></li>
<li>y₂: 維修建議指數（例如 0~1）<br /></li>
</ul></li>
</ul>

<p>
實際應用流程：<br />
</p>
<ol class="org-ol">
<li>感測器每分鐘傳來資料，累積成序列輸入模型。<br /></li>
<li>RNN 模型處理並輸出風險值與建議。<br /></li>
<li>若風險高，系統自動發出警報並派出工程師檢條，防範因機器突然故障導致生產線停機進而導致生產進度落後與金錢損失。<br /></li>
</ol>
</div>
</div>
</div>
<div id="outline-container-orged3d88c" class="outline-2">
<h2 id="orged3d88c"><span class="section-number-2">4.</span> LSTM</h2>
<div class="outline-text-2" id="text-4">
<p>
RNN的一個主要問題是，當序列變得很長時，它們很難記住遠處的資訊。這是因為在 RNN 中，每個時間點的輸出都是由當前輸入和上一個時間點的輸出共同決定的。這意味著當序列變得很長時，RNN 會遺忘遠處的資訊，導致模型無法很好地理解整個序列。<br />
</p>

<p>
為了加強這種RNN的「記憶能力」，人們開發各種各樣的變形體，如非常著名的Long Short-term Memory(LSTM)，用於解決「長期及遠距離的依賴關係」。<br />
</p>
</div>
<div id="outline-container-org4360c79" class="outline-3">
<h3 id="org4360c79"><span class="section-number-3">4.1.</span> LSTM的運作原理</h3>
<div class="outline-text-3" id="text-4-1">
<p>
想象你有一個書包（LSTM的內部結構），你可以決定在上課前放入什麼書籍、何時取出某本書，或者甚至決定更新裡面的某些書，你每天上學就利用書包裡的書來學習新的知識。LSTM也有類似的機制來處理信息，這些機制就是一個個的閘門(Gate)。<br />
</p>

<p>
典型的LSTM架構如圖<a href="#orgdc5612e">9</a>所示，可以看出除了原本的資料輸入(input)，LSTM還多了三個輸入，分別是input(模型輸入），forget gate(遺忘門)，input gate(輸入門)，以及output gate(輸出門)。因此相比普通的神經網路，LSTM的參數量是它們的4倍。這3個門訊號都是處於0～1之間的實數，1代表完全打開，0代表關閉。<br />
</p>

<ol class="org-ol">
<li>遺忘閘（Forget Gate）：這就像是你決定從書包中拿掉不再需要的書。在LSTM中，遺忘閘會查看新的輸入信息和當前的記憶，然後決定保留哪些記憶（有用的）或者遺忘哪些（不再重要的）。<br /></li>
<li>輸入閘（Input Gate）：這是決定將哪些新書放入書包。LSTM會評估當前的輸入（例如新的單詞或資料點），並決定應該添加哪些信息到記憶中，這有助於更新記憶內容。<br /></li>
<li>輸出閘（Output Gate）：決定從書包中拿出哪本書來使用。根據需要的話題或任務，LSTM會決定哪些記憶是目前有用的，然後基於這些記憶提供輸出信息。<br /></li>
<li>記憶單元（Memory Cell）：這是LSTM的核心，它負責記錄和更新所有的記憶。記憶單元是一個長期的記憶存儲，可以通過遺忘閘和輸入閘來更新。LSTM 中的 Memory Cell（也就是記憶狀態，通常記為 Cₜ）的核心功能，就是要 跨時間步保持資訊的狀態，這也是它的關鍵設計。<br /></li>
</ol>


<div id="orgdc5612e" class="figure">
<p><img src="lstm_model.png" alt="lstm_model.png" width="600" /><br />
</p>
<p><span class="figure-number">Figure 9: </span>LSTM架構</p>
</div>

<p>
進一步從時間序列的角度來看，LSTM運作過程中的資料流向如下：<br />
</p>
<ul class="org-ul">
<li>遺忘閘(Forget Gate)：該閘決定在特定時間點(timestamp, 例如圖<a href="#orge472887">10</a>中的\(t\) )，前一個時間點(\(t-1\)) 的模型記憶(也就是狀態, state)是否會被記住保留參與這個時間點的運算，或是直接被遺忘。當遺忘門打開時，前一刻的記憶會被保留，當遺忘門關閉時，前一刻的記憶就會被清空。換句話說，就讓模型具備選擇性遺忘部份訊息的能力，這個機制可以由激活函數sigmoid來實作，其中0代表完全忘記，1代表完全記住。<br /></li>
<li>輸入閘(Input Gate): 決定目前這個時間點有哪些神經元的輸入(\(X\))中有哪些是足夠重要到可以保留下來加入「目前狀態」中，因為在序列輸入中，並不是每個時刻的輸入的資訊都是同等重要的，當輸入完全沒有用時，輸入門關閉，也就是此時刻的輸入資訊被丟棄了。這個機制同樣也可以由sigmoid 激活函數來實作，sigmoid產生的值介於0到1之間，可以被看作是一個閘控信號，這個閘控信號​和tanh函數生成的候選隱藏狀態相乘，確定了從候選狀態中將多少資訊添加到當前的單元狀態​中。<br /></li>
<li>輸出閘(Output Gate): 決定目前神經元的狀態中有哪一部分可以輸出(流向下一個狀態)，同樣由激活函數來sigmoid來決定，這個輸出會通過tanh函數來調整，因為Tanh能夠將單元狀態的值正規化到-1到1之間，這有助於控制神經網絡的激活範圍。再由Tanh來提供輸出權重。<br /></li>
<li>記憶單元(Memory Cell): 這是LSTM的核心，它負責記錄和更新所有的記憶。記憶單元是一個長期的記憶存儲，可以通過遺忘閘和輸入閘來更新。LSTM 中的 Memory Cell（也就是記憶狀態，通常記為 Cₜ）的核心功能，就是要 跨時間步保持資訊的狀態，這也是它的關鍵設計。在數學公式中，LSTM 的記憶更新如下：\(C_t=f_t \times C_{t−1}+i_t \times \tilde{C}_t\)，其中：<br />
<ol class="org-ol">
<li>\(f_t\) ：忘記門輸出（控制保留多少舊記憶）<br /></li>
<li>\(C_{t-1}\) ：上一個時間步的記憶狀態<br /></li>
<li>\(i_t\) ：輸入門輸出（控制加入多少新資訊）<br /></li>
<li>\(\tilde{C}_t\) ：由當前輸入與前一隱藏狀態計算出的新候選記憶<br /></li>
</ol></li>
</ul>


<div id="orge472887" class="figure">
<p><img src="images/LSTM/2024-05-10_21-14-18_LSTM-arch-01.png" alt="2024-05-10_21-14-18_LSTM-arch-01.png" width="800" /><br />
</p>
<p><span class="figure-number">Figure 10: </span>LSTM運作原理</p>
</div>

<p>
因為這樣的機制，讓 LSTM 即使面對很長的序列資料也能有效處理，不遺忘以前的記憶。因為效果卓越，LSTM 非常廣泛地被使用。事實上，當有人跟你說他用 RNN 做了什麼 NLP 專案時，有 9 成機率他是使用 LSTM 或是 GRU（LSTM 的改良版，只使用 2 個閘門） 來實作，而不是使用最簡單的 SimpleRNN。<br />
</p>
</div>
</div>
<div id="outline-container-orgded9922" class="outline-3">
<h3 id="orgded9922"><span class="section-number-3">4.2.</span> LSTM的程式運作示例</h3>
<div class="outline-text-3" id="text-4-2">
<p>
LSTM 的設計引入了三個「門」（gate）：<br />
</p>
<ul class="org-ul">
<li>**遺忘門 forget gate**：決定應該忘記多少過去的記憶<br /></li>
<li>**輸入門 input gate**：決定應該加入多少新的資訊進入記憶<br /></li>
<li>**輸出門 output gate**：決定應該輸出多少目前的記憶內容<br /></li>
</ul>

<p>
此外，LSTM 維持了兩種狀態：<br />
</p>
<ul class="org-ul">
<li>**cell state（長期記憶）C_t**：透過門控機制被有選擇地保留或更新<br /></li>
<li>**hidden state（短期輸出）h_t**：實際傳給下一層網路的輸出<br /></li>
</ul>

<p>
其計算過程可以用以下簡化版 Python 表示：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">lstm_step</span>(x_t, h_t_prev, c_t_prev):
<span class="linenr"> 2: </span>    <span style="color: #dcaeea;">forget_gate</span> = sigmoid(W_f @ x_t + U_f @ h_t_prev + b_f)
<span class="linenr"> 3: </span>    <span style="color: #dcaeea;">input_gate</span> = sigmoid(W_i @ x_t + U_i @ h_t_prev + b_i)
<span class="linenr"> 4: </span>    <span style="color: #dcaeea;">output_gate</span> = sigmoid(W_o @ x_t + U_o @ h_t_prev + b_o)
<span class="linenr"> 5: </span>    <span style="color: #dcaeea;">candidate</span> = tanh(W_c @ x_t + U_c @ h_t_prev + b_c)
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>    <span style="color: #dcaeea;">c_t</span> = forget_gate * c_t_prev + input_gate * candidate
<span class="linenr"> 8: </span>    <span style="color: #dcaeea;">h_t</span> = output_gate * tanh(c_t)
<span class="linenr"> 9: </span>    <span style="color: #51afef;">return</span> h_t, c_t
<span class="linenr">10: </span>
<span class="linenr">11: </span><span style="color: #dcaeea;">h_t</span>, <span style="color: #dcaeea;">c_t</span> = <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21021;&#22987;&#21270;</span>
<span class="linenr">12: </span><span style="color: #51afef;">for</span> x_t <span style="color: #51afef;">in</span> input_sequence:
<span class="linenr">13: </span>    <span style="color: #dcaeea;">h_t</span>, <span style="color: #dcaeea;">c_t</span> = lstm_step(x_t, h_t, c_t)
</pre>
</div>

<p>
LSTM 解決了 RNN 無法長期保留資訊的問題，特別適用於像語言模型、機器翻譯、長時間序列預測等任務。<br />
</p>
</div>
</div>
<div id="outline-container-org547c33a" class="outline-3">
<h3 id="org547c33a"><span class="section-number-3">4.3.</span> LSTM的程式模型架構</h3>
<div class="outline-text-3" id="text-4-3">
<p>
LSTM 是一種改良版的 RNN，可記住更長期的資訊。只需將 SimpleRNN 改為 LSTM 層即可。以下是一個 LSTM 模型的寫法：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span class="linenr"> 2: </span><span style="color: #51afef;">from</span> tensorflow.keras <span style="color: #51afef;">import</span> layers, models
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27169;&#22411;&#36664;&#20837;&#23652; (4&#20491;&#31680;&#40670;)</span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">inputs</span> = layers.Input(shape=(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">4</span>))
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38577;&#34255;&#23652; (LSTM&#23652;, 2&#20491;&#21934;&#20803;)</span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">hidden</span> = layers.LSTM(units=<span style="color: #da8548; font-weight: bold;">2</span>, activation=<span style="color: #98be65;">'tanh'</span>, return_sequences=<span style="color: #a9a1e1;">False</span>)(inputs)
<span class="linenr"> 9: </span>
<span class="linenr">10: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27169;&#22411;&#36664;&#20986;&#23652; (2&#20491;&#31680;&#40670;)</span>
<span class="linenr">11: </span><span style="color: #dcaeea;">outputs</span> = layers.Dense(units=<span style="color: #da8548; font-weight: bold;">2</span>, activation=<span style="color: #98be65;">'linear'</span>)(hidden)
<span class="linenr">12: </span>
<span class="linenr">13: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24314;&#31435;&#20006;&#32232;&#35695;&#27169;&#22411;</span>
<span class="linenr">14: </span><span style="color: #dcaeea;">model</span> = models.Model(inputs=inputs, outputs=outputs)
<span class="linenr">15: </span>model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'adam'</span>, loss=<span style="color: #98be65;">'mse'</span>)
<span class="linenr">16: </span>
<span class="linenr">17: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#27169;&#22411;&#32080;&#27083;</span>
<span class="linenr">18: </span>model.summary()
</pre>
</div>

<pre class="example" id="orgc749217">
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input_layer (InputLayer)        │ (None, 1, 4)           │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lstm (LSTM)                     │ (None, 2)              │            56 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense (Dense)                   │ (None, 2)              │             6 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 62 (248.00 B)
 Trainable params: 62 (248.00 B)
 Non-trainable params: 0 (0.00 B)
</pre>
</div>
</div>
<div id="outline-container-org875d06d" class="outline-3">
<h3 id="org875d06d"><span class="section-number-3">4.4.</span> 實作: 以AI預測股價-隔日漲跌</h3>
<div class="outline-text-3" id="text-4-4">
<p>
<a href="https://colab.research.google.com/drive/1IehBuskagMTm6RK6WB-dsf3NPZKTNVqs?usp=sharing">當AI遇上股票-LSTM.ipynb</a><br />
</p>
</div>
<ol class="org-ol">
<li><a id="org7aeb63c"></a>安裝相關套件<br />
<div class="outline-text-4" id="text-4-4-1">
<div class="org-src-container">
<pre class="src src-shell"><span class="linenr">1: </span>pip install yfinance
</pre>
</div>
</div>
</li>
<li><a id="orga06891d"></a>下載股價資訊<br />
<div class="outline-text-4" id="text-4-4-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> yfinance <span style="color: #51afef;">as</span> yf
<span class="linenr">2: </span>
<span class="linenr">3: </span><span style="color: #dcaeea;">df</span> = yf.Ticker(<span style="color: #98be65;">'2330.TW'</span>).history(period=<span style="color: #98be65;">'10y'</span>)
<span class="linenr">4: </span><span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">type</span>(df))
</pre>
</div>
</div>
<ol class="org-ol">
<li><a id="org273dcf9"></a>查看下載的資料集<br />
<div class="outline-text-5" id="text-4-4-2-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>df
<span class="linenr">2: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">print(df[:5])</span>
</pre>
</div>
</div>
</li>
<li><a id="orgc3eb526"></a>取出需要的特徵值<br />
<div class="outline-text-5" id="text-4-4-2-2">
<p>
此次將成交量納入考慮<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">data</span> = df.<span style="color: #c678dd;">filter</span>([<span style="color: #98be65;">'Close'</span>])
<span class="linenr">2: </span>data
</pre>
</div>
</div>
</li>
</ol>
</li>
<li><a id="org0ddb6dd"></a>觀察原始資料/日K圖<br />
<div class="outline-text-4" id="text-4-4-3">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">2: </span>plt.clf()
<span class="linenr">3: </span>plt.plot(data.Close)
<span class="linenr">4: </span>plt.show()
</pre>
</div>
</div>
</li>
<li><a id="org7cfb7af"></a>將資料標準化<br />
<div class="outline-text-4" id="text-4-4-4">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> MinMaxScaler
<span class="linenr">2: </span><span style="color: #dcaeea;">scaler</span> = MinMaxScaler(feature_range=(<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>))
<span class="linenr">3: </span><span style="color: #dcaeea;">sc_data</span> = scaler.fit_transform(data.values)
<span class="linenr">4: </span>
<span class="linenr">5: </span>sc_data <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35722;&#25104;numpy array</span>
</pre>
</div>
</div>
</li>
<li><a id="org46c43b6"></a>建立、分割資料<br />
<ol class="org-ol">
<li><a id="org5e40543"></a>建立資料集及標籤<br />
<div class="outline-text-5" id="text-4-4-5-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span><span style="color: #dcaeea;">featureDays</span> = <span style="color: #da8548; font-weight: bold;">10</span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">x_data</span>, <span style="color: #dcaeea;">y_data</span> = [], []
<span class="linenr"> 5: </span><span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #c678dd;">len</span>(sc_data) - featureDays):
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">x</span> = sc_data[i:i+featureDays]
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">y</span> = sc_data[i+featureDays]
<span class="linenr"> 8: </span>  x_data.append(x)
<span class="linenr"> 9: </span>  y_data.append(y)
<span class="linenr">10: </span>
<span class="linenr">11: </span><span style="color: #dcaeea;">x_data</span>, <span style="color: #dcaeea;">y_data</span> = np.array(x_data), np.array(y_data)
<span class="linenr">12: </span>
<span class="linenr">13: </span><span style="color: #c678dd;">print</span>(x_data.shape)
<span class="linenr">14: </span><span style="color: #c678dd;">print</span>(y_data.shape)
<span class="linenr">15: </span><span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">len</span>(x_data)) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#20840;&#37096;&#36039;&#26009;&#31558;&#25976;</span>
</pre>
</div>
</div>
</li>
<li><a id="orgd6a1079"></a>分割訓練集與測試集<br />
<div class="outline-text-5" id="text-4-4-5-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #dcaeea;">ratio</span> = <span style="color: #da8548; font-weight: bold;">0.8</span>
<span class="linenr"> 2: </span><span style="color: #dcaeea;">train_size</span> = <span style="color: #c678dd;">round</span>(<span style="color: #c678dd;">len</span>(x_data) * ratio)
<span class="linenr"> 3: </span><span style="color: #c678dd;">print</span>(train_size)
<span class="linenr"> 4: </span><span style="color: #dcaeea;">x_train</span>, <span style="color: #dcaeea;">y_train</span> = x_data[:train_size], y_data[:train_size]
<span class="linenr"> 5: </span><span style="color: #dcaeea;">x_test</span>, <span style="color: #dcaeea;">y_test</span> = x_data[train_size:], y_data[train_size:]
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span><span style="color: #c678dd;">print</span>(x_train.shape)
<span class="linenr"> 8: </span><span style="color: #c678dd;">print</span>(y_train.shape)
<span class="linenr"> 9: </span><span style="color: #c678dd;">print</span>(x_test.shape)
<span class="linenr">10: </span><span style="color: #c678dd;">print</span>(y_test.shape)
</pre>
</div>
</div>
</li>
</ol>
</li>
<li><a id="org078a9b0"></a>建立、編譯、訓練模型<br />
<ol class="org-ol">
<li><a id="orge674352"></a>建立模型<br />
<div class="outline-text-5" id="text-4-4-6-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span class="linenr">2: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#24314;&#27083;LSTM&#27169;&#22411;</span>
<span class="linenr">3: </span><span style="color: #dcaeea;">model</span> = tf.keras.Sequential()
<span class="linenr">4: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">LSTM&#23652;</span>
<span class="linenr">5: </span>model.add(tf.keras.layers.LSTM(units=<span style="color: #da8548; font-weight: bold;">64</span>, unroll = <span style="color: #a9a1e1;">False</span>, input_shape=(featureDays,<span style="color: #da8548; font-weight: bold;">1</span>)))
<span class="linenr">6: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Dense&#23652;</span>
<span class="linenr">7: </span>model.add(tf.keras.layers.Dense(units=<span style="color: #da8548; font-weight: bold;">1</span>))
</pre>
</div>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>model.summary()
</pre>
</div>
</div>
</li>
<li><a id="org1d20521"></a>編譯模型<br />
<div class="outline-text-5" id="text-4-4-6-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'mse'</span>, optimizer=<span style="color: #98be65;">'adam'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
</pre>
</div>
</div>
</li>
<li><a id="org3f3bb35"></a>訓練模型<br />
<div class="outline-text-5" id="text-4-4-6-3">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>model.fit(x_train, y_train,
<span class="linenr">2: </span>          validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr">3: </span>          batch_size=<span style="color: #da8548; font-weight: bold;">200</span>, epochs=<span style="color: #da8548; font-weight: bold;">20</span>)
</pre>
</div>
</div>
</li>
</ol>
</li>
<li><a id="orged7fa44"></a>性能測試<br />
<ol class="org-ol">
<li><a id="org908a5f7"></a>loss<br />
<div class="outline-text-5" id="text-4-4-7-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">score</span> = model.evaluate(x_test, y_test)
<span class="linenr">2: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'loss:'</span>, score[<span style="color: #da8548; font-weight: bold;">0</span>])
</pre>
</div>
</div>
</li>
<li><a id="orgdc2e1cf"></a>predict<br />
<div class="outline-text-5" id="text-4-4-7-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">predict</span> = model.predict(x_test)
<span class="linenr">2: </span><span style="color: #dcaeea;">predict</span> = scaler.inverse_transform(predict)
<span class="linenr">3: </span><span style="color: #dcaeea;">predict</span> = np.reshape(predict, (predict.size,))
<span class="linenr">4: </span><span style="color: #dcaeea;">ans</span> = scaler.inverse_transform(y_test)
<span class="linenr">5: </span><span style="color: #dcaeea;">ans</span> = np.reshape(ans, (ans.size,))
<span class="linenr">6: </span><span style="color: #c678dd;">print</span>(predict[:<span style="color: #da8548; font-weight: bold;">3</span>])
<span class="linenr">7: </span><span style="color: #c678dd;">print</span>(ans[:<span style="color: #da8548; font-weight: bold;">3</span>])
</pre>
</div>
</div>
</li>
<li><a id="org43b85a9"></a>plot<br />
<div class="outline-text-5" id="text-4-4-7-3">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>plt.plot(predict)
<span class="linenr">2: </span>plt.plot(ans)
<span class="linenr">3: </span>plt.show()
</pre>
</div>
</div>
</li>
</ol>
</li>
</ol>
</div>
</div>
<div id="outline-container-orgb95a974" class="outline-2">
<h2 id="orgb95a974"><span class="section-number-2">5.</span> 貼圖</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-orge39366d" class="outline-3">
<h3 id="orge39366d"><span class="section-number-3">5.1.</span> 讀圖</h3>
<div class="outline-text-3" id="text-5-1">
<p>
讀入並分析這張圖，等一下我想利用它來做幾張LINE貼圖上架賣錢<br />
</p>

<p>
設計如下的幾個情境，不用配文字，圖形設計更童稚一點，更接近原圖的拙稚風格<br />
</p>

<ol class="org-ol">
<li>害怕退縮、夾尾巴<br />
<ul class="org-ul">
<li>姿勢建議：<br />
<ul class="org-ul">
<li>狗狗身體縮成一團，尾巴夾在兩腿之間。<br /></li>
<li>眼睛瞪大，嘴巴向下彎表示害怕。<br /></li>
<li>耳朵下垂甚至貼緊頭部，強調害怕感。<br /></li>
</ul></li>
<li>情境細節：<br />
<ul class="org-ul">
<li>可以增加輕微的抖動線條或陰影，表現更真實的害怕感。<br /></li>
</ul></li>
</ul></li>
<li>高興快樂、搖尾巴<br />
<ul class="org-ul">
<li>姿勢建議：<br />
<ul class="org-ul">
<li>狗狗尾巴高舉且左右搖動，尾巴加上動感線條強調擺動感。<br /></li>
<li>眼睛笑瞇瞇、舌頭伸出，耳朵微微上揚，展現活力。<br /></li>
</ul></li>
<li>情境細節：<br />
<ul class="org-ul">
<li>加入代表開心的「♪」或「❤」符號在周圍，突出快樂氛圍。<br /></li>
</ul></li>
</ul></li>
<li>肚子餓，加上一個空碗<br />
<ul class="org-ul">
<li>姿勢建議：<br />
<ul class="org-ul">
<li>狗狗坐著或站著，身前放著一個空碗。<br /></li>
<li>表情無奈或可憐巴巴，眼睛睜大或下垂，舌頭稍微伸出來表示渴望。<br /></li>
</ul></li>
<li>情境細節：<br />
<ul class="org-ul">
<li>空碗上加一兩條細線表示空蕩蕩，增強肚子餓的情境。<br /></li>
</ul></li>
</ul></li>
<li>躲在牆角傷心流淚<br />
<ul class="org-ul">
<li>姿勢建議：<br />
<ul class="org-ul">
<li>狗狗坐在角落，背朝外、面向牆角。<br /></li>
<li>頭微微低下，有幾滴淚珠流下。<br /></li>
<li>尾巴垂下，耳朵耷拉下來，更增添悲傷的情緒。<br /></li>
</ul></li>
<li>情境細節：<br />
<ul class="org-ul">
<li>牆角的線條要明顯，以凸顯出被孤立的感覺。<br /></li>
<li>頭上可以加上幾朵烏雲或一兩滴淚珠，更加明顯地表達傷心。<br /></li>
</ul></li>
</ul></li>
<li>生病感冒，流鼻涕<br />
<ul class="org-ul">
<li>姿勢建議：<br />
<ul class="org-ul">
<li>狗狗坐著或躺著，手上拿著一條紙巾。<br /></li>
<li>鼻子紅腫，眼睛紅腫，嘴巴微微張開，表示呼吸困難。<br /></li>
<li>鼻子下方有一條鼻涕線，表示流鼻涕。<br /></li>
<li>頭上有一個水袋，表示發燒。<br /></li>
</ul></li>
<li>情境細節：<br />
<ul class="org-ul">
<li>紙巾上可以加上一兩條鼻涕線，突出生病的感覺。<br /></li>
</ul></li>
</ul></li>
<li>感到驚訝（配感嘆號）<br />
<ul class="org-ul">
<li>姿勢：<br />
<ul class="org-ul">
<li>狗狗雙眼瞪大，嘴巴微張呈圓形，尾巴向上豎直表示驚訝。<br /></li>
<li>耳朵略向後傾，表達震驚。<br /></li>
</ul></li>
<li>情境細節：<br />
<ul class="org-ul">
<li>頭頂上方繪製粗線條的感嘆號「！」。<br /></li>
</ul></li>
</ul></li>
<li>感到憤怒（配火焰）<br />
<ul class="org-ul">
<li>姿勢：<br />
<ul class="org-ul">
<li>狗狗站立，雙眼微瞇成三角形，嘴巴閉緊或呈現不滿的表情。<br /></li>
<li>尾巴豎起且線條略顯凌亂，表達激烈情緒。<br /></li>
</ul></li>
<li>情境細節：<br />
<ul class="org-ul">
<li>身體周圍或頭頂上方加入代表憤怒情緒的火焰圖案，線條簡單明確。<br /></li>
</ul></li>
</ul></li>
<li>感到驚喜（配驚嘆號）<br />
<ul class="org-ul">
<li>姿勢：<br />
<ul class="org-ul">
<li>狗狗雙眼閃亮、嘴角上揚，呈現開心又驚喜的表情。<br /></li>
<li>前腳稍微抬起或呈現輕跳狀，表達情緒高漲。<br /></li>
<li>尾巴高舉，且微微抖動以展現興奮。<br /></li>
</ul></li>
<li>情境細節：<br />
<ul class="org-ul">
<li>頭頂上方繪製代表驚喜的驚嘆號「！」（可加粗或加上動態線條表現）。<br /></li>
</ul></li>
</ul></li>
<li>熟睡中（有 Zzz）<br />
<ul class="org-ul">
<li>姿勢建議：<br />
<ul class="org-ul">
<li>狗狗蜷縮成一團或側躺，眼睛閉上、表情平靜。<br /></li>
<li>耳朵自然下垂，尾巴環繞身體或放鬆地搭在地上。<br /></li>
</ul></li>
<li>情境細節：<br />
<ul class="org-ul">
<li>頭部上方畫出「Zzz」符號，表達正在熟睡。<br /></li>
<li>可加一點打呼的波浪線或泡泡，讓畫面更溫馨。<br /></li>
</ul></li>
</ul></li>
<li>玩耍中（有球或繩子）<br />
<ul class="org-ul">
<li>姿勢建議：<br />
<ul class="org-ul">
<li>狗狗雙腳張開，身體前傾，尾巴高高翹起，呈現興奮姿態。<br /></li>
<li>嘴巴微張、舌頭吐出，眼睛閃閃發亮。<br /></li>
</ul></li>
<li>情境細節：<br />
<ul class="org-ul">
<li>身旁畫出一顆球或一條狗玩繩，球可畫成條紋球，繩子可用彎曲線條呈現。<br /></li>
<li>可加幾條動態線條表現奔跑或搖尾巴感。<br /></li>
</ul></li>
</ul></li>
<li>跳舞中（有音符）<br />
<ul class="org-ul">
<li>姿勢建議：<br />
<ul class="org-ul">
<li>狗狗呈現單腳起跳或揮舞前腳的姿勢，身體偏一側。<br /></li>
<li>表情開心，眼睛瞇成倒 U 形，舌頭外吐。<br /></li>
</ul></li>
<li>情境細節：<br />
<ul class="org-ul">
<li>周圍加上音符符號 ♪ ♫，表現正在享受音樂。<br /></li>
<li>可以搭配旋轉線條或移動線，加強舞動氛圍。<br /></li>
</ul></li>
</ul></li>
<li>上班中（認真打電腦）<br />
<ul class="org-ul">
<li>姿勢建議：<br />
<ul class="org-ul">
<li>狗狗坐在桌前，前腳放在鍵盤上。<br /></li>
<li>臉部表情專注，眼神直視螢幕。<br /></li>
</ul></li>
<li>情境細節：<br />
<ul class="org-ul">
<li>桌上放一台簡單的筆電或螢幕，畫法可以極簡但清楚。<br /></li>
<li>狗狗可以戴眼鏡增加「上班狗」的認真感，也可以加咖啡杯營造辦公氣氛。<br /></li>
</ul></li>
</ul></li>
<li>健身中（認真舉重）<br />
<ul class="org-ul">
<li>姿勢建議：<br />
<ul class="org-ul">
<li>狗狗坐姿或半蹲，雙前腳抬起舉著啞鈴（可畫成橢圓形加圓點代表）。<br /></li>
<li>嘴巴咬緊，眉毛內縮，眼神專注或咬牙切齒。<br /></li>
</ul></li>
<li>情境細節：<br />
<ul class="org-ul">
<li>啞鈴可以畫得誇張一點增加趣味。<br /></li>
<li>頭頂或身體旁可以加上汗珠線條，呈現努力用力的感覺。<br /></li>
</ul></li>
</ul></li>
<li>表達「我愛妳」<br />
<ul class="org-ul">
<li>姿勢建議：<br />
<ul class="org-ul">
<li>狗狗坐姿或雙手打開像是要抱抱。<br /></li>
<li>表情溫柔、微笑，眼睛可畫成愛心形或閉眼微笑。<br /></li>
</ul></li>
<li>情境細節：<br />
<ul class="org-ul">
<li>頭頂或旁邊畫出愛心符號 ❤，一個或多個皆可。<br /></li>
<li>可加兩隻前腳合掌形狀、或放在胸前像在「比心」。<br /></li>
</ul></li>
</ul></li>
<li>表示感謝<br />
<ul class="org-ul">
<li>姿勢建議：<br />
<ul class="org-ul">
<li>狗狗低頭、合掌（或兩手碰一起），表情感激。<br /></li>
<li>也可設計成略為鞠躬的姿勢。<br /></li>
</ul></li>
<li>情境細節：<br />
<ul class="org-ul">
<li>頭上或旁邊可以畫出閃亮的星星 ✨ 或「感謝」的代表動作線。<br /></li>
<li>臉上可加一點微笑或淚光，表現真摯的情緒。<br /></li>
</ul></li>
</ul></li>
<li>表示抱歉<br />
<ul class="org-ul">
<li>姿勢建議：<br />
<ul class="org-ul">
<li>狗狗低頭，耳朵耷拉，尾巴垂下。<br /></li>
<li>雙手交握或抱在胸前，表情愧疚。<br /></li>
</ul></li>
<li>情境細節：<br />
<ul class="org-ul">
<li>頭頂可畫出一個小汗珠或漩渦，增強歉意。<br /></li>
<li>背後畫出淚滴、或在牆角站立也可以加強效果。<br /></li>
</ul></li>
</ul></li>
<li>表示崇拜（眼裡有星星）<br />
<ul class="org-ul">
<li>姿勢建議：<br />
<ul class="org-ul">
<li>狗狗仰望上方，前腳舉高、表情超興奮。<br /></li>
<li>尾巴翹起，嘴巴張開、舌頭伸出。<br /></li>
</ul></li>
<li>情境細節：<br />
<ul class="org-ul">
<li>雙眼畫成閃亮的星星 ✨✨ 或充滿光芒的圓點。<br /></li>
<li>頭上可加仰望線條或光芒線，展現「哇～」的感覺。<br /></li>
</ul></li>
</ul></li>
<li>正在認真思考，眉頭緊皺<br />
<ul class="org-ul">
<li>姿勢建議：<br />
<ul class="org-ul">
<li>狗狗坐姿、前腳托著下巴或頭歪一邊。<br /></li>
<li>眉毛明顯畫成「八字眉」或集中線條。<br /></li>
</ul></li>
<li>情境細節：<br />
<ul class="org-ul">
<li>頭頂畫出「問號」❓或幾條思考線、齒輪圖示。<br /></li>
<li>也可畫個小泡泡框，有點像漫畫角色思考的樣子。<br /></li>
</ul></li>
</ul></li>
</ol>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://ithelp.ithome.com.tw/articles/10191820">Day 06：處理影像的利器 &#x2013; 卷積神經網路(Convolutional Neural Network)</a><br />
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Yung-Chin Yen</p>
<p class="date">Created: 2025-03-30 Sun 14:38</p>
</div>
</body>
</html>
