<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-02-19 Mon 16:13 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>神經網路</title>
<meta name="author" content="Yung-Chin Yen" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="../css/muse.css" />
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">神經網路</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgdf866dc">1. How machine recognize image</a>
<ul>
<li><a href="#orgb4d2fb5">1.1. 「／」v.s.「＼」</a></li>
<li><a href="#orge272c13">1.2. 將待解問題數學化</a></li>
<li><a href="#org79f21dd">1.3. [課堂作業]如何辨識「及」&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></a></li>
</ul>
</li>
<li><a href="#org0350b21">2. 照片分類任務的兩個難題</a>
<ul>
<li><a href="#org1bd0ae6">2.1. 缺乏可用特徵值</a></li>
<li><a href="#orge0e4fac">2.2. 缺乏適當參數</a></li>
</ul>
</li>
<li><a href="#org557aae3">3. 類神經網路</a>
<ul>
<li><a href="#org15190da">3.1. 概念</a></li>
<li><a href="#orgb18ba55">3.2. 解決第一個問題</a></li>
<li><a href="#org4a1997d">3.3. 解決第二個問題</a></li>
<li><a href="#orgabfad7a">3.4. 神經網路如何學習(正經版)</a></li>
<li><a href="#org083ca8f">3.5. 神經網路的輸出層</a></li>
</ul>
</li>
<li><a href="#org4d84d97">4. 深度神經網路(DNN)</a>
<ul>
<li><a href="#org4163612">4.1. 學習與參數:以迴歸問題為例</a></li>
<li><a href="#orgc5deb4e">4.2. 如何調整參數</a></li>
<li><a href="#orgf462471">4.3. 模型的極限</a></li>
<li><a href="#orgadfff7c">4.4. 神經網路為什麼要有那麼多層</a></li>
</ul>
</li>
<li><a href="#orgac32847">5. DNN實作: 辨識手寫數字</a>
<ul>
<li><a href="#org7665ecd">5.1. Keras</a></li>
<li><a href="#org625d6cf">5.2. Python手刻</a></li>
</ul>
</li>
<li><a href="#org1f225bc">6. DNN的進化之路&#xa0;&#xa0;&#xa0;<span class="tag"><span class="Keras">Keras</span></span></a>
<ul>
<li><a href="#orgfe25855">6.1. 簡介</a></li>
<li><a href="#org32cdd67">6.2. 機器學習模型設計模式</a></li>
<li><a href="#org07677c1">6.3. 基本流程</a></li>
<li><a href="#orgb5df3cf">6.4. 以 Keras 實作 MNist 手寫數字辨識資料集</a></li>
<li><a href="#org1f6e8f0">6.5. 強化 MLP 辨識 solution #1: 增加隠藏層神經元數</a></li>
<li><a href="#orge0b38e3">6.6. 強化 MLP 辨識 solution #2: 加入 DropOut 以避免 overfitting</a></li>
<li><a href="#orga9ff303">6.7. 強化 MLP 辨識 solution #3: 增加隱藏層層數</a></li>
</ul>
</li>
</ul>
</div>
</div>
<a href="https://letranger.github.io/AI/20221025104603-神經網路.html"><img align="right" alt="Hits" src="https://hits.sh/letranger.github.io/AI/20221025104603-神經網路.html.svg"/></a>
<div id="outline-container-orgdf866dc" class="outline-2">
<h2 id="orgdf866dc"><span class="section-number-2">1.</span> How machine recognize image</h2>
<div class="outline-text-2" id="text-1">

<div id="org4730588" class="figure">
<p><img src="images/How_machine_recognize_image/2024-02-16_15-23-39_2024-02-16_15-22-53.png" alt="2024-02-16_15-23-39_2024-02-16_15-22-53.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 1: </span>貓狗特徵如何區別</p>
</div>

<p>
機器是如何能區分貓狗的圖片？現在這似乎是個十分尋常簡單的能力，就和電腦能計算加減乘除那樣。然而在機器學習成熟前，這卻是個十分困難的問題。在回答這個問題之前，我們先來看一個更簡單的圖形識別問題：讓電腦來讀入「／」及「＼」兩個圖像並進行識別。<br />
</p>
</div>
<div id="outline-container-orgb4d2fb5" class="outline-3">
<h3 id="orgb4d2fb5"><span class="section-number-3">1.1.</span> 「／」v.s.「＼」</h3>
<div class="outline-text-3" id="text-1-1">
<p>
首先要先理解，電腦眼中的圖像皆是由像素(pixel)組成，我們先假設「／」及「＼」兩個圖像在電腦眼中其實是如以下的矩陣所示：其中+1表示有像素存在，反之則以-1表示。<br />
</p>

<div id="org4bd2ba7" class="figure">
<p><img src="images/How_machine_recognize_image/2024-02-16_15-25-10_2024-02-16_15-24-54.png" alt="2024-02-16_15-25-10_2024-02-16_15-24-54.png" width="500" /><br />
</p>
</div>

<p>
如同我們之前在<a href="../../../../../Dropbox/notes/roam/20221023101626-監督式學習.html#ID-20221023T101626.420918">監督式學習</a>中的<a href="../../../../../Dropbox/notes/roam/20231204210821-分類.html#ID-1592687a-cca7-4473-83a0-682a36394a28">分類</a>和<a href="../../../../../Dropbox/notes/roam/20221023154410-regression.html#ID-6ae7fb7a-0b38-4448-b19f-073d262513f2">迴歸</a>所做的，我們希望能找到一個機制，將矩陣A、B當成輸入，最後希望能輸出0、1來代表「／」、「＼」，講白了，我們其實一直在做的事情都是找到一個這樣的函數：<br />
</p>
<ul class="org-ul">
<li><p>
若輸入矩陣A，則函數傳回<br />
</p>

<div id="orgee25868" class="figure">
<p><img src="images/How_machine_recognize_image/2024-02-17_09-41-39_2024-02-17_09-41-12.png" alt="2024-02-17_09-41-39_2024-02-17_09-41-12.png" width="400" /><br />
</p>
</div></li>
</ul>
<ul class="org-ul">
<li><p>
若輸入矩陣B，則函數傳回<br />
</p>

<div id="org87adb1e" class="figure">
<p><img src="images/How_machine_recognize_image/2024-02-17_09-42-15_2024-02-17_09-41-24.png" alt="2024-02-17_09-42-15_2024-02-17_09-41-24.png" width="400" /><br />
</p>
</div></li>
</ul>

<p>
能做到這點，也許我們將來就能找出這種函數:<br />
</p>

<div id="orgdd409eb" class="figure">
<p><img src="images/How_machine_recognize_image/2024-02-17_09-43-18_2024-02-17_09-43-06.png" alt="2024-02-17_09-43-18_2024-02-17_09-43-06.png" width="400" /><br />
</p>
</div>

<p>
或是這種函數<br />
</p>

<div id="orgd36dc91" class="figure">
<p><img src="images/How_machine_recognize_image/2024-02-17_09-44-00_2024-02-17_09-43-48.png" alt="2024-02-17_09-44-00_2024-02-17_09-43-48.png" width="400" /><br />
</p>
</div>

<p>
也就是語音辨識，甚至是找出這類函數：<br />
</p>

<div id="orgaff8f63" class="figure">
<p><img src="images/How_machine_recognize_image/2024-02-17_09-44-52_2024-02-17_09-44-41.png" alt="2024-02-17_09-44-52_2024-02-17_09-44-41.png" width="400" /><br />
</p>
</div>

<p>
這雖然只是五子棋，但我們也向AlphaGo邁進一小步了。<br />
</p>
</div>
</div>
<div id="outline-container-orge272c13" class="outline-3">
<h3 id="orge272c13"><span class="section-number-3">1.2.</span> 將待解問題數學化</h3>
<div class="outline-text-3" id="text-1-2">
<p>
在繼續做夢之前，讓我們回到現實，怎麼找出這個函數：<br />
</p>

<div id="org956b9cc" class="figure">
<p><img src="images/How_machine_recognize_image/2024-02-17_09-47-27_2024-02-17_09-47-07.png" alt="2024-02-17_09-47-27_2024-02-17_09-47-07.png" width="400" /><br />
</p>
<p><span class="figure-number">Figure 2: </span>目標函數</p>
</div>

<p>
如何藉由數學運算來將上述兩個矩陣區分為「／」及「＼」兩個圖像？似乎把矩陣裡的數值進行相加或相乘都無法達到要求，因為兩個矩陣元素相加或相乘的結果都相同。顯然我們需要引入額外的變數來協助：加入另一個新的矩陣來與這兩個矩陣進行運算。<br />
</p>

<div id="org377a505" class="figure">
<p><img src="images/How_machine_recognize_image/2024-02-17_09-55-04_2024-02-17_09-54-47.png" alt="2024-02-17_09-55-04_2024-02-17_09-54-47.png" width="120" /><br />
</p>
</div>

<p>
經由如下的矩陣計算，我們可以初步擬定一條分類規則：若是運算所得值大於0，則此圖像為＼；若是運算所得值小於0，則判定為／。<br />
</p>

<div id="org4c92e08" class="figure">
<p><img src="images/How_machine_recognize_image/2024-02-17_09-56-41_2024-02-17_09-56-25.png" alt="2024-02-17_09-56-41_2024-02-17_09-56-25.png" width="500" /><br />
</p>
</div>

<p>
然而，若是電腦所讀入的「／」及「＼」兩符號書寫過於潦草，那麼我們的判斷是否仍然正確呢？假設所讀入的「＼」圖像在右上角的彎曲幅度過大，以致於佔去了三個像素（如下圖矩陣A）；而「／」圖像又因寫的太短，僅佔了左下角一個像素（如下圖矩陣B）。經由如下的計算結果，我們發現仍可將二者進行正確分類。此時，我們可以宣稱這個方法是強韌的(robust)。<br />
</p>

<div id="org5d45b4c" class="figure">
<p><img src="images/How_machine_recognize_image/2024-02-17_09-59-34_2024-02-17_09-59-21.png" alt="2024-02-17_09-59-34_2024-02-17_09-59-21.png" width="500" /><br />
</p>
</div>

<p>
在上述範例中：<br />
</p>
<ul class="org-ul">
<li>矩陣K對矩陣A、B所進行的運算即為卷積(convolution)，矩陣K在<a href="../../../../../Dropbox/notes/roam/20221023101414-卷積神經網路.html#ID-20221023T101414.457264">卷積神經網路</a>中稱之為卷積核(convolution kernel)，其作用即在於萃取出資料特徵。藉此，我們達成了利用數學運算來擷取圖像特徵，也可以理解到為何深度學習能夠過濾資料雜訊而完成圖像識別。<br /></li>
<li>計算得到結果後，我們私自擬定一條分類規則：若是運算所得值大於0，則此圖像為＼；若是運算所得值小於0，則判定為／。在神經網路中，這就是<a href="../../../../../Dropbox/notes/roam/20240215153606-activationfunction.html#ID-d3bcc30a-3d94-4a3c-8e66-baaac7325c75">激勵函數</a>(<a href="../../../../../Dropbox/notes/roam/20240215153606-activationfunction.html#ID-d3bcc30a-3d94-4a3c-8e66-baaac7325c75">Activation Function</a>)。<br /></li>
</ul>

<p>
至此，你有沒有一種<a href="https://zh.wikipedia.org/zh-tw/%E6%97%A2%E8%A6%96%E6%84%9F">Déjà vu</a>的感覺？沒錯，我們實作出了一個<a href="../../../../../Dropbox/notes/roam/20221023103538-感知器.html#ID-20221023T103538.640537">感知器</a>(如圖<a href="#org0e6e788">3</a>)：<br />
</p>
<ul class="org-ul">
<li>矩陣A就是感知器的輸入值<br /></li>
<li>矩陣K裡的值就是感知器中的weight和bias<br /></li>
<li>決定何時輸出&ldquo;\&rdquo;、何時輸出&ldquo;\&rdquo;的功能其實就是<a href="../../../../../Dropbox/notes/roam/20240215153606-activationfunction.html#ID-d3bcc30a-3d94-4a3c-8e66-baaac7325c75">激勵函數</a><br /></li>
</ul>

<div id="org0e6e788" class="figure">
<p><img src="images/How_machine_recognize_image/2024-02-17_13-44-03_2024-02-17_13-21-43.png" alt="2024-02-17_13-44-03_2024-02-17_13-21-43.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 3: </span>感知器基本結構</p>
</div>

<p>
而<a href="../../../../../Dropbox/notes/roam/20221023103538-感知器.html#ID-20221023T103538.640537">感知器</a>只是神經網路中的基本元素。<br />
</p>
</div>
</div>
<div id="outline-container-org79f21dd" class="outline-3">
<h3 id="org79f21dd"><span class="section-number-3">1.3.</span> [課堂作業]如何辨識「及」&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></h3>
<div class="outline-text-3" id="text-1-3">
<p>
思考一下，如果你想要區分「及」兩個符號(如右圖)，你要如何訂制你的卷積核？你的方法夠強韌嗎？<br />
</p>

<div id="orgeb88815" class="figure">
<p><img src="images/How_machine_recognize_image/2024-02-17_10-01-54_2024-02-17_10-01-44.png" alt="2024-02-17_10-01-54_2024-02-17_10-01-44.png" width="300" /><br />
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org0350b21" class="outline-2">
<h2 id="org0350b21"><span class="section-number-2">2.</span> 照片分類任務的兩個難題</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org1bd0ae6" class="outline-3">
<h3 id="org1bd0ae6"><span class="section-number-3">2.1.</span> 缺乏可用特徵值</h3>
<div class="outline-text-3" id="text-2-1">
<p>
回顧<a href="../../../../../Dropbox/notes/roam/20221023101626-監督式學習.html#ID-20221023T101626.420918">監督式學習</a>中的<a href="../../../../../Dropbox/notes/roam/20231204210821-分類.html#ID-1592687a-cca7-4473-83a0-682a36394a28">分類</a>任務中，我們為每張照片中的動物手動定義了不同的特徵，並賦予不同動物不同數值，最後利用這些特徵值來進行分類預測。<br />
</p>

<div id="org1d86d38" class="figure">
<p><img src="images/最短距離分類器/2024-02-05_13-44-20_2024-02-05_13-44-08.png" alt="2024-02-05_13-44-20_2024-02-05_13-44-08.png" width="500" /><br />
</p>
</div>

<p>
就連在<a href="20231204210821-分類.html#IRIS-KNN">KNN分類器:IRIS</a>的任務中，鳶尾花的資料集也是友善的自帶花萼和花瓣資料才來要求我們完成分類工作。<br />
</p>

<div id="org0430b38" class="figure">
<p><img src="images/iris-1.png" alt="iris-1.png" width="300" /><br />
</p>
</div>

<p>
但是，如果我們要辨別的是全新的照片呢？難道我們要進行下圖這種分類時都要手動生成相對應的特徵？所以在輸入照片後還要再手動丈量所需特徵值(頭部及尾巴佔身體比例)？如果是，這種能力也太廢了點。<br />
</p>

<div id="org2457bf5" class="figure">
<p><img src="images/How_machine_recognize_image/2024-02-17_09-43-18_2024-02-17_09-43-06.png" alt="2024-02-17_09-43-18_2024-02-17_09-43-06.png" width="400" /><br />
</p>
</div>

<p>
至此我們發現了真正要實作照片辨識時的第一個問題：缺乏特徵值。<br />
</p>
</div>
</div>
<div id="outline-container-orge0e4fac" class="outline-3">
<h3 id="orge0e4fac"><span class="section-number-3">2.2.</span> 缺乏適當參數</h3>
<div class="outline-text-3" id="text-2-2">
<p>
在<a href="#orgdf866dc">How machine recognize image</a>一節中，我們想出了利用另一個新矩陣K來從兩張影像的矩陣抽取出特徵。<br />
</p>

<div id="orgb4015da" class="figure">
<p><img src="images/How_machine_recognize_image/2024-02-17_09-55-04_2024-02-17_09-54-47.png" alt="2024-02-17_09-55-04_2024-02-17_09-54-47.png" width="120" /><br />
</p>
</div>

<p>
如果你有認真做完<a href="#org79f21dd">[課堂作業]如何辨識「及」</a>，你大概會發現這個矩陣K並不是那麼好踹出來的，而且我們花了這麼多精力踹出一個矩陣也只能抽取出原圖的一個特徵，難道這就是機器學習的玩法？<br />
</p>
</div>
</div>
</div>
<div id="outline-container-org557aae3" class="outline-2">
<h2 id="org557aae3"><span class="section-number-2">3.</span> 類神經網路</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-org15190da" class="outline-3">
<h3 id="org15190da"><span class="section-number-3">3.1.</span> 概念</h3>
<div class="outline-text-3" id="text-3-1">
<p>
類神經網路的機制解決了前述兩大問題，方法是效法人類腦神經系統的運作。<br />
</p>

<p>
在生物神經系統的結構中，神經元(Neuron)之間互相連結，由外部神經元接收信號，再逐層傳導至其他神經元，最後作出反應。<br />
</p>

<div id="org5dd55be" class="figure">
<p><img src="images/Neuron.jpg" alt="Neuron.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 4: </span>神經元與神經網路</p>
</div>

<p>
類神經網絡是一種機器學習機制，它模仿人類大腦的學習方法。人類的大腦從外界接受刺激，並處理這些輸入（通過神經元處理），最終產生輸出<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>。當任務變得複雜的時候，大腦會使用多個神經元來形成一個複雜的網絡，並在神經元之間傳遞訊息，人工神經網絡就是模仿這種處理機制的一種算法。<br />
</p>

<p>
圖<a href="#orgf9b04a3">5</a>為典型的三層類神經網路，輸入層(layer 0)有 2 個神經元，兩個隱藏層(layer 1, layer 2)各有3個(a1, a2, a3)及2個(b1, b2)神經元(灰色圓圈為bias)，輸出層(layer 3)則有兩個神經元。<br />
</p>


<div id="orgf9b04a3" class="figure">
<p><img src="images/3LayerNetwork.png" alt="3LayerNetwork.png" width="400" /><br />
</p>
<p><span class="figure-number">Figure 5: </span>三層神經網路</p>
</div>

<p>
神經網絡的基礎模型是<a href="../../../../../Dropbox/notes/roam/20221023103538-感知器.html#ID-20221023T103538.640537">感知器</a>(perceptron)，因此神經網絡也可以叫做多層感知機(Multi-layer Perceptron)，簡稱MLP<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>。<br />
</p>

<p>
我們可以把圖<a href="#orgf9b04a3">5</a>中的每個神經元都視為一個抽象的<a href="../../../../../Dropbox/notes/roam/20221023103538-感知器.html#ID-20221023T103538.640537">感知器</a>實作，如圖<a href="#org773834f">6</a>，每個神經元都自前方取得輸入資料，經過一番計算後，再將輸出結果傳結後方的神經元，當成後方神經元的輸入。這種訊息在神經網路中傳遞的方式稱為前向傳播(forward propagation)，而實際神經網路中各層的神經元的訊息傳遞則是透過矩陣相乘來進行。<br />
</p>


<div id="org773834f" class="figure">
<p><img src="images/perceptron-3.png" alt="perceptron-3.png" width="400" /><br />
</p>
<p><span class="figure-number">Figure 6: </span>神經元/感知器</p>
</div>
</div>
</div>
<div id="outline-container-orgb18ba55" class="outline-3">
<h3 id="orgb18ba55"><span class="section-number-3">3.2.</span> 解決第一個問題</h3>
<div class="outline-text-3" id="text-3-2">
<p>
如前所述，一個<a href="../../../../../Dropbox/notes/roam/20221023103538-感知器.html#ID-20221023T103538.640537">感知器</a>可以抽取出一份輸入資料的特徵值，那麼，再看一下圖<a href="#orgf9b04a3">5</a>中的forward propagation<br />
</p>
<ul class="org-ul">
<li>第0層(layer 0)裡的兩個神經元(x1, x2)在取得原始資料後，就能抽取出其中的兩項特徵值，而這兩項特徵值就當成(layer 2)的輸入來源<br /></li>
<li>第1層(layer 1)裡的三個神經元(a1, a2, a3)則可抽取出來源資料的三項特徵值(可視為更高階或更抽象的特徵)，而這三項特徵值再當成layer 2的輸入<br /></li>
<li>第2層(layer 2)裡面的兩個神經元(b1, b2)繼續抽取出更抽象的兩項特徵值經過計算送往layer 3<br /></li>
<li>第3層(layer 3)在收到layer 2傳來的結果後，經過內部運算並輸出預測結果。<br /></li>
</ul>


<div id="orgaf98fec" class="figure">
<p><img src="images/nn.gif" alt="nn.gif" width="400" /><br />
</p>
<p><span class="figure-number">Figure 7: </span>Forward propagation</p>
</div>

<p>
藉由這樣的設計，在進行照片分類時，如果我們在每一層都塞入512個神經元，那光是透過第一層就能由每張照片中抽取出512種不同特徵值了，如此就解決了我們面臨的第一個問題：缺乏可用特徵值。<br />
</p>
</div>
</div>
<div id="outline-container-org4a1997d" class="outline-3">
<h3 id="org4a1997d"><span class="section-number-3">3.3.</span> 解決第二個問題</h3>
<div class="outline-text-3" id="text-3-3">
<p>
然而如果事情有這麼簡單，機器學習也不至於在1970年代邁入寒冬。還記得<a href="#org79f21dd">[課堂作業]如何辨識「及」</a>吧，裡面只有一個要處理的矩陣K，一共只有4個要踹出來的數值，想像一下有512個神經元，裡面有512個矩陣K，每個矩陣也不用太大，只要3&times;3就好(9個參數)，那光第一層我們就要處理多少參數呢&#x2026;顯然，要踹出合適的所有參數值不是人力所能及的工作。<br />
</p>

<p>
神經網路解決這個問題的方式非常的率性且不負責任：隨機!!<br />
</p>

<div id="org29e1eb9" class="figure">
<p><img src="images/類神經網路/2024-02-18_09-55-35_2024-02-18_09-55-22.png" alt="2024-02-18_09-55-35_2024-02-18_09-55-22.png" width="100" /><br />
</p>
</div>

<p>
是的，就如同考試時你面對陌生選擇題的反應，神經網路也決定這麼幹，隨便丟一些數值填到矩陣中當成第一批參數。事實上，同樣的策略我們在<a href="../../../../../Dropbox/notes/roam/20221023154410-regression.html#ID-7cd4a142-4cd9-46b6-b9a4-2ad750ae622f">線性迴歸:年齡身高預測/隨機的力量</a>裡已經玩過了，當初在找出方程式的最佳參數組合時，我們也是閉上眼睛隨便選一組。不管整個網路中有多少參數，當我們隨機設定好了所有參數的最初值後，整個神經網就就可以運作了，嗯&#x2026;至少已經可以依照前向傳播的流程輸出第一個預測結果了，你看，我們已經朝完美的人工智慧跨近一大步了-_-<br />
</p>

<div id="org6eb9652" class="figure">
<p><img src="images/類神經網路/2024-02-18_10-23-40_2024-02-18_10-20-52.png" alt="2024-02-18_10-23-40_2024-02-18_10-20-52.png" width="400" /><br />
</p>
</div>

<p>
接下來的流程其實和<a href="../../../../../Dropbox/notes/roam/20221023154410-regression.html#ID-6ae7fb7a-0b38-4448-b19f-073d262513f2">迴歸</a>有點類似，我們評估預測結果的品質，然後回頭修正參數，只是這次的工程有點浩大，我們要修正所有的參數，這個回頭修正所有參數的過程稱為反向傳播(backward propagation)。<br />
</p>


<div id="orga2320d6" class="figure">
<p><img src="images/類神經網路/2024-02-15_15-15-43_2024-02-15_15-07-16.png" alt="2024-02-15_15-15-43_2024-02-15_15-07-16.png" width="400" /><br />
</p>
<p><span class="figure-number">Figure 8: </span>Back Propagation</p>
</div>

<p>
第一次以隨機設定的參數跑出來的結果肯定是慘不忍睹的，因為它的原則就是「有就好」，就好像某些政黨在成立之初找不到信徒，所以什麼殺人犯或貪污犯，只要你肯入黨就照單全收，因為只要親近黨主席，這些人最終都會變成好人。<br />
</p>

<p>
好吧，扯遠了，反正神經網路的第一輪預測品質基本上就是瞎猜，不過沒關係，就像我們在<a href="../../../../../Dropbox/notes/roam/20221023154410-regression.html#ID-6ae7fb7a-0b38-4448-b19f-073d262513f2">迴歸</a>裡做的事一樣，我們每次都會評估預測品質，在這裡我們用損失函數(lost function, 也就是圖<a href="#orga2320d6">8</a>裡的cost function, J)來評估預測品質，當評估結果未達標準，我們就回頭修正參數。在<a href="../../../../../Dropbox/notes/roam/20221023154410-regression.html#ID-6ae7fb7a-0b38-4448-b19f-073d262513f2">迴歸</a>這一章<a href="../../../../../Dropbox/notes/roam/20221023154410-regression.html#ID-e02d602e-c9a4-4a8b-8e5b-336dcac6cc63">保持距離以測安全</a>裡的例子裡，我們只要修正一個參數(\(\hat{y} = w * x\) 裡的\(w\))，用的方法是找出參數的切線斜率，決定修正方向。二者的最終目標其實是一樣的：讓預測誤差達到最小化。<br />
</p>

<div id="orga5c906a" class="figure">
<p><img src="images/carsLoss2.png" alt="carsLoss2.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 9: </span>決定w應往哪個方向移動</p>
</div>

<p>
面對動軏成升上萬的參數，神經網路採用「隨機梯度下降法（Stochastic Gradient Descent，SGD）」來找出各個參數的梯度(我們可以先把它當成前例中的切線斜率)，然後沿著梯度方向修正各個參數，從最後一層反向修正回到第一層(就是反向傳播在做的事)，於是我們就有了第二個版本的參數值了(第一個版本就是隨機指定的)。<br />
</p>

<p>
然後呢?就再從頭玩一次啊：以一批原始資料(圖片)當成輸入再跑一次正向傳播，得到預測、評估預測品質、再反向傳播回去修正所有參數，得到第三個版本的參數值&#x2026;，只要你時間夠多，你可以一直玩下去，這就是神經網路學習的奧義了&#x2026;<br />
</p>

<p>
什麼？你問「隨機梯度下降法」原理？這個等我學完偏微分以後再解釋給你聽&#x2026;.QQ<br />
</p>
</div>
</div>
<div id="outline-container-orgabfad7a" class="outline-3">
<h3 id="orgabfad7a"><span class="section-number-3">3.4.</span> 神經網路如何學習(正經版)</h3>
<div class="outline-text-3" id="text-3-4">
</div>
<div id="outline-container-org1d1626d" class="outline-4">
<h4 id="org1d1626d"><span class="section-number-4">3.4.1.</span> 基本步驟</h4>
<div class="outline-text-4" id="text-3-4-1">
<p>
利用隨機梯度下降法(SGD)求梯度並更新 weight 和 bias 參數<br />
</p>
<ol class="org-ol">
<li>從訓練資料中隨機選擇一部分資料<br /></li>
<li>利用前向傳播，求出預測的輸出值，以損失函數(loss Function)計算預測值與真實答案(label)的誤差，再利用用反向傳播方法，求各參數的梯度。<br /></li>
<li>將所有權重參數沿其各自的梯度方向進行微小更新<br /></li>
<li>重複以上步驟(epoch=100, 500, 1000, &#x2026;)直到誤差最小化<br />
(<a href="http://www.feiguyunai.com/index.php/2019/03/31/python-ml-24th-backp/">http://www.feiguyunai.com/index.php/2019/03/31/python-ml-24th-backp/</a>)<br /></li>
</ol>
</div>
</div>
<div id="outline-container-orgee08817" class="outline-4">
<h4 id="orgee08817"><span class="section-number-4">3.4.2.</span> Error Function</h4>
<div class="outline-text-4" id="text-3-4-2">
<p>
也就是loss function，依不同的資料型別各有遉用的loss function。典型的計算方式如下：<br />
Error(錯誤) = 真正的值(t) + 預測值(y)<br />
</p>
<ul class="org-ul">
<li>\(E=\frac{1}{2}\sum^{n}_{i=1}(y_i = t_i)^2\)<br /></li>
<li>\(E=-\sum_{i=1}^{n}t_i*\log{y_i}\)<br /></li>
</ul>
</div>
</div>
<div id="outline-container-orga00ea55" class="outline-4">
<h4 id="orga00ea55"><span class="section-number-4">3.4.3.</span> 學習步驟</h4>
<div class="outline-text-4" id="text-3-4-3">

<div id="org4980291" class="figure">
<p><img src="images/ANN-1.jpg" alt="ANN-1.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 10: </span>類神經網路</p>
</div>

<p>
以上面這個簡化版的神經元為例：<br />
</p>
<ol class="org-ol">
<li>隨機指定weight與bias的值。<br /></li>
<li>計算出模型結果值<br /></li>
<li><p>
計算出誤差：即計算結果與真實值之差異<br />
</p>

<div id="org6aa6ccf" class="figure">
<p><img src="images/ANN-4.jpg" alt="ANN-4.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 11: </span>類神經網路</p>
</div></li>
<li>定義 weight 與 bias 的更新策略(update the weights and bias)，其更新策略為從錯誤中學習。學習規則為<br />
<ul class="org-ul">
<li>\(\Delta w=\eta X^T (\hat{y} - y)\): \(w\)(新的)=\(w\)(前一回)+\(\Delta w\)(誤差變動)<br /></li>
<li>\(\Delta b=\eta (\hat{y} - y)\): \(b\)(新的)=\(b\)(前靣)+\(\Delta b\)(誤差變動)<br /></li>
<li>\(\eta\) = 學習率<br /></li>
</ul></li>
<li><p>
使用新的 weight 與 bias，計算直到誤差很小(定義何時結束)<br />
</p>

<div id="orgb078269" class="figure">
<p><img src="images/ANN-5.jpg" alt="ANN-5.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 12: </span>類神經網路</p>
</div></li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org083ca8f" class="outline-3">
<h3 id="org083ca8f"><span class="section-number-3">3.5.</span> 神經網路的輸出層</h3>
<div class="outline-text-3" id="text-3-5">
<p>
神經網路可以用來解決分類問題與迴歸問題，端視輸出層所使用的活化函數，解決迴歸問題時使用恆等函數，而分類問題則使用 <i>softmax 函數</i> 。恆等函數對於輸入的內容完全不做任何處理，直接輸出，其神經網路的結構如圖<a href="#org3246076">13</a>所示。<br />
</p>

<div id="org3246076" class="figure">
<p><img src="images/identity-network.png" alt="identity-network.png" width="300" /><br />
</p>
<p><span class="figure-number">Figure 13: </span>恆等函數神經網路圖</p>
</div>

<p>
而分類問題使用的 softmax 函數則如公式\eqref{orgdcfc229}所示，\(exp(x)\)為代表\(e^x\)的指標函數，輸出層有 n 個節點，而每個節點收到的訊息\(y_k\)來自前一層以箭頭連接的所有訊號輸入，由公式\eqref{orgdcfc229}的分母也可以看出，輸出的各個神經元會依以「依各節點訊號量比例」的模式影響下一層的輸入。<br />
</p>

\begin{equation}
\label{orgdcfc229}
y_k = \frac{exp(a_k)}{\sum_{i=1}^{n}exp(a_i)}
\end{equation}


<div id="org62abed7" class="figure">
<p><img src="images/softmax-network.png" alt="softmax-network.png" width="400" /><br />
</p>
<p><span class="figure-number">Figure 14: </span>softmax 函數神經網路圖</p>
</div>

<p>
至於 softmax 的 python 實作則如下程式碼所示，為了避免因矩陣 a 的值過大而導至指數函數運算出現溢位，程式碼第<a href="#coderef-softmax-overflow1" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-softmax-overflow1');" onmouseout="CodeHighlightOff(this, 'coderef-softmax-overflow1');">4</a>行的內容也可以改由第<a href="#coderef-softmax-overflow2" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-softmax-overflow2');" onmouseout="CodeHighlightOff(this, 'coderef-softmax-overflow2');">5</a>行替代。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">python code for softmax funtion</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">softmax</span>(a):
<span id="coderef-softmax-overflow1" class="coderef-off"><span class="linenr"> 4: </span>    <span style="color: #dcaeea;">exp_a</span> = np.exp(a)</span>
<span id="coderef-softmax-overflow2" class="coderef-off"><span class="linenr"> 5: </span>    <span style="color: #5B6268;">#</span><span style="color: #5B6268;">exp_a = np.exp(a - np.max(a))</span></span>
<span class="linenr"> 6: </span>    <span style="color: #dcaeea;">sum_exp_a</span> = np.<span style="color: #c678dd;">sum</span>(exp_a)
<span class="linenr"> 7: </span>    <span style="color: #dcaeea;">y</span> = exp_a / sum_exp_a
<span class="linenr"> 8: </span>    <span style="color: #51afef;">return</span>(y)
<span class="linenr"> 9: </span><span style="color: #dcaeea;">a</span> = np.array([<span style="color: #da8548; font-weight: bold;">0.3</span>, <span style="color: #da8548; font-weight: bold;">2.9</span>, <span style="color: #da8548; font-weight: bold;">4.0</span>])
<span class="linenr">10: </span><span style="color: #c678dd;">print</span>(softmax(a))
</pre>
</div>
<pre class="example">
[0.01821127 0.24519181 0.73659691]
</pre>
</div>
<div id="outline-container-org0aec9b2" class="outline-4">
<h4 id="org0aec9b2"><span class="section-number-4">3.5.1.</span> softmax 函數的特色</h4>
<div class="outline-text-4" id="text-3-5-1">
<p>
softmaxe 的輸出為介於 0 到 1 間的實數，此外，其輸出總和為 1，這個性質使的 softmax 函數的輸出也可解釋為「機率」。例如，前節程式碼的輸出結果為[0.01821127 0.24519181 0.73659691]，從以機率的角度我們可以說：分類的結果有 1.8%的機率為第 0 類；有 24.52%的機率為第 1 類；有 73.66%的機率為第 2 類。換言之，使用 softmax 函數可以針對問題提出對應的機率。<br />
</p>

<p>
softmax 函數的另一個特色是其輸出結果仍保持與輸入訊息一致的大小關係，這是因為指數函數\(y=exp(x)\)為單週函數。一般而言，神經網路會把輸出最大神經元的類別當作辨識結果，然而，因為 softmax 不影響大小順序，所以一般會省略 softmax 函數。<br />
</p>

<p>
輸出層的節點數量取決於要解決的問題，例如，如果要解決的問題為「判斷一個手寫數字的結果」，則輸出層會有 10 個節點(分別代表 0~9)，而輸出訊息最大的結點則為最有可能的答案類別。<br />
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org4d84d97" class="outline-2">
<h2 id="org4d84d97"><span class="section-number-2">4.</span> 深度神經網路(DNN)</h2>
<div class="outline-text-2" id="text-4">
<p>
深度神經網路(Deep Neural Network, DNN)，顧名思義就是有很多層的神經網路。然而，幾層才算是多呢？一般來說有1-2個隱藏層的神經網絡就可以叫做多層，準確的說是(淺層)神經網絡(Shallow Neural Networks)。隨著隱藏層的增多，更深的神經網絡(一般來說超過3層)就都叫做深度神經網路<sup><a id="fnr.2.100" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>。而那些以深度神經網路為模型的機器學習就是我們耳熟能詳的<a href="../../../../../Dropbox/notes/roam/20221023101228-深度學習.html#ID-20221023T101228.247381">深度學習</a>。<br />
</p>
</div>
<div id="outline-container-org4163612" class="outline-3">
<h3 id="org4163612"><span class="section-number-3">4.1.</span> 學習與參數:以迴歸問題為例</h3>
<div class="outline-text-3" id="text-4-1">
<p>
想像一個國中的數學問題：在平面上畫出\(y=2x+3\)的直線，如圖<a href="#orga53b043">15</a>左的直線(\(y=ax+b\))，決定這條直線的因素有二：斜率\(a\)與截距\(b\)，這兩項因素即可視為該直線的參數，像這種由已知參數去畫出對映直線的問題稱之為順向問題；反之，如果目前只知道平面上有幾個點，希望能畫出最符合的這些點的線，這種問題就稱為逆向問題。像圖<a href="#orga53b043">15</a>右圖的紅線明顯就不是一條最符合的線，而解決這個問題就變成透過「尋找最佳參數」來畫出最理想的迴歸線，神經網路便是希望能藉由網路模型的不斷學習來找出最佳參數。<br />
</p>

<div id="orga53b043" class="figure">
<p><img src="images/simplefx-1.png" alt="simplefx-1.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 15: </span>由已知函數畫出直線與由已知點找未知函數</p>
</div>

<p>
同理，如果我們將解題目標改變為「預測學生學測總級分」，那麼，我們得先了解有那些因素會影響學生的學測成績，初步估計也許包括以下因素：<br />
</p>
<ol class="org-ol">
<li>上課狀況<br /></li>
<li>是否認真寫作業<br /></li>
<li>歷次段考成績<br /></li>
<li>校內模考成績<br /></li>
<li>回家後是否努力讀書<br /></li>
<li>是否沉迷網路遊戲或手機遊戲<br /></li>
<li>是否有男/女朋友<br /></li>
</ol>

<p>
此時，我們的預測模型就如圖<a href="#orgb920473">16</a>所示<br />
</p>

<div id="orgb920473" class="figure">
<p><img src="images/exam-Network1.png" alt="exam-Network1.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 16: </span>學測成績預測模型#1</p>
</div>

<p>
然而，上述因素只是一般性的文字描述，畢竟過於模糊而無法對之進行精確計算，所以，我們有必要再對其進行更精確的描述，此處的參數（即影響因素及相對權重）又稱為特徵值。此外，每個因素影響學測結果的程度理應會有所差異，因此也有必要對各因素賦予「加權」（也稱為權重），詳細考慮後的因素及加權列表如下。<br />
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> 學測預測模型因素列表</caption>

<colgroup>
<col  class="org-right" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">no</th>
<th scope="col" class="org-left">因素編號</th>
<th scope="col" class="org-left">模糊描述</th>
<th scope="col" class="org-left">精確描述</th>
<th scope="col" class="org-left">權重</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-left">\(x_1\)</td>
<td class="org-left">上課狀況</td>
<td class="org-left">平均每次上課時認真聽講的時間百分比</td>
<td class="org-left">\(w_1\)</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-left">\(x_2\)</td>
<td class="org-left">是否認真寫作業</td>
<td class="org-left">作業平均成績</td>
<td class="org-left">\(w_2\)</td>
</tr>

<tr>
<td class="org-right">3</td>
<td class="org-left">\(x_3\)</td>
<td class="org-left">歷次段考成績</td>
<td class="org-left">各科段考平均成績</td>
<td class="org-left">\(w_3\)</td>
</tr>

<tr>
<td class="org-right">4</td>
<td class="org-left">\(x_4\)</td>
<td class="org-left">校內模考成績</td>
<td class="org-left">歷次模考平均成績</td>
<td class="org-left">\(w_4\)</td>
</tr>

<tr>
<td class="org-right">5</td>
<td class="org-left">\(x_5\)</td>
<td class="org-left">放學後是否努力讀書</td>
<td class="org-left">放學後花在課業上的時間</td>
<td class="org-left">\(w_5\)</td>
</tr>

<tr>
<td class="org-right">6</td>
<td class="org-left">\(x_6\)</td>
<td class="org-left">是否沉迷網路遊戲或手機遊戲</td>
<td class="org-left">每天平均花在遊戲的時間</td>
<td class="org-left">\(w_6\)</td>
</tr>

<tr>
<td class="org-right">7</td>
<td class="org-left">\(x_7\)</td>
<td class="org-left">是否花太多時間交異性朋友</td>
<td class="org-left">有/無男女朋友</td>
<td class="org-left">\(w_7\)</td>
</tr>
</tbody>
</table>

<p>
此時，我們的預測模型就如圖<a href="#orgeb10722">17</a>所示，換言之，是在解一個\(f(x)=x_1*w_1+x_2*w_2+x_3*w_3+...+x_7*w_7\)的函式問題。我們可以先針對這些特徵值對學生進行問卷調查，並追踪學生的學測成績，最後將取得的大量的特徵值輸入到到我們的函數模型（圖<a href="#orgeb10722">17</a>）中，觀察計算結果與實際資料的吻合程度，藉由不斷的調整參數（權重）來控制函數，讓輸出的計算結果與實際答案完全吻合，以便求得最準確的函數。<br />
</p>

<div id="orgeb10722" class="figure">
<p><img src="images/exam-network2.png" alt="exam-network2.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 17: </span>學測成績預測模型#2</p>
</div>

<p>
然而，無論我們事前研究分析調查的再如何嚴謹，實際的計算結果與真實分數總會存在誤差，如表<a href="#org9d0bc91">2</a>，分別觀察這些誤差值並不容易看出吻合程度，但如果將個別的誤差平方後加總，則可以得到一個明確的誤差函數=\(3^2+(-3)^2+(-2)^2+(-2)^2...\)，至此，解題的任務便轉為：找出能讓誤差函數最小化的一組參數。<br />
</p>
<table id="org9d0bc91" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 2:</span> 誤差的計算</caption>

<colgroup>
<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-center">&#xa0;</th>
<th scope="col" class="org-center">學生 A</th>
<th scope="col" class="org-center">學生 B</th>
<th scope="col" class="org-center">學生 C</th>
<th scope="col" class="org-center">學生 D</th>
<th scope="col" class="org-center">&#x2026;</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-center">資料</td>
<td class="org-center">70</td>
<td class="org-center">65</td>
<td class="org-center">68</td>
<td class="org-center">50</td>
<td class="org-center">&#x2026;</td>
</tr>

<tr>
<td class="org-center">模型</td>
<td class="org-center">67</td>
<td class="org-center">68</td>
<td class="org-center">70</td>
<td class="org-center">52</td>
<td class="org-center">&#x2026;</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-center">誤差</td>
<td class="org-center">3</td>
<td class="org-center">-3</td>
<td class="org-center">-2</td>
<td class="org-center">-2</td>
<td class="org-center">&#x2026;</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-orgc5deb4e" class="outline-3">
<h3 id="orgc5deb4e"><span class="section-number-3">4.2.</span> 如何調整參數</h3>
<div class="outline-text-3" id="text-4-2">
<p>
前節提及，為了能找到最理想的預測函數，我們可以不斷調整權重來把誤差函數降到最低。實務上，我們可以每次以最細微的調幅逐一調整（增加或減少）權重值來試圖減小損失函數，直到其無法再減少為止，此種方式稱為「坡度法」，而這種每次稍微調整一點點再觀察結果變化的手段稱為微分；若是同時微幅調整所有權重以將損失函數降到最低，這種方式則稱為「梯度下降法」。然而類似梯度法並不保證能找到將損失函數降到最小的權重組合，如圖<a href="#org01aa541">18</a>所示，以梯度法可能只會找到 A 點這個局部最小值，然而全體的最小值其實發生在 B 點。<br />
</p>

<div id="org01aa541" class="figure">
<p><img src="images/dotAB.png" alt="dotAB.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 18: </span>極小值與最小值的差異</p>
</div>
</div>
</div>
<div id="outline-container-orgf462471" class="outline-3">
<h3 id="orgf462471"><span class="section-number-3">4.3.</span> 模型的極限</h3>
<div class="outline-text-3" id="text-4-3">
<p>
在我們透過問卷取得大量的數據後，想像一下我們以這些數據來做為調整模型參數的依據，最後，我們如何評估這個模型的效能呢？一般來說，我們會把數據分成兩部份：<br />
</p>
<ol class="org-ol">
<li>訓練資料: 用來給模型(神經網路)調整或學習參數<br /></li>
<li>測試資料: 用來給模型(神經網路)測試或檢驗模型的效能。<br /></li>
</ol>

<p>
之所以用不同的數據進行訓練與測試，是為了避免「過擬合」的狀況，即，因為測試資料與訓練資料一致，導致測試結果十分完美，然而，一旦把模型拿來應用到新的數據上（或是實際應用模型到真實世界中）時，反而效果會不如預期。<br />
</p>

<p>
過擬合就好比學生在學習時只死記課本的習題，對於其他題型完全不予理會，如果考試也考課本的習題，考試成績自然優異，然而如果考試時題型略做變化，則考試結果就可能十分悲慘。<br />
</p>

<p>
實際進行測試時，可以將資料分成數組，將其中一組當成測試資料。例如，分為A、B、C、D 4組，然後輪流拿這4組資料中的一組做為測試資料進行 4 次相同的測試，目的在於提升模型的「泛化能力」，也就是減少其過擬合的可能性。<br />
</p>
</div>
</div>
<div id="outline-container-orgadfff7c" class="outline-3">
<h3 id="orgadfff7c"><span class="section-number-3">4.4.</span> 神經網路為什麼要有那麼多層</h3>
<div class="outline-text-3" id="text-4-4">
<p>
前節提及，我們的預測模型就如圖<a href="#orgeb10722">17</a>所示，換言之，是在解一個\(f(x)=x_1*w_1+x_2*w_2+x_3*w_3+...+x_7*w_7\)的函式問題，那麼，為了能找到最理想的預測函數，可否把函數變的更加複雜，例如，將函數變為二次函數或更複雜的函數以提升預測的精準度？實則，這種社會科學的問題並不如自然科學的物理現象可以用明確的公式來解決，神經網路採用的是以組合的方式來將函數複雜化，例如，把圖<a href="#orgeb10722">17</a>變為下圖的樣式，如此藉由改變各因素以及權值的組合，等於建立了許多新的特徵值，也增加了模型的複雜度。<br />
</p>


<div id="orge795b02" class="figure">
<p><img src="images/exam-Network3.png" alt="exam-Network3.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 19: </span>學測成績預測模型#3</p>
</div>

<p>
然而，如果只是以這種「新增特徵值組合與權重」進而產生新特徵值的方式來改變模型，那麼，再多的層數也能合併為一層，因為這些運算方式均屬於線性轉換，為了有效讓模型更加複雜，此處可以在模型中加入非線性轉換，如圖<a href="#org4fcd7fe">20</a>中的ReLU<a href="../../../../../Dropbox/notes/roam/20240215153606-activationfunction.html#ID-d3bcc30a-3d94-4a3c-8e66-baaac7325c75">激勵函數</a>，其結果如圖<a href="#org2bb2df4">21</a>所示。<br />
</p>


<div id="org4fcd7fe" class="figure">
<p><img src="images/ReLUPlot.png" alt="ReLUPlot.png" width="300" /><br />
</p>
<p><span class="figure-number">Figure 20: </span>ReLU 函數圖</p>
</div>


<div id="org2bb2df4" class="figure">
<p><img src="./images/exam-Network4.png" alt="exam-Network4.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 21: </span>學測成績預測模型#4</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgac32847" class="outline-2">
<h2 id="orgac32847"><span class="section-number-2">5.</span> DNN實作: 辨識手寫數字</h2>
<div class="outline-text-2" id="text-5">
<p>
使用神經網路解決問題分為兩個步驟：「學習」與「推論」，學習指使用訓練資料進行權重參數的學習；而推論指使用學習過的參數進行資料分類。<br />
</p>

<p>
MNIST 是機器學習領域中相當著名的資料集，號稱機器學習領域的「Hello world.」，其重要性不言可喻。MNIST 資料集由 0~9 的數字影像構成(如圖<a href="#orgf25499a">22</a>)，共計 60000 張訓練影像、10000 張測試影像。一般的 MMIST 資料集的用法為：使用訓練影像進行學習，再利用學習後的模型預測能否正確分類測試影像。<br />
</p>

<div id="orgf25499a" class="figure">
<p><img src="images/MNIST.jpg" alt="MNIST.jpg" width="300" /><br />
</p>
<p><span class="figure-number">Figure 22: </span>MNIST 資料集內容範例</p>
</div>
</div>
<div id="outline-container-org7665ecd" class="outline-3">
<h3 id="org7665ecd"><span class="section-number-3">5.1.</span> Keras</h3>
<div class="outline-text-3" id="text-5-1">
<p>
此處以最簡單的 DNN (deep neural network) 作為範例。以 Keras 的核心為模型，應用最常使用 Sequential 模型。藉由.add()我們可以一層一層的將神經網路疊起。在每一層之中我們只需要簡單的設定每層的大小(units)與激活函數(activation function)。需要特別記得的是：第一層要記得寫輸入的向量大小、最後一層的 units 要等於輸出的向量大小。在這邊我們最後一層使用的激活函數(activation function)為 softmax。<br />
相對應程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36617;&#20837;&#36039;&#26009;</span>
<span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> to_categorical

<span style="color: #51afef;">def</span> <span style="color: #c678dd;">load_data</span>():
    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36617;&#20837;minst&#30340;&#36039;&#26009;</span>
    (x_train, y_train), (<span style="color: #dcaeea;">x_test</span>, <span style="color: #dcaeea;">y_test</span>) = mnist.load_data()
    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#22294;&#29255;&#36681;&#25563;&#28858;&#19968;&#20491;60000*784&#30340;&#21521;&#37327;&#65292;&#20006;&#19988;&#27161;&#28310;&#21270;</span>
    <span style="color: #dcaeea;">x_train</span> = x_train.reshape(x_train.shape[<span style="color: #da8548; font-weight: bold;">0</span>], <span style="color: #da8548; font-weight: bold;">28</span>*<span style="color: #da8548; font-weight: bold;">28</span>)
    <span style="color: #dcaeea;">x_test</span> = x_test.reshape(x_test.shape[<span style="color: #da8548; font-weight: bold;">0</span>], <span style="color: #da8548; font-weight: bold;">28</span>*<span style="color: #da8548; font-weight: bold;">28</span>)
    <span style="color: #dcaeea;">x_train</span> = x_train.astype(<span style="color: #98be65;">'float32'</span>)
    <span style="color: #dcaeea;">x_test</span> = x_test.astype(<span style="color: #98be65;">'float32'</span>)
    <span style="color: #dcaeea;">x_train</span> = x_train/<span style="color: #da8548; font-weight: bold;">255</span>
    <span style="color: #dcaeea;">x_test</span> = x_test/<span style="color: #da8548; font-weight: bold;">255</span>
    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;y&#36681;&#25563;&#25104;one-hot encoding</span>
    <span style="color: #dcaeea;">y_train</span> = to_ctaegorical(y_train, <span style="color: #da8548; font-weight: bold;">10</span>)
    <span style="color: #dcaeea;">y_test</span> = to_categorical(y_test, <span style="color: #da8548; font-weight: bold;">10</span>)
    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22238;&#20659;&#34389;&#29702;&#23436;&#30340;&#36039;&#26009;</span>
    <span style="color: #51afef;">return</span> (x_train, y_train), (x_test, y_test)

<span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense, Activation, Dropout, Reshape, Permute
<span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span style="color: #51afef;">from</span> keras.optimizers <span style="color: #51afef;">import</span>  Adam

<span style="color: #51afef;">def</span> <span style="color: #c678dd;">build_model</span>():<span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#24314;&#31435;&#27169;&#22411;</span>
    <span style="color: #dcaeea;">model</span> = Sequential()
    <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#23559;&#27169;&#22411;&#30090;&#36215;</span>
    model.add(Dense(input_dim=<span style="color: #da8548; font-weight: bold;">28</span>*<span style="color: #da8548; font-weight: bold;">28</span>,units=<span style="color: #da8548; font-weight: bold;">500</span>,activation=<span style="color: #98be65;">'relu'</span>))
    model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">500</span>,activation=<span style="color: #98be65;">'relu'</span>))
    model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">500</span>,activation=<span style="color: #98be65;">'relu'</span>))
    model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">10</span>,activation=<span style="color: #98be65;">'softmax'</span>))
    model.summary()
    <span style="color: #51afef;">return</span> model
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38283;&#22987;&#35347;&#32244;&#27169;&#22411;&#65292;&#27492;&#34389;&#20351;&#29992;&#20102;Adam&#20570;&#28858;&#25105;&#20497;&#30340;&#20778;&#21270;&#22120;&#65292;loss function&#36984;&#29992;&#20102;categorical_crossentropy&#12290;</span>
(x_train, y_train), (<span style="color: #dcaeea;">x_test</span>, <span style="color: #dcaeea;">y_test</span>) = load_data()
<span style="color: #dcaeea;">model</span> = build_model()
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38283;&#22987;&#35347;&#32244;&#27169;&#22411;</span>
model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>,optimizer=<span style="color: #98be65;">"adam"</span>,metrics=[<span style="color: #98be65;">'accuracy'</span>])
model.fit(x_train,y_train,batch_size=<span style="color: #da8548; font-weight: bold;">1000</span>,epochs=<span style="color: #da8548; font-weight: bold;">20</span>)
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#39023;&#31034;&#35347;&#32244;&#32080;&#26524;</span>
<span style="color: #dcaeea;">score</span> = model.evaluate(x_train,y_train)
<span style="color: #c678dd;">print</span> (<span style="color: #98be65;">'</span><span style="color: #a9a1e1;">\n</span><span style="color: #98be65;">Train Acc:'</span>, score[<span style="color: #da8548; font-weight: bold;">1</span>])
<span style="color: #dcaeea;">score</span> = model.evaluate(x_test,y_test)
<span style="color: #c678dd;">print</span> (<span style="color: #98be65;">'</span><span style="color: #a9a1e1;">\n</span><span style="color: #98be65;">Test Acc:'</span>, score[<span style="color: #da8548; font-weight: bold;">1</span>])
</pre>
</div>

<pre class="example" id="org449ff70">
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_1 (Dense)              (None, 500)               392500
_________________________________________________________________
dense_2 (Dense)              (None, 500)               250500
_________________________________________________________________
dense_3 (Dense)              (None, 500)               250500
_________________________________________________________________
dense_4 (Dense)              (None, 10)                5010
=================================================================
Total params: 898,510
Trainable params: 898,510
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20

  100/60000 [..............................] - ETA: 2:55 - loss: 2.2917 - acc: 0.1300
  800/60000 [..............................] - ETA: 25s - loss: 1.6424 - ACM: 0.5362
.......
16300/60000 [=======&gt;......................] - ETA: 4s - loss: 0.3752 - acc: 0.8898
17000/60000 [=======&gt;......................] - ETA: 4s - loss: 0.3681 - acc: 0.8916
.......
50600/60000 [========================&gt;.....] - ETA: 0s - loss: 0.2232 - acc: 0.9335
51300/60000 [========================&gt;.....] - ETA: 0s - loss: 0.2220 - acc: 0.9338
.......
59700/60000 [============================&gt;.] - ETA: 0s - loss: 0.2078 - acc: 0.9377
60000/60000 [==============================] - 5s 81us/step - loss: 0.2074 - acc: 0.9379
Epoch 2/20

  100/60000 [..............................] - ETA: 5s - loss: 0.0702 - acc: 0.9800
......
60000/60000 [==============================] - 5s 77us/step - loss: 0.0832 - acc: 0.9740
Epoch 3/20
......
Epoch 29/20

   32/60000 [..............................] - ETA: 1:10
 1440/60000 [..............................] - ETA: 3s
......
58496/60000 [============================&gt;.] - ETA: 0s
60000/60000 [==============================] - 2s 34us/step

Train Acc: 0.9981666666666666

   32/10000 [..............................] - ETA: 0s
 1568/10000 [===&gt;..........................] - ETA: 0s
 3104/10000 [========&gt;.....................] - ETA: 0s
 4640/10000 [============&gt;.................] - ETA: 0s
 6176/10000 [=================&gt;............] - ETA: 0s
 7680/10000 [======================&gt;.......] - ETA: 0s
 9184/10000 [==========================&gt;...] - ETA: 0s
10000/10000 [==============================] - 0s 33us/step

Test Acc: 0.9823
</pre>
</div>
</div>
<div id="outline-container-org625d6cf" class="outline-3">
<h3 id="org625d6cf"><span class="section-number-3">5.2.</span> Python手刻</h3>
<div class="outline-text-3" id="text-5-2">
</div>
<div id="outline-container-org8d76ef5" class="outline-4">
<h4 id="org8d76ef5"><span class="section-number-4">5.2.1.</span> 準備 MNIST 資料</h4>
<div class="outline-text-4" id="text-5-2-1">
<p>
準備資料是訓練模型的第一步，基礎資料可以是網上公開的資料集，也可以是自己的資料集。視覺、語音、語言等各種型別的資料在網上都能找到相應的資料集。<br />
</p>

<p>
MNIST 數據集來自美國國家標準與技術研究所, National Institute of Standards and Technology (NIST). 訓練集 (training set) 由來自 250 個不同人手寫的數字構成, 其中 50% 是高中學生, 50% 來自人口普查局 (the Census Bureau) 的工作人員. 測試集(test set) 也是同樣比例的手寫數字數據。MNIST 數據集可在 <a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a> 獲取, 它包含了四個部分:<br />
</p>
<ol class="org-ol">
<li>Training set images: train-images-idx3-ubyte.gz (9.9 MB, 解壓後 47 MB, 包含 60,000 個樣本)<br /></li>
<li>Training set labels: train-labels-idx1-ubyte.gz (29 KB, 解壓後 60 KB, 包含 60,000 個標籤)<br /></li>
<li>Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 解壓後 7.8 MB, 包含 10,000 個樣本)<br /></li>
<li>Test set labels: t10k-labels-idx1-ubyte.gz (5KB, 解壓後 10 KB, 包含 10,000 個標籤)<br /></li>
</ol>

<p>
MNIST 資料集是一個適合拿來當作 TensotFlow 的練習素材，在 Tensorflow 的現有套件中，也已經有內建好的 MNIST 資料集，我們只要在安裝好 TensorFlow 的 Python 環境中執行以下程式碼，即可將 MNIST 資料成功讀取進來。.<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span class="linenr">2: </span><span style="color: #dcaeea;">mnist</span> = tf.keras.datasets.mnist
<span id="coderef-get-keras-mnist" class="coderef-off"><span class="linenr">3: </span>(x_train, y_train), (<span style="color: #dcaeea;">x_test</span>, <span style="color: #dcaeea;">y_test</span>) = mnist.load_data()</span>
</pre>
</div>

<p>
在訓練模型之前，需要將樣本資料劃分為訓練集、測試集，有些情況下還會劃分為訓練集、測試集、驗證集。由上述程式第<a href="#coderef-get-keras-mnist" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-get-keras-mnist');" onmouseout="CodeHighlightOff(this, 'coderef-get-keras-mnist');">3</a>行可知，下載後的 MNIST 資料分成訓練資料(training data)與測試資料(testing data)，其中 x 為圖片、y為所對應數字。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span class="linenr"> 2: </span><span style="color: #dcaeea;">mnist</span> = tf.keras.datasets.mnist
<span class="linenr"> 3: </span>(x_train, y_train), (<span style="color: #dcaeea;">x_test</span>, <span style="color: #dcaeea;">y_test</span>) = mnist.load_data()
<span class="linenr"> 4: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">=====================================</span>
<span class="linenr"> 5: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21028;&#26039;&#36039;&#26009;&#24418;&#29376;</span>
<span class="linenr"> 6: </span><span style="color: #c678dd;">print</span>(x_train.shape)
<span class="linenr"> 7: </span><span style="color: #c678dd;">print</span>(x_test.shape)
<span class="linenr"> 8: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#31532;&#19968;&#20491;label&#30340;&#20839;&#23481;</span>
<span class="linenr"> 9: </span><span style="color: #c678dd;">print</span>(y_train[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">10: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#24433;&#20687;&#20839;&#23481;</span>
<span class="linenr">11: </span><span style="color: #51afef;">import</span> matplotlib.pylab <span style="color: #51afef;">as</span> plt
<span class="linenr">12: </span><span style="color: #dcaeea;">img</span> = x_train[<span style="color: #da8548; font-weight: bold;">0</span>]
<span class="linenr">13: </span>plt.imshow(img)
<span class="linenr">14: </span>plt.savefig(<span style="color: #98be65;">"MNIST-Image.png"</span>)
</pre>
</div>
<pre class="example">
(60000, 28, 28)
(10000, 28, 28)
5
</pre>


<p>
由上述程式輸出結果可以看到載入的 x 為大小為 28*28 的圖片共 60000 張，每一筆 MNIST 資料的照片(x)由 784 個 pixels 組成（28*28），照片內容如圖<a href="#org512cf2d">23</a>，訓練集的標籤(y)則為其對應的數字(0～9)，此例為 5。<br />
</p>

<div id="org512cf2d" class="figure">
<p><img src="images/MNIST-Image.png" alt="MNIST-Image.png" width="300" /><br />
</p>
<p><span class="figure-number">Figure 23: </span>MNIST 影像示例</p>
</div>

<p>
x 的影像資料為灰階影像，每個像素的數值介於 0~255 之間，矩陣裡每一項的資料則是代表每個 pixel 顏色深淺的數值，如下圖<a href="#org7510b7a">24</a>所示：<br />
</p>

<div id="org7510b7a" class="figure">
<p><img src="images/MNIST-Matrix.png" alt="MNIST-Matrix.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 24: </span>MNIST 資料矩陣</p>
</div>

<p>
載入的 y 為所對應的數字 0~9，在這我們要運用 keras 中的 np_utils.to_categorical 將 y 轉成 one-hot 的形式，將他轉為一個 10 維的 vector，例如：我們所拿到的資料為 y=3，經過 np_utils.to_categorical，會轉換為 y=[0,0,0,1,0,0,0,0,0,0]。這部份的轉換程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 2: </span><span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> to_categorical
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span class="linenr"> 5: </span><span style="color: #dcaeea;">mnist</span> = tf.keras.datasets.mnist
<span class="linenr"> 6: </span>(x_train, y_train), (<span style="color: #dcaeea;">x_test</span>, <span style="color: #dcaeea;">y_test</span>) = mnist.load_data()
<span class="linenr"> 7: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">=====================================</span>
<span class="linenr"> 8: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#22294;&#29255;&#36681;&#25563;&#28858;&#19968;&#20491;60000*784&#30340;&#21521;&#37327;&#65292;&#20006;&#19988;&#27161;&#28310;&#21270;</span>
<span class="linenr"> 9: </span><span style="color: #dcaeea;">x_train</span> = x_train.reshape(x_train.shape[<span style="color: #da8548; font-weight: bold;">0</span>], <span style="color: #da8548; font-weight: bold;">28</span>*<span style="color: #da8548; font-weight: bold;">28</span>)
<span class="linenr">10: </span><span style="color: #dcaeea;">x_test</span> = x_test.reshape(x_test.shape[<span style="color: #da8548; font-weight: bold;">0</span>], <span style="color: #da8548; font-weight: bold;">28</span>*<span style="color: #da8548; font-weight: bold;">28</span>)
<span class="linenr">11: </span><span style="color: #dcaeea;">x_train</span> = x_train.astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">12: </span><span style="color: #dcaeea;">x_test</span> = x_test.astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">13: </span><span style="color: #dcaeea;">x_train</span> = x_train/<span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">14: </span><span style="color: #dcaeea;">x_test</span> = x_test/<span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">15: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;y&#36681;&#25563;&#25104;one-hot encoding</span>
<span class="linenr">16: </span><span style="color: #dcaeea;">y_train</span> = to_categorical(y_train, <span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">17: </span><span style="color: #dcaeea;">y_test</span> = to_categorical(y_test, <span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">18: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22238;&#20659;&#34389;&#29702;&#23436;&#30340;&#36039;&#26009;</span>
<span class="linenr">19: </span><span style="color: #c678dd;">print</span>(y_train[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">20: </span>
<span class="linenr">21: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">22: </span>np.set_printoptions(precision=<span style="color: #da8548; font-weight: bold;">2</span>, linewidth=np.inf)
<span class="linenr">23: </span><span style="color: #c678dd;">print</span>(x_train[<span style="color: #da8548; font-weight: bold;">0</span>].reshape(<span style="color: #da8548; font-weight: bold;">28</span>, <span style="color: #da8548; font-weight: bold;">28</span>))
</pre>
</div>

<pre class="example" id="orgbea9ea0">
[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
[[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.07 0.07 0.07 0.49 0.53 0.69 0.1  0.65 1.   0.97 0.5  0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.12 0.14 0.37 0.6  0.67 0.99 0.99 0.99 0.99 0.99 0.88 0.67 0.99 0.95 0.76 0.25 0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.19 0.93 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.98 0.36 0.32 0.32 0.22 0.15 0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.07 0.86 0.99 0.99 0.99 0.99 0.99 0.78 0.71 0.97 0.95 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.31 0.61 0.42 0.99 0.99 0.8  0.04 0.   0.17 0.6  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.05 0.   0.6  0.99 0.35 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.55 0.99 0.75 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.04 0.75 0.99 0.27 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.14 0.95 0.88 0.63 0.42 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.32 0.94 0.99 0.99 0.47 0.1  0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.18 0.73 0.99 0.99 0.59 0.11 0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.06 0.36 0.99 0.99 0.73 0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.98 0.99 0.98 0.25 0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.18 0.51 0.72 0.99 0.99 0.81 0.01 0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.15 0.58 0.9  0.99 0.99 0.99 0.98 0.71 0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.09 0.45 0.87 0.99 0.99 0.99 0.99 0.79 0.31 0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.09 0.26 0.84 0.99 0.99 0.99 0.99 0.78 0.32 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.07 0.67 0.86 0.99 0.99 0.99 0.99 0.76 0.31 0.04 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.22 0.67 0.89 0.99 0.99 0.99 0.99 0.96 0.52 0.04 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.53 0.99 0.99 0.99 0.83 0.53 0.52 0.06 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]]
</pre>
</div>
</div>
<div id="outline-container-org4485279" class="outline-4">
<h4 id="org4485279"><span class="section-number-4">5.2.2.</span> MNIST 的推論處理</h4>
<div class="outline-text-4" id="text-5-2-2">
<p>
如圖<a href="#org4d45e4e">25</a>所示，MNIST 的推論神經網路最前端的輸入層有 784 (\(28*28=784\))個神經元，最後的輸出端有 10 個神經元(\(0~9\)個數字)，至於中間的隠藏層有兩個，第 1 個隱藏層有 50 個神經元，第 2 層有 100 個。此處的 50、100 可以設定為任意數（如，也可以是 128、64）。<br />
</p>

<div id="org4d45e4e" class="figure">
<p><img src="images/MNIST-CNN.png" alt="MNIST-CNN.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 25: </span>MNIST-NeuralNet</p>
</div>

<p>
為了完成上述推論，此處定義三個函數：get_data()、init_network()、predict()，其中 init_work()直接讀入作者已經訓練好的網絡權重。在以下這段程式碼中，權重與偏權值的參數會儲存成字典型態的變數。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">from</span> keras.datasets.mnist <span style="color: #51afef;">import</span> load_data
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span><span style="color: #51afef;">import</span> pickle
<span class="linenr"> 4: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">sigmoid</span>(x):
<span class="linenr"> 5: </span>    <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">1</span> / (<span style="color: #da8548; font-weight: bold;">1</span> + np.exp(-x))
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38450;&#27490;&#28322;&#20986;&#22411;</span>
<span class="linenr"> 8: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">softmax</span>(x):
<span class="linenr"> 9: </span>    <span style="color: #dcaeea;">c</span> = np.<span style="color: #c678dd;">max</span>(x)
<span class="linenr">10: </span>    <span style="color: #dcaeea;">exp_x</span> = np.exp(x - c)
<span class="linenr">11: </span>    <span style="color: #dcaeea;">sum_exp_x</span> = np.<span style="color: #c678dd;">sum</span>(exp_x)
<span class="linenr">12: </span>    <span style="color: #51afef;">return</span> exp_x / sum_exp_x
<span class="linenr">13: </span>
<span class="linenr">14: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">get_data</span>():
<span class="linenr">15: </span>    (X_train, y_train), (<span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_test</span>) = load_data()
<span class="linenr">16: </span>    <span style="color: #51afef;">return</span> X_test.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>), y_test
<span class="linenr">17: </span>
<span class="linenr">18: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">init_network</span>():
<span class="linenr">19: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">https://github.com/Bingyy/deep-learning-from-scratch/blob/master/ch03/sample_weight.pkl</span>
<span class="linenr">20: </span>    <span style="color: #51afef;">with</span> <span style="color: #c678dd;">open</span>(<span style="color: #98be65;">'./Downloads/sample_weight.pkl'</span>, <span style="color: #98be65;">'rb'</span>) <span style="color: #51afef;">as</span> f:
<span class="linenr">21: </span>        <span style="color: #dcaeea;">network</span> = pickle.load(f)
<span class="linenr">22: </span>        <span style="color: #51afef;">return</span> network
<span class="linenr">23: </span>
<span class="linenr">24: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23384;&#20786;&#30340;&#26159;&#32178;&#32097;&#21443;&#25976;&#23383;&#20856;</span>
<span class="linenr">25: </span><span style="color: #dcaeea;">network</span> = init_network()
<span class="linenr">26: </span>
<span class="linenr">27: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32068;&#21512;&#32178;&#32097;&#27969;&#31243;&#65292;&#29992;&#26044;&#38928;&#28204;</span>
<span id="coderef-MNIST-predict" class="coderef-off"><span class="linenr">28: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">predict</span>(network, x):</span>
<span class="linenr">29: </span>    <span style="color: #dcaeea;">W1</span>, <span style="color: #dcaeea;">W2</span>, <span style="color: #dcaeea;">W3</span> = network[<span style="color: #98be65;">'W1'</span>], network[<span style="color: #98be65;">'W2'</span>], network[<span style="color: #98be65;">'W3'</span>]
<span class="linenr">30: </span>    <span style="color: #dcaeea;">b1</span>, <span style="color: #dcaeea;">b2</span>, <span style="color: #dcaeea;">b3</span> = network[<span style="color: #98be65;">'b1'</span>], network[<span style="color: #98be65;">'b2'</span>], network[<span style="color: #98be65;">'b3'</span>]
<span class="linenr">31: </span>    <span style="color: #dcaeea;">a1</span> = np.dot(x,W1) + b1
<span class="linenr">32: </span>    <span style="color: #dcaeea;">z1</span> = sigmoid(a1)
<span class="linenr">33: </span>    <span style="color: #dcaeea;">a2</span> = np.dot(z1, W2) + b2
<span class="linenr">34: </span>    <span style="color: #dcaeea;">z2</span> = sigmoid(a2)
<span class="linenr">35: </span>    <span style="color: #dcaeea;">a3</span> = np.dot(z2, W3) + b3
<span class="linenr">36: </span>    <span style="color: #dcaeea;">y</span> = softmax(a3) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20998;&#39006;&#29992;&#30340;&#26368;&#24460;&#36664;&#20986;&#23652;&#30340;&#28608;&#27963;&#20989;&#25976;</span>
<span class="linenr">37: </span>    <span style="color: #51afef;">return</span> y
<span class="linenr">38: </span>
<span class="linenr">39: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20351;&#29992;&#32178;&#32097;&#38928;&#28204;</span>
<span class="linenr">40: </span><span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_test</span> = get_data() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24471;&#21040;&#28204;&#35430;&#25976;&#25818;</span>
<span class="linenr">41: </span><span style="color: #dcaeea;">network</span> = init_network()
<span class="linenr">42: </span>
<span class="linenr">43: </span><span style="color: #dcaeea;">accuracy_cnt</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">44: </span><span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #c678dd;">len</span>(X_test)):
<span id="coderef-y-predict" class="coderef-off"><span class="linenr">45: </span>    <span style="color: #dcaeea;">y</span> = predict(network, X_test[i])</span>
<span id="coderef-np-argmax" class="coderef-off"><span class="linenr">46: </span>    <span style="color: #dcaeea;">p</span> = np.argmax(y)</span>
<span class="linenr">47: </span>    np.set_printoptions(precision=<span style="color: #da8548; font-weight: bold;">4</span>, suppress=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">48: </span>    <span style="color: #51afef;">if</span> p == y_test[i]:
<span class="linenr">49: </span>        <span style="color: #dcaeea;">accuracy_cnt</span> += <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">50: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#28310;&#30906;&#29575;&#65306;'</span>, <span style="color: #c678dd;">str</span>(<span style="color: #c678dd;">float</span>(accuracy_cnt) / <span style="color: #c678dd;">len</span>(X_test)))
</pre>
</div>

<pre class="example">
準確率： 0.9207
</pre>


<p>
上述程式中，predict 程序(第<a href="#coderef-MNIST-predict" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-MNIST-predict');" onmouseout="CodeHighlightOff(this, 'coderef-MNIST-predict');">28</a>)透過矩陣相乘運算完成神經網路的參數傳遞，最後必須進行準確率的評估，程式碼第<a href="#coderef-y-predict" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-y-predict');" onmouseout="CodeHighlightOff(this, 'coderef-y-predict');">45</a>行為神經網路針對輸入圖片的預測結果，所傳回的值為各猜測值的機率陣列，如：[0.0004 0.0011 0.9859 0.0065 0.     0.0007 0.0051 0.     0.0003 0.    ]；而程式碼第<a href="#coderef-np-argmax" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-np-argmax');" onmouseout="CodeHighlightOff(this, 'coderef-np-argmax');">46</a>則是該圖片的應對標籤，np.argmax(y)會傳回 y 的最大值所在順序，若 y=[0,0,0,1,0,0,0,0,0,0]，則傳回 3，藉此計算預測正確的百分比。<br />
</p>
</div>
</div>
<div id="outline-container-org438b2e3" class="outline-4">
<h4 id="org438b2e3"><span class="section-number-4">5.2.3.</span> Python 與神經網路運算的批次處理</h4>
<div class="outline-text-4" id="text-5-2-3">
<p>
前節程式碼中最後以 for 迴圈來逐一處理預測結果與比較，輸入(X)為單一圖片，其處理程序如圖<a href="#org363b7f8">26</a>所示：<br />
</p>

<div id="org363b7f8" class="figure">
<p><img src="images/MNIST-single.png" alt="MNIST-single.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 26: </span>MNIST-單一處理架構</p>
</div>

<p>
事實上，在使用批次處理（如一次處理 100 張圖）反而能大幅縮短每張圖片的處理時間，因為多數處理數值運算的函式庫都會針對大型陣列運算進行最佳化，尤其是透過 GPU 來處理時更是如此，這時，傳送單張圖片反而成為效能瓶頸，以批次處理則可減輕匯流排頻寛負擔。若以每次處理 100 張為例，其處理程序則如圖<a href="#org4516d88">27</a>所示。<br />
</p>

<div id="org4516d88" class="figure">
<p><img src="images/MNIST-batch.png" alt="MNIST-batch.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 27: </span>MNIST-批次處理架構</p>
</div>

<p>
至於批次運算的程式碼如下。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25209;&#27425;&#34389;&#29702;&#26550;&#27083;</span>
<span class="linenr">2: </span><span style="color: #dcaeea;">batch_size</span> = <span style="color: #da8548; font-weight: bold;">100</span>
<span class="linenr">3: </span><span style="color: #dcaeea;">accuracy_cnt</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">4: </span><span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #c678dd;">len</span>(X_test), batch_size):
<span id="coderef-b-mnist-x" class="coderef-off"><span class="linenr">5: </span>    <span style="color: #dcaeea;">x_batch</span> = X_test[i:i+batch_size]</span>
<span class="linenr">6: </span>    <span style="color: #dcaeea;">y_batch</span> = predict(network, x_batch)
<span id="coderef-b-mnist-p" class="coderef-off"><span class="linenr">7: </span>    <span style="color: #dcaeea;">p</span> = np.argmax(y_batch, axis=<span style="color: #da8548; font-weight: bold;">1</span>)</span>
<span class="linenr">8: </span>    <span style="color: #dcaeea;">accuracy_cnt</span> += np.<span style="color: #c678dd;">sum</span>(p == y_test[i:i+batch_size])
<span class="linenr">9: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#28310;&#30906;&#29575;&#65306;'</span>, <span style="color: #c678dd;">str</span>(<span style="color: #c678dd;">float</span>(accuracy_cnt) / <span style="color: #c678dd;">len</span>(X_test)))
</pre>
</div>

<pre class="example">
準確率： 0.9207
</pre>


<p>
上述程式中，第<a href="#coderef-b-mnist-x" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-b-mnist-x');" onmouseout="CodeHighlightOff(this, 'coderef-b-mnist-x');">5</a>行每次取出 100 張圖形檔(X 陣列),第<a href="#coderef-b-mnist-p" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-b-mnist-p');" onmouseout="CodeHighlightOff(this, 'coderef-b-mnist-p');">7</a>行則取得這 100 筆資料中各筆資料最大值索引值，若以每次 4 筆資料為例，所得的估計值 p 可能為[7 2 1 0]，相對應的正確標籤值則儲存於 y_test[0:4]中，以此進行準確率的計算。<br />
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org1f225bc" class="outline-2">
<h2 id="org1f225bc"><span class="section-number-2">6.</span> DNN的進化之路&#xa0;&#xa0;&#xa0;<span class="tag"><span class="Keras">Keras</span></span></h2>
<div class="outline-text-2" id="text-6">
<p>
這是個比較囉唆且完整的版本，裡面包含詳細的程式說明。<br />
</p>
</div>
<div id="outline-container-orgfe25855" class="outline-3">
<h3 id="orgfe25855"><span class="section-number-3">6.1.</span> 簡介</h3>
<div class="outline-text-3" id="text-6-1">
<p>
此例以Keras套件建立一個深度學習模型，用以解決手寫數字的辨識。Keras 是 Python 的深度學習框架，提供一種便利的方式來定義和訓練幾秬所有類型的深度學習模型。<br />
</p>
</div>
<div id="outline-container-org295d678" class="outline-4">
<h4 id="org295d678"><span class="section-number-4">6.1.1.</span> 優點</h4>
<div class="outline-text-4" id="text-6-1-1">
<ul class="org-ul">
<li>相同的程式碼可在 CPU 或 GPU 上執行<br /></li>
<li>內建程式庫支擾了卷積神經網路(用於電腦視覺)、循環神經網路(用於序列資料處理)，以及二者的任何組合。<br /></li>
<li>Keras 可以使用最少的程式碼，花最少的時間，就能建立深度學習模型，並進行培訓、評做準確率；相對的，如果使用 TensorFlow，則需要更多程式碼，花費更多時間。<br /></li>
<li>採用寬鬆的 MIT 授權條款，所以可以自由使用在商業專案上。<br /></li>
</ul>

<p>
Keras 是一個 model-level 模型級的深度學習程式庫，Keras 只處理模型的建立、訓練、預測等功能。深度學習程式庫的運作（如張量運算），Keras 必須配合使用「後端引擎」(backend Engine)進行運算。目前 Keras 提供了兩種 backend engine：Theano 與 TensorFlow。其基本架構如下圖所示：<br />
</p>


<div id="org7fd8371" class="figure">
<p><img src="images/KerasArch.png" alt="KerasArch.png" width="360" /><br />
</p>
<p><span class="figure-number">Figure 28: </span>深度學習軟硬體架構</p>
</div>

<p>
由圖<a href="#org7fd8371">28</a>可看出，Keras 並未被綁定在特定張量程式庫中，而是改以模組方式處理，目前可用的後端引擎有 Montreal 大學 MILA 實驗室的 Theano、Google 的 TensorFlow、Microsoft 的 CNTK&#x2026;等，這些後端引擎在應用不同硬體(CPU/GPU)時則會採用不同的低階程式庫(CUDA/Eigen)。<br />
</p>
</div>
</div>
</div>
<div id="outline-container-org32cdd67" class="outline-3">
<h3 id="org32cdd67"><span class="section-number-3">6.2.</span> 機器學習模型設計模式</h3>
<div class="outline-text-3" id="text-6-2">
<p>
開發一個機器學習模型的流程大致如下：<br />
</p>
<ol class="org-ol">
<li>定義問題並建立資料集<br /></li>
<li>選擇一種評量成功的準則(metrics)<br /></li>
<li>決定驗證(validation)程序<br /></li>
<li>準備資料：定義訓練資料：即 input tensor 和 target tensors(label tensors)<br /></li>
<li>開發出優於基準(baseline)的模型：定義神經網路模型的 layers，以便將 input tensor 對應到預測值<br /></li>
<li>選擇 loss function, optimizer 和監控的評量準則(metrics)來建立學習過程<br /></li>
<li>呼叫模型中的 fit()方法來迭代訓練資料<br /></li>
<li>擴大規模：開發一個過度適配的模型<br /></li>
<li>常規化模型並調整參數<br /></li>
</ol>
</div>
<div id="outline-container-org02c45f6" class="outline-4">
<h4 id="org02c45f6"><span class="section-number-4">6.2.1.</span> 定義問題並建立資料集</h4>
<div class="outline-text-4" id="text-6-2-1">
<p>
進行模型建構之初，我們首先要評估的是：<br />
</p>
<ul class="org-ul">
<li>輸入資料是什麼？想要預測什麼？有什麼樣的訓練資料，就只能學習預測該類問題。例如，手上只有電影評論和情緒標註資料，就只能學習對電影評論的情緒分類。<br /></li>
<li>面臨什麼樣的問題？是二元分類？多類別分類？純量迴歸？向量迴歸？多類別多標籤？分群？生成式學習？增強式學習？不同的問題類型會引導我們如何選擇模型架構與損失函數。<br /></li>
</ul>

<p>
在確認上述兩項問題後，我們是基於以下兩個假設來進行模型的建立：<br />
</p>

<ul class="org-ul">
<li>假設機器可以根據給定的輸入資料預測結果<br /></li>
<li>假設手上的資料能提供足夠的資訊，讓機器能學習到輸入與輸出間的關係<br /></li>
</ul>

<p>
在真正建構出一個可用模型之前，上述兩個假設依然只是假設，必須經過驗證後才能確定成立與否。重點是：並非所有的問題都能透過模型來解決，例如：試圖以最近的歷史價格來預測股票市場的走勢就很難成功，因為光是參考歷史價格並不足以提供預測股價所需資訊。<br />
</p>

<p>
另一種要特別留意的問題類型為非平穩問題(nonstationary problems)，例如分析服裝的消售/推薦，這當中存在的最大問題在於人們購買的衣服種類會隨季節而變化，所以服裝購買在幾個月內是非平穩現象，建立的模型內容會隨著時間而變化。在這種情況下，解決方法有：<br />
</p>

<ul class="org-ul">
<li>不斷以最近的資料重新訓練模型，或<br /></li>
<li>在相對平穩的時間區間(具有規律的週期間)收集資料，以購買衣服為例，應該以年為單位進行資料收集才足以補捉到季節變化的規律。<br /></li>
</ul>

<p>
最後，切記：機器學習只能用於學習訓練資料中已存在的模式，也就是只能認出以前見過的模式。通常我們所謂以過去的資料預測未來，是假設未來的行為在過去曾發生過，但實際情況則未必如此。<br />
</p>
</div>
</div>
<div id="outline-container-org140f522" class="outline-4">
<h4 id="org140f522"><span class="section-number-4">6.2.2.</span> 選擇一種評量成功的準則(metrics)</h4>
<div class="outline-text-4" id="text-6-2-2">
<p>
選好評量成功的準則，才有選擇損失函數的依據。在 Keras 中，所謂選擇評量準則，就是在 compile 時選擇適當的 metrics 參數。大概的選擇原則如下：<br />
</p>
<ul class="org-ul">
<li>二元分類問題：accuracy 和 ROC AUC(area under the receiver operating characteristic curve)為兩種常用的度量。<br /></li>
<li>類別不均(class-imbalanced)問題：使用 precision 和 recall 來做度量。<br /></li>
<li>排名問題或多標籤問題：使用平均精度<br /></li>
<li>少問的問題：自行定義指標<br /></li>
</ul>
</div>
</div>
<div id="outline-container-org42d07ed" class="outline-4">
<h4 id="org42d07ed"><span class="section-number-4">6.2.3.</span> 決定驗證(validation)程序</h4>
<div class="outline-text-4" id="text-6-2-3">
<p>
一旦決定目標，就要決定驗證學習進度的方法，三種常見的驗證方法如下所述，但在大多數情況下，第一個方法就有不錯的效能。<br />
</p>
<ul class="org-ul">
<li>Simple hold-out: 資料量大時適用<br /></li>
<li>K-fold cross validation: 樣本資料不夠多時用<br /></li>
<li>Iterated K-fold validation with shuffling: 資料量非常少時用<br /></li>
</ul>
</div>
</div>
<div id="outline-container-orgad0267b" class="outline-4">
<h4 id="orgad0267b"><span class="section-number-4">6.2.4.</span> 準備資料</h4>
<div class="outline-text-4" id="text-6-2-4">
<p>
一旦知道要訓練什麼、要優化什麼、以及如評估效能，就可以著手準備建構模型，但首先要把資料整理成可以輸入神經網路的格式（張量），以監督式學習而言，其輸入的訓練資料會有以下兩類：即 input tensor 和 target tensors(label tensors)<br />
</p>
</div>
</div>
<div id="outline-container-org8a7c99d" class="outline-4">
<h4 id="org8a7c99d"><span class="section-number-4">6.2.5.</span> 開發優於基準(baseline)的模型</h4>
<div class="outline-text-4" id="text-6-2-5">
<p>
此階段的目標在於實現統計功效(statistical power)，以 MNIST 資料集為例，任何準確度大於 0.1 的模型都可以說具有統計功效的（因為一共有 10 個答案類鞏）；而在 IMDB 範例中，只要準確度大於 0.5 即算。雖然 baseline 是一個很低的標準，但我們不見得都能實現這個目標，如果在嚐試過多個合理架構後模型表現仍無法優於隨機基準能力，則很有可能問題出在輸入資料，也許輸入資料沒有所需答案。<br />
</p>

<p>
如果一切順利，則接下來我們要做出三個關鍵選擇來建構第一個模型：<br />
</p>

<ul class="org-ul">
<li>選擇最後一層的啟動函數：這將為神經網路建立輸出的形式。例如，IMDB 分類最後使用 sigmoid 分成兩個值、MNIST 則以 softmax 分為 10 類。<br /></li>
<li>損失函數：要配合問題類型，如 IMDB 使用 binary_crossentropy、迴歸則使用 mse。<br /></li>
<li>優化器設定：大多數情況下，rmsprop 可做為預設選項搭配預設學習率<br /></li>
</ul>

<p>
下表為選擇啟動函數與損失函數的參考<br />
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">問題類型</th>
<th scope="col" class="org-left">輸出層啟動函數</th>
<th scope="col" class="org-left">損失函數</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Binary classification</td>
<td class="org-left">Sigmoid</td>
<td class="org-left">binary_crossentropy</td>
</tr>

<tr>
<td class="org-left">Multiclass, single-label classification</td>
<td class="org-left">softmax</td>
<td class="org-left">categorical_crossentropy</td>
</tr>

<tr>
<td class="org-left">Multiclass, multi-label classification</td>
<td class="org-left">sigmoid</td>
<td class="org-left">binary_crossentropy</td>
</tr>

<tr>
<td class="org-left">Regression to arbitrary values</td>
<td class="org-left">None</td>
<td class="org-left">mse</td>
</tr>

<tr>
<td class="org-left">Regression to values between 0 and 1</td>
<td class="org-left">sigmoid</td>
<td class="org-left">mse or binary_crossentropy</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-orgf513dbc" class="outline-4">
<h4 id="orgf513dbc"><span class="section-number-4">6.2.6.</span> 擴大規模：開發一個過度適配的模型</h4>
<div class="outline-text-4" id="text-6-2-6">
<p>
一旦成功建構了一個超越 baseline 的模型，問題就變成：這個模型夠不夠強大？有沒有足夠的 layer 和參數來正確模擬手上的問題？例如，只有兩個 units 的單隱藏層也許有辨識 MNIST 的統計功效，但不足以很好的解決該問題。而機器學習就是在最佳化和普適性之間做取捨，理想的模型是介於 underfitting 和 overfitting 的交界、介於模型太小(undercapacity)和模型太大(overcapacity)之間，要找出這個位置，勢必要先越過它再回來。所以，要搞清楚需要多大的模型，就要開發一個太大的模型，有幾種方法可以達到這點：<br />
</p>
<ul class="org-ul">
<li>加入更多的 layer<br /></li>
<li>增加每一層的 capacity<br /></li>
<li>訓練更多的週期<br /></li>
</ul>
</div>
</div>
<div id="outline-container-orgc235417" class="outline-4">
<h4 id="orgc235417"><span class="section-number-4">6.2.7.</span> 常規化模型並調整參數</h4>
<div class="outline-text-4" id="text-6-2-7">
<p>
這裡會花掉最多時間：要反覆修改模型、訓練模型、評估驗證資料，然後再次修改，以下有幾種做法：<br />
</p>

<ul class="org-ul">
<li>加入 dropout<br /></li>
<li>嘗試不同架構：新增或刪除 layer<br /></li>
<li>添加 L1 或 L2 regularization，或同時使用<br /></li>
<li>嘗試使用不同的超參數，如每一曾的 units 數或優化器的學習率<br /></li>
<li>著重於特徵工程，如加入新特徵、刪除似乎沒用的特徵<br /></li>
</ul>
<p>
一旦開發出令人滿意的模型配置，就可以在所有可用資料(訓練和驗證)上訓練最終產出的模型，並在測試集上最後一次評估它。<br />
</p>
</div>
</div>
</div>
<div id="outline-container-org07677c1" class="outline-3">
<h3 id="org07677c1"><span class="section-number-3">6.3.</span> 基本流程</h3>
<div class="outline-text-3" id="text-6-3">
<p>
在 Keras 定義 model 有兩種方法：<br />
</p>
<ul class="org-ul">
<li>Sequential class: 適用於線性堆叠的模型<br /></li>
<li>functional API: 適用任何有向無環的神經網路架構<br /></li>
</ul>

<p>
以下為建立 sequential model 的例子：<br />
</p>
</div>
<div id="outline-container-org34a639b" class="outline-4">
<h4 id="org34a639b"><span class="section-number-4">6.3.1.</span> 建立模型</h4>
<div class="outline-text-4" id="text-6-3-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">2: </span><span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">3: </span>
<span class="linenr">4: </span><span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">5: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26032;&#22686;&#19968;&#20491;&#36664;&#20837;&#28858;874&#32173;&#12289;&#36664;&#20986;&#28858;32&#32173;&#30340;Dense layer</span>
<span class="linenr">6: </span>model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">32</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">784</span>,)))
<span class="linenr">7: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25509;&#25976;&#20358;&#33258;&#19978;&#23652;32&#32173;&#30340;&#36664;&#20837;&#65292;&#36664;&#20986;&#19968;&#20491;10&#32173;&#30340;&#36039;&#26009;</span>
<span class="linenr">8: </span>model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">10</span>, activation=<span style="color: #98be65;">'softmax'</span>))
</pre>
</div>

<p>
若使用 API 來定義相同的模型，其語法如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">2: </span><span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">3: </span>
<span class="linenr">4: </span><span style="color: #dcaeea;">input_tensor</span> = layers.Input(shape=(<span style="color: #da8548; font-weight: bold;">784</span>,))
<span class="linenr">5: </span><span style="color: #dcaeea;">x</span> = layers.Dense(<span style="color: #da8548; font-weight: bold;">32</span>, activation=<span style="color: #98be65;">'relu'</span>)(input_tensor)
<span class="linenr">6: </span><span style="color: #dcaeea;">output_tensor</span> = layers.Dense(<span style="color: #da8548; font-weight: bold;">10</span>, activation=<span style="color: #98be65;">'softmax'</span>)(x)
<span class="linenr">7: </span><span style="color: #dcaeea;">model</span> = models.Model(inputs=input_tensor, outputs=output_tensor)
</pre>
</div>
</div>
</div>
<div id="outline-container-orgc8d4a56" class="outline-4">
<h4 id="orgc8d4a56"><span class="section-number-4">6.3.2.</span> 編譯模型</h4>
<div class="outline-text-4" id="text-6-3-2">
<p>
一旦建立好模型架構，則無論是使用 Sequential 或 API，其餘步驟均相同。神經網路是在編譯(model.compile)時建立的，我們可以在其中指定使用的 optimizer 和 loss function，以及訓練期間監看的評量準則(metrics)，典型的範例如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr">2: </span>model.<span style="color: #c678dd;">compile</span>(optimizer=optimizers.RMSprop(learning_rate=<span style="color: #da8548; font-weight: bold;">0.001</span>), loss=<span style="color: #98be65;">'mse'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
</pre>
</div>
<p>
MacOS系統(M1/M2 CPU)下的加速運算方式(): <a href="https://stackoverflow.com/questions/77222463/is-there-a-way-to-change-adam-to-legacy-when-using-mac-m1-m2-in-tensorflow">stackoverflow</a><br />
</p>
</div>
</div>
<div id="outline-container-orgd4ad76d" class="outline-4">
<h4 id="orgd4ad76d"><span class="section-number-4">6.3.3.</span> 訓練模型</h4>
<div class="outline-text-4" id="text-6-3-3">
<p>
最後，整個學習程序經由 fit()將輸入資料以 Numpy 陣列的形式傳給模型：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>model.fit(input_tensor, target_tensor, batch_size=<span style="color: #da8548; font-weight: bold;">128</span>, ephchs=<span style="color: #da8548; font-weight: bold;">10</span>)
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orgb5df3cf" class="outline-3">
<h3 id="orgb5df3cf"><span class="section-number-3">6.4.</span> 以 Keras 實作 MNist 手寫數字辨識資料集</h3>
<div class="outline-text-3" id="text-6-4">
</div>
<div id="outline-container-org9d40dbf" class="outline-4">
<h4 id="org9d40dbf"><span class="section-number-4">6.4.1.</span> 讀入資料與預處理</h4>
<div class="outline-text-4" id="text-6-4-1">
<p>
MNist 手寫數字辨識資料集是由 Yann LeCun 所蒐集，他也是 CNN 的創始人。MNist 資料集共有訓練資料集 60000 筆、測試資料集 10000 筆，每筆資料都由一 28*28 的 image 以及相對應的 label 組成。<br />
</p>
</div>
<div id="outline-container-orga1b5426" class="outline-5">
<h5 id="orga1b5426">完整程式碼概覽</h5>
<div class="outline-text-5" id="text-orga1b5426">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;">### </span><span style="color: #5B6268;">1. &#19979;&#36617;MNist&#36039;&#26009; ###</span>
<span class="linenr"> 2: </span><span style="color: #5B6268;">### </span><span style="color: #5B6268;">1.1 &#28377;&#20837;Keras&#21450;&#30456;&#38364;&#25152;&#38656;&#36039;&#28304;</span>
<span class="linenr"> 3: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25903;&#25588;&#32173;&#24230;&#38499;&#21015;&#20043;&#30697;&#38499;&#36939;&#31639;</span>
<span class="linenr"> 4: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pf
<span class="linenr"> 5: </span><span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> to_categorical <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35201;&#23559;table&#36681;&#28858;one-hot encoding</span>
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>np.random.seed(<span style="color: #da8548; font-weight: bold;">10</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span><span style="color: #5B6268;">### </span><span style="color: #5B6268;">1.2 &#21295;&#20837;Keras&#27169;&#32068;&#20197;&#19979;&#36617;MNist&#36039;&#26009;&#38598;###</span>
<span class="linenr">10: </span><span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr">11: </span><span style="color: #5B6268;">### </span><span style="color: #5B6268;">1.3 &#35712;&#21462;MNist&#36039;&#26009;&#38598;###</span>
<span class="linenr">12: </span>(x_train_image, y_train_label), (<span style="color: #dcaeea;">x_test_image</span>, <span style="color: #dcaeea;">y_test_label</span>) = mnist.load_data()
<span class="linenr">13: </span>
<span class="linenr">14: </span><span style="color: #5B6268;">### </span><span style="color: #5B6268;">1.4 &#26597;&#30475;MNist&#36039;&#26009;&#38598;&#31558;&#25976;###</span>
<span class="linenr">15: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'4. &#26597;&#30475;MNist&#36039;&#26009;&#38598;&#31558;&#25976;'</span>)
<span class="linenr">16: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'train data='</span>, <span style="color: #c678dd;">len</span>(x_train_image))
<span class="linenr">17: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">' test data='</span>, <span style="color: #c678dd;">len</span>(x_test_image))
<span class="linenr">18: </span>
<span class="linenr">19: </span><span style="color: #5B6268;">### </span><span style="color: #5B6268;">2.  &#26597;&#30475;&#35347;&#32244;&#36039;&#26009;###</span>
<span class="linenr">20: </span><span style="color: #5B6268;">### </span><span style="color: #5B6268;">2.1 &#36664;&#20986;&#35347;&#32244;&#36039;&#26009;&#26684;&#24335;###</span>
<span class="linenr">21: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'2.1 &#26597;&#30475;&#35347;&#32244;&#36039;&#26009;&#26684;&#24335;'</span>)
<span class="linenr">22: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'train image='</span>, x_train_image.shape)
<span class="linenr">23: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">' test image='</span>, y_train_label.shape)
<span class="linenr">24: </span>
<span class="linenr">25: </span><span style="color: #5B6268;">### </span><span style="color: #5B6268;">2.2 &#23450;&#32681;plot_image&#20989;&#25976;&#39023;&#31034;&#25976;&#23383;&#24433;&#20687;###</span>
<span class="linenr">26: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">27: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_image</span>(imgname, image):
<span class="linenr">28: </span>    <span style="color: #dcaeea;">fig</span> = plt.gcf() <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#23450;&#35373;&#22294;&#24418;&#22823;&#23567;</span>
<span class="linenr">29: </span>    fig.set_size_inches(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">30: </span>    plt.imshow(image, cmap=<span style="color: #98be65;">'binary'</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">cmap&#21443;&#25976;&#35373;&#23450;&#28858;binary&#20197;&#40657;&#30333;&#28784;&#38542;&#39023;&#31034;</span>
<span class="linenr">31: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plt.show() #for jupyter or colab</span>
<span class="linenr">32: </span>    plt.plot()
<span class="linenr">33: </span>    plt.savefig(imgname+<span style="color: #98be65;">".png"</span>)
<span class="linenr">34: </span>
<span class="linenr">35: </span><span style="color: #5B6268;">### </span><span style="color: #5B6268;">2.3 &#22519;&#34892;plot_image&#20989;&#25976;&#26597;&#30475;&#31532;0&#31558;&#25976;&#23383;&#24433;&#20687;&#21450;&#23565;&#25033;label###</span>
<span class="linenr">36: </span>plot_image(<span style="color: #98be65;">"Keras-mnist-0"</span>, x_train_image[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">37: </span><span style="color: #c678dd;">print</span>(y_train_label[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">38: </span>
<span class="linenr">39: </span><span style="color: #5B6268;">### </span><span style="color: #5B6268;">3. &#26597;&#30475;&#22810;&#31558;&#36039;&#26009;###</span>
<span class="linenr">40: </span><span style="color: #5B6268;">### </span><span style="color: #5B6268;">3.1 &#24314;&#31435;plot_images_labels_prediction()&#20989;&#24335;###</span>
<span class="linenr">41: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_images_labels_prediction</span>(imgname, images, labels, prediction, idx, num = <span style="color: #da8548; font-weight: bold;">10</span>):
<span class="linenr">42: </span>    <span style="color: #dcaeea;">fig</span> = plt.gcf()
<span class="linenr">43: </span>    fig.set_size_inches(<span style="color: #da8548; font-weight: bold;">8</span>,<span style="color: #da8548; font-weight: bold;">4</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35373;&#23450;&#39023;&#31034;&#22294;&#24418;&#22823;&#23567;</span>
<span class="linenr">44: </span>    <span style="color: #51afef;">if</span> num&gt;<span style="color: #da8548; font-weight: bold;">25</span>: num=<span style="color: #da8548; font-weight: bold;">25</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#36039;&#26009;&#31558;&#25976;&#19978;&#38480;</span>
<span class="linenr">45: </span>    <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">0</span>, num):
<span class="linenr">46: </span>        <span style="color: #dcaeea;">ax</span>=plt.subplot(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">1</span>+i) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27492;&#34389;&#39023;&#31034;10&#31558;&#22294;&#24418;&#65292;2*5&#20491;</span>
<span class="linenr">47: </span>        ax.imshow(images[idx], cmap=<span style="color: #98be65;">'binary'</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#22294;</span>
<span class="linenr">48: </span>        <span style="color: #dcaeea;">title</span>= <span style="color: #98be65;">"label="</span> +<span style="color: #c678dd;">str</span>(labels[idx]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21152;&#20837;&#23376;&#22294;&#24418;title</span>
<span class="linenr">49: </span>        <span style="color: #51afef;">if</span> <span style="color: #c678dd;">len</span>(prediction)&gt;<span style="color: #da8548; font-weight: bold;">0</span>:
<span class="linenr">50: </span>            <span style="color: #dcaeea;">title</span>+=<span style="color: #98be65;">",predict="</span>+<span style="color: #c678dd;">str</span>(prediction[idx]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27161;&#38988;title&#21152;&#20837;&#38928;&#28204;&#32080;&#26524;</span>
<span class="linenr">51: </span>        ax.set_title(title,fontsize=<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">52: </span>        ax.set_xticks([]);ax.set_yticks([]) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#19981;&#39023;&#31034;&#21051;&#24230;</span>
<span class="linenr">53: </span>        <span style="color: #dcaeea;">idx</span>+=<span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">54: </span>    <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">55: </span>    <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.plot()</span>
<span class="linenr">56: </span>    plt.savefig(imgname+<span style="color: #98be65;">".png"</span>)
<span class="linenr">57: </span>
<span class="linenr">58: </span><span style="color: #5B6268;">### </span><span style="color: #5B6268;">3.2 &#22519;&#34892;plot_images_labels_prediction&#20989;&#25976;&#26597;&#30475;&#22810;&#31558;images&#21450;labels</span>
<span class="linenr">59: </span>plot_images_labels_prediction(<span style="color: #98be65;">"Keras-mnist-1"</span>,x_train_image,y_train_label,[],<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">60: </span>
<span class="linenr">61: </span><span style="color: #5B6268;">### </span><span style="color: #5B6268;">4. &#22810;&#23652;&#24863;&#30693;&#22120;(Multilayer perception, MLP)&#27169;&#22411;&#36039;&#26009;&#38928;&#34389;&#29702;###</span>
<span class="linenr">62: </span><span style="color: #5B6268;">### </span><span style="color: #5B6268;">4.1 &#20197;reshape&#36681;&#25563;image&#30697;&#38499;</span>
<span class="linenr">63: </span><span style="color: #dcaeea;">x_Train</span> = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">64: </span><span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">65: </span><span style="color: #c678dd;">print</span>(x_Train.shape)
<span class="linenr">66: </span><span style="color: #c678dd;">print</span>(x_Test.shape)
<span class="linenr">67: </span>
<span class="linenr">68: </span><span style="color: #5B6268;">### </span><span style="color: #5B6268;">4.2 &#23559;&#24433;&#20687;&#20043;&#25976;&#23383;&#30697;&#38499;&#27491;&#35215;&#21270;###</span>
<span class="linenr">69: </span><span style="color: #dcaeea;">x_Train_normalize</span> = x_Train/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">70: </span><span style="color: #dcaeea;">x_Test_normalize</span> = x_Test/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">71: </span>
<span class="linenr">72: </span><span style="color: #5B6268;">### </span><span style="color: #5B6268;">4.3 &#21407;&#22987;label&#27396;&#20301;###</span>
<span class="linenr">73: </span><span style="color: #c678dd;">print</span>(y_train_label[:<span style="color: #da8548; font-weight: bold;">5</span>]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36664;&#20986;&#21069;5&#31558;</span>
<span class="linenr">74: </span>
<span class="linenr">75: </span><span style="color: #5B6268;">### </span><span style="color: #5B6268;">4.4 &#36914;&#34892;One-hot encoding###</span>
<span class="linenr">76: </span><span style="color: #dcaeea;">y_TrainOneHot</span> = to_categorical(y_train_label)
<span class="linenr">77: </span><span style="color: #dcaeea;">y_TestOneHot</span> = to_categorical(y_test_label)
<span class="linenr">78: </span><span style="color: #5B6268;">### </span><span style="color: #5B6268;">4.5 &#36681;&#25563;&#24460;&#20043;label&#27396;&#20301;###</span>
<span class="linenr">79: </span><span style="color: #c678dd;">print</span>(y_TrainOneHot[:<span style="color: #da8548; font-weight: bold;">5</span>]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36664;&#20986;&#21069;5&#31558;</span>
</pre>
</div>

<pre class="example" id="org12bb9e4">
4. 查看MNist資料集筆數
train data= 60000
 test data= 10000
2.1 查看訓練資料格式
train image= (60000, 28, 28)
 test image= (60000,)
5
(60000, 784)
(10000, 784)
[5 0 4 1 9]
[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]
</pre>

<div id="orgcf6d534" class="figure">
<p><img src="images/Keras-mnist-0.png" alt="Keras-mnist-0.png" width="100" /><br />
</p>
<p><span class="figure-number">Figure 29: </span>MNist 第一筆資料影像</p>
</div>


<div id="orge564413" class="figure">
<p><img src="images/Keras-mnist-1.png" alt="Keras-mnist-1.png" width="300" /><br />
</p>
<p><span class="figure-number">Figure 30: </span>MNist 前十筆資料影像</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org55c387a" class="outline-4">
<h4 id="org55c387a"><span class="section-number-4">6.4.2.</span> Keras MLP 辨識 MNist</h4>
<div class="outline-text-4" id="text-6-4-2">
</div>
<div id="outline-container-org863ecf8" class="outline-5">
<h5 id="org863ecf8">多層感知器模型</h5>
<div class="outline-text-5" id="text-org863ecf8">
<p>
MNist 的初始模型分為輸入、隠藏及輸出三層, 輸入層有 784 個輸入神經元(\(x_1,x_2,...,x_{784}\))，接收被 reshape 為一維矩陣的手寫圖片(28*28)；隠藏層內部有 256 個神經元，隱藏層的層數與每層的神經元各數在神經網路的建構中主要取決於設計者；輸出層共有 10 個神經元，代表預測的結果(0~9)。<br />
</p>

<div id="org5422706" class="figure">
<p><img src="images/MLP-2.jpg" alt="MLP-2.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 31: </span>MNist MLP 模型</p>
</div>
</div>
</div>
<div id="outline-container-org4d3d244" class="outline-5">
<h5 id="org4d3d244">多層感知器的訓練與預測</h5>
<div class="outline-text-5" id="text-org4d3d244">
<p>
多層感知器模型建立後，必須先訓練模型才能夠進行預測(辨識)手寫數字。而這裡所謂的訓練模型，對於神經網路而言，就是學習。<br />
</p>
</div>
<ul class="org-ul">
<li><a id="orgba212c5"></a>訓練(Traning)<br />
<div class="outline-text-6" id="text-orgba212c5">
<p>
MNist 的資料訓練集共 60000 筆，經資料預處理後會產生 features(數字特徵集)與 label(數字的真實值)，然後將這些資料輸入 MLP 模型進行訓練，訓練完成後的模型才能進行預測。<br />
</p>
</div>
</li>
<li><a id="orgffc77a7"></a>預測(Predict)<br />
<div class="outline-text-6" id="text-orgffc77a7">
<p>
將測試資料集匯入訓練完成的 MLP 模型，最後產生預測結果(此例中為 0~9 的數字)。<br />
</p>
</div>
</li>
</ul>
</div>
</div>
<div id="outline-container-orgef484ac" class="outline-4">
<h4 id="orgef484ac"><span class="section-number-4">6.4.3.</span> MLP 模型旳建立步驟</h4>
<div class="outline-text-4" id="text-6-4-3">
<p>
接下來我們逐步來說明上述程式碼的各項步驟<br />
</p>
</div>
<div id="outline-container-org46ee6c3" class="outline-5">
<h5 id="org46ee6c3">進行資料預處理(preprocess)</h5>
<div class="outline-text-5" id="text-org46ee6c3">
</div>
<ul class="org-ul">
<li><a id="orgfd77b0b"></a>匯入所需模組<br />
<div class="outline-text-6" id="text-orgfd77b0b">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> to_categorical
<span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25903;&#25588;&#32173;&#24230;&#38499;&#21015;&#20043;&#30697;&#38499;&#36939;&#31639;</span>
np.random.seed(<span style="color: #da8548; font-weight: bold;">10</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
</pre>
</div>
</div>
</li>
<li><a id="orgea8c8bf"></a>讀取 mnist 資料<br />
<div class="outline-text-6" id="text-orgea8c8bf">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
(x_train_image, y_train_label), (<span style="color: #dcaeea;">x_test_image</span>, <span style="color: #dcaeea;">y_test_label</span>) = mnist.load_data()
</pre>
</div>
</div>
</li>
<li><a id="org5dd83d2"></a>利用 reshape 轉換影像特徵值(features)<br />
<div class="outline-text-6" id="text-org5dd83d2">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #dcaeea;">x_Train</span> = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
np.set_printoptions(precision=<span style="color: #da8548; font-weight: bold;">2</span>, linewidth=np.inf)
<span style="color: #c678dd;">print</span>(x_Train[<span style="color: #da8548; font-weight: bold;">0</span>].reshape(<span style="color: #da8548; font-weight: bold;">28</span>, <span style="color: #da8548; font-weight: bold;">28</span>))
</pre>
</div>

<pre class="example" id="orgfa3b275">
[[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   3.  18.  18.  18. 126. 136. 175.  26. 166. 255. 247. 127.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.   0.  30.  36.  94. 154. 170. 253. 253. 253. 253. 253. 225. 172. 253. 242. 195.  64.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.  49. 238. 253. 253. 253. 253. 253. 253. 253. 253. 251.  93.  82.  82.  56.  39.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.  18. 219. 253. 253. 253. 253. 253. 198. 182. 247. 241.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.   0.  80. 156. 107. 253. 253. 205.  11.   0.  43. 154.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.   0.   0.  14.   1. 154. 253.  90.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 139. 253. 190.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  11. 190. 253.  70.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  35. 241. 225. 160. 108.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  81. 240. 253. 253. 119.  25.   0.   0.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  45. 186. 253. 253. 150.  27.   0.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  16.  93. 252. 253. 187.   0.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 249. 253. 249.  64.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  46. 130. 183. 253. 253. 207.   2.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  39. 148. 229. 253. 253. 253. 250. 182.   0.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  24. 114. 221. 253. 253. 253. 253. 201.  78.   0.   0.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.   0.  23.  66. 213. 253. 253. 253. 253. 198.  81.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.  18. 171. 219. 253. 253. 253. 253. 195.  80.   9.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.  55. 172. 226. 253. 253. 253. 253. 244. 133.  11.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0. 136. 253. 253. 253. 212. 135. 132.  16.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]
</pre>
</div>
</li>
<li><a id="org12b739b"></a>將 feature 標準化<br />
<div class="outline-text-6" id="text-org12b739b">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #dcaeea;">x_Train_normalize</span> = x_Train/ <span style="color: #da8548; font-weight: bold;">255</span>
<span style="color: #dcaeea;">x_Test_normalize</span> = x_Test/ <span style="color: #da8548; font-weight: bold;">255</span>
np.set_printoptions(precision=<span style="color: #da8548; font-weight: bold;">2</span>, linewidth=np.inf)
<span style="color: #c678dd;">print</span>(x_Train_normalize[<span style="color: #da8548; font-weight: bold;">0</span>].reshape(<span style="color: #da8548; font-weight: bold;">28</span>, <span style="color: #da8548; font-weight: bold;">28</span>))
</pre>
</div>

<pre class="example" id="orgf371c06">
[[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.07 0.07 0.07 0.49 0.53 0.69 0.1  0.65 1.   0.97 0.5  0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.12 0.14 0.37 0.6  0.67 0.99 0.99 0.99 0.99 0.99 0.88 0.67 0.99 0.95 0.76 0.25 0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.19 0.93 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.98 0.36 0.32 0.32 0.22 0.15 0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.07 0.86 0.99 0.99 0.99 0.99 0.99 0.78 0.71 0.97 0.95 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.31 0.61 0.42 0.99 0.99 0.8  0.04 0.   0.17 0.6  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.05 0.   0.6  0.99 0.35 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.55 0.99 0.75 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.04 0.75 0.99 0.27 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.14 0.95 0.88 0.63 0.42 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.32 0.94 0.99 0.99 0.47 0.1  0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.18 0.73 0.99 0.99 0.59 0.11 0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.06 0.36 0.99 0.99 0.73 0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.98 0.99 0.98 0.25 0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.18 0.51 0.72 0.99 0.99 0.81 0.01 0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.15 0.58 0.9  0.99 0.99 0.99 0.98 0.71 0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.09 0.45 0.87 0.99 0.99 0.99 0.99 0.79 0.31 0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.09 0.26 0.84 0.99 0.99 0.99 0.99 0.78 0.32 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.07 0.67 0.86 0.99 0.99 0.99 0.99 0.76 0.31 0.04 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.22 0.67 0.89 0.99 0.99 0.99 0.99 0.96 0.52 0.04 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.53 0.99 0.99 0.99 0.83 0.53 0.52 0.06 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]]
</pre>
</div>
</li>
<li><a id="org3e4c51e"></a>以 one-hot encoding 轉換數字真實值(label)<br />
<div class="outline-text-6" id="text-org3e4c51e">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #c678dd;">print</span>(y_train_label[<span style="color: #da8548; font-weight: bold;">0</span>])
<span style="color: #dcaeea;">y_TrainOneHot</span> = to_categorical(y_train_label)
<span style="color: #dcaeea;">y_TestOneHot</span> = to_categorical(y_test_label)
<span style="color: #c678dd;">print</span>(y_TrainOneHot[<span style="color: #da8548; font-weight: bold;">0</span>])
</pre>
</div>

<pre class="example">
5
[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
</pre>
</div>
</li>
</ul>
</div>
<div id="outline-container-org949ab81" class="outline-5">
<h5 id="org949ab81">建立模型</h5>
<div class="outline-text-5" id="text-org949ab81">
</div>
<ul class="org-ul">
<li><a id="orgc942487"></a>匯入所需模組<br />
<div class="outline-text-6" id="text-orgc942487">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense
</pre>
</div>

<p>
在 Keras 在 Keras 中有兩類主要的模型：Sequential 順序模型 和 使用函數式 API 的 Model 類模型。<br />
</p>
</div>
</li>
<li><a id="orgf67e7e9"></a>建立 Sequential 模型<br />
<div class="outline-text-6" id="text-orgf67e7e9">
<p>
建立一個線性堆叠模型，後續再使用 model.add()方法將各神經網路層加入模型中即可。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #dcaeea;">model</span> = Sequential()
</pre>
</div>
</div>
</li>
<li><a id="orgdf3047e"></a>建立「輸入層」與「隠藏層」<br />
<div class="outline-text-6" id="text-orgdf3047e">
<p>
Dense 神經網路層的特色：所有的上一層與下一層的神經元都完全連接。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#24120;&#24907;&#20998;&#20296;&#30340;&#20098;&#25976;&#21021;&#22987;&#21270;weight&#21644;bias</span>
model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">256</span>, input_dim=<span style="color: #da8548; font-weight: bold;">784</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'relu'</span>))
</pre>
</div>
</div>
</li>
<li><a id="org4ac817f"></a>建立「輸出層」<br />
<div class="outline-text-6" id="text-org4ac817f">
<p>
10 個神經元分別對應 0~9 的答案，softmax 可以將神經元的輸出結果轉換為預測每一個數字的機率。建立這裡的 Dense 網路層時無需設定 input_data，因為 Keras 會自動依照上一層的 units 神經元個數(256)來設定這一層的 input_dim 神經元個數。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#24120;&#24907;&#20998;&#20296;&#30340;&#20098;&#25976;&#21021;&#22987;&#21270;weight&#21644;bias</span>
model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">10</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'softmax'</span>))
</pre>
</div>
</div>
</li>
<li><a id="org0603366"></a>查看模型摘要<br />
<div class="outline-text-6" id="text-org0603366">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #c678dd;">print</span>(model.summary())
</pre>
</div>

<pre class="example" id="org219ab80">
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 dense (Dense)               (None, 256)               200960

 dense_1 (Dense)             (None, 10)                2570

=================================================================
Total params: 203530 (795.04 KB)
Trainable params: 203530 (795.04 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
</pre>
<p>
以上每一層 Param 稱為超參數(Hyper-Parameters)，計算方式為：Param=(上一層神經元數量)\(\times\)(本層的神經元數量)\(+\)(本層的神經元數量)。其中：<br />
</p>
<ul class="org-ul">
<li>隠藏層的 Param 為 200960，即 784(輸入層神經元數量)\(\times\)256(隠藏層神經元數量)+256(隠藏層神經元數量)=200960<br /></li>
<li>輸出層的 Param 為 2570，即 256(隠藏層神經元數量)\(\times\)10(輸出層神經元數量)+10(輸出層神經元數量)=2570<br /></li>
</ul>
</div>
</li>
</ul>
</div>
<div id="outline-container-org3916947" class="outline-5">
<h5 id="org3916947">進行訓練</h5>
<div class="outline-text-5" id="text-org3916947">
<p>
模型建立後，即可利用 Back Propagation 來進行訓練，其步驟如下：<br />
</p>
</div>
<ul class="org-ul">
<li><a id="org55deeb7"></a>定義訓練方式<sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup><br />
<div class="outline-text-6" id="text-org55deeb7">
<p>
在訓練模型前，我們必須使用 compile 方式，設定訓練模式<br />
</p>
<div class="org-src-container">
<pre class="src src-python">model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>, optimizer=<span style="color: #98be65;">'adam'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
</pre>
</div>

<p>
model.compile()接收三個參數：<br />
</p>
<ul class="org-ul">
<li>優化器 optimizer。它可以是現有優化器的字符串標識符，如 rmsprop 或 adagrad，也可以是 Optimizer 類的實例。詳見：optimizers。<br /></li>
<li>損失函數 loss，模型試圖最小化的目標函數。它可以是現有損失函數的字符串標識符，如 categorical_crossentropy 或 mse，也可以是一個目標函數。詳見：losses。<br /></li>
<li>評估標準 metrics。對於任何分類問題，你都希望將其設置為 metrics = [&rsquo;accuracy&rsquo;]。評估標準可以是現有的標準的字符串標識符，也可以是自定義的評估標準函數。<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22810;&#20998;&#39006;&#21839;&#38988;</span>
model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>,
              loss=<span style="color: #98be65;">'categorical_crossentropy'</span>,
              metrics=[<span style="color: #98be65;">'accuracy'</span>])

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20108;&#20998;&#39006;&#21839;&#38988;</span>
model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>,
              loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
              metrics=[<span style="color: #98be65;">'accuracy'</span>])

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22343;&#26041;&#35492;&#24046;&#22238;&#27512;&#21839;&#38988;</span>
model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>,
              loss=<span style="color: #98be65;">'mse'</span>)

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#33258;&#23450;&#32681;&#35413;&#20272;&#27161;&#28310;&#20989;&#25976;</span>
<span style="color: #51afef;">import</span> keras.backend <span style="color: #51afef;">as</span> K

<span style="color: #51afef;">def</span> <span style="color: #c678dd;">mean_pred</span>(y_true, y_pred):
    <span style="color: #51afef;">return</span> K.mean(y_pred)

model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>,
              loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
              metrics=[<span style="color: #98be65;">'accuracy'</span>, mean_pred])
</pre>
</div>
</div>
</li>
<li><a id="org16feda5"></a>開始訓練<br />
<div class="outline-text-6" id="text-org16feda5">
<p>
x,y 分別為輸入之訓練參數資料，split=0.2 表示該批資料的 80%作為訓練用、20%作為驗證用，共執行 10 次訓練週期、verbose=2 則表示要顯示訓練過程。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #dcaeea;">train_history</span> = model.fit(x=x_Train_normalize, y=y_TrainOneHot,
                            validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>, epochs=<span style="color: #da8548; font-weight: bold;">10</span>, verbose=<span style="color: #da8548; font-weight: bold;">2</span>)
</pre>
</div>

<pre class="example" id="orgce86e85">
Epoch 1/10
1500/1500 - 2s - loss: 0.0562 - accuracy: 0.9176 - mean_pred: 0.1000 - val_loss: 0.0293 - val_accuracy: 0.9560 - val_mean_pred: 0.1000 - 2s/epoch - 1ms/step
Epoch 2/10
1500/1500 - 2s - loss: 0.0251 - accuracy: 0.9640 - mean_pred: 0.1000 - val_loss: 0.0233 - val_accuracy: 0.9677 - val_mean_pred: 0.1000 - 2s/epoch - 1ms/step
Epoch 3/10
1500/1500 - 2s - loss: 0.0187 - accuracy: 0.9756 - mean_pred: 0.1000 - val_loss: 0.0195 - val_accuracy: 0.9732 - val_mean_pred: 0.1000 - 2s/epoch - 1ms/step
Epoch 4/10
1500/1500 - 2s - loss: 0.0152 - accuracy: 0.9809 - mean_pred: 0.1000 - val_loss: 0.0200 - val_accuracy: 0.9753 - val_mean_pred: 0.1000 - 2s/epoch - 1ms/step
Epoch 5/10
1500/1500 - 2s - loss: 0.0129 - accuracy: 0.9848 - mean_pred: 0.1000 - val_loss: 0.0198 - val_accuracy: 0.9758 - val_mean_pred: 0.1000 - 2s/epoch - 1ms/step
Epoch 6/10
1500/1500 - 2s - loss: 0.0110 - accuracy: 0.9874 - mean_pred: 0.1000 - val_loss: 0.0199 - val_accuracy: 0.9775 - val_mean_pred: 0.1000 - 2s/epoch - 1ms/step
Epoch 7/10
1500/1500 - 2s - loss: 0.0098 - accuracy: 0.9894 - mean_pred: 0.1000 - val_loss: 0.0215 - val_accuracy: 0.9775 - val_mean_pred: 0.1000 - 2s/epoch - 1ms/step
Epoch 8/10
1500/1500 - 2s - loss: 0.0086 - accuracy: 0.9905 - mean_pred: 0.1000 - val_loss: 0.0216 - val_accuracy: 0.9788 - val_mean_pred: 0.1000 - 2s/epoch - 1ms/step
Epoch 9/10
1500/1500 - 2s - loss: 0.0076 - accuracy: 0.9921 - mean_pred: 0.1000 - val_loss: 0.0223 - val_accuracy: 0.9789 - val_mean_pred: 0.1000 - 2s/epoch - 1ms/step
Epoch 10/10
1500/1500 - 2s - loss: 0.0068 - accuracy: 0.9926 - mean_pred: 0.1000 - val_loss: 0.0231 - val_accuracy: 0.9793 - val_mean_pred: 0.1000 - 2s/epoch - 1ms/step
</pre>

<p>
fit 完整語法如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python">fit(<span style="color: #51afef;">self</span>, x=<span style="color: #a9a1e1;">None</span>, y=<span style="color: #a9a1e1;">None</span>, batch_size=<span style="color: #a9a1e1;">None</span>,
      epochs=<span style="color: #da8548; font-weight: bold;">1</span>, verbose=<span style="color: #da8548; font-weight: bold;">1</span>, callbacks=<span style="color: #a9a1e1;">None</span>,
      validation_split=<span style="color: #da8548; font-weight: bold;">0.0</span>, validation_data=<span style="color: #a9a1e1;">None</span>,
      shuffle=<span style="color: #a9a1e1;">True</span>, class_weight=<span style="color: #a9a1e1;">None</span>, sample_weight=<span style="color: #a9a1e1;">None</span>,
      initial_epoch=<span style="color: #da8548; font-weight: bold;">0</span>, steps_per_epoch=<span style="color: #a9a1e1;">None</span>, validation_steps=<span style="color: #a9a1e1;">None</span>)
</pre>
</div>
<p>
對應參數分別為：<sup><a id="fnr.4" class="footref" href="#fn.4" role="doc-backlink">4</a></sup><br />
</p>
<ul class="org-ul">
<li>x：輸入數據。如果模型只有一個輸入，那麼 x 的類型是 numpy array，如果模型有多個輸入，那麼 x 的類型應當為 list，list 的元素是對應於各個輸入的 numpy array。如果模型的每個輸入都有名字，則可以傳入一個字典，將輸入名與其輸入數據對應起來。<br /></li>
<li>y：標籤，numpy array。如果模型有多個輸出，可以傳入一個 numpy array 的 list。如果模型的輸出擁有名字，則可以傳入一個字典，將輸出名與其標籤對應起來。<br /></li>
<li>batch_size：整數，指定進行梯度下降時每個 batch 包含的樣本數。訓練時一個 batch 的樣本會被計算一次梯度下降，使目標函數優化一步。<br /></li>
<li>epochs：整數，訓練終止時的 epoch 值，訓練將在達到該 epoch 值時停止，當沒有設置 initial_epoch 時，它就是訓練的總輪數，否則訓練的總輪數為 epochs - inital_epoch<br /></li>
<li>verbose：日誌顯示，0為不在標準輸出流輸出日誌信息，1為輸出進度條記錄，2為每個 epoch 輸出一行記錄<br /></li>
<li>callbacks：list，其中的元素是 keras.callbacks.Callback 的對象。這個 list 中的回調函數將會在訓練過程中的適當時機被調用，參考回調函數<br /></li>
<li>validation_split：0~1 之間的浮點數，用來指定訓練集的一定比例數據作為驗證集。驗證集將不參與訓練，並在每個 epoch 結束後測試的模型的指標，如損失函數、精確度等。注意，validation_split 的劃分在 shuffle 之後，因此如果你的數據本身是有序的，需要先手工打亂再指定 validation_split，否則可能會出現驗證集樣本不均勻。<br /></li>
<li>validation_data：形式為（X，y）或（X，y，sample_weights）的 tuple，是指定的驗證集。此參數將覆蓋 validation_spilt。<br /></li>
<li>shuffle：布爾值，表示是否在訓練過程中每個 epoch 前隨機打亂輸入樣本的順序。<br /></li>
<li>class_weight：字典，將不同的類別映射為不同的權值，該參數用來在訓練過程中調整損失函數（只能用於訓練）。該參數在處理非平衡的訓練數據（某些類的訓練樣本數很少）時，可以使得損失函數對樣本數不足的數據更加關注。<br /></li>
<li>sample_weight：權值的 numpy array，用於在訓練時調整損失函數（僅用於訓練）。可以傳遞一個 1D 的與樣本等長的向量用於對樣本進行 1 對 1 的加權，或者在面對時序數據時，傳遞一個的形式為（samples，sequence_length）的矩陣來為每個時間步上的樣本賦不同的權。這種情況下請確定在編譯模型時添加了 sample_weight_mode=&rsquo;temporal&rsquo;。<br /></li>
<li>initial_epoch: 從該參數指定的 epoch 開始訓練，在繼續之前的訓練時有用。<br /></li>
<li>steps_per_epoch: 一個 epoch 包含的步數（每一步是一個 batch 的數據送入），當使用如 TensorFlow 數據 Tensor 之類的輸入張量進行訓練時，預設的 None 代表自動分割，即數據集樣本數/batch 樣本數。<br /></li>
<li>validation_steps: 僅當 steps_per_epoch 被指定時有用，在驗證集上的 step 總數。<br /></li>
</ul>
</div>
</li>
<li><a id="org78eff99"></a>建立、顯示訓練過程：show_train_history<br />
<div class="outline-text-6" id="text-org78eff99">
<p>
上述過程包括 accuracy 及 loss 均儲存於 train_history 變數中，可以下列程式碼將其轉變為圖表：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span style="color: #51afef;">def</span> <span style="color: #c678dd;">show_train_history</span>(train_history, train, validation):
    plt.plot(train_history.history[train])
    plt.plot(train_history.history[validation])
    plt.title(<span style="color: #98be65;">'Train History'</span>)
    plt.ylabel(train)
    plt.xlabel(<span style="color: #98be65;">'Epoch'</span>)
    plt.legend([<span style="color: #98be65;">'train'</span>,<span style="color: #98be65;">'validation'</span>], toc=<span style="color: #98be65;">'upper left'</span>)
    plt.show()
</pre>
</div>
</div>
</li>
<li><a id="orgf30ca50"></a>畫出 accuracy 執行結果<br />
<div class="outline-text-6" id="text-orgf30ca50">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30452;&#25509;&#39023;&#31034;</span>
show_train_history(train_history, <span style="color: #98be65;">'accuracy'</span>, <span style="color: #98be65;">'val_accuracy'</span>)
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21478;&#23384;&#22294;&#27284;</span>
<span style="color: #dcaeea;">img</span> = show_train_history(train_history, <span style="color: #98be65;">'accuracy'</span>, <span style="color: #98be65;">'val_accuracy'</span>)
plt.savefig(<span style="color: #98be65;">"images/Keras-MNist-Train-1.png"</span>)
</pre>
</div>
</div>
</li>
<li><a id="org867d98f"></a>完整執行結果<br />
<div class="outline-text-6" id="text-org867d98f">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">import</span> warnings
<span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> to_categorical <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35201;&#23559;table&#36681;&#28858;one-hot encoding</span>

<span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25903;&#25588;&#32173;&#24230;&#38499;&#21015;&#20043;&#30697;&#38499;&#36939;&#31639;</span>
np.random.seed(<span style="color: #da8548; font-weight: bold;">10</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
<span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
(x_train_image, y_train_label), (<span style="color: #dcaeea;">x_test_image</span>, <span style="color: #dcaeea;">y_test_label</span>) = mnist.load_data()
<span style="color: #dcaeea;">x_Train</span> = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span style="color: #dcaeea;">x_Train_normalize</span> = x_Train/ <span style="color: #da8548; font-weight: bold;">255</span>
<span style="color: #dcaeea;">x_Test_normalize</span> = x_Test/ <span style="color: #da8548; font-weight: bold;">255</span>
<span style="color: #dcaeea;">y_TrainOneHot</span> = to_categorical(y_train_label)
<span style="color: #dcaeea;">y_TestOneHot</span> = to_categorical(y_test_label)
<span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense
<span style="color: #dcaeea;">model</span> = Sequential()
model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">256</span>, input_dim=<span style="color: #da8548; font-weight: bold;">784</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'relu'</span>))
model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">10</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">===&#36914;&#34892;&#35347;&#32244;===</span>
model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>, optimizer=<span style="color: #98be65;">'adam'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span style="color: #dcaeea;">train_history</span> = model.fit(x=x_Train_normalize, y=y_TrainOneHot,
                                validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>, epochs=<span style="color: #da8548; font-weight: bold;">20</span>, verbose=<span style="color: #da8548; font-weight: bold;">2</span>)
<span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span style="color: #51afef;">def</span> <span style="color: #c678dd;">show_train_history</span>(train_history, train, validation):
    plt.plot(train_history.history[train])
    plt.plot(train_history.history[validation])
    plt.title(<span style="color: #98be65;">'Train History'</span>)
    plt.ylabel(train)
    plt.xlabel(<span style="color: #98be65;">'Epoch'</span>)
    plt.legend([<span style="color: #98be65;">'train'</span>,<span style="color: #98be65;">'validation'</span>], loc=<span style="color: #98be65;">'upper left'</span>)
    <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show() # for jupyter notebook</span>
    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#20462;&#25913;for console run</span>
    <span style="color: #dcaeea;">img</span> = plt.plot()
    <span style="color: #51afef;">return</span> img

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;acc</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;val_accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;val_acc</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">show_train_history(train_history, 'accuracy', 'val_accuracy')</span>
<span style="color: #dcaeea;">img</span> = show_train_history(train_history, <span style="color: #98be65;">'accuracy'</span>, <span style="color: #98be65;">'val_accuracy'</span>)
plt.savefig(<span style="color: #98be65;">"Keras-MNist-Train-1.png"</span>)
<span style="color: #dcaeea;">img</span> = show_train_history(train_history, <span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'val_loss'</span>)
plt.savefig(<span style="color: #98be65;">"Keras-MNist-Train-2.png"</span>) <span style="color: #5B6268;">#</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#28204;&#35430;&#36039;&#26009;&#35413;&#20272;&#31934;&#30906;&#29575;</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">save</span>
<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'test before save model: '</span>, model.predict(x_Test[<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #da8548; font-weight: bold;">5</span>]))

<span style="color: #dcaeea;">scores</span> = model.evaluate(x_Test_normalize, y_TestOneHot)
<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'accuracy'</span>,scores)
<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'accuracy'</span>,scores[<span style="color: #da8548; font-weight: bold;">1</span>])

model.save(<span style="color: #98be65;">'Keras_MNist_model.keras'</span>)   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">HDF5 file, you have to pip3 install h5py if don't have it</span>
<span style="color: #51afef;">del</span> model  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">deletes the existing model</span>
</pre>
</div>

<pre class="example" id="org250c8c4">
Epoch 1/20
1500/1500 - 2s - loss: 0.2700 - accuracy: 0.9242 - val_loss: 0.1371 - val_accuracy: 0.9592 - 2s/epoch - 1ms/step
Epoch 2/20
1500/1500 - 2s - loss: 0.1081 - accuracy: 0.9672 - val_loss: 0.1045 - val_accuracy: 0.9694 - 2s/epoch - 1ms/step
Epoch 3/20
1500/1500 - 2s - loss: 0.0703 - accuracy: 0.9790 - val_loss: 0.0842 - val_accuracy: 0.9735 - 2s/epoch - 1ms/step
Epoch 4/20
1500/1500 - 2s - loss: 0.0507 - accuracy: 0.9839 - val_loss: 0.0847 - val_accuracy: 0.9745 - 2s/epoch - 1ms/step
Epoch 5/20
1500/1500 - 1s - loss: 0.0358 - accuracy: 0.9891 - val_loss: 0.0864 - val_accuracy: 0.9746 - 1s/epoch - 999us/step
Epoch 6/20
1500/1500 - 1s - loss: 0.0271 - accuracy: 0.9919 - val_loss: 0.1096 - val_accuracy: 0.9690 - 1s/epoch - 991us/step
Epoch 7/20
1500/1500 - 1s - loss: 0.0201 - accuracy: 0.9938 - val_loss: 0.0771 - val_accuracy: 0.9778 - 1s/epoch - 999us/step
Epoch 8/20
1500/1500 - 1s - loss: 0.0167 - accuracy: 0.9951 - val_loss: 0.0845 - val_accuracy: 0.9778 - 1s/epoch - 998us/step
Epoch 9/20
1500/1500 - 1s - loss: 0.0122 - accuracy: 0.9965 - val_loss: 0.0896 - val_accuracy: 0.9789 - 1s/epoch - 996us/step
Epoch 10/20
1500/1500 - 1s - loss: 0.0107 - accuracy: 0.9967 - val_loss: 0.1125 - val_accuracy: 0.9736 - 1s/epoch - 990us/step
Epoch 11/20
1500/1500 - 1s - loss: 0.0095 - accuracy: 0.9973 - val_loss: 0.0950 - val_accuracy: 0.9787 - 1s/epoch - 991us/step
Epoch 12/20
1500/1500 - 1s - loss: 0.0082 - accuracy: 0.9973 - val_loss: 0.0964 - val_accuracy: 0.9785 - 1s/epoch - 988us/step
Epoch 13/20
1500/1500 - 1s - loss: 0.0065 - accuracy: 0.9981 - val_loss: 0.1082 - val_accuracy: 0.9786 - 1s/epoch - 994us/step
Epoch 14/20
1500/1500 - 1s - loss: 0.0078 - accuracy: 0.9978 - val_loss: 0.1127 - val_accuracy: 0.9766 - 1s/epoch - 990us/step
Epoch 15/20
1500/1500 - 1s - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.1071 - val_accuracy: 0.9797 - 1s/epoch - 998us/step
Epoch 16/20
1500/1500 - 1s - loss: 0.0061 - accuracy: 0.9981 - val_loss: 0.1113 - val_accuracy: 0.9784 - 1s/epoch - 994us/step
Epoch 17/20
1500/1500 - 1s - loss: 0.0067 - accuracy: 0.9978 - val_loss: 0.1168 - val_accuracy: 0.9782 - 1s/epoch - 988us/step
Epoch 18/20
1500/1500 - 1s - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.1174 - val_accuracy: 0.9787 - 1s/epoch - 986us/step
Epoch 19/20
1500/1500 - 1s - loss: 0.0066 - accuracy: 0.9979 - val_loss: 0.1197 - val_accuracy: 0.9785 - 1s/epoch - 990us/step
Epoch 20/20
1500/1500 - 1s - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.1325 - val_accuracy: 0.9768 - 1s/epoch - 993us/step
1/1 [==============================] - 0s 15ms/step
test before save model:  [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
313/313 [==============================] - 0s 447us/step - loss: 0.1032 - accuracy: 0.9802
accuracy [0.10324574261903763, 0.9801999926567078]
accuracy 0.9801999926567078
</pre>


<div id="org2711198" class="figure">
<p><img src="images/Keras-MNist-Train-1.png" alt="Keras-MNist-Train-1.png" width="400" /><br />
</p>
<p><span class="figure-number">Figure 32: </span>Keras Mnist Model 訓練#1: accuracy</p>
</div>

<p>
圖<a href="#org2711198">32</a>為執行 10 次週期後的預測精確度變化，可以看出以下現象：<br />
</p>
<ol class="org-ol">
<li>訓練與驗證的精確率均隨訓練週期增加而提高<br /></li>
<li>訓練精確度較驗證精確度高，原因是用來評估訓練精確率的資料已在訓練階段用過了；而用來評做驗證精確率的資料則否；這就類似，考試時考學過的練習題，學生得分較高。<br /></li>
<li>驗證精確率雖然低，但較符合現實情況，即，考試時考學生沒有做過的題目。<br /></li>
<li>如果訓練精確率持續增高，但驗證精率卻無法提升，可能是出現過度擬合(overfitting)的現象。,<br /></li>
</ol>

<div id="orga02ac58" class="figure">
<p><img src="images/Keras-MNist-Train-2.png" alt="Keras-MNist-Train-2.png" width="400" /><br />
</p>
<p><span class="figure-number">Figure 33: </span>Keras Mnist Model 訓練#1: lost function</p>
</div>

<p>
由圖<a href="#orga02ac58">33</a>亦可看出，訓練誤差與驗證誤差亦隨週期增加而降低，且訓練襄差最終低於驗證誤差。<br />
</p>

<p>
訓練完成後，再以測試資料來評估模型準確率。<br />
</p>
</div>
</li>
</ul>
</div>
<div id="outline-container-org6fddffd" class="outline-5">
<h5 id="org6fddffd">進行預測</h5>
<div class="outline-text-5" id="text-org6fddffd">
<p>
模型在訓練、驗證、測試後，即可以此訓練完之模型進行預測，預測方式如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> load_model

(x_train_image, y_train_label), (<span style="color: #dcaeea;">x_test_image</span>, <span style="color: #dcaeea;">y_test_label</span>) = mnist.load_data()
<span style="color: #dcaeea;">x_Train</span> = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">Define func</span>
<span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_images_labels_prediction</span>(imgname, images, labels, prediction, idx, num = <span style="color: #da8548; font-weight: bold;">10</span>):
      <span style="color: #dcaeea;">fig</span> = plt.gcf()
      fig.set_size_inches(<span style="color: #da8548; font-weight: bold;">8</span>,<span style="color: #da8548; font-weight: bold;">4</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35373;&#23450;&#39023;&#31034;&#22294;&#24418;&#22823;&#23567;</span>
      <span style="color: #51afef;">if</span> num&gt;<span style="color: #da8548; font-weight: bold;">25</span>: num=<span style="color: #da8548; font-weight: bold;">25</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#36039;&#26009;&#31558;&#25976;&#19978;&#38480;</span>
      <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">0</span>, num):
          <span style="color: #dcaeea;">ax</span>=plt.subplot(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">1</span>+i) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27492;&#34389;&#39023;&#31034;10&#31558;&#22294;&#24418;&#65292;2*5&#20491;</span>
          ax.imshow(images[idx], cmap=<span style="color: #98be65;">'binary'</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#22294;</span>
          <span style="color: #dcaeea;">title</span>= <span style="color: #98be65;">"label="</span> +<span style="color: #c678dd;">str</span>(labels[idx]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21152;&#20837;&#23376;&#22294;&#24418;title</span>
          <span style="color: #51afef;">if</span> <span style="color: #c678dd;">len</span>(prediction)&gt;<span style="color: #da8548; font-weight: bold;">0</span>:
              <span style="color: #dcaeea;">title</span>+=<span style="color: #98be65;">"</span><span style="color: #a9a1e1;">\n</span><span style="color: #98be65;">predict="</span>+<span style="color: #c678dd;">str</span>(prediction[idx]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27161;&#38988;title&#21152;&#20837;&#38928;&#28204;&#32080;&#26524;</span>
          ax.set_title(title,fontsize=<span style="color: #da8548; font-weight: bold;">10</span>)
          ax.set_xticks([]);ax.set_yticks([]) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#19981;&#39023;&#31034;&#21051;&#24230;</span>
          <span style="color: #dcaeea;">idx</span>+=<span style="color: #da8548; font-weight: bold;">1</span>
      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
      plt.plot()
      plt.savefig(imgname+<span style="color: #98be65;">".png"</span>)
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36617;&#20837;&#20786;&#23384;&#20043;&#27169;&#22411;</span>
<span style="color: #dcaeea;">model</span> = load_model(<span style="color: #98be65;">'Keras_MNist_model.keras'</span>)
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;340-349&#20849;10&#31558;&#36039;&#26009;</span>
<span style="color: #dcaeea;">predict_x</span>=model.predict(x_Test)
<span style="color: #dcaeea;">prediction</span>=np.argmax(predict_x,axis=<span style="color: #da8548; font-weight: bold;">1</span>)

<span style="color: #5B6268;">#</span><span style="color: #5B6268;">prediction = model.predict_classes(x_Test)</span>
plot_images_labels_prediction(<span style="color: #98be65;">'images/Keras-MNist-Train-3'</span>,x_test_image, y_test_label, prediction, idx=<span style="color: #da8548; font-weight: bold;">340</span>)
</pre>
</div>

<pre class="example">
313/313 [==============================] - 0s 396us/step
</pre>



<div id="orgcce6a57" class="figure">
<p><img src="images/Keras-MNist-Train-3.png" alt="Keras-MNist-Train-3.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 34: </span>Keras-MNist-Train-3</p>
</div>

<pre class="example">
313/313 [==============================] - 0s 397us/step
</pre>


<p>
上述程式碼將訓練好後儲存的模型取出，拿 10 筆記錄去預測，發現第一筆有誤（真實值 label 為 5、預測值為 3）。<br />
</p>
</div>
</div>
<div id="outline-container-org41252e0" class="outline-5">
<h5 id="org41252e0">顯示混淆矩陣(confusion matrix)</h5>
<div class="outline-text-5" id="text-org41252e0">
<p>
若想進一步得知哪些數字容易被混淆，可以使用混淆矩陣(confustion matrix, 也稱為誤差矩陣 error matrix)。實務上可以利用 pandas crosstab 來建立，程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> load_model

(x_train_image, y_train_label), (<span style="color: #dcaeea;">x_test_image</span>, <span style="color: #dcaeea;">y_test_label</span>) = mnist.load_data()

<span style="color: #dcaeea;">x_Train</span> = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)

<span style="color: #dcaeea;">model</span> = load_model(<span style="color: #98be65;">'Keras_MNist_model.keras'</span>)
<span style="color: #dcaeea;">predict_x</span>=model.predict(x_Test)
<span style="color: #dcaeea;">prediction</span>=np.argmax(predict_x,axis=<span style="color: #da8548; font-weight: bold;">1</span>)

<span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span style="color: #dcaeea;">confuse</span> = pd.crosstab(y_test_label, prediction, rownames=[<span style="color: #98be65;">'label'</span>], colnames=[<span style="color: #98be65;">'predict'</span>])
<span style="color: #c678dd;">print</span>(confuse)
</pre>
</div>

<pre class="example" id="org9e40a56">
313/313 [==============================] - 0s 398us/step
predict    0     1     2    3    4    5    6     7    8    9
label
0        971     1     0    0    1    0    3     1    2    1
1          0  1130     2    0    0    0    0     1    2    0
2          4     1  1006    3    2    0    3     7    5    1
3          1     0     6  988    0    6    0     4    3    2
4          2     2     3    0  954    1    3     1    2   14
5          3     1     1    6    1  873    3     0    2    2
6          3     3     1    1    2    4  943     0    1    0
7          1     7     8    1    1    0    0  1004    2    4
8          3     0     3    5    3    3    1     3  952    1
9          3     3     0    2    5    3    0     5    0  988
</pre>

<p>
由輸出結果可以得知，5很容易被誤判為 3(共 29 次)。若想進一步得知到底有那幾張圖為上述狀況，則可以加入限制條件，程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 3: </span><span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> load_model
<span class="linenr"> 4: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 5: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>(x_train_image, y_train_label), (<span style="color: #dcaeea;">x_test_image</span>, <span style="color: #dcaeea;">y_test_label</span>) = mnist.load_data()
<span class="linenr"> 8: </span><span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 9: </span>
<span class="linenr">10: </span><span style="color: #dcaeea;">model</span> = load_model(<span style="color: #98be65;">'Keras_MNist_model.keras'</span>)
<span class="linenr">11: </span><span style="color: #dcaeea;">predict_x</span>=model.predict(x_Test)
<span class="linenr">12: </span><span style="color: #dcaeea;">prediction</span>=np.argmax(predict_x,axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">13: </span>
<span class="linenr">14: </span><span style="color: #dcaeea;">df</span> = pd.DataFrame({<span style="color: #98be65;">'label'</span>: y_test_label, <span style="color: #98be65;">'predict'</span>:prediction})
<span class="linenr">15: </span><span style="color: #c678dd;">print</span>(df[(df.label==<span style="color: #da8548; font-weight: bold;">5</span>)&amp;(df.predict==<span style="color: #da8548; font-weight: bold;">3</span>)])
<span class="linenr">16: </span>
<span class="linenr">17: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_image</span>(imgname, image):
<span class="linenr">18: </span>    <span style="color: #dcaeea;">fig</span> = plt.gcf() <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#23450;&#35373;&#22294;&#24418;&#22823;&#23567;</span>
<span class="linenr">19: </span>    fig.set_size_inches(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">20: </span>    plt.imshow(image, cmap=<span style="color: #98be65;">'binary'</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">cmap&#21443;&#25976;&#35373;&#23450;&#28858;binary&#20197;&#40657;&#30333;&#28784;&#38542;&#39023;&#31034;</span>
<span class="linenr">21: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plt.show() #for jupyter or colab</span>
<span class="linenr">22: </span>    plt.plot()
<span class="linenr">23: </span>    plt.savefig(imgname+<span style="color: #98be65;">".png"</span>)
<span class="linenr">24: </span>
<span class="linenr">25: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#31532;340&#31558;&#36039;&#26009;</span>
<span class="linenr">26: </span>plot_image(<span style="color: #98be65;">'images/Keras-MNist-Train-4'</span>, x_test_image[<span style="color: #da8548; font-weight: bold;">340</span>])
<span class="linenr">27: </span>
</pre>
</div>

<pre class="example">
  1/313 [..............................] - ETA: 9s119/313 [==========&gt;...................] - ETA: 0s245/313 [======================&gt;.......] - ETA: 0s313/313 [==============================] - 0s 407us/step
      label  predict
340       5        3
1393      5        3
2035      5        3
2597      5        3
4360      5        3
5937      5        3
</pre>


<div id="org9cac357" class="figure">
<p><img src="images/Keras-MNist-Train-4.png" alt="Keras-MNist-Train-4.png" width="100" /><br />
</p>
<p><span class="figure-number">Figure 35: </span>Keras Mnist Model 訓練#1: error sample</p>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-org1f6e8f0" class="outline-3">
<h3 id="org1f6e8f0"><span class="section-number-3">6.5.</span> 強化 MLP 辨識 solution #1: 增加隠藏層神經元數</h3>
<div class="outline-text-3" id="text-6-5">
<p>
為了增加 MLP 的準確率，其中一種方法可以增加隠藏層的神經元數至 1000 個，程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> to_categorical
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25903;&#25588;&#32173;&#24230;&#38499;&#21015;&#20043;&#30697;&#38499;&#36939;&#31639;</span>
<span class="linenr"> 3: </span>np.random.seed(<span style="color: #da8548; font-weight: bold;">10</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span><span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 6: </span>(x_train_image, y_train_label), (<span style="color: #dcaeea;">x_test_image</span>, <span style="color: #dcaeea;">y_test_label</span>) = mnist.load_data()
<span class="linenr"> 7: </span><span style="color: #dcaeea;">x_Train</span> = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 8: </span><span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 9: </span><span style="color: #dcaeea;">x_Train_normalize</span> = x_Train/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">10: </span><span style="color: #dcaeea;">x_Test_normalize</span> = x_Test/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">11: </span><span style="color: #dcaeea;">y_TrainOneHot</span> = to_categorical(y_train_label)
<span class="linenr">12: </span><span style="color: #dcaeea;">y_TestOneHot</span> = to_categorical(y_test_label)
<span class="linenr">13: </span>
<span class="linenr">14: </span><span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr">15: </span><span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense
<span class="linenr">16: </span><span style="color: #dcaeea;">model</span> = Sequential()
<span class="linenr">17: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#22686;&#21152;&#31070;&#32147;&#20803;&#25976;</span>
<span class="linenr">18: </span>model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">1000</span>, input_dim=<span style="color: #da8548; font-weight: bold;">784</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">19: </span>model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">10</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr">20: </span><span style="color: #c678dd;">print</span>(model.summary())
<span class="linenr">21: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#26597;&#30475;&#35347;&#32244;&#36942;&#31243;&#21450;&#32080;&#26524;</span>
<span class="linenr">22: </span>model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>, optimizer=<span style="color: #98be65;">'adam'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr">23: </span><span style="color: #dcaeea;">train_history</span> = model.fit(x=x_Train_normalize, y=y_TrainOneHot,
<span class="linenr">24: </span>                                validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>, epochs=<span style="color: #da8548; font-weight: bold;">10</span>, verbose=<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">25: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">26: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">show_train_history</span>(train_history, train, validation):
<span class="linenr">27: </span>    plt.plot(train_history.history[train])
<span class="linenr">28: </span>    plt.plot(train_history.history[validation])
<span class="linenr">29: </span>    plt.title(<span style="color: #98be65;">'Train History'</span>)
<span class="linenr">30: </span>    plt.ylabel(train)
<span class="linenr">31: </span>    plt.xlabel(<span style="color: #98be65;">'Epoch'</span>)
<span class="linenr">32: </span>    plt.legend([<span style="color: #98be65;">'train'</span>,<span style="color: #98be65;">'validation'</span>], loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">33: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plt.show() # for jupyter notebook</span>
<span class="linenr">34: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#20462;&#25913;for console run</span>
<span class="linenr">35: </span>    <span style="color: #dcaeea;">img</span> = plt.plot()
<span class="linenr">36: </span>    <span style="color: #51afef;">return</span> img
<span class="linenr">37: </span>
<span class="linenr">38: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;acc</span>
<span class="linenr">39: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;val_accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;val_acc</span>
<span class="linenr">40: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">show_train_history(train_history, 'accuracy', 'val_accuracy')</span>
<span class="linenr">41: </span><span style="color: #dcaeea;">img</span> = show_train_history(train_history, <span style="color: #98be65;">'accuracy'</span>, <span style="color: #98be65;">'val_accuracy'</span>)
<span class="linenr">42: </span>plt.savefig(<span style="color: #98be65;">"images/Keras-MNist-Train-5.png"</span>)
<span class="linenr">43: </span><span style="color: #dcaeea;">img</span> = show_train_history(train_history, <span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'val_loss'</span>)
<span class="linenr">44: </span>plt.savefig(<span style="color: #98be65;">"images/Keras-MNist-Train-6.png"</span>) <span style="color: #5B6268;">#</span>
<span class="linenr">45: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#28204;&#35430;&#36039;&#26009;&#35413;&#20272;&#31934;&#30906;&#29575;</span>
<span class="linenr">46: </span><span style="color: #dcaeea;">scores</span> = model.evaluate(x_Test_normalize, y_TestOneHot)
<span class="linenr">47: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'accuracy'</span>,scores[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">48: </span>
<span class="linenr">49: </span>model.save(<span style="color: #98be65;">'Keras_MNist_model-1.keras'</span>)
<span class="linenr">50: </span><span style="color: #51afef;">del</span> model  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">deletes the existing model</span>
</pre>
</div>

<pre class="example" id="orgf2414ec">
Model: "sequential_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 dense_6 (Dense)             (None, 1000)              785000

 dense_7 (Dense)             (None, 10)                10010

=================================================================
Total params: 795010 (3.03 MB)
Trainable params: 795010 (3.03 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
Epoch 1/10
1500/1500 - 3s - loss: 0.2009 - accuracy: 0.9403 - val_loss: 0.1142 - val_accuracy: 0.9649 - 3s/epoch - 2ms/step
Epoch 2/10
1500/1500 - 3s - loss: 0.0803 - accuracy: 0.9752 - val_loss: 0.0817 - val_accuracy: 0.9744 - 3s/epoch - 2ms/step
Epoch 3/10
1500/1500 - 3s - loss: 0.0504 - accuracy: 0.9842 - val_loss: 0.1021 - val_accuracy: 0.9682 - 3s/epoch - 2ms/step
Epoch 4/10
1500/1500 - 3s - loss: 0.0327 - accuracy: 0.9894 - val_loss: 0.0861 - val_accuracy: 0.9745 - 3s/epoch - 2ms/step
Epoch 5/10
1500/1500 - 3s - loss: 0.0284 - accuracy: 0.9907 - val_loss: 0.0880 - val_accuracy: 0.9774 - 3s/epoch - 2ms/step
Epoch 6/10
1500/1500 - 3s - loss: 0.0211 - accuracy: 0.9930 - val_loss: 0.0875 - val_accuracy: 0.9768 - 3s/epoch - 2ms/step
Epoch 7/10
1500/1500 - 3s - loss: 0.0163 - accuracy: 0.9946 - val_loss: 0.0824 - val_accuracy: 0.9795 - 3s/epoch - 2ms/step
Epoch 8/10
1500/1500 - 3s - loss: 0.0130 - accuracy: 0.9957 - val_loss: 0.0876 - val_accuracy: 0.9803 - 3s/epoch - 2ms/step
Epoch 9/10
1500/1500 - 3s - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.1053 - val_accuracy: 0.9781 - 3s/epoch - 2ms/step
Epoch 10/10
1500/1500 - 3s - loss: 0.0126 - accuracy: 0.9959 - val_loss: 0.1093 - val_accuracy: 0.9772 - 3s/epoch - 2ms/step
313/313 [==============================] - 0s 898us/step - loss: 0.0983 - accuracy: 0.9787
accuracy 0.9786999821662903
</pre>


<div id="org8941db7" class="figure">
<p><img src="images/Keras-MNist-Train-5.png" alt="Keras-MNist-Train-5.png" width="400" /><br />
</p>
<p><span class="figure-number">Figure 36: </span>Keras Mnist Model 訓練#2: accuracy</p>
</div>

<div id="org7820690" class="figure">
<p><img src="images/Keras-MNist-Train-6.png" alt="Keras-MNist-Train-6.png" width="400" /><br />
</p>
<p><span class="figure-number">Figure 37: </span>Keras Mnist Model 訓練#2: loss information</p>
</div>

<p>
將神經元數增加至 1000 後，精確率由 0.97 提升至 0.98。但測驗準確率仍未提升，可見 overfitting 仍然嚴重。<br />
</p>
</div>
</div>
<div id="outline-container-orge0b38e3" class="outline-3">
<h3 id="orge0b38e3"><span class="section-number-3">6.6.</span> 強化 MLP 辨識 solution #2: 加入 DropOut 以避免 overfitting</h3>
<div class="outline-text-3" id="text-6-6">
<p>
為解決 Overfitting 問題，此處再加入 Dropout(0.5)指令，其功能為在每次訓練迭代時，會隨機地在隱藏層中放棄 50%的神經元，以避免 overfitting，程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> to_categorical
<span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25903;&#25588;&#32173;&#24230;&#38499;&#21015;&#20043;&#30697;&#38499;&#36939;&#31639;</span>

np.random.seed(<span style="color: #da8548; font-weight: bold;">10</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
<span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
(x_train_image, y_train_label), (<span style="color: #dcaeea;">x_test_image</span>, <span style="color: #dcaeea;">y_test_label</span>) = mnist.load_data()
<span style="color: #dcaeea;">x_Train</span> = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span style="color: #dcaeea;">x_Train_normalize</span> = x_Train/ <span style="color: #da8548; font-weight: bold;">255</span>
<span style="color: #dcaeea;">x_Test_normalize</span> = x_Test/ <span style="color: #da8548; font-weight: bold;">255</span>
<span style="color: #dcaeea;">y_TrainOneHot</span> = to_categorical(y_train_label)
<span style="color: #dcaeea;">y_TestOneHot</span> = to_categorical(y_test_label)

<span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense
<span style="color: #dcaeea;">model</span> = Sequential()
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#22686;&#21152;&#31070;&#32147;&#20803;&#25976;</span>
model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">1000</span>, input_dim=<span style="color: #da8548; font-weight: bold;">784</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#21152;&#20837;Dropout</span>
<span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dropout
model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#21152;&#20837;&#36664;&#20986;&#23652;</span>
model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">10</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span style="color: #c678dd;">print</span>(model.summary())
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#35347;&#32244;&#27169;&#22411;&#12289;&#26597;&#30475;&#32080;&#26524;</span>
model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>, optimizer=<span style="color: #98be65;">'adam'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span style="color: #dcaeea;">train_history</span> = model.fit(x=x_Train_normalize, y=y_TrainOneHot,
                              validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>, epochs=<span style="color: #da8548; font-weight: bold;">10</span>, verbose=<span style="color: #da8548; font-weight: bold;">2</span>)
<span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span style="color: #51afef;">def</span> <span style="color: #c678dd;">show_train_history</span>(train_history, train, validation):
    plt.plot(train_history.history[train])
    plt.plot(train_history.history[validation])
    plt.title(<span style="color: #98be65;">'Train History'</span>)
    plt.ylabel(train)
    plt.xlabel(<span style="color: #98be65;">'Epoch'</span>)
    plt.legend([<span style="color: #98be65;">'train'</span>,<span style="color: #98be65;">'validation'</span>], loc=<span style="color: #98be65;">'upper left'</span>)
    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plt.show() # for jupyter notebook</span>
    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#20462;&#25913;for console run</span>
    <span style="color: #dcaeea;">img</span> = plt.plot()
    <span style="color: #51afef;">return</span> img

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;acc</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;val_accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;val_acc</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">show_train_history(train_history, 'accuracy', 'val_accuracy')</span>
<span style="color: #dcaeea;">img</span> = show_train_history(train_history, <span style="color: #98be65;">'accuracy'</span>, <span style="color: #98be65;">'val_accuracy'</span>)
plt.savefig(<span style="color: #98be65;">"images/Keras-MNist-Train-7.png"</span>)
<span style="color: #dcaeea;">img</span> = show_train_history(train_history, <span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'val_loss'</span>)
plt.savefig(<span style="color: #98be65;">"images/Keras-MNist-Train-8.png"</span>) <span style="color: #5B6268;">#</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#28204;&#35430;&#36039;&#26009;&#35413;&#20272;&#31934;&#30906;&#29575;</span>
<span style="color: #dcaeea;">scores</span> = model.evaluate(x_Test_normalize, y_TestOneHot)
<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'accuracy='</span>,scores[<span style="color: #da8548; font-weight: bold;">1</span>])
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36914;&#34892;&#38928;&#28204;</span>
<span style="color: #dcaeea;">predict_x</span>=model.predict(x_Test)
<span style="color: #dcaeea;">prediction</span>=np.argmax(predict_x,axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36664;&#20986;confuse mqtrix</span>
<span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
pd.crosstab(y_test_label,prediction,
            rownames=[<span style="color: #98be65;">'label'</span>],colnames=[<span style="color: #98be65;">'predict'</span>])

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20786;&#23384;&#35347;&#32244;&#22909;&#30340;&#27169;&#24335;</span>
model.save(<span style="color: #98be65;">'Keras_MNist_model-2.keras'</span>)
<span style="color: #51afef;">del</span> model  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">deletes the existing model</span>
</pre>
</div>

<pre class="example" id="org1636de1">
Model: "sequential_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 dense_10 (Dense)            (None, 1000)              785000

 dropout_1 (Dropout)         (None, 1000)              0

 dense_11 (Dense)            (None, 10)                10010

=================================================================
Total params: 795010 (3.03 MB)
Trainable params: 795010 (3.03 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
Epoch 1/10
1500/1500 - 3s - loss: 0.2691 - accuracy: 0.9188 - val_loss: 0.1277 - val_accuracy: 0.9624 - 3s/epoch - 2ms/step
Epoch 2/10
1500/1500 - 3s - loss: 0.1346 - accuracy: 0.9579 - val_loss: 0.0932 - val_accuracy: 0.9711 - 3s/epoch - 2ms/step
Epoch 3/10
1500/1500 - 3s - loss: 0.1030 - accuracy: 0.9677 - val_loss: 0.0850 - val_accuracy: 0.9746 - 3s/epoch - 2ms/step
Epoch 4/10
1500/1500 - 3s - loss: 0.0864 - accuracy: 0.9729 - val_loss: 0.0870 - val_accuracy: 0.9747 - 3s/epoch - 2ms/step
Epoch 5/10
1500/1500 - 3s - loss: 0.0751 - accuracy: 0.9760 - val_loss: 0.0794 - val_accuracy: 0.9786 - 3s/epoch - 2ms/step
Epoch 6/10
1500/1500 - 3s - loss: 0.0688 - accuracy: 0.9779 - val_loss: 0.0852 - val_accuracy: 0.9774 - 3s/epoch - 2ms/step
Epoch 7/10
1500/1500 - 3s - loss: 0.0633 - accuracy: 0.9795 - val_loss: 0.0819 - val_accuracy: 0.9775 - 3s/epoch - 2ms/step
Epoch 8/10
1500/1500 - 3s - loss: 0.0563 - accuracy: 0.9816 - val_loss: 0.0864 - val_accuracy: 0.9765 - 3s/epoch - 2ms/step
Epoch 9/10
1500/1500 - 3s - loss: 0.0527 - accuracy: 0.9830 - val_loss: 0.0838 - val_accuracy: 0.9790 - 3s/epoch - 2ms/step
Epoch 10/10
1500/1500 - 3s - loss: 0.0506 - accuracy: 0.9836 - val_loss: 0.0848 - val_accuracy: 0.9799 - 3s/epoch - 2ms/step
313/313 [==============================] - 0s 859us/step - loss: 0.0696 - accuracy: 0.9825
accuracy= 0.9825000166893005
313/313 [==============================] - 0s 831us/step
</pre>

<p>
由訓練過程可以看出，驗證精確率(0.9805)已接近訓練精確率(0.9838)，可見已改善了 overfitting 問題。<br />
</p>


<div id="orgc790b55" class="figure">
<p><img src="images/Keras-MNist-Train-7.png" alt="Keras-MNist-Train-7.png" width="400" /><br />
</p>
<p><span class="figure-number">Figure 38: </span>Keras Mnist Model 訓練#3: accuracy</p>
</div>

<div id="org49b2aa7" class="figure">
<p><img src="images/Keras-MNist-Train-8.png" alt="Keras-MNist-Train-8.png" width="400" /><br />
</p>
<p><span class="figure-number">Figure 39: </span>Keras Mnist Model 訓練#3: loss information</p>
</div>

<p>
由圖<a href="#orgc790b55">38</a>也可看出，驗證精確率已隨訓練週期提高，驗證誤差也隨訓練週期降低。<br />
</p>
</div>
</div>
<div id="outline-container-orga9ff303" class="outline-3">
<h3 id="orga9ff303"><span class="section-number-3">6.7.</span> 強化 MLP 辨識 solution #3: 增加隱藏層層數</h3>
<div class="outline-text-3" id="text-6-7">
<p>
本例 MLP 模型的預測能力仍有改善空間：增加隠藏層層數。以下程式碼將隠藏層數提高至 2 層：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> to_categorical
<span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25903;&#25588;&#32173;&#24230;&#38499;&#21015;&#20043;&#30697;&#38499;&#36939;&#31639;</span>

np.random.seed(<span style="color: #da8548; font-weight: bold;">10</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
<span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
(x_train_image, y_train_label), (<span style="color: #dcaeea;">x_test_image</span>, <span style="color: #dcaeea;">y_test_label</span>) = mnist.load_data()
<span style="color: #dcaeea;">x_Train</span> = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span style="color: #dcaeea;">x_Train_normalize</span> = x_Train/ <span style="color: #da8548; font-weight: bold;">255</span>
<span style="color: #dcaeea;">x_Test_normalize</span> = x_Test/ <span style="color: #da8548; font-weight: bold;">255</span>
<span style="color: #dcaeea;">y_TrainOneHot</span> = to_categorical(y_train_label)
<span style="color: #dcaeea;">y_TestOneHot</span> = to_categorical(y_test_label)

<span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense
<span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dropout

<span style="color: #dcaeea;">model</span> = Sequential()
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#22686;&#21152;&#31532;1&#23652;&#38577;&#34255;&#23652;&#31070;&#32147;&#20803;&#25976;</span>
model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">1000</span>, input_dim=<span style="color: #da8548; font-weight: bold;">784</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#21152;&#20837;Dropout</span>
model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#22686;&#21152;&#31532;2&#23652;&#38577;&#34255;&#23652;</span>
model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">1000</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#21152;&#20837;&#31532;2&#23652;Dropout</span>
model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#21152;&#20837;&#36664;&#20986;&#23652;</span>
model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">10</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span style="color: #c678dd;">print</span>(model.summary())
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#35347;&#32244;&#27169;&#22411;&#12289;&#26597;&#30475;&#32080;&#26524;</span>
model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>, optimizer=<span style="color: #98be65;">'adam'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span style="color: #dcaeea;">train_history</span> = model.fit(x=x_Train_normalize, y=y_TrainOneHot,
                          validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>, epochs=<span style="color: #da8548; font-weight: bold;">10</span>, verbose=<span style="color: #da8548; font-weight: bold;">2</span>)
<span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span style="color: #51afef;">def</span> <span style="color: #c678dd;">show_train_history</span>(train_history, train, validation):
    plt.plot(train_history.history[train])
    plt.plot(train_history.history[validation])
    plt.title(<span style="color: #98be65;">'Train History'</span>)
    plt.ylabel(train)
    plt.xlabel(<span style="color: #98be65;">'Epoch'</span>)
    plt.legend([<span style="color: #98be65;">'train'</span>,<span style="color: #98be65;">'validation'</span>], loc=<span style="color: #98be65;">'upper left'</span>)
    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plt.show() # for jupyter notebook</span>
    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#20462;&#25913;for console run</span>
    <span style="color: #dcaeea;">img</span> = plt.plot()
    <span style="color: #51afef;">return</span> img

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;acc</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;val_accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;val_acc</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">show_train_history(train_history, 'accuracy', 'val_accuracy')</span>
<span style="color: #dcaeea;">img</span> = show_train_history(train_history, <span style="color: #98be65;">'accuracy'</span>, <span style="color: #98be65;">'val_accuracy'</span>)
plt.savefig(<span style="color: #98be65;">"images/Keras-MNist-Train-9.png"</span>)
<span style="color: #dcaeea;">img</span> = show_train_history(train_history, <span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'val_loss'</span>)
plt.savefig(<span style="color: #98be65;">"images/Keras-MNist-Train-a.png"</span>) <span style="color: #5B6268;">#</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#28204;&#35430;&#36039;&#26009;&#35413;&#20272;&#31934;&#30906;&#29575;</span>
<span style="color: #dcaeea;">scores</span> = model.evaluate(x_Test_normalize, y_TestOneHot)
<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'accuracy='</span>,scores[<span style="color: #da8548; font-weight: bold;">1</span>])
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36914;&#34892;&#38928;&#28204;</span>
<span style="color: #dcaeea;">predict_x</span>=model.predict(x_Test)
<span style="color: #dcaeea;">prediction</span>=np.argmax(predict_x,axis=<span style="color: #da8548; font-weight: bold;">1</span>)

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36664;&#20986;confuse mqtrix</span>
<span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
pd.crosstab(y_test_label,prediction,
            rownames=[<span style="color: #98be65;">'label'</span>],colnames=[<span style="color: #98be65;">'predict'</span>])
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20786;&#23384;&#35347;&#32244;&#22909;&#30340;&#27169;&#24335;</span>
model.save(<span style="color: #98be65;">'Keras_MNist_model-3.keras'</span>)
<span style="color: #51afef;">del</span> model  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">deletes the existing model</span>
</pre>
</div>

<pre class="example" id="org4d11448">
Model: "sequential_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 dense_12 (Dense)            (None, 1000)              785000

 dropout_2 (Dropout)         (None, 1000)              0

 dense_13 (Dense)            (None, 1000)              1001000

 dropout_3 (Dropout)         (None, 1000)              0

 dense_14 (Dense)            (None, 10)                10010

=================================================================
Total params: 1796010 (6.85 MB)
Trainable params: 1796010 (6.85 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
Epoch 1/10
1500/1500 - 6s - loss: 0.3203 - accuracy: 0.9016 - val_loss: 0.1277 - val_accuracy: 0.9622 - 6s/epoch - 4ms/step
Epoch 2/10
1500/1500 - 6s - loss: 0.1769 - accuracy: 0.9470 - val_loss: 0.1071 - val_accuracy: 0.9671 - 6s/epoch - 4ms/step
Epoch 3/10
1500/1500 - 6s - loss: 0.1495 - accuracy: 0.9555 - val_loss: 0.1007 - val_accuracy: 0.9684 - 6s/epoch - 4ms/step
Epoch 4/10
1500/1500 - 6s - loss: 0.1367 - accuracy: 0.9602 - val_loss: 0.0919 - val_accuracy: 0.9737 - 6s/epoch - 4ms/step
Epoch 5/10
1500/1500 - 6s - loss: 0.1224 - accuracy: 0.9646 - val_loss: 0.0889 - val_accuracy: 0.9743 - 6s/epoch - 4ms/step
Epoch 6/10
1500/1500 - 6s - loss: 0.1166 - accuracy: 0.9659 - val_loss: 0.0956 - val_accuracy: 0.9737 - 6s/epoch - 4ms/step
Epoch 7/10
1500/1500 - 6s - loss: 0.1114 - accuracy: 0.9687 - val_loss: 0.0892 - val_accuracy: 0.9756 - 6s/epoch - 4ms/step
Epoch 8/10
1500/1500 - 6s - loss: 0.1035 - accuracy: 0.9712 - val_loss: 0.1030 - val_accuracy: 0.9750 - 6s/epoch - 4ms/step
Epoch 9/10
1500/1500 - 6s - loss: 0.1066 - accuracy: 0.9709 - val_loss: 0.0979 - val_accuracy: 0.9746 - 6s/epoch - 4ms/step
Epoch 10/10
1500/1500 - 6s - loss: 0.1013 - accuracy: 0.9726 - val_loss: 0.0961 - val_accuracy: 0.9758 - 6s/epoch - 4ms/step
313/313 [==============================] - 1s 2ms/step - loss: 0.0871 - accuracy: 0.9769
accuracy= 0.9768999814987183
313/313 [==============================] - 1s 2ms/step
</pre>


<div id="orge1af192" class="figure">
<p><img src="images/Keras-MNist-Train-9.png" alt="Keras-MNist-Train-9.png" width="400" /><br />
</p>
<p><span class="figure-number">Figure 40: </span>Keras Mnist Model 訓練#3: accuracy</p>
</div>

<div id="org2f3be06" class="figure">
<p><img src="images/Keras-MNist-Train-a.png" alt="Keras-MNist-Train-a.png" width="400" /><br />
</p>
<p><span class="figure-number">Figure 41: </span>Keras Mnist Model 訓練#3: loss information</p>
</div>

<p>
由訓練成果判斷，雖然精確率並未提升，但是由圖<a href="#orge1af192">40</a>可以看出驗證精確率已經比訓練精確率高，已確實可以解決 overfitting 的問題。<br />
</p>

<p>
隨著 MLP 模型的改進，雖然精確率可逐步提升，也可藉由加入 Dropout 解決 overfitting 的問題，但 MLP 仍有其極限，如果要進一步提升準確率，就要使用卷積神經網路 CNN (convolutional neural network)。<br />
</p>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://kknews.cc/code/o8m4gpq.html">五分鐘理解深度學習中激活函數以及不同激活函數的使用場景</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://kknews.cc/zh-tw/tech/b4zkbom.html">主流的深度學習模型有哪些？</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://ithelp.ithome.com.tw/articles/10191820">Day 06：處理影像的利器 &#x2013; 卷積神經網路(Convolutional Neural Network)</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4" role="doc-backlink">4</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://medium.com/@syshen/%E5%85%A5%E9%96%80%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-2-d694cad7d1e5">入門深度學習 — 2</a><br />
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Yung-Chin Yen</p>
<p class="date">Created: 2024-02-19 Mon 16:13</p>
</div>
</body>
</html>
