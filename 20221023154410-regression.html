<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-02-18 Tue 17:59 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>迴歸</title>
<meta name="author" content="Yung-Chin Yen" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="../css/muse.css" />
<script src="../css/copy_code.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">迴歸</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgf3204bb">1. 關於迴歸</a>
<ul>
<li><a href="#org935c84b">1.1. 迴歸類型</a></li>
</ul>
</li>
<li><a href="#org29b46bb">2. 迴歸原理</a>
<ul>
<li><a href="#orge57d256">2.1. Step 1: Model, Data</a></li>
<li><a href="#org0188a0b">2.2. Step 2: Goodness of Function</a></li>
</ul>
</li>
<li><a href="#orgd28a98a">3. 線性迴歸: 學期成績預估</a>
<ul>
<li><a href="#orgd9b6c4b">3.1. 函數: AI的本質</a></li>
<li><a href="#org66ce2b4">3.2. 逐步找出最佳的a</a></li>
<li><a href="#org8428bda">3.3. 兩項成績推估</a></li>
<li><a href="#orgb24deb3">3.4. 神經網路的視角</a></li>
</ul>
</li>
<li><a href="#orgc6dc189">4. 線性迴歸實作: 波士頓房價預測</a>
<ul>
<li><a href="#org7a01048">4.1. 下載資料</a></li>
<li><a href="#org3f45ee0">4.2. 大概觀察一下資料集</a></li>
<li><a href="#orgec5cff6">4.3. 資料預處理</a></li>
<li><a href="#org0e251da">4.4. 觀察資料</a></li>
<li><a href="#org94a6136">4.5. 分割訓練集與測試集</a></li>
<li><a href="#orgd00aa43">4.6. 建立模型</a></li>
<li><a href="#org29cef4a">4.7. 測試效能</a></li>
<li><a href="#org9868f2f">4.8. 找出線性模型</a></li>
</ul>
</li>
<li><a href="#org41b9dd4">5. 關於迴歸模型的特徵選擇</a>
<ul>
<li><a href="#org7b53428">5.1. 前向選擇法(Forward Selection)</a></li>
<li><a href="#org801eecd">5.2. 後向選擇法(Backward Selection)</a></li>
<li><a href="#orgdafd9c3">5.3. 特徵選擇的注意事項</a></li>
<li><a href="#orgb1b4610">5.4. 多重共線性(Multicollinearity)</a></li>
</ul>
</li>
<li><a href="#orgb247da7">6. [作業]依據期中考成績預測期末考成績&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></a>
<ul>
<li><a href="#orgca2b7eb">6.1. Data</a></li>
<li><a href="#orgacae913">6.2. Task</a></li>
</ul>
</li>
<li><a href="#org0a20d3a">7. [卜聖卦活動2]以迴歸預測成績&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></a>
<ul>
<li><a href="#org741d46e">7.1. 資料集</a></li>
<li><a href="#org7da66da">7.2. 任務：</a></li>
</ul>
</li>
</ul>
</div>
</div>
<a href="https://letranger.github.io/AI/20221023154410-regression.html"><img align="right" alt="Hits" src="https://hits.sh/letranger.github.io/AI/20221023154410-regression.html.svg"/></a>
<div id="outline-container-orgf3204bb" class="outline-2">
<h2 id="orgf3204bb"><span class="section-number-2">1.</span> 關於迴歸</h2>
<div class="outline-text-2" id="text-1">
<p>
即，根據一組預測特徵（predictor，如里程數、車齡、品牌）來預測目標數值（如二手車車價）<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>、根據歷史股價來預測明天股價、根據路況來預測方向盤轉向及車速。<br />
</p>

<p>
部份迴歸演算法也可以用來分類，例如Logistic，它可以輸出一個數值，以這個數值來表示對應到特定類別的機率，例如，某封email為垃圾郵件的機率為20%、某張圖片為狗的機率為70%。<br />
</p>
</div>
<div id="outline-container-org935c84b" class="outline-3">
<h3 id="org935c84b"><span class="section-number-3">1.1.</span> 迴歸類型</h3>
<div class="outline-text-3" id="text-1-1">
<p>
迴歸問題可分為兩類：<br />
</p>
<ul class="org-ul">
<li>Linear regression:<br /></li>
<li>Logistic regression<br /></li>
</ul>
</div>
<div id="outline-container-org35c3221" class="outline-4">
<h4 id="org35c3221"><span class="section-number-4">1.1.1.</span> Linear regression:</h4>
<div class="outline-text-4" id="text-1-1-1">
<ul class="org-ul">
<li>假設輸入變量(x)與單一輸出變量(y)間存在線性關係，並以此建立模型。<br /></li>
<li>優點: 簡單、容易解釋<br /></li>
<li>缺點: 輸入與輸出變量關係為線性時會導致低度擬合<br /></li>
<li>例: 身高與體重間的關係<br /></li>
</ul>
<p>
Linear regression可再細分為<br />
</p>
</div>
<div id="outline-container-orge5825bc" class="outline-5">
<h5 id="orge5825bc">Simple Linear regression (簡單線性迴歸):</h5>
<div class="outline-text-5" id="text-orge5825bc">
</div>
<ul class="org-ul">
<li><a id="org98e7eff"></a>迴歸<br />
<div class="outline-text-6" id="text-org98e7eff">
<p>
討論兩個變數間的關係(輸入變量(x)與單一輸出變量(y)間)的關係<br />
</p>

<div id="org46fde89" class="figure">
<p><img src="images/regvsml1.png" alt="regvsml1.png" /><br />
</p>
</div>
</div>
</li>
<li><a id="org5ab77bb"></a>在統計中:<br />
<div class="outline-text-6" id="text-org5ab77bb">
<ul class="org-ul">
<li>x為Independent Variables: 自變數<br /></li>
<li>y為Dependent Variables: 應變數<br /></li>
</ul>

<div id="org6b23387" class="figure">
<p><img src="images/regvsml2.png" alt="regvsml2.png" /><br />
</p>
</div>
</div>
</li>
<li><a id="org0331fd5"></a>在機器學習中<br />
<div class="outline-text-6" id="text-org0331fd5">
<ul class="org-ul">
<li>x叫Features: 特徵值<br /></li>
<li>y叫Label: 標籤<br /></li>
</ul>

<div id="org734e92d" class="figure">
<p><img src="images/regvsml3.png" alt="regvsml3.png" /><br />
</p>
</div>
</div>
</li>
</ul>
</div>
<div id="outline-container-org278b7a7" class="outline-5">
<h5 id="org278b7a7">Multiple Linear regression (多元線性迴歸)</h5>
<div class="outline-text-5" id="text-org278b7a7">

<div id="org97e540c" class="figure">
<p><img src="./images/regvsm3.png" alt="regvsm3.png" width="200" /><br />
</p>
<p><span class="figure-number">Figure 1: </span>多元線性迴歸</p>
</div>

<p>
討論多個變數間的關係，Multiple Linear Regression 是線性迴歸的一種延伸，當輸出變量 \(y\) 與多個輸入變量 \(x_1,x_2,…,x_n\) 之間存在線性關係時，可使用多元線性迴歸來建模:<br />
\[y=ax_1+bx_2+cx_3+⋯+zx_n+ϵ\]<br />
其中<br />
</p>
<ul class="org-ul">
<li>\(y\): 輸出變量 (應變數)<br /></li>
<li>\(x_1,x_2,…,x_n\)​: 輸入變量 (特徵值)<br /></li>
<li>\(a,b,c​,…,z\)​: 係數 (權重)<br /></li>
<li>\(ϵ\): 誤差項 (Residual)<br /></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orge7f3c8b" class="outline-4">
<h4 id="orge7f3c8b"><span class="section-number-4">1.1.2.</span> Logistic regression</h4>
<div class="outline-text-4" id="text-1-1-2">
<ul class="org-ul">
<li>也是線性方法，但使用logist function轉換輸出的預測結果，其輸出結果為類別機率(class probabilities)<br /></li>
<li>優點: 簡單、容易解釋<br /></li>
<li>缺點: 輸入與輸出變量關係為線性時無法處理分類問題<br /></li>
</ul>
</div>
</div>
<div id="outline-container-orgdb3f9b4" class="outline-4">
<h4 id="orgdb3f9b4"><span class="section-number-4">1.1.3.</span> 迴歸的目的</h4>
<div class="outline-text-4" id="text-1-1-3">
<p>
建立迴歸的目的在於從現有資料中找出規則，然後依此規則來對後續的新進資料進行預測。如圖<a href="#org0eca1d2">2</a>中有一些資料分佈，x、y軸為資料的兩個特徵值。<br />
</p>

<div id="org0eca1d2" class="figure">
<p><img src="images/關於迴歸/2024-02-07_15-18-38_2024-02-07_15-18-19.png" alt="2024-02-07_15-18-38_2024-02-07_15-18-19.png" width="400" /><br />
</p>
<p><span class="figure-number">Figure 2: </span>原始資料分佈</p>
</div>

<p>
我們可以畫出幾條直線來代表這些資料的趨勢，問題是：<br />
</p>
<ul class="org-ul">
<li>怎麼畫<br /></li>
<li>怎麼知道哪一條最有代表性<br /></li>
</ul>

<div id="org8ffc87a" class="figure">
<p><img src="images/關於迴歸/2024-02-07_15-19-44_2024-02-07_15-18-27.png" alt="2024-02-07_15-19-44_2024-02-07_15-18-27.png" width="400" /><br />
</p>
<p><span class="figure-number">Figure 3: </span>根據原始資料畫出的幾條迴歸線</p>
</div>

<p>
典型迴歸案例: Boston Housing Data<br />
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org29b46bb" class="outline-2">
<h2 id="org29b46bb"><span class="section-number-2">2.</span> 迴歸原理</h2>
<div class="outline-text-2" id="text-2">
<p>
練習投藍的時後，我們需要知道籃筐位置，誤差多少，1做出丟球的修正。<br />
做 Machine Learning 也是一樣道理，我們需要 :<br />
</p>
<ul class="org-ul">
<li>建立模型<br /></li>
<li>計算誤差: 在<a href="20221023101456-機器學習.html#ID-20221023T101456.955364">機器學習</a>中，我們用loss function來計算<br /></li>
<li>做出修正: 在<a href="20221023101456-機器學習.html#ID-20221023T101456.955364">機器學習</a>中，我們用optimizer來不斷對模型進行修正<br /></li>
</ul>
</div>
<div id="outline-container-orge57d256" class="outline-3">
<h3 id="orge57d256"><span class="section-number-3">2.1.</span> Step 1: Model, Data</h3>
<div class="outline-text-3" id="text-2-1">
<ol class="org-ol">
<li>Model: \(y = w*x+b\)<br /></li>
<li>Data: 找一堆現成的資料<br /></li>
</ol>
</div>
</div>
<div id="outline-container-org0188a0b" class="outline-3">
<h3 id="org0188a0b"><span class="section-number-3">2.2.</span> Step 2: Goodness of Function</h3>
<div class="outline-text-3" id="text-2-2">
<ol class="org-ol">
<li>Training Data<br /></li>
<li>Loss function L: 越小越好<br />
input: a function / output: how bad it is<br /></li>
<li>Pick the <b>Best Function</b> :<br />
\(f* = \arg\min L(f)\)<br />
上述可以微分來求最佳解，即求 function L 的最小值<br /></li>
<li>數值最佳解: Gradient Descent(找拋物線/面最低點)<br /></li>
</ol>
</div>
</div>
</div>
<div id="outline-container-orgd28a98a" class="outline-2">
<h2 id="orgd28a98a"><span class="section-number-2">3.</span> 線性迴歸: 學期成績預估</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-orgd9b6c4b" class="outline-3">
<h3 id="orgd9b6c4b"><span class="section-number-3">3.1.</span> 函數: AI的本質</h3>
<div class="outline-text-3" id="text-3-1">
<p>
人工智慧本質上就是在找出一個特定函數。例如，我們想利用人工智慧來預估自己這個學期的資訊科成績，其實相當於在找一個類似這樣的函數：<br />
\[f(期中考成績,期末考成績)→學期總成績\]<br />
有了這個函數，只要我們輸入這學期的兩次段成績，就能算出自己的學期總成績。但像這樣的函數f有無限多個，其中一種可能的計算方式如下：<br />
\[期末成績=0.4×期中考成績+0.6×期末考成績\]<br />
，此處的0.4, 0.6就稱為這個函數的參數（parameter）。<br />
</p>

<p>
為了找出這個函數，我們首先應蒐集歷屆學長姐的成績資料（包括兩次段考平均成績與學期總成績），這些資料稱之為訓練資料（training data）。在將訓練資料視覺化後，我們發現x與y的分佈如圖<a href="#org4559e7e">4</a>所示（假設我們詢問了10位學長姐、收集了10筆訓練資料），初步猜測二者可能存在線性關係。接下來的任務就是畫出一條能儘量接近圖上所有資料點的線，也就是找出最佳的參數a（即這條迴歸線的斜率）。<br />
</p>


<div id="org4559e7e" class="figure">
<p><img src="images/2scores.png" alt="2scores.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 4: </span>兩次段考平均與學期總成績關係</p>
</div>
</div>
</div>
<div id="outline-container-org66ce2b4" class="outline-3">
<h3 id="org66ce2b4"><span class="section-number-3">3.2.</span> 逐步找出最佳的a</h3>
<div class="outline-text-3" id="text-3-2">
<p>
雖然在此例中最佳的a可以很快用數學公式求出，但此處我們打算介紹另一種逐步找出最佳a的方式，步驟詳列如下：<br />
</p>
</div>
<div id="outline-container-orged3c883" class="outline-4">
<h4 id="orged3c883"><span class="section-number-4">3.2.1.</span> a=0.5</h4>
<div class="outline-text-4" id="text-3-2-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> matplotlib
<span class="linenr">2: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'cc'</span>)
</pre>
</div>

<pre class="example">
cc
</pre>


<p>
首先，面對未知的困難，我們要有嚴格的解題SOP，也就是要遵循「科學、理性、務實」的精神：閉著眼睛隨便給個 \(a\)。例如：a=0.5，畫出的迴歸線結果如圖<a href="#org0366e0b">5</a>。<br />
</p>

<div id="org0366e0b" class="figure">
<p><img src="images/2scores_with_regression.png" alt="2scores_with_regression.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 5: </span>a=0.5的迴歸線</p>
</div>

<p>
有了這條線，我們就可以找出這10筆資料與這條線有多遠。理想狀況下我們希望每個點都離線越近越好，把所有點與迴歸線的距離（圖<a href="#org0366e0b">5</a>中的橘色線段）加總起來，我們把它稱為誤差。此處以均方差（Mean Squared Error, MSE）來計算，誤差值為367.76。MSE公式如下：<br />
\[MSE=\frac{1}{n}\sum_{i=1}^{n}{(y_i-\widehat{y_i})}^2\]<br />
，這裡的MSE就是這個模型的損失函數（Loss Function） 。<br />
</p>
</div>
</div>
<div id="outline-container-org01ba9c1" class="outline-4">
<h4 id="org01ba9c1"><span class="section-number-4">3.2.2.</span> a=0.4, a=0.6</h4>
<div class="outline-text-4" id="text-3-2-2">
<p>
但是目前的a(0.5)是最好的嗎？我們可以試著調整a值，對它各加、減0.1，再來觀察誤差的變化，結果如圖<a href="#orge4a87f5">6</a>，可以發現a再大一些結果似乎會更好。<br />
</p>

<div id="orge4a87f5" class="figure">
<p><img src="images/2scores_with_regression12.png" alt="2scores_with_regression12.png" width="800" /><br />
</p>
<p><span class="figure-number">Figure 6: </span>a=0.4 v.s. a=0.6</p>
</div>
</div>
</div>
<div id="outline-container-orgcb8e779" class="outline-4">
<h4 id="orgcb8e779"><span class="section-number-4">3.2.3.</span> 全部的a</h4>
<div class="outline-text-4" id="text-3-2-3">
<p>
當我們把全部合理的  a  值（如\(0.3 \le a \le 1.4\)）對應的損失函數逐一計算出來後，就可以將這些結果畫成一條如圖<a href="#org72785a8">7</a>的曲線，這就是這個模型的損失函數曲線（Loss Function Curve）。不難看出：只要將a逐步往損失函數較小的方向調整，這個模型的預測效果就會更好。另一種調整a值的方式是透過資料點在曲線上的切線（圖中綠色虛線）斜率來判斷該往左或右移動a值，直到找到切線斜率為0的點，這便是曲線的最低點，也是最佳a值。此函數如下，也是我們用來預估分數的模型：<br />
\[y=f\left(x\right)=0.84x\]<br />
</p>


<div id="org72785a8" class="figure">
<p><img src="images/scoreLoss.png" alt="scoreLoss.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 7: </span>合理a值畫出的誤差函數</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org8428bda" class="outline-3">
<h3 id="org8428bda"><span class="section-number-3">3.3.</span> 兩項成績推估</h3>
<div class="outline-text-3" id="text-3-3">
<p>
如果老師的計分是針對期中期末考給予不同權重呢？那我們新的目標函數就要改成如下式子，此時的任務在求出最佳的參數a, b。<br />
\[y=f\left(x\right)=ax_1+bx_2\]<br />
，其中 \(x_1\) 為兩次段考平均成績, \(x_2\) 為學期總成績。<br />
</p>

<p>
比照上述步驟的做法，我們可以求出所有合理的a, b值（0～1）及其所對應的損失函數，將結果畫成如圖 ‎2.2 5的曲面。實際求解時，一樣先隨機指定一組權重值a, b（圖<a href="#org9d2460c">8</a>藍色點），依該點在曲面上的斜率（此處稱為梯度，Gradient） ，沿著梯度的反方向往下找，就能找到這個曲面的最低點（圖<a href="#org72785a8">7</a>紅色點），該點便是函數的最佳參數。此種做法也就是神經網路中找出模型最佳參數的核心思想：梯度下降法（Gradient Descent）。<br />
</p>


<div id="org9d2460c" class="figure">
<p><img src="images/3dScoreLoss.png" alt="3dScoreLoss.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 8: </span>兩個參數時的誤差函數</p>
</div>
</div>
</div>
<div id="outline-container-orgb24deb3" class="outline-3">
<h3 id="orgb24deb3"><span class="section-number-3">3.4.</span> 神經網路的視角</h3>
<div class="outline-text-3" id="text-3-4">
<p>
從神經網路的角度來看，上述函數也可以視為如圖 ‎2.2 6的模型，模型中只有一層隱藏層，裡面只有一個神經元，輸出結果為期末成績，這便是一個能進行迴歸計算的神經網路。<br />
</p>

<div id="orgfee3bcc" class="figure">
<p><img src="images/datapreprocessing.png" alt="datapreprocessing.png" /><br />
</p>
</div>



<div id="org3346906" class="figure">
<p><img src="images/scoreNN.png" alt="scoreNN.png" width="300" /><br />
</p>
<p><span class="figure-number">Figure 9: </span>以兩項成績預估學期成績的神經網路</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgc6dc189" class="outline-2">
<h2 id="orgc6dc189"><span class="section-number-2">4.</span> 線性迴歸實作: 波士頓房價預測</h2>
<div class="outline-text-2" id="text-4">
<ul class="org-ul">
<li>本例中部份程式碼及文字來自<a href="https://medium.com/li-ting-liao-tiffany/python-%E5%BF%AB%E9%80%9F%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-boston-housing%E6%B3%A2%E5%A3%AB%E9%A0%93%E6%88%BF%E5%83%B9-9c535fb7ceb7">What impacts Boston Housing Prices</a><br /></li>
<li>本例使用資料集為 1970 年中期 Boston 郊區資料，包含犯罪率、當地財產稅等，用以預測某郊區房價中位數，本例有 506 筆資料，分為 404 個訓練樣本和 102 個測試樣本，但每個 feature 的單位不同，故須先進行資料預調整。<br /></li>
</ul>
</div>
<div id="outline-container-org7a01048" class="outline-3">
<h3 id="org7a01048"><span class="section-number-3">4.1.</span> 下載資料</h3>
<div class="outline-text-3" id="text-4-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">2: </span>
<span class="linenr">3: </span><span style="color: #dcaeea;">housing</span> = pd.read_csv(<span style="color: #98be65;">'https://raw.githubusercontent.com/letranger/AI/gh-pages/Downloads/boston_housing.csv'</span>)
</pre>
</div>

<p>
也可以用tensorflow的load_data()直接下載，但這組沒有column title<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">2: </span><span style="color: #51afef;">from</span> tensorflow.keras.datasets <span style="color: #51afef;">import</span> boston_housing
<span class="linenr">3: </span>
<span class="linenr">4: </span>(train_x, train_y), (<span style="color: #dcaeea;">test_x</span>, <span style="color: #dcaeea;">test_y</span>) = boston_housing.load_data()
</pre>
</div>
</div>
</div>
<div id="outline-container-org3f45ee0" class="outline-3">
<h3 id="org3f45ee0"><span class="section-number-3">4.2.</span> 大概觀察一下資料集</h3>
<div class="outline-text-3" id="text-4-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">type</span>(housing))
<span class="linenr">2: </span><span style="color: #c678dd;">print</span>(housing.shape)
<span class="linenr">3: </span><span style="color: #c678dd;">print</span>(housing.iloc[<span style="color: #da8548; font-weight: bold;">0</span>])
</pre>
</div>

<pre class="example" id="org056d0af">
&lt;class 'pandas.core.frame.DataFrame'&gt;
(506, 14)
crim         0.00632
zn          18.00000
indus        2.31000
chas         0.00000
nox          0.53800
rm           6.57500
age         65.20000
dis          4.09000
rad          1.00000
tax        296.00000
ptratio     15.30000
b          396.90000
lstat        4.98000
medv        24.00000
Name: 0, dtype: float64
</pre>

<p>
這個資料集共有506筆資料，前13個為特徵值，最後一個medv為房價。其他特徵值分別代表:<br />
</p>
<ul class="org-ul">
<li>CRIM: 每個城鎮的人均犯罪率<br /></li>
<li>ZN: 佔地超過 25,000 平方英尺的住宅用地比例<br /></li>
<li>INDUS: 每個城鎮的非零售業商業用地比例(工業區)<br /></li>
<li>CHAS: 查爾斯河虛擬變量（= 1 如果地段邊界是河流；否則為 0）<br /></li>
<li>NOX: 一氧化氮濃度（每 1000 萬分之一）<br /></li>
<li>RM: 每套住宅的平均房間數<br /></li>
<li>AGE: 1940 年前建成的自有住宅單位比例<br /></li>
<li>DIS: 到五個波士頓就業中心的加權距離<br /></li>
<li>RAD: 徑向公路可達性指數<br /></li>
<li>TAX: 每 $10,000 的全價房產稅率<br /></li>
<li>PTRATIO: 每個城鎮的師生比例<br /></li>
<li>B: 1000(Bk - 0.63)^2，其中 Bk 是每個城鎮的黑人比例<br /></li>
<li>LSTAT: 低社會地位人口的百分比<br /></li>
<li>MEDV: 房值（以 $1000 為單位）<br /></li>
</ul>
</div>
</div>
<div id="outline-container-orgec5cff6" class="outline-3">
<h3 id="orgec5cff6"><span class="section-number-3">4.3.</span> 資料預處理</h3>
<div class="outline-text-3" id="text-4-3">
</div>
<div id="outline-container-orgfdc9d6e" class="outline-4">
<h4 id="orgfdc9d6e"><span class="section-number-4">4.3.1.</span> 處理缺漏值</h4>
<div class="outline-text-4" id="text-4-3-1">
<p>
快速檢查是否有缺漏值<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #c678dd;">print</span>(housing.isnull().<span style="color: #c678dd;">sum</span>())
</pre>
</div>
<pre class="example" id="org4dc5441">
crim        0
zn          0
indus       0
chas        0
nox         0
rm          0
age         0
dis         0
rad         0
tax         0
ptratio     0
b           0
lstat       0
medv       54
dtype: int64
</pre>
<p>
刪掉有缺失值的資料<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>housing.dropna(axis=<span style="color: #da8548; font-weight: bold;">0</span>, inplace=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">2: </span><span style="color: #c678dd;">print</span>(housing.isnull().<span style="color: #c678dd;">sum</span>())
<span class="linenr">3: </span><span style="color: #c678dd;">print</span>(housing.shape)
</pre>
</div>

<pre class="example" id="orgcac6748">
crim       0
zn         0
indus      0
chas       0
nox        0
rm         0
age        0
dis        0
rad        0
tax        0
ptratio    0
b          0
lstat      0
medv       0
dtype: int64
(452, 14)
</pre>
</div>
</div>
<div id="outline-container-orgd0b1e43" class="outline-4">
<h4 id="orgd0b1e43"><span class="section-number-4">4.3.2.</span> 資料標準化</h4>
<div class="outline-text-4" id="text-4-3-2">
<p>
由第一筆訓練資料特徵housing.iloc[0]可以看出，每項特徵值的差異甚大，我們可以先對這些資料特徵進行標準化：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #c678dd;">print</span>(housing.iloc[<span style="color: #da8548; font-weight: bold;">0</span>,:-<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">2: </span>
<span class="linenr">3: </span><span style="color: #dcaeea;">mean</span> = housing.iloc[:,:-<span style="color: #da8548; font-weight: bold;">1</span>].mean(axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">4: </span>housing.<span style="color: #dcaeea;">iloc</span>[:,:-<span style="color: #da8548; font-weight: bold;">1</span>] -= mean
<span class="linenr">5: </span><span style="color: #dcaeea;">std</span> = housing.iloc[:,:-<span style="color: #da8548; font-weight: bold;">1</span>].std(axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">6: </span>housing.<span style="color: #dcaeea;">iloc</span>[:,:-<span style="color: #da8548; font-weight: bold;">1</span>] /= std
<span class="linenr">7: </span>
<span class="linenr">8: </span><span style="color: #c678dd;">print</span>(housing.iloc[<span style="color: #da8548; font-weight: bold;">0</span>,:-<span style="color: #da8548; font-weight: bold;">1</span>])
</pre>
</div>

<pre class="example" id="org2a64c58">
crim         0.00632
zn          18.00000
indus        2.31000
chas         0.00000
nox          0.53800
rm           6.57500
age         65.20000
dis          4.09000
rad          1.00000
tax        296.00000
ptratio     15.30000
b          396.90000
lstat        4.98000
Name: 0, dtype: float64
&lt;string&gt;:5: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0     -6.823009
1     -5.823009
2     -5.823009
3     -4.823009
4     -4.823009
         ...
501   -6.823009
502   -6.823009
503   -6.823009
504   -6.823009
505   -6.823009
Name: rad, Length: 452, dtype: float64' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.
crim      -0.566733
zn         0.217000
indus     -1.176220
chas      -0.289391
nox       -0.024739
rm         0.347120
age       -0.012727
dis        0.022210
rad       -0.904489
tax       -0.538187
ptratio   -1.339563
b          0.394920
lstat     -1.049614
Name: 0, dtype: float64
</pre>
</div>
</div>
</div>
<div id="outline-container-org0e251da" class="outline-3">
<h3 id="org0e251da"><span class="section-number-3">4.4.</span> 觀察資料</h3>
<div class="outline-text-3" id="text-4-4">
</div>
<div id="outline-container-org09e28f0" class="outline-4">
<h4 id="org09e28f0"><span class="section-number-4">4.4.1.</span> 初步看一下房價的分佈</h4>
<div class="outline-text-4" id="text-4-4-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">2: </span><span style="color: #51afef;">import</span> seaborn <span style="color: #51afef;">as</span> sns
<span class="linenr">3: </span>
<span class="linenr">4: </span>sns.histplot(housing[<span style="color: #98be65;">'medv'</span>])
<span class="linenr">5: </span>plt.savefig(<span style="color: #98be65;">"images/housing-price.png"</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>


<div id="orgc389d45" class="figure">
<p><img src="images/housing-price.png" alt="housing-price.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 10: </span>房價分佈概況</p>
</div>
</div>
</div>
<div id="outline-container-org43352d3" class="outline-4">
<h4 id="org43352d3"><span class="section-number-4">4.4.2.</span> 各特徵值間的關係</h4>
<div class="outline-text-4" id="text-4-4-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">correlation_matrix</span> = housing.corr().<span style="color: #c678dd;">round</span>(<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">2: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">annot = True &#35731;&#25105;&#20497;&#21487;&#20197;&#25226;&#25976;&#23383;&#27161;&#36914;&#27599;&#20491;&#26684;&#23376;&#35041;</span>
<span class="linenr">3: </span>sns.heatmap(data=correlation_matrix, annot = <span style="color: #a9a1e1;">True</span>)
<span class="linenr">4: </span>plt.savefig(<span style="color: #98be65;">"images/housing-corr.png"</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>

<div id="org443e843" class="figure">
<p><img src="images/housing-corr.png" alt="housing-corr.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 11: </span>特徵值間的闗係</p>
</div>

<p>
由圖<a href="#org443e843">11</a>可以看出：<br />
</p>
<ul class="org-ul">
<li>跟MEDV（房價）高度相關的是LSTAT（中低收入戶佔當地居住人口的比例）和RM（房子有幾間房間）這兩個變數。<br /></li>
<li>此外也看到DIS（到波士頓商業中心的距離）和AGE（屋齡），INDUS（非零售業土地使用比例）和ZN（居住使用土地比例）這兩組變數有多元共線性問題，所以未來如果要做其他模型，避免同時使用這兩組中的變數。<br /></li>
</ul>

<p>
所以我們 <b>直覺的想法</b> 是：應該可以用LSTAT和RM來做出預測MEDV的模型。再次把這兩個變數跟房價變數的關係畫出來，可以看到兩者和房價變數都接近線性關係：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35373;&#23450;&#25972;&#24373;&#22294;&#30340;&#38263;&#23532;</span>
<span class="linenr"> 2: </span>plt.figure(figsize=(<span style="color: #da8548; font-weight: bold;">20</span>, <span style="color: #da8548; font-weight: bold;">10</span>))
<span class="linenr"> 3: </span><span style="color: #dcaeea;">features</span> = [<span style="color: #98be65;">'lstat'</span>, <span style="color: #98be65;">'rm'</span>]
<span class="linenr"> 4: </span><span style="color: #dcaeea;">target</span> = housing[<span style="color: #98be65;">'medv'</span>]
<span class="linenr"> 5: </span><span style="color: #51afef;">for</span> i, col <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(features):
<span class="linenr"> 6: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25490;&#29256;1 row, 2 columns, nth plot&#65306;&#22312;jupyter notebook&#19978;&#20841;&#24373;&#20006;&#25490;</span>
<span class="linenr"> 7: </span>    plt.subplot(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(features) , i+<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 8: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">add data column into plot</span>
<span class="linenr"> 9: </span>    <span style="color: #dcaeea;">x</span> = housing[col]
<span class="linenr">10: </span>    <span style="color: #dcaeea;">y</span> = target
<span class="linenr">11: </span>    plt.scatter(x, y, marker=<span style="color: #98be65;">'o'</span>)
<span class="linenr">12: </span>    plt.title(col)
<span class="linenr">13: </span>    plt.xlabel(col)
<span class="linenr">14: </span>    plt.ylabel(<span style="color: #98be65;">'medv'</span>)
<span class="linenr">15: </span>plt.savefig(<span style="color: #98be65;">'images/housing-2var.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>


<div id="orgec5961b" class="figure">
<p><img src="images/housing-2var.png" alt="housing-2var.png" width="600" /><br />
</p>
<p><span class="figure-number">Figure 12: </span>LSTAT和RM與房價的關係</p>
</div>
</div>
</div>
<div id="outline-container-org9eaeca7" class="outline-4">
<h4 id="org9eaeca7"><span class="section-number-4">4.4.3.</span> 準備訓練用的資料</h4>
<div class="outline-text-4" id="text-4-4-3">
<p>
先拿兩項特徵值來試一下水溫: lstat和rm<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">2: </span><span style="color: #dcaeea;">X</span> = housing[[<span style="color: #98be65;">'lstat'</span>, <span style="color: #98be65;">'rm'</span>]]
<span class="linenr">3: </span><span style="color: #dcaeea;">Y</span> = housing[<span style="color: #98be65;">'medv'</span>]
<span class="linenr">4: </span><span style="color: #c678dd;">print</span>(X)
<span class="linenr">5: </span><span style="color: #c678dd;">print</span>(Y)
</pre>
</div>

<pre class="example" id="org216de1a">
        lstat        rm
0   -1.049614  0.347120
1   -0.373898  0.116169
2   -1.203924  1.261927
3   -1.380974  0.981486
4   -0.992763  1.204939
..        ...       ...
501 -0.287809  0.374115
502 -0.383644 -0.335236
503 -0.942409  0.948493
504 -0.805966  0.675551
505 -0.578562 -0.470207

[452 rows x 2 columns]
0      24.0
1      21.6
2      34.7
3      33.4
4      36.2
       ...
501    22.4
502    20.6
503    23.9
504    22.0
505    11.9
Name: medv, Length: 452, dtype: float64
</pre>
</div>
</div>
</div>
<div id="outline-container-org94a6136" class="outline-3">
<h3 id="org94a6136"><span class="section-number-3">4.5.</span> 分割訓練集與測試集</h3>
<div class="outline-text-3" id="text-4-5">
<p>
訓練集佔80%、測試集佔20%<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">train_test_split</span>
<span class="linenr">2: </span><span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr">3: </span><span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">Y_train</span>, <span style="color: #dcaeea;">Y_test</span> = train_test_split(X, Y, test_size = <span style="color: #da8548; font-weight: bold;">0.2</span>, random_state=<span style="color: #da8548; font-weight: bold;">5</span>)
<span class="linenr">4: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20877;&#29992;.shape&#30475;&#20999;&#20986;&#20358;&#30340;&#36039;&#26009;&#30340;&#38263;&#30456;&#65288;&#21015;, &#27396;&#65289;</span>
<span class="linenr">5: </span><span style="color: #c678dd;">print</span>(X_train.shape)
<span class="linenr">6: </span><span style="color: #c678dd;">print</span>(X_test.shape)
<span class="linenr">7: </span><span style="color: #c678dd;">print</span>(Y_train.shape)
<span class="linenr">8: </span><span style="color: #c678dd;">print</span>(Y_test.shape)
</pre>
</div>

<pre class="example">
(361, 2)
(91, 2)
(361,)
(91,)
</pre>
</div>
</div>
<div id="outline-container-orgd00aa43" class="outline-3">
<h3 id="orgd00aa43"><span class="section-number-3">4.6.</span> 建立模型</h3>
<div class="outline-text-3" id="text-4-6">
<p>
new出一個LinearRegression的物件後，用特徵變數的訓練資料和目標變數的訓練資料產生一個模型。接著將特徵變數的測試資料倒進這個新產生的模型當中，得到預測的目標變數資料<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Modeling</span>
<span class="linenr">2: </span><span style="color: #51afef;">from</span> sklearn.linear_model <span style="color: #51afef;">import</span> LinearRegression
<span class="linenr">3: </span><span style="color: #dcaeea;">reg</span> = LinearRegression()<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23416;&#32722;/&#35347;&#32244;Fitting linear model</span>
<span class="linenr">4: </span>reg.fit(X_train,Y_train)
</pre>
</div>
</div>
</div>
<div id="outline-container-org29cef4a" class="outline-3">
<h3 id="org29cef4a"><span class="section-number-3">4.7.</span> 測試效能</h3>
<div class="outline-text-3" id="text-4-7">
<p>
將這個預測的目標變數資料（預測結果）和目標變數的測試資料（真實結果）做R2-score：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38928;&#28204;&#32080;&#26524;Predicting using the linear model</span>
<span class="linenr">2: </span>reg.predict(X_test)<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30495;&#23526;&#32080;&#26524;&#65306;Y_test# &#28204;&#35430;&#28310;&#30906;&#24230;&#65306;</span>
<span class="linenr">3: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'R2:'</span>, reg.score(X_test, Y_test))
</pre>
</div>

<pre class="example">
R2: 0.6048366146231109
</pre>

<p>
得到的這個R2-score讓我們可以知道特徵變數對於目標變數的解釋程度為何，而越接近1代表越準確。這裡大約是66%，解釋程度算是相當好的<sup><a id="fnr.2.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>。<br />
</p>
</div>
<div id="outline-container-orge1a9c57" class="outline-4">
<h4 id="orge1a9c57"><span class="section-number-4">4.7.1.</span> 模型效能視覺化</h4>
<div class="outline-text-4" id="text-4-7-1">
<p>
把剛剛的預測的目標變數資料和測試的目標變數資料畫成散佈圖<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">plotting the y_test vs y_pred</span>
<span class="linenr"> 2: </span><span style="color: #dcaeea;">Y_pred</span> = reg.predict(X_test)
<span class="linenr"> 3: </span>plt.cla()
<span class="linenr"> 4: </span>plt.tight_layout()
<span class="linenr"> 5: </span>plt.figure(figsize=(<span style="color: #da8548; font-weight: bold;">10</span>,<span style="color: #da8548; font-weight: bold;">8</span>))
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>plt.scatter(Y_pred, Y_test)
<span class="linenr"> 8: </span>plt.xlabel(<span style="color: #98be65;">'Y_pred'</span>)
<span class="linenr"> 9: </span>plt.ylabel(<span style="color: #98be65;">'Y_test'</span>)
<span class="linenr">10: </span>plt.savefig(<span style="color: #98be65;">'images/boston-perf.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>


<div id="org97368f6" class="figure">
<p><img src="images/boston-perf.png" alt="boston-perf.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 13: </span>Caption</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org9868f2f" class="outline-3">
<h3 id="org9868f2f"><span class="section-number-3">4.8.</span> 找出線性模型</h3>
<div class="outline-text-3" id="text-4-8">
<p>
由LinearRegression()找出線性模型的intercept和coefficient<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'intercept:'</span>,reg.intercept_)
<span class="linenr">2: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'coefficient::'</span>,reg.coef_)
<span class="linenr">3: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'lstat:'</span>,reg.coef_[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">4: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'rm:'</span>,reg.coef_[<span style="color: #da8548; font-weight: bold;">1</span>])
</pre>
</div>

<pre class="example">
intercept: 23.662167506495486
coefficient:: [-3.2284783   4.66331239]
lstat: -3.228478297753095
rm: 4.663312387946355
</pre>

<p>
線性模型為：\(medv=23.66 + -3.22 \times lstat + 4.66 \times rm + error\)<br />
</p>
</div>
</div>
</div>
<div id="outline-container-org41b9dd4" class="outline-2">
<h2 id="org41b9dd4"><span class="section-number-2">5.</span> 關於迴歸模型的特徵選擇</h2>
<div class="outline-text-2" id="text-5">
<p>
在進行迴歸分析時，我們可以選擇不同的特徵來建立模型，這裡介紹兩種方法：<br />
</p>
<ol class="org-ol">
<li>前向選擇法(Forward Selection)<br /></li>
<li>後向選擇法(Backward Selection)<br /></li>
</ol>
</div>
<div id="outline-container-org7b53428" class="outline-3">
<h3 id="org7b53428"><span class="section-number-3">5.1.</span> 前向選擇法(Forward Selection)</h3>
<div class="outline-text-3" id="text-5-1">
<p>
前向選擇法是一種逐步選擇特徵的方法，它從空模型開始，然後逐步添加特徵，直到達到某個標準為止。這種方法的優點是可以減少過擬合的風險，但是需要注意的是，如果特徵之間存在 <b>多重共線性(Multicollinearity)</b> ，則可能會導致模型不穩定。<br />
</p>
</div>
</div>
<div id="outline-container-org801eecd" class="outline-3">
<h3 id="org801eecd"><span class="section-number-3">5.2.</span> 後向選擇法(Backward Selection)</h3>
<div class="outline-text-3" id="text-5-2">
<p>
後向選擇法是一種逐步刪除特徵的方法，它從包含所有特徵的模型開始，然後逐步刪除特徵，直到達到某個標準為止。這種方法的優點是可以減少過擬合的風險，但是需要注意的是，如果特徵之間存在多重共線性，則可能會導致模型不穩定。<br />
</p>
</div>
</div>
<div id="outline-container-orgdafd9c3" class="outline-3">
<h3 id="orgdafd9c3"><span class="section-number-3">5.3.</span> 特徵選擇的注意事項</h3>
<div class="outline-text-3" id="text-5-3">
<p>
在進行特徵選擇時，需要注意以下幾點：<br />
</p>
</div>
<div id="outline-container-org0cc81e3" class="outline-4">
<h4 id="org0cc81e3"><span class="section-number-4">5.3.1.</span> 特徵之間的相關性</h4>
<div class="outline-text-4" id="text-5-3-1">
<p>
特徵之間的相關性可能會導致模型不穩定，因此在進行特徵選擇時，需要注意特徵之間的相關性，避免選擇相關性較高的特徵。<br />
</p>

<p>
以lstat, rm, indus, ptratio這四個特徵為例，我們可以使用 皮爾遜相關係數（Pearson Correlation） 來衡量 lstat 與 rm、indus、ptratio 的線性關係。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> seaborn <span style="color: #51afef;">as</span> sns
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35336;&#31639; lstat &#33287;&#20854;&#20182;&#35722;&#25976;&#30340;&#30456;&#38364;&#24615;</span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">lstat_corr</span> = housing[[<span style="color: #98be65;">'lstat'</span>, <span style="color: #98be65;">'rm'</span>, <span style="color: #98be65;">'indus'</span>, <span style="color: #98be65;">'ptratio'</span>]].corr()
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#30456;&#38364;&#20418;&#25976;&#34920;</span>
<span class="linenr"> 8: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"lstat &#33287;&#20854;&#20182;&#35722;&#25976;&#30340;&#30456;&#38364;&#24615;&#65306;"</span>)
<span class="linenr"> 9: </span><span style="color: #c678dd;">print</span>(lstat_corr)
<span class="linenr">10: </span>
<span class="linenr">11: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32362;&#35069;&#30456;&#38364;&#20418;&#25976;&#29105;&#22294;</span>
<span class="linenr">12: </span>plt.figure(figsize=(<span style="color: #da8548; font-weight: bold;">8</span>, <span style="color: #da8548; font-weight: bold;">6</span>))
<span class="linenr">13: </span>sns.heatmap(lstat_corr, annot=<span style="color: #a9a1e1;">True</span>, cmap=<span style="color: #98be65;">"coolwarm"</span>, fmt=<span style="color: #98be65;">".2f"</span>)
<span class="linenr">14: </span>plt.title(<span style="color: #98be65;">"lstat correlation with other variables"</span>)
<span class="linenr">15: </span>plt.savefig(<span style="color: #98be65;">'images/lstat-corr.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">16: </span>
</pre>
</div>

<pre class="example">
lstat 與其他變數的相關性：
            lstat        rm     indus   ptratio
lstat    1.000000 -0.607289  0.565402  0.303043
rm      -0.607289  1.000000 -0.364895 -0.334164
indus    0.565402 -0.364895  1.000000  0.317336
ptratio  0.303043 -0.334164  0.317336  1.000000
</pre>


<div id="org47f9a07" class="figure">
<p><img src="images/lstat-corr.png" alt="lstat-corr.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 14: </span>lstat 與其他變數的相關性</p>
</div>
</div>
</div>
<div id="outline-container-orgf467d59" class="outline-4">
<h4 id="orgf467d59"><span class="section-number-4">5.3.2.</span> 如何解讀相關性？</h4>
<div class="outline-text-4" id="text-5-3-2">
<p>
相關係數範圍：-1（完全負相關）到 +1（完全正相關）<br />
</p>
<ul class="org-ul">
<li>|r| &gt; 0.8：高度相關（可能有多重共線性）<br /></li>
<li>0.5 &lt; |r| &lt; 0.8：中等相關（可能需要進一步檢查）<br /></li>
<li>|r| &lt; 0.5：低相關（通常不會造成共線性問題）<br /></li>
</ul>
</div>
</div>
<div id="outline-container-org5c6efab" class="outline-4">
<h4 id="org5c6efab"><span class="section-number-4">5.3.3.</span> 為什麼變數之間的共變（高相關性）會影響迴歸結果？</h4>
<div class="outline-text-4" id="text-5-3-3">
<ol class="org-ol">
<li><p>
變數之間高共變影響迴歸模型<br />
當兩個變數彼此高度相關時（例如 rm 和 lstat 相關性 -0.73）：<br />
</p>

<p>
這兩個變數都能解釋目標變數 MEDV（房價）。<br />
但因為它們彼此也高度相關，回歸模型無法區分到底是哪個變數真正影響房價，所以模型可能會：<br />
</p>
<ul class="org-ul">
<li>讓其中一個變數的迴歸係數變得極端大或極端小，以補償另一個變數。<br /></li>
<li>係數的正負號可能與直覺相反。<br /></li>
<li>在不同的訓練集上，模型的回歸係數可能會大幅波動，導致 泛化能力下降。<br /></li>
</ul></li>
<li><p>
迴歸係數的不穩定性<br />
舉個例子，假設你的房價預測模型中有兩個變數：<br />
\(x_1\) = 房屋面積（平方公尺）<br />
\(x_2\) = 房間數（通常與房屋面積高度相關）<br />
</p>

<p>
回歸模型可能學到：<br />
</p>
<ul class="org-ul">
<li>MEDV=5000×\(x_1\)+10000×\(x_2\)+20000<br /></li>
<li>MEDV=5000×\(x_1\)+10000×\(x_2\)+20000<br /></li>
</ul>

<p>
但如果  \(x_1\)  和 \(x_2\) 幾乎完全相關，模型可能學到：<br />
</p>
<ul class="org-ul">
<li>MEDV=8000×\(x_1\)+2000×\(x_2\)+20000<br /></li>
<li>MEDV=8000×\(x_1\)+2000×\(x_2\)+20000<br /></li>
</ul>

<p>
或者：<br />
</p>
<ul class="org-ul">
<li>MEDV=3000×\(x_1\)+15000×\(x_2\)+20000<br /></li>
<li>MEDV=3000×\(x_1\)+15000×\(x_2\)+20000<br /></li>
</ul>

<p>
這代表：房價可以被正確預測，但\(x_1\)和 \(x_2\) 的係數變動很大，影響可解釋性。<br />
</p>
<ul class="org-ul">
<li>如果我們想解釋房價是因為房屋面積還是因為房間數變動，這時候我們就無法下結論。<br /></li>
<li>如果用這個模型來預測新的房子，可能會產生不穩定的結果。<br /></li>
</ul></li>
<li><p>
多重共線性會影響模型泛化能力<br />
</p>
<ul class="org-ul">
<li>假設我們的模型發現： \(x_1\) （房屋面積）和 \(x_2\)（房間數）高度相關<br /></li>
<li>由於數據集的特性，模型可能學到：<br />
<ul class="org-ul">
<li>\(x_1\)影響比較大，\(x_2\) 影響比較小<br /></li>
<li>但換一組數據集時，模型可能學到 \(x_2\) 影響比較大，\(x_1\)影響比較小<br /></li>
</ul></li>
</ul>
<p>
這會導致：<br />
</p>
<ul class="org-ul">
<li>在不同的測試集上，模型預測結果不穩定。<br /></li>
<li>當我們遇到新數據時，模型的預測結果可能變化很大。<br /></li>
</ul></li>
</ol>
</div>
</div>
<div id="outline-container-org083949a" class="outline-4">
<h4 id="org083949a"><span class="section-number-4">5.3.4.</span> 如何解決？</h4>
<div class="outline-text-4" id="text-5-3-4">
</div>
<div id="outline-container-org61df6cd" class="outline-5">
<h5 id="org61df6cd">方法 1：刪除其中一個變數</h5>
<div class="outline-text-5" id="text-org61df6cd">
<p>
如果 \(x_1\) 和 \(x_2\) 高度相關（相關係數 &gt; 0.8），可以考慮刪除其中一個。例如：<br />
</p>
<ul class="org-ul">
<li>刪除 \(x_2\)（房間數），只保留 \(x_1\)（房屋面積）。<br /></li>
<li>選擇一個對 MEDV 影響更大的變數（透過 corr() 和 VIF 來選擇）。<br /></li>
</ul>
</div>
</div>
<div id="outline-container-orgdfbd524" class="outline-5">
<h5 id="orgdfbd524">方法 2：合併變數</h5>
<div class="outline-text-5" id="text-orgdfbd524">
<p>
有時候可以建立一個新變數，代表這些高度相關變數的組合資訊。例如：<br />
</p>
<ul class="org-ul">
<li>用「每平方公尺的房間數」(\(x_2\) / \(x_1\) 來替代 \(x_1\) 和 \(x_2\)，這樣就避免兩個變數的高度相關性。<br /></li>
</ul>
</div>
</div>
<div id="outline-container-org43219ac" class="outline-5">
<h5 id="org43219ac">方法 3：使用 Ridge Regression（L2 正則化）</h5>
<div class="outline-text-5" id="text-org43219ac">
<ul class="org-ul">
<li>Ridge 迴歸會對回歸係數加上一個懲罰，使它們不會變得過大，從而減少共線性的影響。<br /></li>
<li>適合當 VIF 在 5~10 之間的變數，因為它不會直接移除變數，而是調整其影響力。<br /></li>
</ul>
</div>
</div>
<div id="outline-container-orge11e620" class="outline-5">
<h5 id="orge11e620">方法 4：使用 PCA（主成分分析）降維</h5>
<div class="outline-text-5" id="text-orge11e620">
<ul class="org-ul">
<li>透過 PCA 將高度相關的變數轉換為不相關的新特徵（如 PC1、PC2）。<br /></li>
<li>這樣可以讓回歸模型使用的變數之間不會有多重共線性問題。<br /></li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-orgb1b4610" class="outline-3">
<h3 id="orgb1b4610"><span class="section-number-3">5.4.</span> 多重共線性(Multicollinearity)</h3>
<div class="outline-text-3" id="text-5-4">
<p>
多重共線性是指特徵之間存在高度相關性的情況，這種情況下，模型的參數估計可能會變得不穩定，並且可能會導致模型的預測能力下降。因此，在進行特徵選擇時，需要注意特徵之間的相關性，避免多重共線性的問題。<br />
</p>

<p>
當我們檢查 lstat（低收入族群比例）與其他變數的相關性時，指的是 lstat 與 其餘三個變數 (rm, indus, ptratio) 的 共變關係（即相關係數, Correlation Coefficient）。<br />
</p>
</div>
<div id="outline-container-orgfc4596e" class="outline-4">
<h4 id="orgfc4596e"><span class="section-number-4">5.4.1.</span> 檢查多重共線性（VIF, Variance Inflation Factor）</h4>
<div class="outline-text-4" id="text-5-4-1">
<ul class="org-ul">
<li>若兩個或多個變數之間的相關性過高，可能會影響迴歸模型的穩定性。<br /></li>
<li>VIF &gt; 5 通常表示該變數與其他變數的相關性過高，應考慮移除。<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span><span style="color: #51afef;">import</span> seaborn <span style="color: #51afef;">as</span> sns
<span class="linenr"> 4: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 5: </span><span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr"> 6: </span><span style="color: #51afef;">from</span> statsmodels.stats.outliers_influence <span style="color: #51afef;">import</span> variance_inflation_factor
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#21462; Boston Housing &#36039;&#26009;</span>
<span class="linenr"> 9: </span><span style="color: #dcaeea;">housing</span> = pd.read_csv(<span style="color: #98be65;">'https://raw.githubusercontent.com/letranger/AI/gh-pages/Downloads/boston_housing.csv'</span>)
<span class="linenr">10: </span>
<span class="linenr">11: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30446;&#27161;&#35722;&#25976;</span>
<span class="linenr">12: </span><span style="color: #dcaeea;">target_column</span> = <span style="color: #98be65;">'medv'</span>
<span class="linenr">13: </span>
<span class="linenr">14: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35336;&#31639;&#30456;&#38364;&#20418;&#25976;</span>
<span class="linenr">15: </span><span style="color: #dcaeea;">correlation_matrix</span> = housing.corr()
<span class="linenr">16: </span>
<span class="linenr">17: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21482;&#39023;&#31034;&#33287;&#30446;&#27161;&#35722;&#25976;&#30340;&#30456;&#38364;&#24615;</span>
<span class="linenr">18: </span><span style="color: #dcaeea;">corr_target</span> = correlation_matrix[target_column].drop(target_column).sort_values(ascending=<span style="color: #a9a1e1;">False</span>)
<span class="linenr">19: </span>
<span class="linenr">20: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35373;&#23450;&#30456;&#38364;&#24615;&#38334;&#20540;&#65288;&#20363;&#22914;&#65306;0.4 &#20197;&#19978;&#28858;&#36611;&#24375;&#30456;&#38364;&#65289;</span>
<span class="linenr">21: </span><span style="color: #dcaeea;">threshold</span> = <span style="color: #da8548; font-weight: bold;">0.4</span>
<span class="linenr">22: </span><span style="color: #dcaeea;">selected_features</span> = corr_target[<span style="color: #c678dd;">abs</span>(corr_target) &gt; threshold].index.tolist()
<span class="linenr">23: </span>
<span class="linenr">24: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36942;&#28670;&#24460;&#30340;&#29305;&#24501;</span>
<span class="linenr">25: </span><span style="color: #dcaeea;">filtered_data</span> = housing[selected_features + [target_column]]
<span class="linenr">26: </span>
<span class="linenr">27: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#30456;&#38364;&#24615;&#36611;&#39640;&#30340;&#29305;&#24501;</span>
<span class="linenr">28: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#33287;&#30446;&#27161;&#35722;&#25976;&#30456;&#38364;&#24615;&#36611;&#39640;&#30340;&#29305;&#24501;&#65306;"</span>)
<span class="linenr">29: </span><span style="color: #c678dd;">print</span>(filtered_data.head())
<span class="linenr">30: </span>
<span class="linenr">31: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#20986;&#30456;&#38364;&#20418;&#25976;&#29105;&#22294;</span>
<span class="linenr">32: </span>plt.figure(figsize=(<span style="color: #da8548; font-weight: bold;">10</span>, <span style="color: #da8548; font-weight: bold;">6</span>))
<span class="linenr">33: </span>sns.heatmap(filtered_data.corr(), annot=<span style="color: #a9a1e1;">True</span>, cmap=<span style="color: #98be65;">"coolwarm"</span>, fmt=<span style="color: #98be65;">".2f"</span>)
<span class="linenr">34: </span>plt.title(<span style="color: #98be65;">"The correlation heatmap of features related to MEDV"</span>)
<span class="linenr">35: </span>plt.savefig(<span style="color: #98be65;">'images/boston-heatmap.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">36: </span>
<span class="linenr">37: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35336;&#31639; VIF&#65288;&#35722;&#30064;&#33192;&#33081;&#22240;&#23376;&#65289;</span>
<span class="linenr">38: </span><span style="color: #dcaeea;">X</span> = housing[selected_features]
<span class="linenr">39: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">vif_data = pd.DataFrame()</span>
<span class="linenr">40: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">vif_data["Feature"] = X.columns</span>
<span class="linenr">41: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]</span>
<span class="linenr">42: </span>
<span class="linenr">43: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24314;&#31435; VIF DataFrame(&#31777;&#26131;&#29256;)</span>
<span class="linenr">44: </span><span style="color: #dcaeea;">vif_data</span> = pd.DataFrame()
<span class="linenr">45: </span><span style="color: #dcaeea;">vif_data</span>[<span style="color: #98be65;">"Feature"</span>] = X.columns  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35352;&#37636;&#35722;&#25976;&#21517;&#31281;</span>
<span class="linenr">46: </span><span style="color: #dcaeea;">vif_values</span> = []  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23384;&#25918;&#27599;&#20491;&#35722;&#25976;&#30340; VIF &#20540;</span>
<span class="linenr">47: </span>
<span class="linenr">48: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20351;&#29992; for &#36852;&#22280;&#36880;&#20491;&#35722;&#25976;&#35336;&#31639; VIF</span>
<span class="linenr">49: </span><span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(X.shape[<span style="color: #da8548; font-weight: bold;">1</span>]):
<span class="linenr">50: </span>    <span style="color: #dcaeea;">vif</span> = variance_inflation_factor(X.values, i)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35336;&#31639; VIF</span>
<span class="linenr">51: </span>    vif_values.append(vif)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21152;&#20837;&#21015;&#34920;</span>
<span class="linenr">52: </span>
<span class="linenr">53: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559; VIF &#20540;&#23384;&#20837; DataFrame</span>
<span class="linenr">54: </span><span style="color: #dcaeea;">vif_data</span>[<span style="color: #98be65;">"VIF"</span>] = vif_values
<span class="linenr">55: </span>
<span class="linenr">56: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034; VIF &#32080;&#26524;</span>
<span class="linenr">57: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"</span><span style="color: #a9a1e1;">\n</span><span style="color: #98be65;">&#35722;&#30064;&#33192;&#33081;&#22240;&#23376;&#65288;VIF&#65289;&#27298;&#28204;&#32080;&#26524;&#65306;"</span>)
<span class="linenr">58: </span><span style="color: #c678dd;">print</span>(vif_data)
</pre>
</div>

<pre class="example" id="orgfdd247f">
與目標變數相關性較高的特徵：
      rm  indus  ptratio  lstat  medv
0  6.575   2.31     15.3   4.98  24.0
1  6.421   7.07     17.8   9.14  21.6
2  7.185   7.07     17.8   4.03  34.7
3  6.998   2.18     18.7   2.94  33.4
4  7.147   2.18     18.7   5.33  36.2

變異膨脹因子（VIF）檢測結果：
   Feature       VIF
0       rm  4.022386
1    indus  4.063422
2  ptratio  3.545233
3    lstat  5.000509
</pre>

<div id="org17c1848" class="figure">
<p><img src="images/boston-heatmap.png" alt="boston-heatmap.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 15: </span>Caption</p>
</div>
</div>
</div>
<div id="outline-container-orgb80967a" class="outline-4">
<h4 id="orgb80967a"><span class="section-number-4">5.4.2.</span> 如何判斷 VIF 是否過高？</h4>
<div class="outline-text-4" id="text-5-4-2">
<ul class="org-ul">
<li>VIF &lt; 5：通常可以接受，代表該變數與其他變數的相關性不高。<br /></li>
<li>VIF 5~10：表示有些許多重共線性，建議進一步檢查，但不一定要刪除變數。<br /></li>
<li>VIF &gt; 10：代表嚴重的多重共線性，應該考慮移除或合併變數。<br /></li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-orgb247da7" class="outline-2">
<h2 id="orgb247da7"><span class="section-number-2">6.</span> [作業]依據期中考成績預測期末考成績&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-orgca2b7eb" class="outline-3">
<h3 id="orgca2b7eb"><span class="section-number-3">6.1.</span> Data</h3>
<div class="outline-text-3" id="text-6-1">
<ul class="org-ul">
<li>線上資料: <a href="https://letranger.github.io/AI/2023A-CS-Data.csv">https://letranger.github.io/AI/Downloads/PythonScores.csv</a><br /></li>
<li>資料中有424筆記錄，每筆記錄分別為學生的<br />
<ol class="org-ol">
<li>id: 學號<br /></li>
<li>class: 平時成績<br /></li>
<li>task: 作業成績<br /></li>
<li>mid: 期中考成績<br /></li>
<li>final: 期末考成績<br /></li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-orgacae913" class="outline-3">
<h3 id="orgacae913"><span class="section-number-3">6.2.</span> Task</h3>
<div class="outline-text-3" id="text-6-2">
<p>
你的任務是建立一個模型，輸入一個或多個特徵值(class, task, mid)來預測期末考成績(final)，其他相關任務包括:<br />
</p>
<ol class="org-ol">
<li>部份學生的期中、期末考有缺考行為，請將這些缺考記錄填入0分<br /></li>
<li>畫出所有特徵資料的分佈狀況(直方圖)<br /></li>
<li>將所有分數間的相關以視覺化方式表現出來<br /></li>
<li>將資料集分割為訓練集(70%)及測試集(30%)<br /></li>
<li>請自行決定你要用多少個特徵值來預測，並以測試集來評估模型效能，輸出分數(R2-score)<br /></li>
<li>列出你找出的模型方程式<br /></li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org0a20d3a" class="outline-2">
<h2 id="org0a20d3a"><span class="section-number-2">7.</span> [卜聖卦活動2]以迴歸預測成績&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></h2>
<div class="outline-text-2" id="text-7">
</div>
<div id="outline-container-org741d46e" class="outline-3">
<h3 id="org741d46e"><span class="section-number-3">7.1.</span> 資料集</h3>
<div class="outline-text-3" id="text-7-1">
<ul class="org-ul">
<li>10Asub1: 高一上英文<br /></li>
<li>10Asub2: 高一上數學<br /></li>
<li>10Asub3: 高一上國文<br /></li>
<li>10BSub1: 高一下英文<br /></li>
<li>10BSub2: 高一下數學<br /></li>
<li>10BSub3: 高一下國文<br /></li>
<li>11ASub1: 高二上英文<br /></li>
<li>11ASub2: 高二上數學<br /></li>
<li>11ASub3: 高二上國文<br /></li>
<li>12ASub1: 高三上英文<br /></li>
<li>12ASub2: 高三上數學<br /></li>
<li>12ASub3: 高三上國文<br /></li>
<li>12BSub1: 高三下英文<br /></li>
<li>12BSub2: 高三下數學<br /></li>
<li>12BSub3: 高三下國文<br /></li>
<li>TestNG3: (學測)自然組國英數三科<br /></li>
<li>TestSG3: (學測)社會組國英數三科<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">2: </span>
<span class="linenr">3: </span><span style="color: #dcaeea;">df</span> = pd.read_csv(<span style="color: #98be65;">'./10Ato11A.csv'</span>)
<span class="linenr">4: </span><span style="color: #c678dd;">print</span>(df.columns)
<span class="linenr">5: </span><span style="color: #c678dd;">print</span>(df.head())
</pre>
</div>

<pre class="example" id="org03a721a">
Index(['10ASub1', '10ASub2', '10ASub3', '10BSub1', '10BSub2', '10BSub3',
       '11ASub1', '11ASub2', '11ASub3', '11BSub1', '11BSub2', '11BSub3'],
      dtype='object')
   10ASub1  10ASub2  10ASub3  10BSub1  ...  11ASub3  11BSub1  11BSub2  11BSub3
0       31       38       61       34  ...       54       41       33       51
1       58       49       63       50  ...       59        4        1        0
2       32       43       68       29  ...       56       39       64       66
3       55       46       74       56  ...       68       65       65       74
4       79       74       82       75  ...       82       65       74       82

[5 rows x 12 columns]
</pre>
</div>
</div>
<div id="outline-container-org7da66da" class="outline-3">
<h3 id="org7da66da"><span class="section-number-3">7.2.</span> 任務：</h3>
<div class="outline-text-3" id="text-7-2">
<ol class="org-ol">
<li>預估二下英文科成績<br /></li>
<li>預估二下數學科成績<br /></li>
<li>預估二下國文科成績<br /></li>
<li>預估學測成績<br /></li>
</ol>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Hands-On Machine Learning with Scikit-Learn: Aurelien Geron<br />
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://medium.com/li-ting-liao-tiffany/python-%E5%BF%AB%E9%80%9F%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-boston-housing%E6%B3%A2%E5%A3%AB%E9%A0%93%E6%88%BF%E5%83%B9-9c535fb7ceb7">What impacts Boston Housing Prices</a><br />
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Yung-Chin Yen</p>
<p class="date">Created: 2025-02-18 Tue 17:59</p>
</div>
</body>
</html>
