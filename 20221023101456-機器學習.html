<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2023-12-03 Sun 21:59 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>機器學習</title>
<meta name="author" content="Yung-Chin Yen" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="../css/muse.css" />
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">機器學習</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org437e05c">1. 機器學習</a>
<ul>
<li><a href="#orga78f22a">1.1. 機器學習簡介影片</a></li>
<li><a href="#orgffcacdf">1.2. 簡介</a></li>
<li><a href="#org405a32d">1.3. 何謂機器學習</a></li>
<li><a href="#orgc52bbe4">1.4. 何謂機器學習</a></li>
</ul>
</li>
<li><a href="#orgd742f4a">2. 機器是如何學習的</a>
<ul>
<li><a href="#org4e9035b">2.1. 傳統程式設計</a></li>
<li><a href="#orgf8ed6cf">2.2. 機器學習的策略</a></li>
<li><a href="#orgdeb1fa6">2.3. 實作1</a></li>
<li><a href="#org7b81a90">2.4. 實作2</a></li>
<li><a href="#org215fb00">2.5. 實作3</a></li>
<li><a href="#org659b64f">2.6. 程式解說</a></li>
</ul>
</li>
<li><a href="#org33bae59">3. 機器學習概念釐清</a>
<ul>
<li><a href="#orgeb404c6">3.1. Machine Learning v.s. Statistics</a></li>
<li><a href="#org4ce6b1b">3.2. Machine Learning v.s. Traditional Programming</a></li>
<li><a href="#orgbbe6490">3.3. Training Phase v.s. Inference Phase</a></li>
</ul>
</li>
<li><a href="#org6fa7933">4. 典型的機器學習MODEL Training</a>
<ul>
<li><a href="#org2d39039">4.1. Process of learning</a></li>
<li><a href="#orgbaa27b5">4.2. Example of training</a></li>
<li><a href="#org7de995d">4.3. Model v.s. Layer: Training step by step</a></li>
<li><a href="#org5eab7fa">4.4. compile v.s. fit</a></li>
</ul>
</li>
<li><a href="#org5eff329">5. 機器學習如何解決問題</a>
<ul>
<li><a href="#org2732507">5.1. 使用機器學習處理問題的幾個步驟：</a></li>
<li><a href="#orgcde8d0e">5.2. 現實生活中的應用</a></li>
</ul>
</li>
<li><a href="#orgde5f3d9">6. 機器學習的類型</a>
<ul>
<li><a href="#org9ed061f">6.1. 監督式學習(Supervised learning)</a></li>
<li><a href="#org4534484">6.2. 非監督式學習(Unsupervised learning)</a></li>
<li><a href="#orge27a3dc">6.3. 增強式學習</a></li>
<li><a href="#org60c14d6">6.4. Batch and Online Learning</a></li>
<li><a href="#org3b6ccb3">6.5. Instance-Based Versus model-Based Learning</a></li>
</ul>
</li>
<li><a href="#org8076c0a">7. Data Representations</a>
<ul>
<li><a href="#orgbdef392">7.1. Images</a></li>
<li><a href="#orgf0b6670">7.2. Words and Documents</a></li>
<li><a href="#orgfa25004">7.3. Yes/No or Ratings</a></li>
<li><a href="#orga376da7">7.4. One-Hot Encodings</a></li>
</ul>
</li>
<li><a href="#org3a14425">8. 機器學習的主要挑戰</a>
<ul>
<li><a href="#org9541efa">8.1. Insufficient quantity of training data</a></li>
<li><a href="#org1c846b4">8.2. Nonrepresentative training data</a></li>
<li><a href="#orgc79f0a0">8.3. Poor-quality data</a></li>
<li><a href="#org65b8d00">8.4. Irrelevant features</a></li>
<li><a href="#org03d2183">8.5. Overfitting the training data</a></li>
<li><a href="#org89b1e29">8.6. Underfitting the training data</a></li>
</ul>
</li>
<li><a href="#org1ee91ac">9. Machine Learning 的基本類型</a></li>
<li><a href="#org120a34d">10. Classfication</a>
<ul>
<li><a href="#org691ca61">10.1. Model construction: describing a set of predetermined classes</a></li>
<li><a href="#org3982615">10.2. Model usage: for classifying future or unknown objects</a></li>
</ul>
</li>
<li><a href="#org2ea22be">11. 學習資源</a>
<ul>
<li><a href="#org263d5fa">11.1. Machine Learning [台大李宏毅]</a></li>
<li><a href="#org5049831">11.2. Digital Speech Processing</a></li>
</ul>
</li>
<li><a href="#org2630463">12. code references</a></li>
</ul>
</div>
</div>
<a href="https://letranger.github.io/AI/20221023101456-機器學習.html"><img align="right" alt="Hits" src="https://hits.sh/letranger.github.io/AI/20221023101456-機器學習.html.svg"/></a>
<div id="outline-container-org437e05c" class="outline-2">
<h2 id="org437e05c"><span class="section-number-2">1.</span> 機器學習</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-orga78f22a" class="outline-3">
<h3 id="orga78f22a"><span class="section-number-3">1.1.</span> 機器學習簡介影片</h3>
<div class="outline-text-3" id="text-1-1">
<p>
<a href="https://www.youtube.com/watch?v=OXNC_sefxi4">机器学习简介（机器学习从零到一第一集)</a><br />
<a href="https://www.youtube.com/watch?v=ifj5bAzrzMw">机器学习中的基本计算机视觉概念（机器学习从零到一第二集）</a><br />
</p>
</div>
</div>
<div id="outline-container-orgffcacdf" class="outline-3">
<h3 id="orgffcacdf"><span class="section-number-3">1.2.</span> 簡介</h3>
<div class="outline-text-3" id="text-1-2">
<ul class="org-ul">
<li>機器學習是<a href="20221023101139-人工智慧.html#ID-20221023T101138.945879">人工智慧</a>的一個分支，在<a href="20221023101139-人工智慧.html#ID-20221023T101138.945879">人工智慧</a>的研究歷史有著一條從以「推理」為重點，到以「知識」為重點，再到以 「學習」為重點的自然、清晰的脈絡。<br /></li>
<li>機器學習是實現<a href="AI-Introduction.html">人工智慧</a>的一個途徑，即以機器學習為手段解決<a href="AI-Introduction.html">人工智慧</a>中的問題。<br /></li>
<li>機器學習理論主要是設計和分析一些讓電腦可以自動「學習」 的演算法。機器學習演算法是一類從資料中自動分析獲得規 律，並利用規律對未知資料進行預測的演算法。<br /></li>
<li>機器學習已廣泛應用於資料探勘、電腦視覺、自然語言處理、<br /></li>
<li>生物特徵辨識、搜尋引擎、醫學診斷、檢測信用卡欺詐、證券市場分析、DNA 序列測序、語音和手寫辨識、戰略遊戲和機器人等領域<br /></li>
<li>機器學習是一門<a href="AI-Introduction.html">人工智慧</a>的科學，該領域的主要研究物件是<a href="AI-Introduction.html">人工智慧</a>，特別是如何=在經驗學習中=改善具體演算法的效能。<br /></li>
<li>機器學習是對能通過經驗=自動改進=的電腦演算法研究<br /></li>
<li>機器學習是用資料或以往的經驗，以此最佳化電 腦程式的效能標準。<br /></li>
</ul>
</div>
</div>
<div id="outline-container-org405a32d" class="outline-3">
<h3 id="org405a32d"><span class="section-number-3">1.3.</span> 何謂機器學習</h3>
<div class="outline-text-3" id="text-1-3">
<ul class="org-ul">
<li>機器學習的定義之一：一個可以與其環境做互動的系統。具有<a href="20221023103538-感知器.html#ID-20221023T103538.640537">感知器</a>(sensor)，可以讓機器了解它們所處的環境，以及它們也具有相關的工具可以讓機器做出回應<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>。<br /></li>
<li>機器學習可以大致分為三類：<a href="20221023101626-監督式學習.html#ID-20221023T101626.420918">監督式學習</a>、<a href="20221023101716-非監督式學習.html#ID-20221023T101716.467694">非監督式學習</a>、<a href="20230210172919-增強式學習.html#ID-0a5c37c0-741a-4a1a-bec7-f98074830132">增強式學習</a>，如圖<a href="#orga2200d7">1</a><sup><a id="fnr.1.100" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>。<br /></li>
</ul>


<div id="orga2200d7" class="figure">
<p><img src="images/MLTypes.png" alt="MLTypes.png" width="700" /><br />
</p>
<p><span class="figure-number">Figure 1: </span>三種不同的機器學習</p>
</div>
</div>
</div>
<div id="outline-container-orgc52bbe4" class="outline-3">
<h3 id="orgc52bbe4"><span class="section-number-3">1.4.</span> 何謂機器學習</h3>
<div class="outline-text-3" id="text-1-4">
<ul class="org-ul">
<li>機器學習就是從一組function中找出一個最合適的function的過程[李宏毅]。<br /></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgd742f4a" class="outline-2">
<h2 id="orgd742f4a"><span class="section-number-2">2.</span> 機器是如何學習的</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org4e9035b" class="outline-3">
<h3 id="org4e9035b"><span class="section-number-3">2.1.</span> 傳統程式設計</h3>
<div class="outline-text-3" id="text-2-1">
<p>
假設有一個用來描述直線的函數(模型): \(y=Wx+B\)，直線上的每個點都可以用 \(x\) 值乘以\(W\)(權重值)再加上\(B\)(偏差值)，得到相應的 \(y\) 值。<br />
現在假設直線上有兩個點，分別是 \(x=2, y=3\) 和 \(x=3, y=5\) (如圖<a href="#orgb12b131">2</a>)，那麼，請問當 \(x=10\) 時，\(y\) 的值是多少呢？<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">x</span> = [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>]
<span class="linenr"> 5: </span><span style="color: #dcaeea;">y</span> = [<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">5</span>]
<span class="linenr"> 6: </span>plt.plot(x, y, <span style="color: #98be65;">'-*'</span>)
<span class="linenr"> 7: </span>plt.xlim(<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">5</span>)
<span class="linenr"> 8: </span>plt.ylim(<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">7</span>)
<span class="linenr"> 9: </span>plt.text(<span style="color: #da8548; font-weight: bold;">2.1</span>, <span style="color: #da8548; font-weight: bold;">3</span>-<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #98be65;">'(2, 3)'</span>)
<span class="linenr">10: </span>plt.text(<span style="color: #da8548; font-weight: bold;">3.1</span>, <span style="color: #da8548; font-weight: bold;">5</span>-<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #98be65;">'(3, 5)'</span>)
<span class="linenr">11: </span>plt.savefig(<span style="color: #98be65;">'images/TrandPlot.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>

<div id="orgb12b131" class="figure">
<p><img src="images/TrandPlot.png" alt="TrandPlot.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 2: </span>一個直線函數問題</p>
</div>

<p>
以下是傳統的程式設計模式，受了十數年高深數學教育的你，第一反應大概是會想先求出連接這兩個點的直線所相應的 \(W\) 值與 \(B\) 值(也就是模型中的兩個參數)。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="orgfb82864"></a>解法1: class版<br />
<div class="outline-text-4" id="text-2-1-1">
<p>
看不懂的可以看底下的tuple版<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">def</span> <span style="color: #c678dd;">get_slope</span>(p1, p2):
    <span style="color: #dcaeea;">W</span> = (p2.y - p1.y) / (p2.x - p1.x)
    <span style="color: #51afef;">return</span> W

<span style="color: #51afef;">def</span> <span style="color: #c678dd;">get_bias</span>(p1, W):
    <span style="color: #dcaeea;">B</span> = p1.y - (W*p1.x)
    <span style="color: #51afef;">return</span> B

<span style="color: #51afef;">def</span> <span style="color: #c678dd;">get_y</span>(x, W, B):
    <span style="color: #dcaeea;">y</span> = W*x + B

<span style="color: #51afef;">from</span> dataclasses <span style="color: #51afef;">import</span> dataclass
<span style="color: #ECBE7B;">@dataclass</span>
<span style="color: #51afef;">class</span> <span style="color: #ECBE7B;">Point</span>:
    x: <span style="color: #c678dd;">float</span>
    y: <span style="color: #c678dd;">float</span>

<span style="color: #dcaeea;">p1</span> = Point(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>)
<span style="color: #dcaeea;">p2</span> = Point(<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">5</span>)

<span style="color: #dcaeea;">W</span> = get_slope(p1, p2)
<span style="color: #dcaeea;">B</span> = get_bias(p1, W)

<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Slope:"</span>, W)
<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Bias:"</span>, B)
<span style="color: #c678dd;">print</span>(f<span style="color: #98be65;">'&#30070;x=10&#26178;&#65292;y=</span>{W}<span style="color: #98be65;">*10+</span>{B}<span style="color: #98be65;">=</span>{W*10+B}<span style="color: #98be65;">'</span>)
</pre>
</div>

<pre class="example">
Slope: 2.0
Bias: -1.0
當x=10時，y=2.0*10+-1.0=19.0
</pre>
</div>
</li>
<li><a id="org97b3796"></a>解法2: tuple版<br />
<div class="outline-text-4" id="text-2-1-2">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">def</span> <span style="color: #c678dd;">get_slope</span>(p1, p2):
    <span style="color: #dcaeea;">p1x</span>, <span style="color: #dcaeea;">p1y</span> = p1 <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#21462;&#20986;tuple&#20013;&#30340;(x, y)</span>
    <span style="color: #dcaeea;">p2x</span>, <span style="color: #dcaeea;">p2y</span> = p2 <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#21462;&#20986;tuple&#20013;&#30340;(x, y)</span>
    <span style="color: #dcaeea;">W</span> = (p2y - p1y) / (p2x - p1x)
    <span style="color: #51afef;">return</span> W

<span style="color: #51afef;">def</span> <span style="color: #c678dd;">get_bias</span>(p1, W):
    <span style="color: #dcaeea;">p1x</span>, <span style="color: #dcaeea;">p1y</span> = p1 <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#21462;&#20986;tuple&#20013;&#30340;(x, y)</span>
    <span style="color: #dcaeea;">B</span> = p1y - (W*p1x)
    <span style="color: #51afef;">return</span> B

<span style="color: #51afef;">def</span> <span style="color: #c678dd;">get_y</span>(x, W, B):
    <span style="color: #dcaeea;">y</span> = W*x + B

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;tuple&#20358;&#25551;&#36848;(x, y)</span>
<span style="color: #dcaeea;">p1</span> = (<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>)
<span style="color: #dcaeea;">p2</span> = (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">5</span>)

<span style="color: #dcaeea;">W</span> = get_slope(p1, p2)
<span style="color: #dcaeea;">B</span> = get_bias(p1, W)

<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Slope:"</span>, W)
<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Bias:"</span>, B)
<span style="color: #c678dd;">print</span>(f<span style="color: #98be65;">'&#30070;x=10&#26178;&#65292;y=</span>{W}<span style="color: #98be65;">*10+</span>{B}<span style="color: #98be65;">=</span>{W*10+B}<span style="color: #98be65;">'</span>)
</pre>
</div>

<pre class="example">
Slope: 2.0
Bias: -1.0
當x=10時，y=2.0*10+-1.0=19.0
</pre>
</div>
</li>
<li><a id="orgefa2ba7"></a>傳統的程式設計模式<br />
<div class="outline-text-4" id="text-2-1-3">
<p>
由以上的程式碼可以看出傳統的解題模式為：<br />
</p>
<ol class="org-ol">
<li>給資料(兩個點)<br /></li>
<li>給規則(公式)<br /></li>
</ol>
<p>
接下來就以程式來解出答案，那麼，面臨一樣的問題，機器學習的解題策略又是如何呢？<br />
</p>


<div id="orgdb7cb0a" class="figure">
<p><img src="images/TradProg.png" alt="TradProg.png" /><br />
</p>
</div>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgf8ed6cf" class="outline-3">
<h3 id="orgf8ed6cf"><span class="section-number-3">2.2.</span> 機器學習的策略</h3>
<div class="outline-text-3" id="text-2-2">
<p>
機器學習把這個線性函數當成一個模型，而我們想求的 \(W\) 和 \(B\) 則為模型的兩個參數，如果要以機器學習的方式來解題，其策略大致如下：<br />
</p>
<ol class="org-ol">
<li>第一步：猜答案<br />
一開始我們並不知道正確答案是什麼，所以用猜的：以隨機亂數來做為 \(W\) 和\(B\) 值，例如：\(y=10x+5\)。<br /></li>
<li>第二步：評估猜測答案的品質<br />
上個步驟中所猜的 \(W\) 和\(B\) 值夠不夠準確呢？我們可以用\(y=10x+5\) 這個函數來計算出每個 \(x\) 值相對應的 \(y\) 值，再和 <b>正確</b> 的 \(y\) 值比較，看看還差多少。這個比較的方式稱為「損失」(loss)或「誤差」(error)。<br /></li>
<li>第三步：對所猜測的策略進行最佳化調整<br />
依據上一步驟猜測結果的品質(loss或error)做出更好的猜測，這個步驟稱為「最佳化」(optimization)。微積分可以用「梯度遞減」(gradient descent)的方式來進行。<br /></li>
</ol>

<p>
整個流程大致如下：<br />
</p>

<div id="org5f8aec6" class="figure">
<p><img src="images/MLProg.png" alt="MLProg.png" /><br />
</p>
</div>
</div>
</div>
<div id="outline-container-orgdeb1fa6" class="outline-3">
<h3 id="orgdeb1fa6"><span class="section-number-3">2.3.</span> 實作1</h3>
<div class="outline-text-3" id="text-2-3">
<div class="org-src-container">
<pre class="src src-shell">pip3 install tensorflow
</pre>
</div>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span style="color: #51afef;">from</span> tensorflow.keras <span style="color: #51afef;">import</span> Sequential
<span style="color: #51afef;">from</span> tensorflow.keras.layers <span style="color: #51afef;">import</span> Dense

<span style="color: #dcaeea;">l0</span> = Dense(units=<span style="color: #da8548; font-weight: bold;">1</span>, input_shape=[<span style="color: #da8548; font-weight: bold;">1</span>])
<span style="color: #dcaeea;">model</span> = Sequential([l0])
model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'sgd'</span>, loss=<span style="color: #98be65;">'mean_squared_error'</span>)

<span style="color: #dcaeea;">xs</span> = np.array([<span style="color: #da8548; font-weight: bold;">2.0</span>, <span style="color: #da8548; font-weight: bold;">3.0</span>], dtype=<span style="color: #c678dd;">float</span>)
<span style="color: #dcaeea;">ys</span> = np.array([<span style="color: #da8548; font-weight: bold;">3.0</span>, <span style="color: #da8548; font-weight: bold;">5.0</span>], dtype=<span style="color: #c678dd;">float</span>)

model.fit(xs, ys, epochs=<span style="color: #da8548; font-weight: bold;">500</span>, verbose=<span style="color: #da8548; font-weight: bold;">0</span>)

<span style="color: #c678dd;">print</span>(f<span style="color: #98be65;">'y&#30340;&#38928;&#28204;&#32080;&#26524;&#28858;: </span>{model.predict([10.0])}<span style="color: #98be65;">'</span>)
<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#27169;&#22411;&#30340;&#20841;&#20491;&#21443;&#25976;W,B: {}"</span>.<span style="color: #c678dd;">format</span>(l0.get_weights()))
</pre>
</div>

<pre class="example">
1/1 [==============================] - 0s 172ms/step
y的預測結果為: 14.709847
模型的兩個參數W,B: [array([[1.4212971]], dtype=float32), array([0.49687672], dtype=float32)]
</pre>


<p>
但是正確答案為19，預測的結果好像不太準&#x2026;.QQ<br />
</p>
</div>
</div>
<div id="outline-container-org7b81a90" class="outline-3">
<h3 id="org7b81a90"><span class="section-number-3">2.4.</span> 實作2</h3>
<div class="outline-text-3" id="text-2-4">
<p>
如果我們向上帝偷偷多要一個模型的參照點(4, 7)，以三點當成標準答案來進行預測，結果會不會好一點呢?<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span style="color: #51afef;">from</span> tensorflow.keras <span style="color: #51afef;">import</span> Sequential
<span style="color: #51afef;">from</span> tensorflow.keras.layers <span style="color: #51afef;">import</span> Dense

<span style="color: #dcaeea;">l0</span> = Dense(units=<span style="color: #da8548; font-weight: bold;">1</span>, input_shape=[<span style="color: #da8548; font-weight: bold;">1</span>])
<span style="color: #dcaeea;">model</span> = Sequential([l0])
model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'sgd'</span>, loss=<span style="color: #98be65;">'mean_squared_error'</span>)

<span style="color: #dcaeea;">xs</span> = np.array([<span style="color: #da8548; font-weight: bold;">2.0</span>, <span style="color: #da8548; font-weight: bold;">3.0</span>, <span style="color: #da8548; font-weight: bold;">4.0</span>], dtype=<span style="color: #c678dd;">float</span>)
<span style="color: #dcaeea;">ys</span> = np.array([<span style="color: #da8548; font-weight: bold;">3.0</span>, <span style="color: #da8548; font-weight: bold;">5.0</span>, <span style="color: #da8548; font-weight: bold;">7.0</span>], dtype=<span style="color: #c678dd;">float</span>)

model.fit(xs, ys, epochs=<span style="color: #da8548; font-weight: bold;">500</span>, verbose=<span style="color: #da8548; font-weight: bold;">0</span>)

<span style="color: #c678dd;">print</span>(f<span style="color: #98be65;">'y&#30340;&#38928;&#28204;&#32080;&#26524;&#28858;: </span>{model.predict([10.0])}<span style="color: #98be65;">'</span>)
<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#27169;&#22411;&#30340;&#20841;&#20491;&#21443;&#25976;W,B: {}"</span>.<span style="color: #c678dd;">format</span>(l0.get_weights()))
</pre>
</div>

<pre class="example">
y的預測結果為: [[17.07221]]
模型的兩個參數W,B: [array([[1.7164488]], dtype=float32), array([-0.09227743], dtype=float32)]
</pre>


<p>
預測結果y的值為17.072221，而正確答案為19，好像也不怎麼樣&#x2026;.-_-<br />
兩個參數的值為(1.7164488, -0.9227743)，與正確答案(2, -1)相近。<br />
</p>
</div>
</div>
<div id="outline-container-org215fb00" class="outline-3">
<h3 id="org215fb00"><span class="section-number-3">2.5.</span> 實作3</h3>
<div class="outline-text-3" id="text-2-5">
<p>
讓我們壯起膽子再跟上帝多要兩個線上的點:(-1, -3)、(0, -1)，以五點當成標準答案來進行預測，其實作的程式碼為：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span><span style="color: #51afef;">from</span> tensorflow.keras <span style="color: #51afef;">import</span> Sequential
<span class="linenr"> 4: </span><span style="color: #51afef;">from</span> tensorflow.keras.layers <span style="color: #51afef;">import</span> Dense
<span class="linenr"> 5: </span>
<span id="coderef-neuron" class="coderef-off"><span class="linenr"> 6: </span><span style="color: #dcaeea;">l0</span> = Dense(units=<span style="color: #da8548; font-weight: bold;">1</span>, input_shape=[<span style="color: #da8548; font-weight: bold;">1</span>])</span>
<span id="coderef-model" class="coderef-off"><span class="linenr"> 7: </span><span style="color: #dcaeea;">model</span> = Sequential([l0])</span>
<span id="coderef-modelCompile" class="coderef-off"><span class="linenr"> 8: </span>model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'sgd'</span>, loss=<span style="color: #98be65;">'mean_squared_error'</span>)</span>
<span class="linenr"> 9: </span>
<span class="linenr">10: </span><span style="color: #dcaeea;">xs</span> = np.array([-<span style="color: #da8548; font-weight: bold;">1.0</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.0</span>, <span style="color: #da8548; font-weight: bold;">2.0</span>, <span style="color: #da8548; font-weight: bold;">3.0</span>, <span style="color: #da8548; font-weight: bold;">4.0</span>], dtype=<span style="color: #c678dd;">float</span>)
<span class="linenr">11: </span><span style="color: #dcaeea;">ys</span> = np.array([-<span style="color: #da8548; font-weight: bold;">3.0</span>, -<span style="color: #da8548; font-weight: bold;">1.0</span>, <span style="color: #da8548; font-weight: bold;">1.0</span>, <span style="color: #da8548; font-weight: bold;">3.0</span>, <span style="color: #da8548; font-weight: bold;">5.0</span>, <span style="color: #da8548; font-weight: bold;">7.0</span>], dtype=<span style="color: #c678dd;">float</span>)
<span class="linenr">12: </span>
<span id="coderef-modelFit" class="coderef-off"><span class="linenr">13: </span>model.fit(xs, ys, epochs=<span style="color: #da8548; font-weight: bold;">500</span>, verbose=<span style="color: #da8548; font-weight: bold;">0</span>)</span>
<span class="linenr">14: </span>
<span id="coderef-modelPredict" class="coderef-off"><span class="linenr">15: </span><span style="color: #c678dd;">print</span>(f<span style="color: #98be65;">'y&#30340;&#38928;&#28204;&#32080;&#26524;&#28858;: </span>{model.predict([10.0])}<span style="color: #98be65;">'</span>)</span>
<span id="coderef-modelWeights" class="coderef-off"><span class="linenr">16: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#27169;&#22411;&#30340;&#20841;&#20491;&#21443;&#25976;W,B: {}"</span>.<span style="color: #c678dd;">format</span>(l0.get_weights()))</span>
</pre>
</div>

<pre class="example">
y的預測結果為:[[18.979391]]
模型的兩個參數W,B: [array([[1.9970131]], dtype=float32), array([-0.99074], dtype=float32)]
</pre>


<p>
預測結果為(1.9970131, -0.99074)，與正確答案(2, -1)已經十分相近。<br />
</p>
</div>
</div>
<div id="outline-container-org659b64f" class="outline-3">
<h3 id="org659b64f"><span class="section-number-3">2.6.</span> 程式解說</h3>
<div class="outline-text-3" id="text-2-6">
<ol class="org-ol">
<li>上述程式使用tensorflow這個模組為主要預測工具，建立了一個模型(第<a href="#coderef-model" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-model');" onmouseout="CodeHighlightOff(this, 'coderef-model');">7</a>行)，這個模型中只有一層、裡面有一個神經元(第<a href="#coderef-neuron" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-neuron');" onmouseout="CodeHighlightOff(this, 'coderef-neuron');">6</a>行)<br /></li>
<li>定義好模型後，我們以SGD(隨機梯度下降法)為優化器、用它來找出最佳參數；以MSE(均方差)做為損失函數，計算模型訓練過程中生成的參數優劣判斷標準(第<a href="#coderef-modelCompile" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-modelCompile');" onmouseout="CodeHighlightOff(this, 'coderef-modelCompile');">8</a>行)，並編譯此模型<br /></li>
<li>編譯好模型，我們就拿手僅有的5組資料來訓練模型，訓練500回(第<a href="#coderef-modelFit" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-modelFit');" onmouseout="CodeHighlightOff(this, 'coderef-modelFit');">13</a>行)<br /></li>
<li>訓練好的模型就能拿來預測了，我們拿 \(x=10\) 當成預測資料，要求模型給出預測的結果 \(y\) (第<a href="#coderef-modelPredict" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-modelPredict');" onmouseout="CodeHighlightOff(this, 'coderef-modelPredict');">15</a>行)<br /></li>
<li>偷偷看一下這個模型給出答案的關鍵、也就是模型的兩個參數(Weight)(第<a href="#coderef-modelWeights" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-modelWeights');" onmouseout="CodeHighlightOff(this, 'coderef-modelWeights');">16</a>行)<br /></li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org33bae59" class="outline-2">
<h2 id="org33bae59"><span class="section-number-2">3.</span> 機器學習概念釐清</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-orgeb404c6" class="outline-3">
<h3 id="orgeb404c6"><span class="section-number-3">3.1.</span> Machine Learning v.s. Statistics</h3>
<div class="outline-text-3" id="text-3-1">
<ul class="org-ul">
<li>統計: 目的在由樣本(sample)找出真相(universe)，如，由台灣 2350 萬人口中，只選出 1000 人，得知全台生活型態、政治傾向。<br /></li>
<li>Machine learning: 目的在讓電腦由歷史資料中學習處理新的資料以解決問題。如，由醫生判讀 X 光片的資料學習判讀新的 X 光片；由一個人的刷卡及繳費行為預測他的信貸是否能準時還款？<br /></li>
<li>Machine learning 不在意真實資料的分佈，而在於根據已知推測未知。<br /></li>
</ul>
</div>
</div>
<div id="outline-container-org4ce6b1b" class="outline-3">
<h3 id="org4ce6b1b"><span class="section-number-3">3.2.</span> Machine Learning v.s. Traditional Programming</h3>
<div class="outline-text-3" id="text-3-2">
<ul class="org-ul">
<li>傳統的程式設計pattern是將資料及規則餵給程式，經由程式判斷(if-else)產生答案<br /></li>
<li>Meachine learning是將資料(features)和答案(label)餵給model，然後由model學習出規則<br /></li>
</ul>

<div id="org84d3146" class="figure">
<p><img src="images/MLTP.png" alt="MLTP.png" width="600" /><br />
</p>
<p><span class="figure-number">Figure 3: </span>Traditional Programming v.s. Machine Learning</p>
</div>
</div>
</div>
<div id="outline-container-orgbbe6490" class="outline-3">
<h3 id="orgbbe6490"><span class="section-number-3">3.3.</span> Training Phase v.s. Inference Phase</h3>
<div class="outline-text-3" id="text-3-3">

<div id="org2ca0e0a" class="figure">
<p><img src="images/TPIP.png" alt="TPIP.png" width="600" /><br />
</p>
<p><span class="figure-number">Figure 4: </span>Training and Inference Phase of ML</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org6fa7933" class="outline-2">
<h2 id="org6fa7933"><span class="section-number-2">4.</span> 典型的機器學習MODEL Training</h2>
<div class="outline-text-2" id="text-4">
<p>
以&ldquo;rock, paper, and scissors&rdquo;辨識為例<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>:<br />
</p>
<ol class="org-ol">
<li>將三種圖案的image讀入model中<br /></li>
<li>於model中建立神經元(可視為function)，學習不同image的特徵<br /></li>
<li>根據上述神經元讀取的特徵值(features)，配合該image之答案(label)，進行學習<br /></li>
</ol>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">model</span> = tf.keras.models.Sequence([
<span class="linenr">2: </span>    tf.keras.layers.Flatten(input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>)),
<span class="linenr">3: </span>    tf.keras.layers.Dense(<span style="color: #da8548; font-weight: bold;">512</span>, activation=<span style="color: #98be65;">'relu),</span>
<span class="linenr">4: </span><span style="color: #98be65;">    tf.keras.layers.Dense(3, activatio='</span>softmax<span style="color: #98be65;">')</span>
<span class="linenr">5: </span><span style="color: #98be65;">])</span>
<span class="linenr">6: </span>
<span class="linenr">7: </span><span style="color: #98be65;">model.compile(loss='</span>categorical_crossentropy<span style="color: #98be65;">', optimizer='</span>rmsprop<span style="color: #98be65;">')</span>
<span class="linenr">8: </span>
<span class="linenr">9: </span><span style="color: #98be65;">model.fit(...., epochs=100)</span>
</pre>
</div>
</div>
<div id="outline-container-org2d39039" class="outline-3">
<h3 id="org2d39039"><span class="section-number-3">4.1.</span> Process of learning</h3>
<div class="outline-text-3" id="text-4-1">
<p>
可先將512個神經元視為512個function，每個function裡有許多變數，這些變數的初始值為random assign，接下來每個function陸續讀取image的所有feature(即圖的每個pixel，這裡可以將之視為傳入該function的parameters)，然後根據function中的變數值，開始學習要如何調整function裡的變數值才能輸出這個image的正確答案(label)，每個神經元同時進行學習與變數值調校。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org0a77dd0"></a>function 示意圖<br />
<div class="outline-text-4" id="text-4-1-1">

<div id="org0729f76" class="figure">
<p><img src="images/function-dg.png" alt="function-dg.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 5: </span>Function</p>
</div>

<p>
在訓練的過程中，我們根據每個function所輸入的參數(parameters)，調整function裡的變數值(這些最初是random得來的)，希望能得到與該輸入值(所有的parameters)所相對應的答案。<br />
</p>

<p>
在傳統的程式設計觀點，輸入function的數值為參數；但在神經網路的觀點，輸入值為features，function稱為neuron，neuron裡的變數稱為parameters，所有neuron裡的parameter均為神經網路訓練過程中要調整的對象。<br />
</p>

<div id="org859c6ac" class="figure">
<p><img src="images/neuron-dg.png" alt="neuron-dg.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 6: </span>Neuron</p>
</div>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgbaa27b5" class="outline-3">
<h3 id="orgbaa27b5"><span class="section-number-3">4.2.</span> Example of training</h3>
<div class="outline-text-3" id="text-4-2">
<ul class="org-ul">
<li><p>
輸入image為rock<br />
</p>

<div id="orgf8df8f9" class="figure">
<p><img src="images/stone.jpg" alt="stone.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 7: </span>Stone</p>
</div></li>
<li><p>
輸入image為paper<br />
</p>

<div id="orga9d8ff1" class="figure">
<p><img src="images/paper.jpg" alt="paper.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 8: </span>Paper</p>
</div></li>
</ul>

<p>
接下來，<br />
</p>
</div>
</div>
<div id="outline-container-org7de995d" class="outline-3">
<h3 id="org7de995d"><span class="section-number-3">4.3.</span> Model v.s. Layer: Training step by step</h3>
<div class="outline-text-3" id="text-4-3">
<ul class="org-ul">
<li><p>
Input layer<br />
此的輸入為image，對model來說，其feature為150*150*3<br />
</p>

<div id="org89c5847" class="figure">
<p><img src="images/input_shape.jpg" alt="input_shape.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 9: </span>input_shape</p>
</div></li>
<li><p>
Hidden layer<br />
中間層(hidden layer)有512個神經元，可視為512個function<br />
</p>

<div id="org337139a" class="figure">
<p><img src="images/hidden_layer.jpg" alt="hidden_layer.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 10: </span>hidden_layer</p>
</div></li>
<li><p>
Output layer<br />
輸出層(output layer)則輸出三種可能的答案<br />
</p>

<div id="orga749d74" class="figure">
<p><img src="images/output_layer.jpg" alt="output_layer.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 11: </span>output_layer</p>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org5eab7fa" class="outline-3">
<h3 id="org5eab7fa"><span class="section-number-3">4.4.</span> compile v.s. fit</h3>
<div class="outline-text-3" id="text-4-4">
<ul class="org-ul">
<li>compile: Compile與傳統程式設計概念不同，此處旨在訂定兩個model training中最重要的關鍵: loss function與optimizer。剛才提及每個function(或neuron)都對於產生答案(label)做出了貢獻，然而這個答案到底好或不好，必須要有一個評估機制，loss function的目的就在評估每一次所有neuron所產生的答案是否足夠好(以此例來看至少正確率必須高過1/3)，然後把評估結果丟給optimizer，由它來決定下一次猜測時neuron裡的parameters要如何調整。<br /></li>
<li>fit: 如此重複不斷的進行&ldquo;輸入feature-&gt;猜測答案-&gt;評估答案-&gt;修正parameter-&gt;輸入features-&gt;猜測答案-&gt;評估答案-&gt;修正parameter&#x2026;.&rdquo;，希望最終能找到各neuron中最佳的parameters值，每一次的訓練稱為一次epoch。這就是fit在做的事，也是實際訓練的動作，訓練所需時間視輸入值、model複雜程度與硬體效能而定，也許僅需數分鐘，也許要耗時數週。<br /></li>
</ul>

<div id="org273cfa9" class="figure">
<p><img src="images/compile_phase.jpg" alt="compile_phase.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 12: </span>compile-phase</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org5eff329" class="outline-2">
<h2 id="org5eff329"><span class="section-number-2">5.</span> 機器學習如何解決問題</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-org2732507" class="outline-3">
<h3 id="org2732507"><span class="section-number-3">5.1.</span> 使用機器學習處理問題的幾個步驟：</h3>
<div class="outline-text-3" id="text-5-1">
<ul class="org-ul">
<li>數據收集(Data Collection): 在監督式學習下還要蒐集正確的標記<br /></li>
<li>數據處理(Data Processing): 包含「數據清理」，例如：刪除「冗餘」或「高度相關的特徵」，或補滿「遺漏值」。<br /></li>
<li>建立測試案例(Creation of the test case): 通常包括：「訓練數據集」(training dataset)用來訓練演算法、「測試數據集」(test dataset)用來測試訓練完成的演算法、以及「驗證數據集」(validation dataset)用來進行最終測試(在不斷的訓練-測試之後)。<br /></li>
</ul>
</div>
</div>
<div id="outline-container-orgcde8d0e" class="outline-3">
<h3 id="orgcde8d0e"><span class="section-number-3">5.2.</span> 現實生活中的應用</h3>
<div class="outline-text-3" id="text-5-2">
<p>
近年來的機器學習範例：<br />
</p>
<ul class="org-ul">
<li>AlphaGo: 基於深度學習所製作的「<a href="AI-Introduction.html">人工智慧</a>機」，在 2016 年擊敗世界圍棋冠軍 Lee Sedol。AlphaGo 的優勢在於這個程式並不是專門開發來下圍棋的，而是運用「強化學習」與「深度學習」，透過下了數以千計次的圍棋，學習到如何下圍棋。<br /></li>
<li>澳大利亞：2015 年舉辦一場「預測西澳大利亞租屋價錢」的比賽<br /></li>
<li>2009 年 Netflix 推出一項總奬金 100 萬美元的競賽，用來改進預測使用者喜愛影片的正確性<br /></li>
<li>AlhpaGo Zero: 2017 年 10 月 19 日，AlphaGo 團隊在《自然》上發表文章介紹了 AlphaGo Zero，文中指出此版本不採用人類玩家的棋譜，且比之前的所有版本都要強大。透過自我對弈，AlphaGo Zero 在三天內以 100 比 0 的戰績戰勝了 AlphaGo Lee，花了 21 天達到 AlphaGo Master 的水平，用 40 天超越了所有舊版本。DeepMind 聯合創始人兼 CEO 傑米斯·哈薩比斯說，AlphaGo Zero「不再受限於人類認知」，很強大。由於專家數據「經常很貴、不可靠或是無法取得」，不藉助人類專家的數據集訓練<a href="AI-Introduction.html">人工智慧</a>，對於<a href="AI-Introduction.html">人工智慧</a>開發超人技能具有重大意義[4]，因為這樣的 AI 不是學習人，是透過對自我的反思和獨有的創造力直接超越人類<sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup>。<br /></li>
<li>繼 AlphaGo 後，同一團隊(DeepMind)繼續打造出 AlphaZero，不再依賴人類棋士的知識與棋譜，只給遊戲規則。在 34 小時的訓練後（約自我訓練 2100 萬局[1]:Table S3），AlphaZero 以 60 勝 40 敗的成績打敗 AlphaGo Zero<sup><a id="fnr.4" class="footref" href="#fn.4" role="doc-backlink">4</a></sup>。<br /></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgde5f3d9" class="outline-2">
<h2 id="orgde5f3d9"><span class="section-number-2">6.</span> 機器學習的類型</h2>
<div class="outline-text-2" id="text-6">

<div id="org8221e78" class="figure">
<p><img src="images/2022-10-03_09-26-40.png" alt="2022-10-03_09-26-40.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 13: </span>Caption</p>
</div>
</div>
<div id="outline-container-org9ed061f" class="outline-3">
<h3 id="org9ed061f"><span class="section-number-3">6.1.</span> <a href="20221023101626-監督式學習.html#ID-20221023T101626.420918">監督式學習</a>(Supervised learning)</h3>
<div class="outline-text-3" id="text-6-1">
<ul class="org-ul">
<li>目前九成以上的機器學習應用均屬此類<br /></li>
<li>監督式學習指在訓練過程中直接告訴機器答案，也就是將資料進行標註(label)，例如，在 1000 張訓練集照片中標註「貓/狗」。<br /></li>
<li>從給定的訓練資料集中學習出一個函式，當新的資料到來時，可以根據這個函式預測結果。<br /></li>
<li>監督學習的訓練集要求是包括輸入和輸出， 也可以說是特徵和目標。 訓練集中的目標是由人標註的。<br /></li>
<li>為迄今為止最常見的機器學習，泛指一群的機器學習演算法，是從一組「已標記」的「訓練數據集」(training dataset)來學習(訓練)，並導出模型。然後，以此一模型對「未標記」的類似數據進行預測分類，其運作流程如圖<a href="#org43e31b4">14</a><sup><a id="fnr.1.100" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>所示。典型的例子為早期電子郵件的垃圾信件是讓使用者先去標記某些信為垃圾郵件，然後藉由這些被標記的郵件來推論找出其他可能的垃圾郵件；由此看來，我們以為 Gmail 很好心的提供給我們為信件加註「垃圾」、「廣告」的功能，其實是 Google 利用我們當免費勞工為他們提供信件加註標籤的工作。<br /></li>
</ul>

<div id="org43e31b4" class="figure">
<p><img src="images/FlowChartOfSupervisedLearning.png" alt="FlowChartOfSupervisedLearning.png" width="600" /><br />
</p>
<p><span class="figure-number">Figure 14: </span>監督式學習流程圖</p>
</div>
</div>
<ol class="org-ol">
<li><a id="org06f13b7"></a>方法<br />
<div class="outline-text-4" id="text-6-1-1">
<ul class="org-ul">
<li>類神經網路<br /></li>
<li>單純貝氏分類器(Naive Bayes Classifier)<br /></li>
<li>邏輯迴歸(Logisitc Regression)<br /></li>
<li>決策樹(Decision tree)<br /></li>
<li>支援向量機(SVM, Support Vector Machine)<br /></li>
</ul>
</div>
</li>
<li><a id="orgbc5e4e3"></a>典型應用<br />
<div class="outline-text-4" id="text-6-1-2">
<ul class="org-ul">
<li>Credit/loan approval:信用評比與貸款通過<br /></li>
<li>Medical diagnosis: if a tumor is cancerous or benign(是否有 XX 癌症)<br /></li>
<li>Fraud detection: if a transaction is fraudulent 詐騙或正常交易<br /></li>
<li>垃圾郵件(SPAM)或正常郵件<br /></li>
<li>Web page categorization 網站分類: which category it is<br /></li>
<li>資安應用: 取得有漏洞程式碼資料集(label)，評估其他程式是否有漏洞<br /></li>
</ul>
</div>
</li>
<li><a id="orgde4bcb7"></a>範例：<br />
<div class="outline-text-4" id="text-6-1-3">
<ul class="org-ul">
<li>分類：基於從訓練集資料觀測到的分類類別來標籤、預測新數據的類別，如上述的垃圾郵件即為典型的二元分類工作(如圖<a href="#orgf9ba43e">15</a>)，類別分類工作也可以進行「多類別分類」(multi-class classification)，如典型的 MNist 手寫數字辨識，即是將手寫的 0-9 數字進行預測辨識，並給出一個 0-9 的類別標籤。<br /></li>
</ul>

<div id="orgf9ba43e" class="figure">
<p><img src="images/01_03.png" alt="01_03.png" width="600" /><br />
</p>
<p><span class="figure-number">Figure 15: </span>典型的分類</p>
</div>
<ul class="org-ul">
<li>線性迴歸: 利用輸入數據的「特徵」來預測出一個「值」，例如，根據房屋的地點、坪數、樓層、房間數等變數, 發掘出這些變數之間的關係，進而預測房價，如圖<a href="#org0f772ad">16</a>。<br /></li>
</ul>

<div id="org0f772ad" class="figure">
<p><img src="images/01_04.png" alt="01_04.png" width="600" /><br />
</p>
<p><span class="figure-number">Figure 16: </span>典型的迴歸</p>
</div>
<ul class="org-ul">
<li>決策樹: 精典例子為「鳶尾花數據集」(<a href="http://archive.ics.uci.deu/ml/datasets/Iris">http://archive.ics.uci.deu/ml/datasets/Iris</a>)。<br /></li>
<li>支援向量機: 主要用來處理分類問題，它不僅能將數據分門別類，甚至遇可以找到最大化的「分離超平面」（類似於三維以上空間中的一個平面），會最大化每個樣本點與該「超平面」的差。此外，當數據是「不可線性分離」時，支援向量機還可以透過「軟邊界」(soft margin)和「核技巧」(kernel trick)來處理。<br /></li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org4534484" class="outline-3">
<h3 id="org4534484"><span class="section-number-3">6.2.</span> <a href="20221023101716-非監督式學習.html#ID-20221023T101716.467694">非監督式學習</a>(Unsupervised learning)</h3>
<div class="outline-text-3" id="text-6-2">
<ul class="org-ul">
<li>非監親式學習只有觀測值，單純給電腦大量觀測資料，然後從這些資料找出潛在規則。例如：將 10 萬張片依據電腦自己歸納的規則分為數個不同的群組。<br /></li>
<li>在「監督式學習」中，我們事先會知道訓練集數據的正確答案(label)，並依此訓練我們的模型；在強化學習的環境中，我們會為代理人定義如何度量特定行動的奬勵；然而，在「非監督式學習」的環境中，我們面對的是未標記類別的數據或未知結構的數據。而是讓演算法導出結論。最典型的例子就是「集群」(clustering)，即，讓演算法自己根據數據的特性把它們依某種特質分類為不同子集合，這裡的子集合不一定要是有限集合，也可能是無界子集(unbounded subsets)。「受限玻爾茲曼機」以及「深度信念網路」(deep belief networks, DBN)都屬此類。<br /></li>
<li>非監督式學習經常被運用於資料分析的前置階段，用來先將資料分群或降低維度(減少變數量)，以利後續的分析或監督式學習的進行。<br /></li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org682e86d"></a>方法：<br />
<div class="outline-text-4" id="text-6-2-1">
<ul class="org-ul">
<li>K平均法(K-means): Clustering<br /></li>
<li>自動編碼器(Autoencoder)<br /></li>
</ul>
</div>
</li>
<li><a id="org1a66585"></a>範例：<br />
<div class="outline-text-4" id="text-6-2-2">
<ul class="org-ul">
<li>集群(cluster): 是一種「探索式數據分析」(exploratory data analysis)技術，它允許我們先組織一堆資訊到一個有意義的「子集群」(clusters)中，而無需任何先驗知識。<br /></li>
<li>Cluster analysis (or clustering, data segmentation, &#x2026;): Finding similarities between data according to the characteristics found in the data and grouping similar data objects into clusters<br /></li>
<li>K-means: 將數據集中的每個樣本分類到 k 個不同的子集合中，它隨機選擇 k 個點，這些點稱為「質心」(centroid)，代表這 k 個不同子集合的中心點，然後對於每個「質心」，我們選擇最接近它的一點，群組起來&#x2026;..。<br /></li>
<li>生成對抗網路(GAN)<br /></li>
<li>資料沒有答案(如解讀古文)<br /></li>
<li>Principal Component Analysis 主成份分析<br /></li>
</ul>
</div>
</li>
<li><a id="org809d09f"></a>非監督式學習分群的原理<br />
<ol class="org-ol">
<li><a id="orgeaff1c0"></a>Linear projection<br />
<div class="outline-text-5" id="text-6-2-3-1">
<ul class="org-ul">
<li>PCA: Principal component analysis，用來識別整個feature set中哪些特徵最為重要，且最能解釋資料實例間的可變性。<br />
<ul class="org-ul">
<li>standard PCA<br /></li>
<li>incremental PCA<br /></li>
<li>sparse PCA<br /></li>
<li>kernel PCA<br /></li>
</ul></li>
<li>SVD: Singular value decomposition，降低原來features matrix的rank，使原來的matrix可以分解成數個較小rank的matrix。<br /></li>
<li>Random projection<br /></li>
<li>PCA<br /></li>
</ul>

<div id="orgc0f7dfd" class="figure">
<p><img src="images/PCA.jpg" alt="PCA.jpg" width="600" /><br />
</p>
<p><span class="figure-number">Figure 17: </span>PCA</p>
</div>
</div>
</li>
<li><a id="orge5bdd8e"></a>Nonlinear dimensionality reduction<br />
<div class="outline-text-5" id="text-6-2-3-2">
<ul class="org-ul">
<li>Isomap<br /></li>
<li>t-SNE: t-distributed stochastic neighbor embedding<br /></li>
<li>Dictionary learning<br /></li>
<li>ICA: Independent component analysis<br /></li>
<li>LDA: Latent Dirichlet allocation<br /></li>
<li>MDS: Multidimensional scaling<br /></li>
<li>LLE: Locally linear embedding<br /></li>
</ul>
</div>
</li>
<li><a id="orgba42c3b"></a>Dimensionality reduction<br />
<div class="outline-text-5" id="text-6-2-3-3">
<ul class="org-ul">
<li>找出原始資料裡最主要的方面來代替原始資料，使得在損失少部分原始資訊的基礎上，最大程度的降低原始資料的維度。<br /></li>
<li>維度縮減讓非監督式學習能更正確地辨識patterns，並更有效率地解決大規模資料所導致的昂貴運算問題<br /></li>
<li>降維原理<br /></li>
</ul>

<div id="org0a55ed4" class="figure">
<p><img src="images/DimensionalityReduction-1.jpg" alt="DimensionalityReduction-1.jpg" width="600" /><br />
</p>
<p><span class="figure-number">Figure 18: </span>基本的降維#1</p>
</div>
<ul class="org-ul">
<li>降維的選擇<br /></li>
</ul>

<div id="orgaec2f68" class="figure">
<p><img src="images/DimensionalityReduction-2.jpg" alt="DimensionalityReduction-2.jpg" width="600" /><br />
</p>
<p><span class="figure-number">Figure 19: </span>基本的降維#2</p>
</div>
</div>
</li>
</ol>
</li>
<li><a id="orgd2949a0"></a>face recognition 做法<br />
<div class="outline-text-4" id="text-6-2-4">
<ol class="org-ol">
<li>2D 圖形 轉 1 維向量<br /></li>
<li>經由 PCA 計算，取得主要 factor，進行降維度(projection)<br /></li>
<li>取得合理的維度，進行特徵描述<br /></li>
</ol>
</div>
</li>
</ol>
</div>
<div id="outline-container-orge27a3dc" class="outline-3">
<h3 id="orge27a3dc"><span class="section-number-3">6.3.</span> <a href="20230210172919-增強式學習.html#ID-0a5c37c0-741a-4a1a-bec7-f98074830132">增強式學習</a></h3>
<div class="outline-text-3" id="text-6-3">
<ul class="org-ul">
<li>機器為了達成目標，隨著環境的變動，而逐步調整其行為，並評估每一個行動之後所到 的回饋是正向的或負向的，即，在 try-and-error 的過程中一步步從失敗中找出成功的路徑。<br /></li>
<li>較常用於以下領域：電腦遊戲、下棋、自駕車、機器人。<br /></li>
<li>AlphaGo：先以監督式學習(以人類棋譜來訓練)訓練出早期版的 AlphaGo，接下來以增強式學習兩個最期版的 AlphaGo 對奕(40 天內對奕 3000 萬盤棋)。<br /></li>
<li>2017 年的 AlphaZero 則放棄監督式學習(人類棋譜)，完全採取強化學習的模式，三天後摸索出自己的圍棋下法，成為有史以來棋力最強的版本。(不再以人類為師，所以才能超越人類？)<br /></li>
<li>Google 也將強化學習用於機房伺服器管理，持續偵測機房室內外用電、溫度、建立模型，由模型決定每台伺服器的運轉(全速、低速、休眠、關機)，並達到省電 40%的目標。<br /></li>
<li>強化學習的目標在於開發一個系統（或代理人，agent）,他會藉由與環境的互動來改進自身的效能。由於當前的環境狀態資訊通常就包含了所謂「奬勵信號」(reward signal)，強化學習的目的就是找到一個最好的 Policy(策略)，可以讓 reward 最多。所以也可以把「強化學習」視為與「監督式學習」相關聯的領域，然而在強化學習中，環境回饋不能視為真實正的事實（或是說，正確的標籤），只能將之視為：測量函數對特定行動所觀測到並回報的一個度量值。如圖<a href="#org74ad3e1">21</a>，Agent<br /></li>
<li>最常見的應用是教機器如何「玩遊戲」，在這種情境下，我們不會對某個動作貼標籤說它是「好」或「壞」，而是根據遊戲的結果（輸或是贏）或是遊戲中的信號（得分或失分）來做為回饋。<br /></li>
</ul>


<div id="orgd057fd2" class="figure">
<p><img src="images/ReinforcementLearning.png" alt="ReinforcementLearning.png" width="600" /><br />
</p>
<p><span class="figure-number">Figure 20: </span>強化學習流程圖</p>
</div>
</div>
<ol class="org-ol">
<li><a id="org6f9afbf"></a>範例<br />
<div class="outline-text-4" id="text-6-3-1">
<p>
典型的 reinforcement learning 包括<br />
</p>
<ul class="org-ul">
<li>DQN<br /></li>
<li>q-learning<br /></li>
</ul>
</div>
</li>
<li><a id="org751a4c0"></a>Flappy bird:<br />
<div class="outline-text-4" id="text-6-3-2">
<p>
在這個遊戲中，我們需要簡單的點擊操作來控制小鳥，躲過各種水管，飛的越遠越好，因為飛的越遠就能獲得更高的積分獎勵。這就是一個典型的強化學習場景：<br />
</p>
<ul class="org-ul">
<li>機器有一個明確的小鳥角色——代理<br /></li>
<li>需要控制小鳥飛的更遠——目標<br /></li>
<li>整個遊戲過程中需要躲避各種水管——環境<br /></li>
<li>躲避水管的方法是讓小鳥用力飛一下——行動<br /></li>
<li>飛的越遠，就會獲得越多的積分——獎勵<br /></li>
</ul>

<div id="org74ad3e1" class="figure">
<p><img src="images/ReinforceLearningGame.jpg" alt="ReinforceLearningGame.jpg" /><br />
</p>
<p><span class="figure-number">Figure 21: </span>強化學習流程圖</p>
</div>
</div>
</li>
</ol>
</div>
<div id="outline-container-org60c14d6" class="outline-3">
<h3 id="org60c14d6"><span class="section-number-3">6.4.</span> Batch and Online Learning<sup><a id="fnr.5" class="footref" href="#fn.5" role="doc-backlink">5</a></sup></h3>
<div class="outline-text-3" id="text-6-4">
</div>
<ol class="org-ol">
<li><a id="orgacb399f"></a>Batch learning: In <i>batch learning</i>, the system is incapable of learning incrementally; it must be trained using all the available data.<br /></li>
<li><a id="org8400f90"></a>Online learning:<br />
<div class="outline-text-4" id="text-6-4-2">
<ul class="org-ul">
<li>In <i>online learning</i>, you train the system incrementally by feeding it data instances sequentially, either individually or in small groups called <i>mini-batches</i>.<br /></li>
<li>Online learning is great for systems that receive data as a continuous flow (e.g., stock prices) and need to adapt to change rapidly or autonomously.<br /></li>
<li>Online learning algorithms can also be used to train systems on huge datasets that cannot fit in one machine&rsquo;s main memory (this is called <i>out-of-core</i> learning).<br /></li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org3b6ccb3" class="outline-3">
<h3 id="org3b6ccb3"><span class="section-number-3">6.5.</span> Instance-Based Versus model-Based Learning<sup><a id="fnr.5.100" class="footref" href="#fn.5" role="doc-backlink">5</a></sup></h3>
<div class="outline-text-3" id="text-6-5">
</div>
<ol class="org-ol">
<li><a id="orgf6d3a14"></a>Instance-based learning: the system learns the examples by heart, then generalizes to new cases by using a similarity measure to compare them the the learned examples.<br />
<div class="outline-text-4" id="text-6-5-1">
<ul class="org-ul">
<li>以垃圾郵件偵測為例，早期做法是由使用者將特定寄件人設定為垃圾來源，之後將同一寄件者寄出之郵件全標為垃圾；<br /></li>
<li>而 AI 的做法則是可以找出與現有被標為垃圾郵件十分「相似」的郵件，也將之標為垃圾郵件。<br /></li>
<li>重點在於相似性的計算<br /></li>
</ul>
</div>
</li>
<li><a id="org3410a89"></a>Model-based learning: Another way to generalize from a set of examples is to build a model of these examples and then use that model to make <i>predictions</i>.<br />
<div class="outline-text-4" id="text-6-5-2">
<ul class="org-ul">
<li>例，以 linear model 來預測人圴生產總值與該國人民滿意度的關係：<br />
<ol class="org-ol">
<li>model selection: decide to select what kind of model: \[lifeSatisfaction = \theta_0 + \theta_1 \times GDP \]<br /></li>
<li>define parameter value: how do you know which vsalues (\[\theta_0, \theta_1\]) will make your model perform best?<br /></li>
<li>evaluate model performance: you can either define a utility function (or <i>fitness function</i>) that measures how <i>good</i> your model is, or you can define a <i>cost function</i> that measures how <i>bad</i> it is.<br /></li>
</ol></li>
</ul>

<div id="orge94bc37" class="figure">
<p><img src="images/GDP.jpg" alt="GDP.jpg" width="600" /><br />
</p>
<p><span class="figure-number">Figure 22: </span>The linear model that fits the training data best</p>
</div>
<ul class="org-ul">
<li>CODE<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Download the data</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> urllib.request
<span class="linenr"> 3: </span><span style="color: #51afef;">import</span> os
<span class="linenr"> 4: </span><span style="color: #dcaeea;">datapath</span> = os.path.join(<span style="color: #98be65;">"datasets"</span>, <span style="color: #98be65;">"lifesat"</span>, <span style="color: #98be65;">""</span>)
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">prepare_country_stats</span>(oecd_bli, gdp_per_capita):
<span class="linenr"> 7: </span>    <span style="color: #dcaeea;">oecd_bli</span> = oecd_bli[oecd_bli[<span style="color: #98be65;">"INEQUALITY"</span>]==<span style="color: #98be65;">"TOT"</span>]
<span class="linenr"> 8: </span>    <span style="color: #dcaeea;">oecd_bli</span> = oecd_bli.pivot(index=<span style="color: #98be65;">"Country"</span>, columns=<span style="color: #98be65;">"Indicator"</span>, values=<span style="color: #98be65;">"Value"</span>)
<span class="linenr"> 9: </span>    gdp_per_capita.rename(columns={<span style="color: #98be65;">"2015"</span>: <span style="color: #98be65;">"GDP per capita"</span>}, inplace=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">10: </span>    gdp_per_capita.set_index(<span style="color: #98be65;">"Country"</span>, inplace=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">11: </span>    <span style="color: #dcaeea;">full_country_stats</span> = pd.merge(left=oecd_bli, right=gdp_per_capita,
<span class="linenr">12: </span>                                  left_index=<span style="color: #a9a1e1;">True</span>, right_index=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">13: </span>    full_country_stats.sort_values(by=<span style="color: #98be65;">"GDP per capita"</span>, inplace=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">14: </span>    <span style="color: #dcaeea;">remove_indices</span> = [<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">6</span>, <span style="color: #da8548; font-weight: bold;">8</span>, <span style="color: #da8548; font-weight: bold;">33</span>, <span style="color: #da8548; font-weight: bold;">34</span>, <span style="color: #da8548; font-weight: bold;">35</span>]
<span class="linenr">15: </span>    <span style="color: #dcaeea;">keep_indices</span> = <span style="color: #c678dd;">list</span>(<span style="color: #c678dd;">set</span>(<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">36</span>)) - <span style="color: #c678dd;">set</span>(remove_indices))
<span class="linenr">16: </span>    <span style="color: #51afef;">return</span> full_country_stats[[<span style="color: #98be65;">"GDP per capita"</span>, <span style="color: #98be65;">'Life satisfaction'</span>]].iloc[keep_indices]
<span class="linenr">17: </span>
<span class="linenr">18: </span><span style="color: #dcaeea;">DOWNLOAD_ROOT</span> = <span style="color: #98be65;">"https://raw.githubusercontent.com/ageron/handson-ml2/master/"</span>
<span class="linenr">19: </span>os.makedirs(datapath, exist_ok=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">20: </span><span style="color: #51afef;">for</span> filename <span style="color: #51afef;">in</span> (<span style="color: #98be65;">"oecd_bli_2015.csv"</span>, <span style="color: #98be65;">"gdp_per_capita.csv"</span>):
<span class="linenr">21: </span>    <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Downloading"</span>, filename)
<span class="linenr">22: </span>    <span style="color: #dcaeea;">url</span> = DOWNLOAD_ROOT + <span style="color: #98be65;">"datasets/lifesat/"</span> + filename
<span class="linenr">23: </span>    urllib.request.urlretrieve(url, datapath + filename)
<span class="linenr">24: </span>
<span class="linenr">25: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">26: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">27: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">28: </span><span style="color: #51afef;">import</span> sklearn.linear_model
<span class="linenr">29: </span>
<span class="linenr">30: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Load the data</span>
<span class="linenr">31: </span><span style="color: #dcaeea;">oecd_bli</span> = pd.read_csv(<span style="color: #98be65;">"./datasets/lifesat/oecd_bli_2015.csv"</span>, thousands=<span style="color: #98be65;">','</span>)
<span class="linenr">32: </span><span style="color: #dcaeea;">gdp_per_capita</span> = pd.read_csv(<span style="color: #98be65;">"./datasets/lifesat/gdp_per_capita.csv"</span>,thousands=<span style="color: #98be65;">','</span>,delimiter=<span style="color: #98be65;">'</span><span style="color: #a9a1e1;">\t</span><span style="color: #98be65;">'</span>, encoding=<span style="color: #98be65;">'latin1'</span>, na_values=<span style="color: #98be65;">"n/a"</span>)
<span class="linenr">33: </span>
<span class="linenr">34: </span>
<span class="linenr">35: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Prepare the data</span>
<span class="linenr">36: </span><span style="color: #dcaeea;">country_stats</span> = prepare_country_stats(oecd_bli, gdp_per_capita)
<span class="linenr">37: </span><span style="color: #dcaeea;">X</span> = np.c_[country_stats[<span style="color: #98be65;">"GDP per capita"</span>]]
<span class="linenr">38: </span><span style="color: #dcaeea;">y</span> = np.c_[country_stats[<span style="color: #98be65;">"Life satisfaction"</span>]]
<span class="linenr">39: </span>
<span class="linenr">40: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Visualize the data</span>
<span class="linenr">41: </span>plt.clf()
<span class="linenr">42: </span>
<span class="linenr">43: </span>country_stats.plot(kind=<span style="color: #98be65;">'scatter'</span>, x=<span style="color: #98be65;">"GDP per capita"</span>, y=<span style="color: #98be65;">'Life satisfaction'</span>)
<span class="linenr">44: </span><span style="color: #51afef;">import</span> matplotlib <span style="color: #51afef;">as</span> mpl
<span class="linenr">45: </span>mpl.rc(<span style="color: #98be65;">'axes'</span>, labelsize=<span style="color: #da8548; font-weight: bold;">14</span>)
<span class="linenr">46: </span>mpl.rc(<span style="color: #98be65;">'xtick'</span>, labelsize=<span style="color: #da8548; font-weight: bold;">12</span>)
<span class="linenr">47: </span>mpl.rc(<span style="color: #98be65;">'ytick'</span>, labelsize=<span style="color: #da8548; font-weight: bold;">12</span>)
<span class="linenr">48: </span>plt.plot()
<span class="linenr">49: </span>
<span class="linenr">50: </span>plt.savefig(<span style="color: #98be65;">"images/GDP.png"</span>)
<span class="linenr">51: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Select a linear model</span>
<span class="linenr">52: </span><span style="color: #dcaeea;">model</span> = sklearn.linear_model.LinearRegression()
<span class="linenr">53: </span>
<span class="linenr">54: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Train the model</span>
<span class="linenr">55: </span>model.fit(X, y)
<span class="linenr">56: </span>
<span class="linenr">57: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Make a prediction for Cyprus</span>
<span class="linenr">58: </span><span style="color: #dcaeea;">X_new</span> = [[<span style="color: #da8548; font-weight: bold;">22587</span>]]  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Cyprus's GDP per capita</span>
<span class="linenr">59: </span><span style="color: #c678dd;">print</span>(model.predict(X_new)) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">outputs [[ 5.96242338]]</span>
<span class="linenr">60: </span>
</pre>
</div>

<pre class="example">
Downloading oecd_bli_2015.csv
Downloading gdp_per_capita.csv
[[5.96242338]]
</pre>


<div id="orgb5cbd9d" class="figure">
<p><img src="images/GDP.png" alt="GDP.png" width="600" /><br />
</p>
<p><span class="figure-number">Figure 23: </span>The linear model that fits the training data best</p>
</div>
</div>
</li>
</ol>
</div>
</div>
<div id="outline-container-org8076c0a" class="outline-2">
<h2 id="org8076c0a"><span class="section-number-2">7.</span> Data Representations</h2>
<div class="outline-text-2" id="text-7">
<p>
How can we represent data (images, text, user preferences, etc.) in a way that computers can understand? -&gt; Organize information into a vector<sup><a id="fnr.2.100" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>.<br />
</p>
<ul class="org-ul">
<li>A vector is a 1-dimensional array of numbers. It has both a magnitude(length) and a direction.<br /></li>
<li><p>
A feature vector is a vector whose entries represent the &ldquo;features&rdquo; of some object.<br />
</p>

<div id="orgdd44c2c" class="figure">
<p><img src="images/feature-vector.jpg" alt="feature-vector.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 24: </span>Feature Vector</p>
</div></li>
</ul>
</div>
<div id="outline-container-orgbdef392" class="outline-3">
<h3 id="orgbdef392"><span class="section-number-3">7.1.</span> Images</h3>
<div class="outline-text-3" id="text-7-1">
<p>
In black and white images, <b>black and white pixels</b> correspond to 0s and 1s. Grayscale pixels are numbers between 0 and 255. Both assemble into a 1-dimensional array of numbers.<br />
</p>

<div id="org5968153" class="figure">
<p><img src="images/image-data-rep.jpg" alt="image-data-rep.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 25: </span>Image data representations</p>
</div>
</div>
</div>
<div id="outline-container-orgf0b6670" class="outline-3">
<h3 id="orgf0b6670"><span class="section-number-3">7.2.</span> Words and Documents</h3>
<div class="outline-text-3" id="text-7-2">
<p>
Given a collection of doucments (e.g. Wikipedia articles), assign to every word a vector whose \(i^{th}\) entry is the number of times the word appears in the \(i^{th}\) document.<br />
</p>
<p width="500">
<img src="images/word-rep.jpg" alt="word-rep.jpg" width="500" /><br />
These vectors can assemble into a large matrix, useful for <b>latent semantic analysis</b>.<br />
</p>
</div>
</div>
<div id="outline-container-orgfa25004" class="outline-3">
<h3 id="orgfa25004"><span class="section-number-3">7.3.</span> Yes/No or Ratings</h3>
<div class="outline-text-3" id="text-7-3">
<p>
Given users and items (e.g. movies), vectors can indicate if a user has interacted with the item (1=yes, 0=no) or the user&rsquo;s ratings, say a number between 0 and 5.<br />
</p>

<div id="org9988632" class="figure">
<p><img src="images/yn-rep.jpg" alt="yn-rep.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 26: </span>Y/N or Ratings</p>
</div>
</div>
</div>
<div id="outline-container-orga376da7" class="outline-3">
<h3 id="orga376da7"><span class="section-number-3">7.4.</span> One-Hot Encodings</h3>
<div class="outline-text-3" id="text-7-4">
<p>
Assign to each word a vector with one 1 and 0s elsewhere. This is called a one-hot encoding (or a &ldquo;standard basis vector&rdquo;). For example, suppose our language only has four words:<br />
</p>

<div id="org75b67ed" class="figure">
<p><img src="images/word-one-hot.jpg" alt="word-one-hot.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 27: </span>words one-hot encoding</p>
</div>
</div>
<ol class="org-ol">
<li><a id="orga811643"></a>drawbacks to consider:<br />
<ol class="org-ol">
<li><a id="orgb93e83c"></a>These vectors can be very sparse<br />
<div class="outline-text-5" id="text-7-4-1-1">
<ul class="org-ul">
<li>A &ldquo;sparse&rdquo; vector is one with lots of zeros<br /></li>
<li>500,000 users and 1,000,000 movies<br /></li>
</ul>
</div>
</li>
<li><a id="org49a8910"></a>Possible lack lf <b>meaningful relationships between</b> vectors<br />
<div class="outline-text-5" id="text-7-4-1-2">
<ul class="org-ul">
<li>One-hot encodings are never &ldquo;similar.&rdquo;<br /></li>
<li>Similarity is measured by the <b>dot product.</b><br /></li>
</ul>
</div>
</li>
</ol>
</li>
</ol>
</div>
</div>
<div id="outline-container-org3a14425" class="outline-2">
<h2 id="org3a14425"><span class="section-number-2">8.</span> 機器學習的主要挑戰<sup><a id="fnr.5.100" class="footref" href="#fn.5" role="doc-backlink">5</a></sup></h2>
<div class="outline-text-2" id="text-8">
</div>
<div id="outline-container-org9541efa" class="outline-3">
<h3 id="org9541efa"><span class="section-number-3">8.1.</span> Insufficient quantity of training data</h3>
<div class="outline-text-3" id="text-8-1">
<p>
Even for very simple problems you typically need thousands of examples, and for complex problems such as image or speech recognition you may need millions of examples. The idea that data matters more than algorithms for complex problems was further popularized by Peter norvig et al. in a paper titled &ldquo;The Unreasonable Effectiveness of Data&rdquo;, published in 2009.<br />
</p>
</div>
</div>
<div id="outline-container-org1c846b4" class="outline-3">
<h3 id="org1c846b4"><span class="section-number-3">8.2.</span> Nonrepresentative training data</h3>
<div class="outline-text-3" id="text-8-2">
<ul class="org-ul">
<li>In order to generalize well, it is crucial that your training data be representative of the new cases you want to generalize to.<br /></li>
<li>Perhaps the most famous example of sampling bias happened during the US presidential election in 1936, which pitted Landon against Roosevelt: the <i>Literary Digest</i> conducted a very large poll, sending mail to about 10 million people. The flaw was in the <i>Literary Digest&rsquo;s</i> sampling method:<br />
<ol class="org-ol">
<li>First, to obtain the addresses to send the polls to, the <i>Literary Digest</i> use telephone directories, lists of magazine subscribers, club membership lists, and the like.<br /></li>
<li>Second, less than 25% of the people who were polled answered.<br /></li>
</ol></li>
</ul>

<div id="orgab6ae08" class="figure">
<p><img src="images/GDP2.jpg" alt="GDP2.jpg" width="600" /><br />
</p>
<p><span class="figure-number">Figure 28: </span>加入更多資料會使 modle 更具代表性</p>
</div>
</div>
</div>
<div id="outline-container-orgc79f0a0" class="outline-3">
<h3 id="orgc79f0a0"><span class="section-number-3">8.3.</span> Poor-quality data</h3>
<div class="outline-text-3" id="text-8-3">
<p>
Obviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-quality measurements), it will make it harder for the system to detect the underlying patterns, so your system is less likely to perform well.<br />
</p>
</div>
</div>
<div id="outline-container-org65b8d00" class="outline-3">
<h3 id="org65b8d00"><span class="section-number-3">8.4.</span> Irrelevant features</h3>
</div>

<div id="outline-container-org03d2183" class="outline-3">
<h3 id="org03d2183"><span class="section-number-3">8.5.</span> Overfitting the training data</h3>
<div class="outline-text-3" id="text-8-5">
<ul class="org-ul">
<li>Overfitting happens when the model is too complex relative to the amount and noisiness of the training data. Here are possible solutions:<br />
<ol class="org-ol">
<li>Simplify the model by selecting one with fewer parameters.<br /></li>
<li>Gather more training data.<br /></li>
<li>Reduce the noise in the training data (e.g., fix data errors and remove outliers).<br /></li>
</ol></li>
<li>Constraining a model to make it simpler and reduce the risk of overfitting is called <i>regularization</i>.<br /></li>
<li>The amount of regularization to apply during learning can be controlled by a <i>hyperparameter</i>. A hyperparameter is a parameter of a learning algorithm (not of the model).<br /></li>
</ul>
</div>
</div>
<div id="outline-container-org89b1e29" class="outline-3">
<h3 id="org89b1e29"><span class="section-number-3">8.6.</span> Underfitting the training data</h3>
<div class="outline-text-3" id="text-8-6">
<p>
Underfitting occurs when your model is too simple to learn the underlying structure of the data.<br />
</p>
</div>
</div>
</div>
<div id="outline-container-org1ee91ac" class="outline-2">
<h2 id="org1ee91ac"><span class="section-number-2">9.</span> Machine Learning 的基本類型</h2>
</div>

<div id="outline-container-org120a34d" class="outline-2">
<h2 id="org120a34d"><span class="section-number-2">10.</span> Classfication</h2>
<div class="outline-text-2" id="text-10">
<p>
兩步驟<br />
</p>
</div>
<div id="outline-container-org691ca61" class="outline-3">
<h3 id="org691ca61"><span class="section-number-3">10.1.</span> Model construction: describing a set of predetermined classes</h3>
<div class="outline-text-3" id="text-10-1">
<ul class="org-ul">
<li>Each tuple/sample is assumed to belong to a predefined class, as determined by the class label attribute<br /></li>
<li>The set of tuples used for model construction is training set<br /></li>
<li>The model is represented as classfication urles, decision trees, or mathematical formulae.<br /></li>
</ul>
</div>
</div>
<div id="outline-container-org3982615" class="outline-3">
<h3 id="org3982615"><span class="section-number-3">10.2.</span> Model usage: for classifying future or unknown objects</h3>
<div class="outline-text-3" id="text-10-2">
<ul class="org-ul">
<li>Estimate accuracy of the model<br />
<ul class="org-ul">
<li>The Known label of test sample is compared with the classified result from the model<br /></li>
<li>Accuracy rate is the percentage of test set samples that are conrrectly classified by the model<br /></li>
<li>Test set is independent of training set (otherwise overfitting)<br /></li>
</ul></li>
<li>If the test set is used to select models, it is called validation (set) set.<br /></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org2ea22be" class="outline-2">
<h2 id="org2ea22be"><span class="section-number-2">11.</span> 學習資源</h2>
<div class="outline-text-2" id="text-11">
</div>
<div id="outline-container-org263d5fa" class="outline-3">
<h3 id="org263d5fa"><span class="section-number-3">11.1.</span> Machine Learning [台大李宏毅]</h3>
<div class="outline-text-3" id="text-11-1">
</div>
<ol class="org-ol">
<li><a id="org6d1a157"></a>Lecture 0<br />
<div class="outline-text-4" id="text-11-1-1">
<ul class="org-ul">
<li><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML19.html">ML19: 台大機器學習課程大綱</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=CXgbekl66jc">ML Lecture 0-1: Introduction of Machine Learning</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=On1N8u1z2Ng">ML Lecture 0-2: Why we need to learn machine learning?</a><br /></li>
</ul>
</div>
</li>
<li><a id="org6a25967"></a>Lecture 1<br />
<div class="outline-text-4" id="text-11-1-2">
<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=fegAeph9UaA">ML Lecture 1: Regression - Case Study</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=1UqCjFQiiy0">ML Lecture 1: Regression - Demo</a><br /></li>
</ul>
</div>
</li>
<li><a id="org493e9ce"></a>Lecture 2<br />
<div class="outline-text-4" id="text-11-1-3">
<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=D_S6y0Jm6dQ">ML Lecture 2: Where does the error come from?</a><br /></li>
</ul>
</div>
</li>
<li><a id="org64dfb9b"></a>Lecture 3<br />
<div class="outline-text-4" id="text-11-1-4">
<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=yKKNr-QKz2Q">ML Lecture 3-1: Gradient Descent</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=1_HBTJyWgNA">ML Lecture 3-2: Gradient Descent (Demo by AOE)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=wzPAInDF_gI">ML Lecture 3-3: Gradient Descent (Demo by Minecraft)</a><br /></li>
</ul>
</div>
</li>
<li><a id="orgdecfb98"></a>Lecture 4<br />
<div class="outline-text-4" id="text-11-1-5">
<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=fZAZUYEeIMg">ML Lecture 4: Classification</a><br /></li>
</ul>
</div>
</li>
<li><a id="org5bb136f"></a>Lecture 5<br />
<div class="outline-text-4" id="text-11-1-6">
<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=hSXFuypLukA">ML Lecture 5: Logistic Regression</a><br /></li>
</ul>
</div>
</li>
<li><a id="orge3563e2"></a>Lecture 6<br />
<div class="outline-text-4" id="text-11-1-7">
<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=Dr-WRlEFefw">ML Lecture 6: Brief Introduction of Deep Learning</a><br /></li>
</ul>
</div>
</li>
<li><a id="orgbf8127f"></a>Lecture 7<br />
<div class="outline-text-4" id="text-11-1-8">
<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=ibJpTrp5mcE">ML Lecture 7: Backpropagation</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=gDp2LXGnVLQ&amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;index=3&amp;t=0s">Anomaly Detection (1/7)</a><br /></li>
<li><a href="https://www.youtube.com/playlist?list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4">Next Step of Machine Learning (Hung-yi Lee, NTU, 2019)</a><br /></li>
</ul>
</div>
</li>
<li><a id="org751d370"></a>Lecture 8<br />
<div class="outline-text-4" id="text-11-1-9">
<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=Lx3l4lOrquw">ML Lecture 8-1: “Hello world” of deep learning</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=5BJDJd-dzzg">ML Lecture 8-2: Keras 2.0</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=L8unuZNpWw8">ML Lecture 8-3: Keras Demo</a><br /></li>
</ul>
</div>
</li>
<li><a id="orgd0c2a72"></a>Explainable ML<br />
<div class="outline-text-4" id="text-11-1-10">
<ol class="org-ol">
<li><a href="https://www.youtube.com/watch?v=lnjrn3bF9lA">Explainable ML (1/8)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=pNpk6DPYUh8&amp;list=PL9McrqOpq3mUCXF5E8rVLjw8f878zkBfX&amp;index=16">Explainable ML (2/8)</a><br /></li>
<li>Explainable ML (3/8)<br /></li>
<li><a href="https://www.youtube.com/watch?v=yORbWn7UsBs">Explainable ML (4/8)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=1xnhQbAV1m0">Explainable ML (5/8)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=K1mWgthGS-A">Explainable ML (6/8)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=1xnhQbAV1m0">Explainable ML (7/8)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=ah_Ttx6cIVU">Explainable ML (8/8)</a><br /></li>
</ol>
</div>
</li>
<li><a id="orgeb59aa3"></a><span class="todo TODO">TODO</span> Attack ML Models<br />
<div class="outline-text-4" id="text-11-1-11">
<ol class="org-ol">
<li><a href="https://www.youtube.com/watch?v=NI6yb0WgMBM">Attack ML Models (1/8)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=zOdg05BwE7I">Attack ML Models (2/8)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=F9N5zF7N0qY">Attack ML Models (3/8)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=qjnMoWmn1FQ">Attack ML Models (4/8)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=2mgLPZJOHNk">Attack ML Models (5/8)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=z2nmPDLEXI0">Attack ML Models (6/8)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=KH48zq2RfBA&amp;t=1s">Attack ML Models (7/8)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=ah_Ttx6cIVU">Attack ML Models (8/8)</a><br /></li>
</ol>
</div>
</li>
<li><a id="org868dc75"></a>Lecture 9<br />
<div class="outline-text-4" id="text-11-1-12">
<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=xki61j7z-30">ML Lecture 9-1: Tips for Training DNN</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=Ky1ku1miDow">ML Lecture 9-2: Keras Demo 2</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=F1vek6ULo9w">ML Lecture 9-3: Fizz Buzz in Tensorflow (sequel)</a><br /></li>
</ul>
</div>
</li>
<li><a id="org1e1b2f8"></a>Lecture 10<br />
<div class="outline-text-4" id="text-11-1-13">
<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=FrKWiRv254g">ML Lecture 10: Convolutional Neural Network</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=XsC9byQkUH8">ML Lecture 11: Why Deep?</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=fX_guE7JNnY">ML Lecture 12: Semi-supervised</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=iwh5o_M4BNU">ML Lecture 13: Unsupervised Learning - Linear Methods</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=X7PH3NuYW0Q">ML Lecture 14: Unsupervised Learning - Word Embedding</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=GBUEjkpoxXc">ML Lecture 15: Unsupervised Learning - Neighbor Embedding</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=yyKaACh_j3M&amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;index=45&amp;t=0s">Meta Learning – Metric-based (1/3)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=Tk5B4seA-AU">ML Lecture 16: Unsupervised Learning - Auto-encoder</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=YNUek8ioAJk">ML Lecture 17: Unsupervised Learning - Deep Generative Model (Part I)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=8zomhgKrsmQ">ML Lecture 18: Unsupervised Learning - Deep Generative Model (Part II)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=6ZWu4L7XOiQ&amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;index=48&amp;t=0s">More about Auto-encoder (1/4)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=qD6iD4TFsdQ">ML Lecture 19: Transfer Learning</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=7qT5P9KJnWo&amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;index=25">Life Long Learning (1/7)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=ZjfjPzXw6og&amp;feature=youtu.be">Sequence-to-sequence Learning</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=EkAqYbpCYAc&amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;index=33&amp;t=0s">Meta Learning – MAML (1/9)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=QSEPStBgwRQ">ML Lecture 20: Support Vector Machine (SVM)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=xCGidAeyS4M">ML Lecture 21-1: Recurrent Neural Network (Part I)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=rTqmWlnwz_0">ML Lecture 21-2: Recurrent Neural Network (Part II)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=YIuBHB9Ejok&amp;feature=youtu.be">Unsupervised Syntactic Parsing (ft. 莊永松同學)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=tH9FH1DH5n0">ML Lecture 22: Ensemble</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=W8XF3ME8G2I">ML Lecture 23-1: Deep Reinforcement Learning</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=y8UPGr36ccI">ML Lecture 23-2: Policy Gradient (Supplementary Explanation)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=2-JNBzCq77c">ML Lecture 23-3: Reinforcement Learning (including Q-learning)</a><br /></li>
<li><a href="https://www.youtube.com/playlist?list=PLJV_el3uVTsODxQFgzMzPLa16h6B8kWM_">Deep Reinforcement Learning, 2018</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=dPp8rCAnU_A&amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;index=52&amp;t=0s">Network Compression (1/6)</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=ufcKFjdpT98&amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;index=58&amp;t=0s">GAN (Quick Review)</a><br /></li>
<li><a href="https://www.youtube.com/playlist?list=PLJV_el3uVTsMq6JEFPW35BCiOQTsoqwNw">Generative Adversarial Network (GAN), 2018</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=ugWDIIOHtPA&amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;index=58">Transformer</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=UYPa347-DdE&amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;index=62&amp;t=0s">ELMO, BERT, GPT</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=uXY18nzdSsM&amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;index=59">Flow-based Generative Model</a><br /></li>
<li><a href="https://brohrer.mcknote.com/zh-Hant/statistics/how_bayesian_inference_works.html">貝葉斯推斷的運作原理</a><br /></li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org5049831" class="outline-3">
<h3 id="org5049831"><span class="section-number-3">11.2.</span> Digital Speech Processing</h3>
<div class="outline-text-3" id="text-11-2">
<ul class="org-ul">
<li><a href="http://ocw.aca.ntu.edu.tw/ntu-ocw/ocw/cou/104S204/1">第一章 Introduction to Digital Speech Processing  </a><br /></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org2630463" class="outline-2">
<h2 id="org2630463"><span class="section-number-2">12.</span> code references</h2>
<div class="outline-text-2" id="text-12">
<ul class="org-ul">
<li><a href="https://towardsdatascience.com/visualizing-intermediate-activation-in-convolutional-neural-networks-with-keras-260b36d60d0">Visualizing intermediate activation in Convolutional Neural Networks with Keras</a>: 查看 training 中 image 的成像<br /></li>
</ul>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://github.com/rasbt/python-machine-learning-book-2nd-edition">python-machine-learning-book-2nd-edition</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://youtu.be/LlKAna21fLE">A friendly introduction to linear algebra for ML (ML Tech Talks)</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://zh.wikipedia.org/zh-tw/AlphaGo_Zero">AlphaGo Zero</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4" role="doc-backlink">4</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://zh.wikipedia.org/wiki/AlphaZero">AlphaZero</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.5" class="footnum" href="#fnr.5" role="doc-backlink">5</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://www.books.com.tw/products/F014278520">Hands-On Machine Learning with Scikit-Learn, Keras, and Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems</a><br />
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Yung-Chin Yen</p>
<p class="date">Created: 2023-12-03 Sun 21:59</p>
</div>
</body>
</html>