<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-02-02 Fri 10:25 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>機器學習</title>
<meta name="author" content="Yung-Chin Yen" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="../css/muse.css" />
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">機器學習</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org74b3943">1. 機器學習</a>
<ul>
<li><a href="#org9b1c25f">1.1. 簡介</a></li>
<li><a href="#orgeb96274">1.2. 何謂機器學習</a></li>
</ul>
</li>
<li><a href="#org61cc06b">2. 機器是如何學習的</a>
<ul>
<li><a href="#org3ffaab5">2.1. 傳統程式設計</a></li>
<li><a href="#orga999ba5">2.2. 機器學習的策略</a></li>
<li><a href="#org61ea1d6">2.3. 實作1</a></li>
<li><a href="#orgbb9e10f">2.4. 實作2</a></li>
<li><a href="#org50485dc">2.5. 實作3</a></li>
<li><a href="#orgb6dfe7d">2.6. 程式解說</a></li>
</ul>
</li>
<li><a href="#orge00729e">3. 機器學習的類型</a>
<ul>
<li><a href="#org1a95ffa">3.1. 監督式學習(Supervised learning)</a></li>
<li><a href="#org0903869">3.2. 非監督式學習(Unsupervised learning)</a></li>
<li><a href="#org29d58f0">3.3. 半監督式學習</a></li>
<li><a href="#orgef27935">3.4. 增強式學習</a></li>
<li><a href="#org4eb3a76">3.5. 依據AI模型是否能由即時資料流進行增量學習來區分</a></li>
</ul>
</li>
<li><a href="#org8740613">4. 機器學習的程序與資料呈現方式</a>
<ul>
<li><a href="#org1795275">4.1. 機器學習如何解決問題</a></li>
<li><a href="#org3585221">4.2. 機器學習的資料呈現方式</a></li>
<li><a href="#orge591824">4.3. Images</a></li>
<li><a href="#orgc626493">4.4. Words and Documents</a></li>
<li><a href="#org4749478">4.5. Yes/No or Ratings</a></li>
<li><a href="#orgb74e6c7">4.6. One-Hot Encodings</a></li>
</ul>
</li>
<li><a href="#orge234c43">5. 機器學習的主要挑戰</a>
<ul>
<li><a href="#org000335c">5.1. Insufficient quantity of training data</a></li>
<li><a href="#org2635b5b">5.2. Nonrepresentative training data</a></li>
<li><a href="#org0345078">5.3. Poor-quality data</a></li>
<li><a href="#org3af3347">5.4. Irrelevant features</a></li>
<li><a href="#org4763540">5.5. Overfitting the training data</a></li>
<li><a href="#org5d16800">5.6. Underfitting the training data</a></li>
</ul>
</li>
<li><a href="#org2d55279">6. 典型的機器學習MODEL Training[實作]</a>
<ul>
<li><a href="#org903741d">6.1. Process of learning</a></li>
<li><a href="#org6bba271">6.2. Example of training</a></li>
<li><a href="#org3222b8e">6.3. Model v.s. Layer: Training step by step</a></li>
<li><a href="#org93bd9f3">6.4. compile v.s. fit</a></li>
</ul>
</li>
</ul>
</div>
</div>
<a href="https://letranger.github.io/AI/20221023101456-機器學習.html"><img align="right" alt="Hits" src="https://hits.sh/letranger.github.io/AI/20221023101456-機器學習.html.svg"/></a>
<div id="outline-container-org74b3943" class="outline-2">
<h2 id="org74b3943"><span class="section-number-2">1.</span> 機器學習</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org9b1c25f" class="outline-3">
<h3 id="org9b1c25f"><span class="section-number-3">1.1.</span> 簡介</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li>機器學習是<a href="20221023101139-人工智慧.html#ID-20221023T101138.945879">人工智慧</a>的一個分支，在<a href="20221023101139-人工智慧.html#ID-20221023T101138.945879">人工智慧</a>的研究歷史有著一條從以「推理」為重點，到以「知識」為重點，再到以 「學習」為重點的自然、清晰的脈絡。<br /></li>
<li>機器學習是實現<a href="AI-Introduction.html">人工智慧</a>的一個途徑，即以機器學習為手段解決<a href="AI-Introduction.html">人工智慧</a>中的問題。<br /></li>
<li>機器學習理論主要是設計和分析一些讓電腦可以自動「學習」 的演算法。機器學習演算法是一類從資料中自動分析獲得規 律，並利用規律對未知資料進行預測的演算法。<br /></li>
<li>機器學習已廣泛應用於資料探勘、電腦視覺、自然語言處理、<br /></li>
<li>生物特徵辨識、搜尋引擎、醫學診斷、檢測信用卡欺詐、證券市場分析、DNA 序列測序、語音和手寫辨識、戰略遊戲和機器人等領域<br /></li>
<li>機器學習是一門<a href="AI-Introduction.html">人工智慧</a>的科學，該領域的主要研究物件是<a href="AI-Introduction.html">人工智慧</a>，特別是如何=在經驗學習中=改善具體演算法的效能。<br /></li>
<li>機器學習是對能通過經驗=自動改進=的電腦演算法研究<br /></li>
<li>機器學習是用資料或以往的經驗，以此最佳化電 腦程式的效能標準。<br /></li>
</ul>
</div>
</div>
<div id="outline-container-orgeb96274" class="outline-3">
<h3 id="orgeb96274"><span class="section-number-3">1.2.</span> 何謂機器學習</h3>
<div class="outline-text-3" id="text-1-2">
<ul class="org-ul">
<li>機器學習的定義之一：一個可以與其環境做互動的系統。具有<a href="20221023103538-感知器.html#ID-20221023T103538.640537">感知器</a>(sensor)，可以讓機器了解它們所處的環境，以及它們也具有相關的工具可以讓機器做出回應<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>。<br /></li>
<li>機器學習可以大致分為三類：<a href="20221023101626-監督式學習.html#ID-20221023T101626.420918">監督式學習</a>、<a href="20221023101716-非監督式學習.html#ID-20221023T101716.467694">非監督式學習</a>、<a href="20230210172919-增強式學習.html#ID-0a5c37c0-741a-4a1a-bec7-f98074830132">增強式學習</a>，如圖<a href="#org922aae7">1</a><sup><a id="fnr.1.100" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>。<br /></li>
</ul>


<div id="org922aae7" class="figure">
<p><img src="images/MLTypes.png" alt="MLTypes.png" width="700" /><br />
</p>
<p><span class="figure-number">Figure 1: </span>三種不同的機器學習</p>
</div>

<ul class="org-ul">
<li>機器學習就是從一組function中找出一個最合適的function的過程[李宏毅]。<br /></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org61cc06b" class="outline-2">
<h2 id="org61cc06b"><span class="section-number-2">2.</span> 機器是如何學習的</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org3ffaab5" class="outline-3">
<h3 id="org3ffaab5"><span class="section-number-3">2.1.</span> 傳統程式設計</h3>
<div class="outline-text-3" id="text-2-1">
<p>
假設有一個用來描述直線的函數(模型): \(y=wx+b\)，直線上的每個點都可以用 \(x\) 值乘以\(w\)(權重值)再加上\(b\)(偏差值)，得到相應的 \(y\) 值。<br />
現在假設直線上有兩個點，分別是 \(x=2, y=3\) 和 \(x=3, y=5\) (如圖<a href="#org33f63dd">2</a>)，那麼，請問當 \(x=10\) 時，\(y\) 的值是多少呢？<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">x</span> = [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>]
<span class="linenr"> 5: </span><span style="color: #dcaeea;">y</span> = [<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">5</span>]
<span class="linenr"> 6: </span>plt.plot(x, y, <span style="color: #98be65;">'-*'</span>)
<span class="linenr"> 7: </span>plt.xlim(<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">5</span>)
<span class="linenr"> 8: </span>plt.ylim(<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">7</span>)
<span class="linenr"> 9: </span>plt.text(<span style="color: #da8548; font-weight: bold;">2.1</span>, <span style="color: #da8548; font-weight: bold;">3</span>-<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #98be65;">'(2, 3)'</span>)
<span class="linenr">10: </span>plt.text(<span style="color: #da8548; font-weight: bold;">3.1</span>, <span style="color: #da8548; font-weight: bold;">5</span>-<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #98be65;">'(3, 5)'</span>)
<span class="linenr">11: </span>plt.savefig(<span style="color: #98be65;">'images/TrandPlot.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>

<div id="org33f63dd" class="figure">
<p><img src="images/TrandPlot.png" alt="TrandPlot.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 2: </span>一個直線函數問題</p>
</div>

<p>
以下是傳統的程式設計模式，受了十數年高深數學教育的你，第一反應大概是會想先求出連接這兩個點的直線所相應的 \(w\) 值與 \(b\) 值(也就是模型中的兩個參數)。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org1b6751b"></a>解法1: class版<br />
<div class="outline-text-4" id="text-2-1-1">
<p>
看不懂的可以看底下的tuple版<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">def</span> <span style="color: #c678dd;">get_slope</span>(p1, p2):
    <span style="color: #dcaeea;">w</span> = (p2.y - p1.y) / (p2.x - p1.x)
    <span style="color: #51afef;">return</span> w

<span style="color: #51afef;">def</span> <span style="color: #c678dd;">get_bias</span>(p1, w):
    <span style="color: #dcaeea;">b</span> = p1.y - (w*p1.x)
    <span style="color: #51afef;">return</span> b

<span style="color: #51afef;">def</span> <span style="color: #c678dd;">get_y</span>(x, w, b):
    <span style="color: #dcaeea;">y</span> = w*x + b

<span style="color: #51afef;">from</span> dataclasses <span style="color: #51afef;">import</span> dataclass
<span style="color: #ECBE7B;">@dataclass</span>
<span style="color: #51afef;">class</span> <span style="color: #ECBE7B;">Point</span>:
    x: <span style="color: #c678dd;">float</span>
    y: <span style="color: #c678dd;">float</span>

<span style="color: #dcaeea;">p1</span> = Point(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>)
<span style="color: #dcaeea;">p2</span> = Point(<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">5</span>)

<span style="color: #dcaeea;">w</span> = get_slope(p1, p2)
<span style="color: #dcaeea;">b</span> = get_bias(p1, w)

<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Slope:"</span>, w)
<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Bias:"</span>, b)
<span style="color: #c678dd;">print</span>(f<span style="color: #98be65;">'&#30070;x=10&#26178;&#65292;y=</span>{w}<span style="color: #98be65;">*10+</span>{b}<span style="color: #98be65;">=</span>{w*10+b}<span style="color: #98be65;">'</span>)
</pre>
</div>

<pre class="example">
Slope: 2.0
Bias: -1.0
當x=10時，y=2.0*10+-1.0=19.0
</pre>
</div>
</li>
<li><a id="orgf7900f2"></a>解法2: tuple版<br />
<div class="outline-text-4" id="text-2-1-2">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">def</span> <span style="color: #c678dd;">get_slope</span>(p1, p2):
    <span style="color: #dcaeea;">p1x</span>, <span style="color: #dcaeea;">p1y</span> = p1 <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#21462;&#20986;tuple&#20013;&#30340;(x, y)</span>
    <span style="color: #dcaeea;">p2x</span>, <span style="color: #dcaeea;">p2y</span> = p2 <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#21462;&#20986;tuple&#20013;&#30340;(x, y)</span>
    <span style="color: #dcaeea;">w</span> = (p2y - p1y) / (p2x - p1x)
    <span style="color: #51afef;">return</span> w

<span style="color: #51afef;">def</span> <span style="color: #c678dd;">get_bias</span>(p1, w):
    <span style="color: #dcaeea;">p1x</span>, <span style="color: #dcaeea;">p1y</span> = p1 <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#21462;&#20986;tuple&#20013;&#30340;(x, y)</span>
    <span style="color: #dcaeea;">b</span> = p1y - (w*p1x)
    <span style="color: #51afef;">return</span> b

<span style="color: #51afef;">def</span> <span style="color: #c678dd;">get_y</span>(x, w, b):
    <span style="color: #dcaeea;">y</span> = w*x + b

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;tuple&#20358;&#25551;&#36848;(x, y)</span>
<span style="color: #dcaeea;">p1</span> = (<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>)
<span style="color: #dcaeea;">p2</span> = (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">5</span>)

<span style="color: #dcaeea;">w</span> = get_slope(p1, p2)
<span style="color: #dcaeea;">b</span> = get_bias(p1, w)

<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Slope:"</span>, w)
<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Bias:"</span>, b)
<span style="color: #c678dd;">print</span>(f<span style="color: #98be65;">'&#30070;x=10&#26178;&#65292;y=</span>{w}<span style="color: #98be65;">*10+</span>{b}<span style="color: #98be65;">=</span>{w*10+b}<span style="color: #98be65;">'</span>)
</pre>
</div>

<pre class="example">
Slope: 2.0
Bias: -1.0
當x=10時，y=2.0*10+-1.0=19.0
</pre>
</div>
</li>
<li><a id="org9585ab2"></a>傳統的程式設計模式<br />
<div class="outline-text-4" id="text-2-1-3">
<p>
由以上的程式碼可以看出傳統的解題模式為：<br />
</p>
<ol class="org-ol">
<li>給資料(兩個點)<br /></li>
<li>給規則(公式)<br /></li>
</ol>
<p>
接下來就以程式來解出答案，那麼，面臨一樣的問題，機器學習的解題策略又是如何呢？<br />
</p>


<div id="org7fd0de2" class="figure">
<p><img src="images/TradProg.png" alt="TradProg.png" /><br />
</p>
</div>
</div>
</li>
</ol>
</div>
<div id="outline-container-orga999ba5" class="outline-3">
<h3 id="orga999ba5"><span class="section-number-3">2.2.</span> 機器學習的策略</h3>
<div class="outline-text-3" id="text-2-2">
<p>
機器學習把這個線性函數當成一個模型，而我們想求的 \(w\) 和 \(b\) 則為模型的兩個參數，如果要以機器學習的方式來解題，其策略大致如下：<br />
</p>
<ol class="org-ol">
<li>第一步：猜答案<br />
一開始我們並不知道正確答案是什麼，所以用猜的：以隨機亂數來做為 \(w\) 和\(b\) 值，例如：\(y=10x+5\)。<br /></li>
<li>第二步：評估猜測答案的品質<br />
上個步驟中所猜的 \(w\) 和\(b\) 值夠不夠準確呢？我們可以用\(y=10x+5\) 這個函數來計算出每個 \(x\) 值相對應的 \(y\) 值，再和 <b>正確</b> 的 \(y\) 值比較，看看還差多少。這個比較的方式稱為「損失」(loss)或「誤差」(error)。<br /></li>
<li>第三步：對所猜測的策略進行最佳化調整<br />
依據上一步驟猜測結果的品質(loss或error)做出更好的猜測，這個步驟稱為「最佳化」(optimization)。微積分可以用「梯度遞減」(gradient descent)的方式來進行。<br /></li>
</ol>

<p>
整個流程大致如下：<br />
</p>

<div id="org01c7247" class="figure">
<p><img src="images/MLProg.png" alt="MLProg.png" /><br />
</p>
</div>
</div>
</div>
<div id="outline-container-org61ea1d6" class="outline-3">
<h3 id="org61ea1d6"><span class="section-number-3">2.3.</span> 實作1</h3>
<div class="outline-text-3" id="text-2-3">
<div class="org-src-container">
<pre class="src src-shell">pip3 install tensorflow
</pre>
</div>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span style="color: #51afef;">from</span> tensorflow.keras <span style="color: #51afef;">import</span> Sequential
<span style="color: #51afef;">from</span> tensorflow.keras.layers <span style="color: #51afef;">import</span> Dense

<span style="color: #dcaeea;">l0</span> = Dense(units=<span style="color: #da8548; font-weight: bold;">1</span>, input_shape=[<span style="color: #da8548; font-weight: bold;">1</span>])
<span style="color: #dcaeea;">model</span> = Sequential([l0])
model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'sgd'</span>, loss=<span style="color: #98be65;">'mean_squared_error'</span>)

<span style="color: #dcaeea;">xs</span> = np.array([<span style="color: #da8548; font-weight: bold;">2.0</span>, <span style="color: #da8548; font-weight: bold;">3.0</span>], dtype=<span style="color: #c678dd;">float</span>)
<span style="color: #dcaeea;">ys</span> = np.array([<span style="color: #da8548; font-weight: bold;">3.0</span>, <span style="color: #da8548; font-weight: bold;">5.0</span>], dtype=<span style="color: #c678dd;">float</span>)

model.fit(xs, ys, epochs=<span style="color: #da8548; font-weight: bold;">500</span>, verbose=<span style="color: #da8548; font-weight: bold;">0</span>)

<span style="color: #c678dd;">print</span>(f<span style="color: #98be65;">'y&#30340;&#38928;&#28204;&#32080;&#26524;&#28858;: </span>{model.predict([10.0])}<span style="color: #98be65;">'</span>)
<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#27169;&#22411;&#30340;&#20841;&#20491;&#21443;&#25976;w,b: {}"</span>.<span style="color: #c678dd;">format</span>(l0.get_weights()))
</pre>
</div>

<pre class="example">
1/1 [==============================] - 0s 172ms/step
y的預測結果為: 14.709847
模型的兩個參數w, b: [array([[1.4212971]], dtype=float32), array([0.49687672], dtype=float32)]
</pre>


<p>
但是正確答案為19，預測的結果好像不太準&#x2026;.QQ<br />
</p>
</div>
</div>
<div id="outline-container-orgbb9e10f" class="outline-3">
<h3 id="orgbb9e10f"><span class="section-number-3">2.4.</span> 實作2</h3>
<div class="outline-text-3" id="text-2-4">
<p>
如果我們向上帝偷偷多要一個模型的參照點(4, 7)，以三點當成標準答案來進行預測，結果會不會好一點呢?<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span style="color: #51afef;">from</span> tensorflow.keras <span style="color: #51afef;">import</span> Sequential
<span style="color: #51afef;">from</span> tensorflow.keras.layers <span style="color: #51afef;">import</span> Dense

<span style="color: #dcaeea;">l0</span> = Dense(units=<span style="color: #da8548; font-weight: bold;">1</span>, input_shape=[<span style="color: #da8548; font-weight: bold;">1</span>])
<span style="color: #dcaeea;">model</span> = Sequential([l0])
model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'sgd'</span>, loss=<span style="color: #98be65;">'mean_squared_error'</span>)

<span style="color: #dcaeea;">xs</span> = np.array([<span style="color: #da8548; font-weight: bold;">2.0</span>, <span style="color: #da8548; font-weight: bold;">3.0</span>, <span style="color: #da8548; font-weight: bold;">4.0</span>], dtype=<span style="color: #c678dd;">float</span>)
<span style="color: #dcaeea;">ys</span> = np.array([<span style="color: #da8548; font-weight: bold;">3.0</span>, <span style="color: #da8548; font-weight: bold;">5.0</span>, <span style="color: #da8548; font-weight: bold;">7.0</span>], dtype=<span style="color: #c678dd;">float</span>)

model.fit(xs, ys, epochs=<span style="color: #da8548; font-weight: bold;">500</span>, verbose=<span style="color: #da8548; font-weight: bold;">0</span>)

<span style="color: #c678dd;">print</span>(f<span style="color: #98be65;">'y&#30340;&#38928;&#28204;&#32080;&#26524;&#28858;: </span>{model.predict([10.0])}<span style="color: #98be65;">'</span>)
<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#27169;&#22411;&#30340;&#20841;&#20491;&#21443;&#25976;w,b: {}"</span>.<span style="color: #c678dd;">format</span>(l0.get_weights()))
</pre>
</div>

<pre class="example">
y的預測結果為: [[17.07221]]
模型的兩個參數w,b: [array([[1.7164488]], dtype=float32), array([-0.09227743], dtype=float32)]
</pre>


<p>
預測結果y的值為17.072221，而正確答案為19，好像也不怎麼樣&#x2026;.-_-<br />
兩個參數的值為(1.7164488, -0.9227743)，與正確答案(2, -1)相近。<br />
</p>
</div>
</div>
<div id="outline-container-org50485dc" class="outline-3">
<h3 id="org50485dc"><span class="section-number-3">2.5.</span> 實作3</h3>
<div class="outline-text-3" id="text-2-5">
<p>
讓我們壯起膽子再跟上帝多要兩個線上的點:(-1, -3)、(0, -1)，以五點當成標準答案來進行預測，其實作的程式碼為：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span><span style="color: #51afef;">from</span> tensorflow.keras <span style="color: #51afef;">import</span> Sequential
<span class="linenr"> 4: </span><span style="color: #51afef;">from</span> tensorflow.keras.layers <span style="color: #51afef;">import</span> Dense
<span class="linenr"> 5: </span>
<span id="coderef-neuron" class="coderef-off"><span class="linenr"> 6: </span><span style="color: #dcaeea;">l0</span> = Dense(units=<span style="color: #da8548; font-weight: bold;">1</span>, input_shape=[<span style="color: #da8548; font-weight: bold;">1</span>])</span>
<span id="coderef-model" class="coderef-off"><span class="linenr"> 7: </span><span style="color: #dcaeea;">model</span> = Sequential([l0])</span>
<span id="coderef-modelCompile" class="coderef-off"><span class="linenr"> 8: </span>model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'sgd'</span>, loss=<span style="color: #98be65;">'mean_squared_error'</span>)</span>
<span class="linenr"> 9: </span>
<span class="linenr">10: </span><span style="color: #dcaeea;">xs</span> = np.array([-<span style="color: #da8548; font-weight: bold;">1.0</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.0</span>, <span style="color: #da8548; font-weight: bold;">2.0</span>, <span style="color: #da8548; font-weight: bold;">3.0</span>, <span style="color: #da8548; font-weight: bold;">4.0</span>], dtype=<span style="color: #c678dd;">float</span>)
<span class="linenr">11: </span><span style="color: #dcaeea;">ys</span> = np.array([-<span style="color: #da8548; font-weight: bold;">3.0</span>, -<span style="color: #da8548; font-weight: bold;">1.0</span>, <span style="color: #da8548; font-weight: bold;">1.0</span>, <span style="color: #da8548; font-weight: bold;">3.0</span>, <span style="color: #da8548; font-weight: bold;">5.0</span>, <span style="color: #da8548; font-weight: bold;">7.0</span>], dtype=<span style="color: #c678dd;">float</span>)
<span class="linenr">12: </span>
<span id="coderef-modelFit" class="coderef-off"><span class="linenr">13: </span>model.fit(xs, ys, epochs=<span style="color: #da8548; font-weight: bold;">500</span>, verbose=<span style="color: #da8548; font-weight: bold;">0</span>)</span>
<span class="linenr">14: </span>
<span id="coderef-modelPredict" class="coderef-off"><span class="linenr">15: </span><span style="color: #c678dd;">print</span>(f<span style="color: #98be65;">'y&#30340;&#38928;&#28204;&#32080;&#26524;&#28858;: </span>{model.predict([10.0])}<span style="color: #98be65;">'</span>)</span>
<span id="coderef-modelWeights" class="coderef-off"><span class="linenr">16: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#27169;&#22411;&#30340;&#20841;&#20491;&#21443;&#25976;w,b: {}"</span>.<span style="color: #c678dd;">format</span>(l0.get_weights()))</span>
</pre>
</div>

<pre class="example">
y的預測結果為:[[18.979391]]
模型的兩個參數w,b: [array([[1.9970131]], dtype=float32), array([-0.99074], dtype=float32)]
</pre>


<p>
模型對 \(y\) 的預測結果為18.979391，與正確答案19已經十分相近。<br />
</p>
</div>
</div>
<div id="outline-container-orgb6dfe7d" class="outline-3">
<h3 id="orgb6dfe7d"><span class="section-number-3">2.6.</span> 程式解說</h3>
<div class="outline-text-3" id="text-2-6">
<ol class="org-ol">
<li>上述程式使用tensorflow這個模組為主要預測工具，建立了一個模型(第<a href="#coderef-model" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-model');" onmouseout="CodeHighlightOff(this, 'coderef-model');">7</a>行)，這個模型中只有一層、裡面有一個神經元(第<a href="#coderef-neuron" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-neuron');" onmouseout="CodeHighlightOff(this, 'coderef-neuron');">6</a>行)<br /></li>
<li>定義好模型後，我們以SGD(隨機梯度下降法)為優化器、用它來找出最佳參數；以MSE(均方差)做為損失函數，計算模型訓練過程中生成的參數優劣判斷標準(第<a href="#coderef-modelCompile" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-modelCompile');" onmouseout="CodeHighlightOff(this, 'coderef-modelCompile');">8</a>行)，並編譯此模型<br /></li>
<li>編譯好模型，我們就拿手僅有的5組資料來訓練模型，訓練500回(第<a href="#coderef-modelFit" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-modelFit');" onmouseout="CodeHighlightOff(this, 'coderef-modelFit');">13</a>行)<br /></li>
<li>訓練好的模型就能拿來預測了，我們拿 \(x=10\) 當成預測資料，要求模型給出預測的結果 \(y\) (第<a href="#coderef-modelPredict" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-modelPredict');" onmouseout="CodeHighlightOff(this, 'coderef-modelPredict');">15</a>行)<br /></li>
<li>偷偷看一下這個模型給出答案的關鍵、也就是模型的兩個參數(Weight)(第<a href="#coderef-modelWeights" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-modelWeights');" onmouseout="CodeHighlightOff(this, 'coderef-modelWeights');">16</a>行)<br /></li>
</ol>
</div>
</div>
</div>
<div id="outline-container-orge00729e" class="outline-2">
<h2 id="orge00729e"><span class="section-number-2">3.</span> 機器學習的類型</h2>
<div class="outline-text-2" id="text-3">

<div id="orgcbb5ff5" class="figure">
<p><img src="images/2022-10-03_09-26-40.png" alt="2022-10-03_09-26-40.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 3: </span>機器學習的類型</p>
</div>


<div id="org1e13019" class="figure">
<p><img src="images/機器學習的類型/2024-02-01_19-39-09_2024-02-01_17-23-24.png" alt="2024-02-01_19-39-09_2024-02-01_17-23-24.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 4: </span>監督、非監督、半監督</p>
</div>
</div>
<div id="outline-container-org1a95ffa" class="outline-3">
<h3 id="org1a95ffa"><span class="section-number-3">3.1.</span> <a href="20221023101626-監督式學習.html#ID-20221023T101626.420918">監督式學習</a>(Supervised learning)</h3>
<div class="outline-text-3" id="text-3-1">

<div id="org68656aa" class="figure">
<p><img src="images/FlowChartOfSupervisedLearning.png" alt="FlowChartOfSupervisedLearning.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 5: </span>監督式學習流程圖</p>
</div>
<ul class="org-ul">
<li><a href="20221023101626-監督式學習.html#ID-20221023T101626.420918">監督式學習</a>指在訓練過程中直接告訴機器答案，也就是將資料進行標註(label:人力標註)，例如，在 1000 張訓練集照片中標註「貓/狗」。目前九成以上的機器學習應用均屬此類。<br /></li>
<li>監督學習的訓練集要求是包括輸入和輸出， 也可以說是特徵和目標。做法是從給定的訓練資料集中學習出一個函式，當新的資料到來時，可以根據這個函式預測結果。<br /></li>
<li>為迄今為止最常見的機器學習，泛指一群的機器學習演算法，是從一組「已標記」的「訓練數據集」(training dataset)來學習(訓練)，並導出模型。然後，以此一模型對「未標記」的類似數據進行預測分類，其運作流程如圖<a href="#org68656aa">5</a><sup><a id="fnr.1.100" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>所示。典型的例子為早期電子郵件的垃圾信件是讓使用者先去標記某些信為垃圾郵件，然後藉由這些被標記的郵件來推論找出其他可能的垃圾郵件；由此看來，我們以為 Gmail 很好心的提供給我們為信件加註「垃圾」、「廣告」的功能，其實是 Google 利用我們當免費勞工為他們提供信件加註標籤的工作。<br /></li>
</ul>
</div>
<ol class="org-ol">
<li><a id="orgbcd2d28"></a>方法<br />
<div class="outline-text-4" id="text-3-1-1">
<ul class="org-ul">
<li>分類: 最短距離分類器、KNN分類器、決策樹<br /></li>
<li>類神經網路<br /></li>
<li>單純貝氏分類器(Naive Bayes Classifier)<br /></li>
<li>邏輯迴歸(Logisitc Regression)<br /></li>
<li>決策樹(Decision tree)<br /></li>
<li>支援向量機(SVM, Support Vector Machine)<br /></li>
</ul>
</div>
</li>
<li><a id="orged020b9"></a>典型應用<br />
<div class="outline-text-4" id="text-3-1-2">
<ul class="org-ul">
<li>Credit/loan approval:信用評比與貸款通過<br /></li>
<li>Medical diagnosis: if a tumor is cancerous or benign(是否有 XX 癌症)<br /></li>
<li>Fraud detection: if a transaction is fraudulent 詐騙或正常交易<br /></li>
<li>垃圾郵件(SPAM)或正常郵件<br /></li>
<li>Web page categorization 網站分類: which category it is<br /></li>
<li>資安應用: 取得有漏洞程式碼資料集(label)，評估其他程式是否有漏洞<br /></li>
</ul>
</div>
</li>
<li><a id="orgd71b7e5"></a>類型：<br />
<ol class="org-ol">
<li><a id="orgd432ba2"></a>分類<br />
<div class="outline-text-5" id="text-3-1-3-1">
<p>
基於從訓練集資料觀測到的分類類別來標籤、預測新數據的類別，如上述的垃圾郵件即為典型的二元分類工作(如圖<a href="#org81c7526">6</a>)，類別分類工作也可以進行「多類別分類」(multi-class classification)，如典型的 MNist 手寫數字辨識，即是將手寫的 0-9 數字進行預測辨識，並給出一個 0-9 的類別標籤。<br />
</p>

<div id="org81c7526" class="figure">
<p><img src="images/01_03.png" alt="01_03.png" width="300" /><br />
</p>
<p><span class="figure-number">Figure 6: </span>典型的分類</p>
</div>
</div>
</li>
<li><a id="org695eb19"></a>迴歸<br />
<div class="outline-text-5" id="text-3-1-3-2">
<p>
利用輸入數據的「特徵」來預測出一個「值」，例如，根據房屋的地點、坪數、樓層、房間數等變數, 發掘出這些變數之間的關係，進而預測房價，如圖<a href="#org2c0f641">7</a>。<br />
</p>

<div id="org2c0f641" class="figure">
<p><img src="images/01_04.png" alt="01_04.png" width="300" /><br />
</p>
<p><span class="figure-number">Figure 7: </span>典型的迴歸</p>
</div>
</div>
</li>
</ol>
</li>
<li><a id="org1807fc7"></a>決策樹<br />
<div class="outline-text-4" id="text-3-1-4">

<div id="org45b6e67" class="figure">
<p><img src="images/DecisionTree.png" alt="DecisionTree.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 8: </span>Decision Tree</p>
</div>

<p>
圖<a href="#org45b6e67">8</a>為「鳶尾花數據集」(<a href="http://archive.ics.uci.deu/ml/datasets/Iris">http://archive.ics.uci.deu/ml/datasets/Iris</a>)的分類結果，依鳶尾花的四個特徵：花萼(sepal)長度、寬度、花瓣(petal)長度、寬度，對其進行鳶尾花種類判斷。<br />
</p>
</div>
</li>
<li><a id="orgff36c55"></a>支援向量機(Support Vector Machine, SVM)<br />
<div class="outline-text-4" id="text-3-1-5">

<div id="orgb227374" class="figure">
<p><img src="images/機器學習的類型/2024-01-31_13-03-24_2024-01-31_13-03-07.png" alt="2024-01-31_13-03-24_2024-01-31_13-03-07.png" width="400" /><br />
</p>
<p><span class="figure-number">Figure 9: </span>SVM</p>
</div>

<p>
支援向量機是一種基於統計學習理論基礎的機器學習模型，針對小樣本、非線性、高維度與局部最小點等問題具有相對的優勢，主要用來處理分類問題。除了在文字分類、圖像分類及醫學中分類蛋白質等領域有不錯的成效外，因具有計算速度快且空間成本低等優勢，在工業界也有廣泛的應用<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>。<br />
</p>


<div id="orge53406d" class="figure">
<p><img src="images/機器學習的類型/2024-01-31_13-05-33_2024-01-31_13-05-24.png" alt="2024-01-31_13-05-33_2024-01-31_13-05-24.png" width="400" /><br />
</p>
<p><span class="figure-number">Figure 10: </span>SVM超平面</p>
</div>


<div id="org4984379" class="figure">
<p><img src="images/機器學習的類型/2024-01-31_13-07-55_2024-01-31_13-07-31.png" alt="2024-01-31_13-07-55_2024-01-31_13-07-31.png" width="400" /><br />
</p>
<p><span class="figure-number">Figure 11: </span>將超平面最大化</p>
</div>

<p>
SVM不僅能將數據分門別類，甚至還可以找到最大化的「分離超平面」（類似於三維以上空間中的一個平面）<sup><a id="fnr.2.100" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>，會最大化每個樣本點與該「超平面」的差。此外，當數據是「不可線性分離」時，支援向量機還可以透過「軟邊界」(soft margin)和「核技巧」(kernel trick)來處理。<br />
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-org0903869" class="outline-3">
<h3 id="org0903869"><span class="section-number-3">3.2.</span> <a href="20221023101716-非監督式學習.html#ID-20221023T101716.467694">非監督式學習</a>(Unsupervised learning)</h3>
<div class="outline-text-3" id="text-3-2">

<div id="orge39d36b" class="figure">
<p><img src="images/機器學習的類型/2024-01-31_21-19-35_2024-01-31_21-18-52.png" alt="2024-01-31_21-19-35_2024-01-31_21-18-52.png" width="400" /><br />
</p>
<p><span class="figure-number">Figure 12: </span>資料分群</p>
</div>

<ul class="org-ul">
<li>非監督式學習只有觀測值，單純給電腦大量觀測資料，然後從這些資料找出潛在規則。例如：將 10 萬張照片依據電腦自己歸納的規則分為數個不同的群組(如圖<a href="#orge39d36b">12</a>左<sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup>)或是分成5群(如圖<a href="#orge39d36b">12</a>右)。<br /></li>
<li>在<a href="20221023101626-監督式學習.html#ID-20221023T101626.420918">監督式學習</a>中，我們事先會知道訓練集數據的正確答案(label)，並依此訓練我們的模型；在<a href="20230210172919-增強式學習.html#ID-0a5c37c0-741a-4a1a-bec7-f98074830132">強化學習</a>的環境中，我們會為代理人定義如何度量特定行動的奬勵；然而，在「非監督式學習」的環境中，我們面對的是未標記類別的數據或未知結構的數據，目的是讓演算法導出結論。最典型的例子就是「集群」(clustering)，即，讓演算法自己根據數據的特性把它們依某種特質分類為不同子集合，這裡的子集合不一定要是有限集合，也可能是無界子集(unbounded subsets)。「受限玻爾茲曼機」以及「深度信念網路」(deep belief networks, DBN)都屬此類。<br /></li>
<li>非監督式學習經常被運用於資料分析的前置階段，用來先將資料分群或降低維度(減少變數量)，以利後續的分析或監督式學習的進行。<br /></li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org8c0a93a"></a>方法：<br />
<div class="outline-text-4" id="text-3-2-1">
<ul class="org-ul">
<li>群集(Clustering): K平均法(K-means)、階層式分群<br />
cluster analysis 是一種精簡資料的方法，主要目的是將一大筆資料依據樣本之間的共同屬性精簡成少數幾個同質性次群體 homogeneous subgroups ，即以相似性 similarity 衡量，形成集群(cluster)，以便從雜亂無章的一大堆原始資料中，做到分類、分群的目標。而所謂的相似性通常是以「距離」作為衡量，相對距離愈近，相似程度愈高，分群之後可以使得群內差異小、群間差異大<sup><a id="fnr.4" class="footref" href="#fn.4" role="doc-backlink">4</a></sup>。<br /></li>
<li>階層式分群​​(Hierarchical Clustering​​)<br />
將資料在一個階層式的樹狀上，反覆的利用拆分以及聚合的方式建立出一個分類系統。階層式分群的優勢在於它使用上的簡單性以及能夠在小數據上操作，然而卻非常難處理大型的資料<sup><a id="fnr.5" class="footref" href="#fn.5" role="doc-backlink">5</a></sup>。<br />
各群組間的距離計算方式有以下三種：Single linkage、Maximum linkage、Average Linkage<br /></li>
<li>自動編碼器(Autoencoder)<br />
由編碼器(encoder)與解碼器(decoder)構成，<br /></li>
</ul>
</div>
</li>
<li><a id="orgf1fd3a6"></a>範例：<br />
<div class="outline-text-4" id="text-3-2-2">
<ul class="org-ul">
<li>集群(cluster): 是一種「探索式數據分析」(exploratory data analysis)技術，它允許我們先組織一堆資訊到一個有意義的「子集群」(clusters)中，而無需任何先驗知識。<br /></li>
<li>K-means: 將數據集中的每個樣本分類到 k 個不同的子集合中，它隨機選擇 k 個點，這些點稱為「質心」(centroid)，代表這 k 個不同子集合的中心點，然後對於每個「質心」，我們選擇最接近它的一點，群組起來。<br /></li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org29d58f0" class="outline-3">
<h3 id="org29d58f0"><span class="section-number-3">3.3.</span> 半監督式學習</h3>
<div class="outline-text-3" id="text-3-3">

<div id="org7c35746" class="figure">
<p><img src="images/機器學習的類型/2024-02-01_19-46-27_2024-02-01_19-45-17.png" alt="2024-02-01_19-46-27_2024-02-01_19-45-17.png" width="300" /><br />
</p>
<p><span class="figure-number">Figure 13: </span>半監督式學習</p>
</div>

<p>
Semi-Supervised Learning如圖<a href="#org7c35746">13</a>所示，只有部分的data有label，而其他的data則是未標註的(unlabeled)。半監督學習的目標與監督學習完全相同，差別在資料集中存在部分的標注缺失<sup><a id="fnr.6" class="footref" href="#fn.6" role="doc-backlink">6</a></sup>。<br />
</p>
</div>
</div>
<div id="outline-container-orgef27935" class="outline-3">
<h3 id="orgef27935"><span class="section-number-3">3.4.</span> <a href="20230210172919-增強式學習.html#ID-0a5c37c0-741a-4a1a-bec7-f98074830132">增強式學習</a></h3>
<div class="outline-text-3" id="text-3-4">
<ul class="org-ul">
<li>機器為了達成目標，隨著環境的變動，而逐步調整其行為，並評估每一個行動之後所到 的回饋是正向的或負向的，即，在 try-and-error 的過程中一步步從失敗中找出成功的路徑。<br /></li>
<li>較常用於以下領域：電腦遊戲、下棋、自駕車、機器人。<br /></li>
<li>AlphaGo：先以監督式學習(以人類棋譜來訓練)訓練出早期版的 AlphaGo，接下來以增強式學習兩個最期版的 AlphaGo 對奕(40 天內對奕 3000 萬盤棋)。<br /></li>
<li>2017 年的 AlphaZero 則放棄監督式學習(人類棋譜)，完全採取強化學習的模式，三天後摸索出自己的圍棋下法，成為有史以來棋力最強的版本。(不再以人類為師，所以才能超越人類？)<br /></li>
<li>Google 也將強化學習用於機房伺服器管理，持續偵測機房室內外用電、溫度、建立模型，由模型決定每台伺服器的運轉(全速、低速、休眠、關機)，並達到省電 40%的目標。<br /></li>
<li>強化學習的目標在於開發一個系統（或代理人，agent）,他會藉由與環境的互動來改進自身的效能。由於當前的環境狀態資訊通常就包含了所謂「奬勵信號」(reward signal)，強化學習的目的就是找到一個最好的 Policy(策略)，可以讓 reward 最多。所以也可以把「強化學習」視為與「監督式學習」相關聯的領域，然而在強化學習中，環境回饋不能視為真實正的事實（或是說，正確的標籤），只能將之視為：測量函數對特定行動所觀測到並回報的一個度量值。如圖<a href="#org32879df">15</a>，Agent<br /></li>
<li>最常見的應用是教機器如何「玩遊戲」，在這種情境下，我們不會對某個動作貼標籤說它是「好」或「壞」，而是根據遊戲的結果（輸或是贏）或是遊戲中的信號（得分或失分）來做為回饋。<br /></li>
</ul>


<div id="org23b7277" class="figure">
<p><img src="images/ReinforcementLearning.png" alt="ReinforcementLearning.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 14: </span>強化學習流程圖</p>
</div>
</div>
<ol class="org-ol">
<li><a id="orgdd67909"></a>範例<br />
<div class="outline-text-4" id="text-3-4-1">
<p>
典型的 reinforcement learning 包括<br />
</p>
<ul class="org-ul">
<li>DQN<br /></li>
<li>q-learning<br /></li>
</ul>
</div>
</li>
<li><a id="org486a219"></a>Flappy bird:<br />
<div class="outline-text-4" id="text-3-4-2">
<p>
在這個遊戲中，我們需要簡單的點擊操作來控制小鳥，躲過各種水管，飛的越遠越好，因為飛的越遠就能獲得更高的積分獎勵。這就是一個典型的強化學習場景：<br />
</p>
<ul class="org-ul">
<li>機器有一個明確的小鳥角色——代理<br /></li>
<li>需要控制小鳥飛的更遠——目標<br /></li>
<li>整個遊戲過程中需要躲避各種水管——環境<br /></li>
<li>躲避水管的方法是讓小鳥用力飛一下——行動<br /></li>
<li>飛的越遠，就會獲得越多的積分——獎勵<br /></li>
</ul>

<div id="org32879df" class="figure">
<p><img src="images/ReinforceLearningGame.jpg" alt="ReinforceLearningGame.jpg" /><br />
</p>
<p><span class="figure-number">Figure 15: </span>強化學習流程圖</p>
</div>
</div>
</li>
</ol>
</div>
<div id="outline-container-org4eb3a76" class="outline-3">
<h3 id="org4eb3a76"><span class="section-number-3">3.5.</span> 依據AI模型是否能由即時資料流進行增量學習來區分<sup><a id="fnr.7" class="footref" href="#fn.7" role="doc-backlink">7</a></sup></h3>
<div class="outline-text-3" id="text-3-5">
</div>
<ol class="org-ol">
<li><a id="org74903c9"></a>Offline (Batch) learning:<br />
<div class="outline-text-4" id="text-3-5-1">
<p>
In <i>batch learning</i>, the system is incapable of learning incrementally; it must be trained using all the available data.<br />
</p>
</div>
</li>
<li><a id="org3a13be5"></a>Online learning:<br />
<div class="outline-text-4" id="text-3-5-2">
<ul class="org-ul">
<li>In <i>online learning</i>, you train the system incrementally by feeding it data instances sequentially, either individually or in small groups called <i>mini-batches</i>.<br /></li>
<li>Online learning is great for systems that receive data as a continuous flow (e.g., stock prices) and need to adapt to change rapidly or autonomously.<br /></li>
<li>Online learning algorithms can also be used to train systems on huge datasets that cannot fit in one machine&rsquo;s main memory (this is called <i>out-of-core</i> learning).<br /></li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div id="outline-container-org8740613" class="outline-2">
<h2 id="org8740613"><span class="section-number-2">4.</span> 機器學習的程序與資料呈現方式</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-org1795275" class="outline-3">
<h3 id="org1795275"><span class="section-number-3">4.1.</span> 機器學習如何解決問題</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li>數據收集(Data Collection): 在監督式學習下還要蒐集正確的標記<br /></li>
<li>數據處理(Data Processing): 包含「數據清理」，例如：刪除「冗餘」或「高度相關的特徵」，或補滿「遺漏值」。<br /></li>
<li>建立測試案例(Creation of the test case): 通常包括：「訓練數據集」(training dataset)用來訓練演算法、「測試數據集」(test dataset)用來測試訓練完成的演算法、以及「驗證數據集」(validation dataset)用來進行最終測試(在不斷的訓練-測試之後)。<br /></li>
<li>建立模型<br /></li>
<li>訓練模型<br /></li>
<li>評估、 修正模型<br /></li>
</ul>
</div>
</div>
<div id="outline-container-org3585221" class="outline-3">
<h3 id="org3585221"><span class="section-number-3">4.2.</span> 機器學習的資料呈現方式</h3>
<div class="outline-text-3" id="text-4-2">
<p>
How can we represent data (images, text, user preferences, etc.) in a way that computers can understand? -&gt; Organize information into a vector<sup><a id="fnr.8" class="footref" href="#fn.8" role="doc-backlink">8</a></sup>.<br />
</p>
<ul class="org-ul">
<li>A vector is a 1-dimensional array of numbers. It has both a magnitude(length) and a direction.<br /></li>
<li><p>
A feature vector is a vector whose entries represent the &ldquo;features&rdquo; of some object.<br />
</p>

<div id="org4bc9a31" class="figure">
<p><img src="images/feature-vector.jpg" alt="feature-vector.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 16: </span>Feature Vector</p>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-orge591824" class="outline-3">
<h3 id="orge591824"><span class="section-number-3">4.3.</span> Images</h3>
<div class="outline-text-3" id="text-4-3">
<p>
In black and white images, <b>black and white pixels</b> correspond to 0s and 1s. Grayscale pixels are numbers between 0 and 255. Both assemble into a 1-dimensional array of numbers.<br />
</p>

<div id="orga2820b8" class="figure">
<p><img src="images/image-data-rep.jpg" alt="image-data-rep.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 17: </span>Image data representations</p>
</div>
</div>
</div>
<div id="outline-container-orgc626493" class="outline-3">
<h3 id="orgc626493"><span class="section-number-3">4.4.</span> Words and Documents</h3>
<div class="outline-text-3" id="text-4-4">
<p>
Given a collection of documents (e.g. Wikipedia articles), assign to every word a vector whose \(i^{th}\) entry is the number of times the word appears in the \(i^{th}\) document.<br />
</p>

<div id="orgf57099c" class="figure">
<p><img src="images/word-rep.jpg" alt="word-rep.jpg" width="400" /><br />
</p>
<p><span class="figure-number">Figure 18: </span>Words and documents representations</p>
</div>

<p>
These vectors can assemble into a large matrix, useful for <b>latent semantic analysis</b>.<br />
</p>
</div>
</div>
<div id="outline-container-org4749478" class="outline-3">
<h3 id="org4749478"><span class="section-number-3">4.5.</span> Yes/No or Ratings</h3>
<div class="outline-text-3" id="text-4-5">
<p>
Given users and items (e.g. movies), vectors can indicate if a user has interacted with the item (1=yes, 0=no) or the user&rsquo;s ratings, say a number between 0 and 5.<br />
</p>

<div id="org2825154" class="figure">
<p><img src="images/yn-rep.jpg" alt="yn-rep.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 19: </span>Y/N or Ratings</p>
</div>
</div>
</div>
<div id="outline-container-orgb74e6c7" class="outline-3">
<h3 id="orgb74e6c7"><span class="section-number-3">4.6.</span> One-Hot Encodings</h3>
<div class="outline-text-3" id="text-4-6">
<p>
Assign to each word a vector with one 1 and 0s elsewhere. This is called a one-hot encoding (or a &ldquo;standard basis vector&rdquo;). For example, suppose our language only has four words:<br />
</p>

<div id="org480ee4b" class="figure">
<p><img src="images/word-one-hot.jpg" alt="word-one-hot.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 20: </span>words one-hot encoding</p>
</div>
</div>
<ol class="org-ol">
<li><a id="orgad6d04e"></a>drawbacks to consider:<br />
<ol class="org-ol">
<li><a id="org499e5ad"></a>These vectors can be very sparse<br />
<div class="outline-text-5" id="text-4-6-1-1">
<ul class="org-ul">
<li>A &ldquo;sparse&rdquo; vector is one with lots of zeros<br /></li>
<li>500,000 users and 1,000,000 movies<br /></li>
</ul>
</div>
</li>
<li><a id="org5c1438a"></a>Possible lack of <b>meaningful relationships between</b> vectors<br />
<div class="outline-text-5" id="text-4-6-1-2">
<ul class="org-ul">
<li>One-hot encodings are never &ldquo;similar.&rdquo;<br /></li>
<li>Similarity is measured by the <b>dot product.</b><br /></li>
</ul>
</div>
</li>
</ol>
</li>
</ol>
</div>
</div>
<div id="outline-container-orge234c43" class="outline-2">
<h2 id="orge234c43"><span class="section-number-2">5.</span> 機器學習的主要挑戰<sup><a id="fnr.7.100" class="footref" href="#fn.7" role="doc-backlink">7</a></sup></h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-org000335c" class="outline-3">
<h3 id="org000335c"><span class="section-number-3">5.1.</span> Insufficient quantity of training data</h3>
<div class="outline-text-3" id="text-5-1">
<p>
Even for very simple problems you typically need thousands of examples, and for complex problems such as image or speech recognition you may need millions of examples. The idea that data matters more than algorithms for complex problems was further popularized by Peter norvig et al. in a paper titled &ldquo;The Unreasonable Effectiveness of Data&rdquo;, published in 2009.<br />
</p>
</div>
</div>
<div id="outline-container-org2635b5b" class="outline-3">
<h3 id="org2635b5b"><span class="section-number-3">5.2.</span> Nonrepresentative training data</h3>
<div class="outline-text-3" id="text-5-2">
<ul class="org-ul">
<li>In order to generalize well, it is crucial that your training data be representative of the new cases you want to generalize to.<br /></li>
<li>Perhaps the most famous example of sampling bias happened during the US presidential election in 1936, which pitted Landon against Roosevelt: the <i>Literary Digest</i> conducted a very large poll, sending mail to about 10 million people. The flaw was in the <i>Literary Digest&rsquo;s</i> sampling method:<br />
<ol class="org-ol">
<li>First, to obtain the addresses to send the polls to, the <i>Literary Digest</i> use telephone directories, lists of magazine subscribers, club membership lists, and the like.<br /></li>
<li>Second, less than 25% of the people who were polled answered.<br /></li>
</ol></li>
</ul>

<div id="orgfa5d56a" class="figure">
<p><img src="images/GDP2.jpg" alt="GDP2.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 21: </span>加入更多資料會使 modle 更具代表性</p>
</div>
</div>
</div>
<div id="outline-container-org0345078" class="outline-3">
<h3 id="org0345078"><span class="section-number-3">5.3.</span> Poor-quality data</h3>
<div class="outline-text-3" id="text-5-3">
<p>
Obviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-quality measurements), it will make it harder for the system to detect the underlying patterns, so your system is less likely to perform well.<br />
</p>
</div>
</div>
<div id="outline-container-org3af3347" class="outline-3">
<h3 id="org3af3347"><span class="section-number-3">5.4.</span> Irrelevant features</h3>
<div class="outline-text-3" id="text-5-4">
<p>
As the saying goes: garbage in, garbage out. Your system will only be capable of learning if the training data contains enough relevant features and not too many irrelevant ones.<br />
</p>
</div>
</div>
<div id="outline-container-org4763540" class="outline-3">
<h3 id="org4763540"><span class="section-number-3">5.5.</span> Overfitting the training data</h3>
<div class="outline-text-3" id="text-5-5">
<ul class="org-ul">
<li>Overfitting happens when the model is too complex relative to the amount and noisiness of the training data. Here are possible solutions:<br />
<ol class="org-ol">
<li>Simplify the model by selecting one with fewer parameters.<br /></li>
<li>Gather more training data.<br /></li>
<li>Reduce the noise in the training data (e.g., fix data errors and remove outliers).<br /></li>
</ol></li>
<li>Constraining a model to make it simpler and reduce the risk of overfitting is called <i>regularization</i>.<br /></li>
<li>The amount of regularization to apply during learning can be controlled by a <i>hyperparameter</i>. A hyperparameter is a parameter of a learning algorithm (not of the model).<br /></li>
</ul>
</div>
</div>
<div id="outline-container-org5d16800" class="outline-3">
<h3 id="org5d16800"><span class="section-number-3">5.6.</span> Underfitting the training data</h3>
<div class="outline-text-3" id="text-5-6">
<p>
Underfitting occurs when your model is too simple to learn the underlying structure of the data.<br />
</p>
</div>
</div>
</div>
<div id="outline-container-org2d55279" class="outline-2">
<h2 id="org2d55279"><span class="section-number-2">6.</span> 典型的機器學習MODEL Training[實作]</h2>
<div class="outline-text-2" id="text-6">
<p>
以&ldquo;rock, paper, and scissors&rdquo;辨識為例<sup><a id="fnr.8.100" class="footref" href="#fn.8" role="doc-backlink">8</a></sup>:<br />
</p>
<ol class="org-ol">
<li>將三種圖案的image讀入model中<br /></li>
<li>於model中建立神經元(可視為function)，學習不同image的特徵<br /></li>
<li>根據上述神經元讀取的特徵值(features)，配合該image之答案(label)，進行學習<br /></li>
</ol>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">model</span> = tf.keras.models.Sequence([
<span class="linenr">2: </span>    tf.keras.layers.Flatten(input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>)),
<span class="linenr">3: </span>    tf.keras.layers.Dense(<span style="color: #da8548; font-weight: bold;">512</span>, activation=<span style="color: #98be65;">'relu'</span>),
<span class="linenr">4: </span>    tf.keras.layers.Dense(<span style="color: #da8548; font-weight: bold;">3</span>, activatio=<span style="color: #98be65;">'softmax'</span>)
<span class="linenr">5: </span>])
<span class="linenr">6: </span>
<span class="linenr">7: </span>model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>, optimizer=<span style="color: #98be65;">'rmsprop'</span>)
<span class="linenr">8: </span>
<span class="linenr">9: </span>model.fit(...., epochs=<span style="color: #da8548; font-weight: bold;">100</span>)
</pre>
</div>
</div>
<div id="outline-container-org903741d" class="outline-3">
<h3 id="org903741d"><span class="section-number-3">6.1.</span> Process of learning</h3>
<div class="outline-text-3" id="text-6-1">
<p>
可先將512個神經元視為512個function，每個function裡有許多變數，這些變數的初始值為random assign，接下來每個function陸續讀取image的所有feature(即圖的每個pixel，這裡可以將之視為傳入該function的parameters)，然後根據function中的變數值，開始學習要如何調整function裡的變數值才能輸出這個image的正確答案(label)，每個神經元同時進行學習與變數值調校。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org34740f0"></a>function 示意圖<br />
<div class="outline-text-4" id="text-6-1-1">

<div id="org0a4e606" class="figure">
<p><img src="images/function-dg.png" alt="function-dg.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 22: </span>Function</p>
</div>

<p>
在訓練的過程中，我們根據每個function所輸入的參數(parameters)，調整function裡的變數值(這些最初是random得來的)，希望能得到與該輸入值(所有的parameters)所相對應的答案。<br />
</p>

<p>
在傳統的程式設計觀點，輸入function的數值為參數；但在神經網路的觀點，輸入值為features，function稱為neuron，neuron裡的變數稱為parameters，所有neuron裡的parameter均為神經網路訓練過程中要調整的對象。<br />
</p>

<div id="org344501d" class="figure">
<p><img src="images/neuron-dg.png" alt="neuron-dg.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 23: </span>Neuron</p>
</div>
</div>
</li>
</ol>
</div>
<div id="outline-container-org6bba271" class="outline-3">
<h3 id="org6bba271"><span class="section-number-3">6.2.</span> Example of training</h3>
<div class="outline-text-3" id="text-6-2">
<ul class="org-ul">
<li><p>
輸入image為rock<br />
</p>

<div id="orgd64b820" class="figure">
<p><img src="images/stone.jpg" alt="stone.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 24: </span>Stone</p>
</div></li>
<li><p>
輸入image為paper<br />
</p>

<div id="org9fc7cd2" class="figure">
<p><img src="images/paper.jpg" alt="paper.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 25: </span>Paper</p>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org3222b8e" class="outline-3">
<h3 id="org3222b8e"><span class="section-number-3">6.3.</span> Model v.s. Layer: Training step by step</h3>
<div class="outline-text-3" id="text-6-3">
<ul class="org-ul">
<li><p>
Input layer<br />
此的輸入為image，對model來說，其feature為150*150*3<br />
</p>

<div id="org4884bb3" class="figure">
<p><img src="images/input_shape.jpg" alt="input_shape.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 26: </span>input_shape</p>
</div></li>
<li><p>
Hidden layer<br />
中間層(hidden layer)有512個神經元，可視為512個function<br />
</p>

<div id="orgacb3f48" class="figure">
<p><img src="images/hidden_layer.jpg" alt="hidden_layer.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 27: </span>hidden_layer</p>
</div></li>
<li><p>
Output layer<br />
輸出層(output layer)則輸出三種可能的答案<br />
</p>

<div id="org89855ab" class="figure">
<p><img src="images/output_layer.jpg" alt="output_layer.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 28: </span>output_layer</p>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org93bd9f3" class="outline-3">
<h3 id="org93bd9f3"><span class="section-number-3">6.4.</span> compile v.s. fit</h3>
<div class="outline-text-3" id="text-6-4">
<ul class="org-ul">
<li>compile: Compile與傳統程式設計概念不同，此處旨在訂定兩個model training中最重要的關鍵: loss function與optimizer。剛才提及每個function(或neuron)都對於產生答案(label)做出了貢獻，然而這個答案到底好或不好，必須要有一個評估機制，loss function的目的就在評估每一次所有neuron所產生的答案是否足夠好(以此例來看至少正確率必須高過1/3)，然後把評估結果丟給optimizer，由它來決定下一次猜測時neuron裡的parameters要如何調整。<br /></li>
<li>fit: 如此重複不斷的進行&ldquo;輸入feature-&gt;猜測答案-&gt;評估答案-&gt;修正parameter-&gt;輸入features-&gt;猜測答案-&gt;評估答案-&gt;修正parameter&#x2026;.&rdquo;，希望最終能找到各neuron中最佳的parameters值，每一次的訓練稱為一次epoch。這就是fit在做的事，也是實際訓練的動作，訓練所需時間視輸入值、model複雜程度與硬體效能而定，也許僅需數分鐘，也許要耗時數週。<br /></li>
</ul>

<div id="org3a5bcd2" class="figure">
<p><img src="images/compile_phase.jpg" alt="compile_phase.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 29: </span>compile-phase</p>
</div>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://github.com/rasbt/python-machine-learning-book-2nd-edition">python-machine-learning-book-2nd-edition</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://pyecontech.com/2020/03/24/svm/">[機器學習首部曲] 支援向量機 SVM</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://alankrantas.medium.com/kmeans-%E8%83%BD%E5%BE%9E%E8%B3%87%E6%96%99%E4%B8%AD%E6%89%BE%E5%87%BA-k-%E5%80%8B%E5%88%86%E9%A1%9E%E7%9A%84%E9%9D%9E%E7%9B%A3%E7%9D%A3%E5%BC%8F%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95-%E6%89%80%E4%BB%A5%E5%AE%83%E5%88%B0%E5%BA%95%E6%9C%89%E5%95%A5%E7%94%A8-%E4%BD%BF%E7%94%A8-scikit-learn-%E8%88%87-python-5dd8c0c8b167">KMeans：能從資料中找出 K 個分類的非監督式機器學習演算法 — — 所以它到底有啥用？</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4" role="doc-backlink">4</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://ithelp.ithome.com.tw/articles/10291318">Day 19 聚類（集群）分析 Cluster Analysis</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.5" class="footnum" href="#fnr.5" role="doc-backlink">5</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://zh.oosga.com/docs/unsupervised-learning/">非監督式學習（Unsupervised Learning）的定義為何？<br />
</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.6" class="footnum" href="#fnr.6" role="doc-backlink">6</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://u9534056.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E4%BB%BB%E5%8B%99-%E7%9B%A3%E7%9D%A3%E5%AD%B8%E7%BF%92-%E5%8D%8A%E7%9B%A3%E7%9D%A3%E5%AD%B8%E7%BF%92-%E7%84%A1%E7%9B%A3%E7%9D%A3%E5%AD%B8%E7%BF%92-9b75972f91d6">機器學習任務：監督學習/半監督學習/無監督學習</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.7" class="footnum" href="#fnr.7" role="doc-backlink">7</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://www.books.com.tw/products/F014278520">Hands-On Machine Learning with Scikit-Learn, Keras, and Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.8" class="footnum" href="#fnr.8" role="doc-backlink">8</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://youtu.be/LlKAna21fLE">A friendly introduction to linear algebra for ML (ML Tech Talks)</a><br />
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Yung-Chin Yen</p>
<p class="date">Created: 2024-02-02 Fri 10:25</p>
</div>
</body>
</html>
