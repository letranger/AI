<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-02-25 Sun 15:51 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>深度學習</title>
<meta name="author" content="Yung-Chin Yen" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="../css/muse.css" />
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">深度學習</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orga84a248">1. 深度學習</a>
<ul>
<li><a href="#orga57ed46">1.1. 深度學習的知名模型</a></li>
<li><a href="#org9e69298">1.2. 深度學習的高速化</a></li>
</ul>
</li>
<li><a href="#org9d15cd9">2. 深度學習運作原理</a>
<ul>
<li><a href="#org06cec31">2.1. Layer, 損失函數與優化器</a></li>
</ul>
</li>
<li><a href="#org57f889b">3. 實作範例</a>
<ul>
<li><a href="#orgcec3b60">3.1. 二元分類：IMDB</a></li>
<li><a href="#org504d94a">3.2. 多類別分類：數位新聞</a></li>
<li><a href="#orgbaacb9a">3.3. 以 Keras 解決迴歸問題：預測房價</a></li>
</ul>
</li>
</ul>
</div>
</div>
<a href="https://letranger.github.io/AI/20221023101228-深度學習.html"><img align="right" alt="Hits" src="https://hits.sh/letranger.github.io/AI/20221023101228-深度學習.html.svg"/></a>
<div id="outline-container-orga84a248" class="outline-2">
<h2 id="orga84a248"><span class="section-number-2">1.</span> 深度學習</h2>
<div class="outline-text-2" id="text-1">

<div id="orgbb4af27" class="figure">
<p><img src="images/AI,_Machine_Learning與Deep_Learning/2024-02-19_16-24-48_2024-02-19_16-23-09.png" alt="2024-02-19_16-24-48_2024-02-19_16-23-09.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 1: </span>AI, Machine Learning與Deep Learning</p>
</div>

<p>
在<a href="20221025104603-神經網路.html#ID-d6daa102-05bb-475d-b619-db8b61e86030">神經網路</a>中我們曾經提及：<br />
</p>
<blockquote>
<p>
深度神經網路(Deep Neural Network, DNN)，顧名思義就是有很多層的神經網路。然而，幾層才算是多呢？一般來說有1-2個隱藏層的神經網絡就可以叫做多層，準確的說是(淺層)神經網絡(Shallow Neural Networks)。隨著隱藏層的增多，更深的神經網絡(一般來說超過3層)就都叫做深度神經網路<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>。而那些以深度神經網路為模型的機器學習就是我們耳熟能詳的<a href="20221023101228-深度學習.html#ID-20221023T101228.247381">深度學習</a>。<br />
</p>
</blockquote>

<p>
那麼幾層才算是夠深呢？實際上，「深度」只是一個商業概念，很多時候工業界把3層隱藏層也叫做「深度學習」，在機器學習領域的約定俗成是，名字中有深度(Deep)的網絡僅代表其有超過5-7層的隱藏層<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>。<br />
</p>

<p>
典型的深度學習如圖<a href="#orgab61813">2</a>，在此例中，輸入為一張手寫數字的影像，經由 4 層的深度學習模型後得知此數字為 4。<br />
</p>


<div id="orgab61813" class="figure">
<p><img src="images/img-191107113927.jpg" alt="img-191107113927.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 2: </span>典型的深度神經網路-1</p>
</div>

<p>
圖<a href="#orgf1743fa">3</a>進一步說明網路模型中每一層的作用，可以將每一層網路視為對影像的特殊運算，如此一層一層逐一精煉(purified)，最後得到結果。<br />
</p>


<div id="orgf1743fa" class="figure">
<p><img src="images/img-1911071139277.jpg" alt="img-1911071139277.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 3: </span>典型的深度神經網路-2</p>
</div>

<p>
關於增加層數的重要性，目前還缺乏理論佐證，但從過往的研究或實驗中，有幾點可以說明。<br />
</p>
<ol class="org-ol">
<li>在 ILSVRC 這種大型視覺辨識競賽結果中，加深層數的比例多與辨識效能成正比。<br /></li>
<li>加深層數可以在減少網路參數的狀況下得到相同成效，透過重叠層級，可以讓 ReLU 等活化函數夾在卷積層之間，進一步提高網路的表現力，因為透過活化函數，可以在網路增加「非線性」的能力，重叠非線性函數，也能達到更複雜的表現力。<br /></li>
<li>學習的效率也是加深層數的優點之一，卷積層的神經元會反應出邊界等單純形狀，隨著層數增加，可以反應出紋理、物體部位等特質，依照階層逐漸變複雜。<br /></li>
<li>以辨識「狗」為例子，如果要以層數較少的網路來解決這個問題，卷積層就要一次「理解」眾多特徵，還要因應不同拍攝環境帶來的變化，一次處理這些龐大的資料會花費許多學習時間； 如果加深層數，就能用階層分解必須學習的問題，每一層可以處理單純的問題，例如，最初的層級可以只學習邊界，利用少量的學習資料來進行效率化的學習。<br /></li>
<li>加深層數可以階層性的傳遞資料，例如，擷取出邊界的下一層會使用邊界資料來學習更高階的問題（如判斷形狀）。<br /></li>
</ol>
</div>
<div id="outline-container-orga57ed46" class="outline-3">
<h3 id="orga57ed46"><span class="section-number-3">1.1.</span> 深度學習的知名模型</h3>
<div class="outline-text-3" id="text-1-1">
<p>
幾個知名的深度學習模型如下：<br />
</p>
</div>
<div id="outline-container-org96a56d6" class="outline-4">
<h4 id="org96a56d6"><span class="section-number-4">1.1.1.</span> VGG</h4>
<div class="outline-text-4" id="text-1-1-1">
<p>
VGG 為由卷積層與池化層構成的基本 CNN。特色是含權重層（卷積層及全連接層）共 16-19 層，有時會稱為 VGG16 或 VGG19。VGG 由於結構非常簡單，應用性高，所以多數技術人員喜歡使用以 VGG 為最基礎的網路。<br />
</p>
</div>
</div>
<div id="outline-container-org12c71a1" class="outline-4">
<h4 id="org12c71a1"><span class="section-number-4">1.1.2.</span> GoogLeNet</h4>
<div class="outline-text-4" id="text-1-1-2">
<p>
GoogLeLeNet 基本上與 CNN 相同，其特色是不僅會往垂直方向加深網路，也會往水平方向加深。GoogLeNet 往水平方向的做法稱為「Inception 結構」。<br />
</p>
</div>
</div>
<div id="outline-container-orga3594b1" class="outline-4">
<h4 id="orga3594b1"><span class="section-number-4">1.1.3.</span> ResNet</h4>
<div class="outline-text-4" id="text-1-1-3">
<p>
ResNet 是由 Microsoft 團隊開發的網路，特色是具有能加深比過去更多層的「結構」，為了解決因加深過多層數無法順利學習的問題，ResNet 導入了「跳躍結構」（也稱為捷徑或分流）。跳躍結構是「直接」傳遞輸入資料，所以在反向傳播時，也會將上層的梯度「直接」傳遞給下層。透過這種跳躍結構，不用擔心梯度變小（或變得太大），可以把「具有意義的梯度」傳遞給上層。因此，跳躍結構能減少之前因為加深層數，使得梯度變小，出現梯度消失的問題。<br />
</p>
</div>
</div>
</div>
<div id="outline-container-org9e69298" class="outline-3">
<h3 id="org9e69298"><span class="section-number-3">1.2.</span> 深度學習的高速化</h3>
<div class="outline-text-3" id="text-1-2">
<p>
由於大數據(big data)與大型網路的關係，使得深度學習必須進行大量運算，過去我們使用 CPU 來進行運算，如今多數深度學習的框架多支援 GPU，甚至支援以多個 GPU 與多台裝置進行分散式學習。GPU 原本是圖形專用處理器，可以快速處理平行運算，GPU 運算的目標是把其強大的效能運用在各種用途。比較 CPU 與 GPU 在 AlexNet 的學習，CPU 需花費 40 天以上，GPU 則可以在 6 天內完成。<br />
</p>

<p>
利用 GPU 除了可以大幅提升深度學習的運算速度，但是一旦變成多層網路時，就需要花費數天或數週的時間來學習，Google 的 TensorFlow、Microsoft 的 CNTK 便是針對分散式學習來開發的，100 個分散式的 GPU 可以提升比單一 GPU 高到 56 倍的速度，意味著原本要有天才能完成的學習，只要 3 小時就可以結束。<br />
</p>

<p>
在深度學習的高速化過程中，包含運算量在內，記憶體容量、匯流排頻寬等，都會造成瓶頸，就記憶體容量來說，必須考慮到大量權重參數及中間資料會儲存在記憶體的情況。至於匯流排頻寛，一旦通過 GPU(或 CPU)的匯流排資料超過一定的限制，該處就會形成瓶頸，所以，最好能儘量減少通過網路的資料位元數。<br />
</p>
</div>
<div id="outline-container-org9cf8692" class="outline-4">
<h4 id="org9cf8692"><span class="section-number-4">1.2.1.</span> GPU v.s. CPU</h4>
<div class="outline-text-4" id="text-1-2-1">

<div id="org630489d" class="figure">
<p><img src="images/深度學習/2024-02-20_10-26-27_2024-02-20_10-25-03.png" alt="2024-02-20_10-26-27_2024-02-20_10-25-03.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 4: </span>CPU 與 GPU 在架構上的設計差異</p>
</div>

<p>
如圖<a href="#org630489d">4</a>，CPU 和 GPU 的差異起源於其相異的設計目標與應用場景， CPU的設計目的是處理各種不同的數據運算、邏輯判斷和中斷要求;而 GPU 的設計目的則是為了圖形運算， 其優勢在於能快速對同類型的數據進行平行運算<sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup>。二者主要差異大致如下：<br />
</p>
<ul class="org-ul">
<li>CPU 是由幾個每次可處理數個獨立「執行緒」(threads)的核心(core)所組成；GPU 則有數百個這樣的核心，同時可以處理上千個執行緒<br /></li>
<li>CPU 主要是線性執行； GPU 則是個高度平行化的單元<br /></li>
<li>CPU 的發展主要致力於最佳化系統的遲滯時間，讓系統能有迅速流暢的反應；GPU 的發展則是朝頻寬最佳化努力。在深度神經網路中，頻寬為主要的系統瓶頸<br /></li>
<li>GPU 的 Level 1 cache 比 CPU 快且大，在深度神經網路中，大部份的資料都會再次被使用到<br /></li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-org9d15cd9" class="outline-2">
<h2 id="org9d15cd9"><span class="section-number-2">2.</span> 深度學習運作原理</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org06cec31" class="outline-3">
<h3 id="org06cec31"><span class="section-number-3">2.1.</span> Layer, 損失函數與優化器</h3>
<div class="outline-text-3" id="text-2-1">
<p>
前節深度學習中的每一「層」(layer)如何運作，取決於儲存於該層的權重(weight)，而權重是由多個數字組成。從技術層面來看，layer 是由各個權重參數(parameters)來和輸入的資料(如圖<a href="#org49c1de5">5</a>中的X)進行運算以執行資料轉換的工作(如圖<a href="#org49c1de5">5</a>)。而所謂的學習，指的就是幫助神經網路的每一層找出適當的權重值，讓神經網路可以將輸入的訓練資料經由與權重的運作推導出接近標準答案的運算結果(即圖<a href="#org49c1de5">5</a>中的預測 Y)。<br />
</p>

<p>
然而，這在實際運作上是十分困難的，因為一個深度神經網路可以包含數千萬個權重，此外，其中一個權重被改變後，往往會影響其他權重的運作。<br />
</p>


<div id="org49c1de5" class="figure">
<p><img src="images/img-191107115233.jpg" alt="img-191107115233.jpg" width="400" /><br />
</p>
<p><span class="figure-number">Figure 5: </span>nn 中 layer 的 parameter</p>
</div>

<p>
為了提高神經網路的效能(預測的準確率)，我們要即時的掌握目前的輸出(Y)與真正的標準答案Y還差多少，這個評估由神經網路的損失函數(loss function;或稱目標函數, objective function;或稱成本函數, cost function<sup><a id="fnr.4" class="footref" href="#fn.4" role="doc-backlink">4</a></sup>)來負責，如圖<a href="#org24c4875">6</a>。損失函數會取得神經網路的預測結果與標準答案二者的損失分數(又稱差距分數)，做為每一次學習的表現效能之評估標準。<br />
</p>


<div id="org24c4875" class="figure">
<p><img src="images/img-191107115304.jpg" alt="img-191107115304.jpg" width="400" /><br />
</p>
<p><span class="figure-number">Figure 6: </span>損失函數</p>
</div>

<p>
而深度學習的基本工作就是使用損失函數做為回饋訊息來一步步微調權重，逐步降低每次學習的損失分數，最終目標在於讓損失函數結果達到最小，而這個微調工作則由優化器(optimizer，也稱最佳化函數)來執行。優化器實作了反向傳播演算法(Backpropagation)，這也是深度學習中的核心演算法，藉此來週整權重。<br />
</p>


<div id="org198b016" class="figure">
<p><img src="images/img-1911071153041.jpg" alt="img-1911071153041.jpg" width="400" /><br />
</p>
<p><span class="figure-number">Figure 7: </span>優化器</p>
</div>

<p>
事實上，同樣的流程我們也曾在<a href="20221023154410-regression.html#ID-6ae7fb7a-0b38-4448-b19f-073d262513f2">迴歸</a>裡看過，在找到一條理想的迴歸方程式時，我們也是先隨便找一條，然後用loss function去評估這條方程式的優劣，再「求切線斜率」的方式來修正方程式的係數。差別只在於：在<a href="20221023154410-regression.html#ID-6ae7fb7a-0b38-4448-b19f-073d262513f2">迴歸</a>時我們要修正的係數只有一、兩個，而在深度學習中，我們要同時修正成千上萬個權重。<br />
</p>

<p>
那麼，在最初一次的學習，權重的值是如何設定的呢？可以先全數設為零，但更常用的做法是隨機指定，隨著多次學習後，權重會逐步往正確的方向調整，損失分數也會慢慢降低。<br />
</p>

<p>
我們再複習一下<a href="20221025104603-神經網路.html#ID-d6daa102-05bb-475d-b619-db8b61e86030">神經網路</a>這章裡的文字：<br />
</p>

<blockquote>
<p>
是的，就如同考試時你面對陌生選擇題的反應，神經網路也決定這麼幹，隨便丟一些數值填到矩陣中當成第一批參數。事實上，同樣的策略我們在<a href="20221023154410-regression.html#ID-7cd4a142-4cd9-46b6-b9a4-2ad750ae622f">線性迴歸:年齡身高預測/隨機的力量</a>裡已經玩過了，當初在找出方程式的最佳參數組合時，我們也是閉上眼睛隨便選一組。不管整個網路中有多少參數，當我們隨機設定好了所有參數的最初值後，整個神經網就就可以運作了，嗯&#x2026;至少已經可以依照前向傳播的流程輸出第一個預測結果了，你看，我們已經朝完美的人工智慧跨近一大步了-_-<br />
</p>

<div id="org67c1ced" class="figure">
<p><img src="images/類神經網路/2024-02-18_10-23-40_2024-02-18_10-20-52.png" alt="2024-02-18_10-23-40_2024-02-18_10-20-52.png" width="400" /><br />
</p>
</div>

<p>
接下來的流程其實和<a href="20221023154410-regression.html#ID-6ae7fb7a-0b38-4448-b19f-073d262513f2">迴歸</a>有點類似，我們評估預測結果的品質，然後回頭修正參數，只是這次的工程有點浩大，我們要修正所有的參數，這個回頭修正所有參數的過程稱為反向傳播(backward propagation)。<br />
</p>
</blockquote>
</div>
</div>
</div>
<div id="outline-container-org57f889b" class="outline-2">
<h2 id="org57f889b"><span class="section-number-2">3.</span> 實作範例</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-orgcec3b60" class="outline-3">
<h3 id="orgcec3b60"><span class="section-number-3">3.1.</span> 二元分類：IMDB</h3>
<div class="outline-text-3" id="text-3-1">
<p>
自 IMDB 資料集中取得 50000 個正/負評論，各 25000 個，該資料集已內建於 Keras 中，且資料已先預處理，電影評論內容為由單字構成的 list 結構，例如，若評論內容為&ldquo;In a Wonderful morning&#x2026;&rdquo;，其 list 結構可能為(8, 3, 386, 1969&#x2026;)，每個單字都會依據其出現頻率給定一個編號，編號越小越常見。(與 IMDb 相關的 paper 參見<a href="https://paperswithcode.com/sota/sentiment-analysis-on-imdb">Sentiment Analysis on IMDb / paperswithcode</a><br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> imdb
<span class="linenr">2: </span>(train_data, train_labels), (<span style="color: #dcaeea;">test_data</span>, <span style="color: #dcaeea;">test_labels</span>) = imdb.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)
<span class="linenr">3: </span><span style="color: #c678dd;">print</span>(train_data[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">4: </span><span style="color: #c678dd;">print</span>(train_labels[<span style="color: #da8548; font-weight: bold;">0</span>])
</pre>
</div>

<pre class="example">
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz
17464789/17464789 [==============================] - 35s 2us/step
[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]
1
</pre>


<p>
如上為第一筆評論的單字代號與評論結果，若要將原始資料的單字代號還原，其程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">word_index is a dictionary mapping words to an integer index</span>
<span id="coderef-wordIndex" class="coderef-off"><span class="linenr"> 2: </span><span style="color: #dcaeea;">word_index</span> = imdb.get_word_index()</span>
<span class="linenr"> 3: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#23383;&#20856;&#20013;key&#28858;this&#23565;&#25033;&#30340;value:"</span>,word_index[<span style="color: #98be65;">'this'</span>])
<span class="linenr"> 4: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">We reverse it, mapping integer indices to words</span>
<span id="coderef-reverseWordIndex" class="coderef-off"><span class="linenr"> 5: </span><span style="color: #dcaeea;">reverse_word_index</span> = <span style="color: #c678dd;">dict</span>([(value, key) <span style="color: #51afef;">for</span> (key, value) <span style="color: #51afef;">in</span> word_index.items()])</span>
<span class="linenr"> 6: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#21453;&#36681;&#23383;&#20856;&#20013;key&#28858;11&#25152;&#23565;&#25033;&#21040;&#30340;value:"</span>,reverse_word_index[<span style="color: #da8548; font-weight: bold;">11</span>])
<span class="linenr"> 7: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#21453;&#36681;&#23383;&#20856;&#20013;key&#28858;1&#25152;&#23565;&#25033;&#21040;&#30340;value:"</span>,reverse_word_index[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr"> 8: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#21453;&#36681;&#23383;&#20856;&#20013;key&#28858;2&#25152;&#23565;&#25033;&#21040;&#30340;value:"</span>,reverse_word_index[<span style="color: #da8548; font-weight: bold;">2</span>])
<span class="linenr"> 9: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">We decode the review; note that our indices were offset by 3</span>
<span class="linenr">10: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">because 0, 1 and 2 are reserved indices for "padding", "start of sequence", and "unknown".</span>
<span id="coderef-decodedReview" class="coderef-off"><span class="linenr">11: </span><span style="color: #dcaeea;">decoded_review</span> = <span style="color: #98be65;">' '</span>.join([reverse_word_index.get(i - <span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #98be65;">'?'</span>) <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> train_data[<span style="color: #da8548; font-weight: bold;">0</span>]])</span>
<span class="linenr">12: </span><span style="color: #c678dd;">print</span>(decoded_review)
</pre>
</div>

<pre class="example">
字典中key為this對應的value: 11
反轉字典中key為11所對應到的value: this
反轉字典中key為1所對應到的value: the
反轉字典中key為2所對應到的value: and
? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all
</pre>


<p>
上述程式中第<a href="#coderef-wordIndex" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-wordIndex');" onmouseout="CodeHighlightOff(this, 'coderef-wordIndex');">2</a>行主要負責取得單字(key)的對應數字(value)的字典，再藉由第<a href="#coderef-reverseWordIndex" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-reverseWordIndex');" onmouseout="CodeHighlightOff(this, 'coderef-reverseWordIndex');">5</a>行將(key:value)轉換為(value:key)，最後第<a href="#coderef-decodedReview" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-decodedReview');" onmouseout="CodeHighlightOff(this, 'coderef-decodedReview');">11</a>行將字典中的單字回復至原始評論，程式中(i-3)的原因是imdb.load_data已預留了第 0~2 個位置做特殊用途。<br />
</p>
</div>
<div id="outline-container-org0344984" class="outline-4">
<h4 id="org0344984"><span class="section-number-4">3.1.1.</span> 準備資料</h4>
<div class="outline-text-4" id="text-3-1-1">
<p>
由於 IMDB 匯入 train_data 及 test_data 均為 list 型態，要先轉換為 tensor 才能輸入至神經網路，方法有二：<br />
</p>
<ol class="org-ol">
<li>填補資料中每個子 list 內容使其具有相同長度，再做reshape<br /></li>
<li>對每個子 list 做 one-hot encoding，其程式碼如下：<br /></li>
</ol>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">vectorize_sequences</span>(sequences, dimension=<span style="color: #da8548; font-weight: bold;">10000</span>):
<span class="linenr"> 3: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Create an all-zero matrix of shape (len(sequences), dimension)</span>
<span class="linenr"> 4: </span>    <span style="color: #dcaeea;">results</span> = np.zeros((<span style="color: #c678dd;">len</span>(sequences), dimension))
<span class="linenr"> 5: </span>    <span style="color: #51afef;">for</span> i, sequence <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(sequences):
<span class="linenr"> 6: </span>        <span style="color: #dcaeea;">results</span>[<span style="color: #dcaeea;">i</span>, <span style="color: #dcaeea;">sequence</span>] = <span style="color: #da8548; font-weight: bold;">1</span>.  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">set specific indices of results[i] to 1s</span>
<span class="linenr"> 7: </span>    <span style="color: #51afef;">return</span> results
<span class="linenr"> 8: </span><span style="color: #c678dd;">print</span>(train_data.shape)
<span class="linenr"> 9: </span><span style="color: #c678dd;">print</span>(train_data[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">10: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized training data</span>
<span class="linenr">11: </span>
<span class="linenr">12: </span><span style="color: #dcaeea;">x_train</span> = vectorize_sequences(train_data)
<span class="linenr">13: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized test data</span>
<span class="linenr">14: </span><span style="color: #dcaeea;">x_test</span> = vectorize_sequences(test_data)
<span class="linenr">15: </span><span style="color: #c678dd;">print</span>(x_train.shape)
<span class="linenr">16: </span><span style="color: #c678dd;">print</span>(x_train[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">17: </span>
<span class="linenr">18: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26368;&#24460;&#20877;&#23559;&#27161;&#31844;&#36039;&#26009;&#20063;&#21521;&#37327;&#21270;</span>
<span class="linenr">19: </span><span style="color: #dcaeea;">y_train</span> = np.asarray(train_labels).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">20: </span><span style="color: #dcaeea;">y_test</span> = np.asarray(test_labels).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">21: </span>
<span class="linenr">22: </span><span style="color: #c678dd;">print</span>(y_train[<span style="color: #da8548; font-weight: bold;">0</span>])
</pre>
</div>


<pre class="example">
(25000,)
[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]
(25000, 10000)
[0. 1. 1. ... 0. 0. 0.]
1.0
</pre>
</div>
</div>
<div id="outline-container-orgafde35a" class="outline-4">
<h4 id="orgafde35a"><span class="section-number-4">3.1.2.</span> 建立神經網路</h4>
<div class="outline-text-4" id="text-3-1-2">
<p>
要建構一個 Dense 層堆疊架構的神經網路，要考慮兩個關鍵：<br />
</p>
<ol class="org-ol">
<li>要用多少層？<br /></li>
<li>每一層要有多少神經元？<br /></li>
</ol>
<p>
此處使用兩個中間層、一個輸出層，如圖<a href="#orge96060e">8</a>，一般的神經網路中，對那些介於輸入層和輸出層間的layer，我們習慣上稱之為隱藏層(hidden layers)，但此處 Keras 的輸入層也有隱藏層的特性。圖<a href="#orge96060e">8</a>的 hidden layer 以 relu 為啟動函數，輸出層以 sigmoid 啟動函數輸出機率值。<br />
</p>


<div id="orge96060e" class="figure">
<p><img src="images/nn3-6.png" alt="nn3-6.png" width="300" /><br />
</p>
<p><span class="figure-number">Figure 8: </span>IMDB model 架構</p>
</div>

<p>
由於輸入資料為向量、標籤為純量(1, 0)，對這樣的問題，適合用 relu 啟動函數的全連接層(Dense)堆疊架構：Dense(16, activation=&rsquo;relu&rsquo;)。其中 16 指該層神經元的數量(也可看成該層的寬度)，典型旳寫法為：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21152;&#20837;Dense&#38577;&#34255;&#23652;&#65292;&#35442;&#23652;&#26377;16&#20491;&#31070;&#32147;&#20803;</span>
model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, activation=<span style="color: #98be65;">'relu'</span>))
</pre>
</div>

<p>
擁有 16 個神經單元表示權重矩陣 W 的 shape 為(input_dimension, 16)，在 W 和 input 做內積後，input 資料會被映射到 16 維的空間上，最後加上 b、套用 relu 運算來產生輸出值。每一層的神經元數越多，可以讓神經網路學習更複雜的資料表示法，但也使計算成本更高。<br />
</p>


<div id="orgbd02035" class="figure">
<p><img src="images/ReLUPlot.png" alt="ReLUPlot.png" width="300" /><br />
</p>
<p><span class="figure-number">Figure 9: </span>ReLU 函數圖</p>
</div>

<p>
為何要有 relu 等啟動函數？原因之一是這類函數為非線性函數(如圖<a href="#orgbd02035">9</a>)，回顧<a href="20221025104603-神經網路.html#ID-d6daa102-05bb-475d-b619-db8b61e86030">神經網路</a>中的「學測成績預測模型」，像圖<a href="#orga06b22f">10</a>的模型，我們也只是在解一個如\(f(x)=x_1*w_1+x_2*w_2+x_3*w_3+...+x_7*w_7\)這樣的函式問題。<br />
</p>


<div id="orga06b22f" class="figure">
<p><img src="images/exam-Network2.png" alt="exam-Network2.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 10: </span>學測成績預測模型#2</p>
</div>

<p>
就算我們把模型2進化為模型3(如圖<a href="#orgf574799">11</a>)，本質上也仍只是一層，再多的層數也能合併為一層，此類模型並無助於複雜的學習。為了有效讓模型更加複雜，此處可以在模型中加入非線性轉換，如圖<a href="#orgbd02035">9</a>中的ReLU激勵函數。<br />
</p>


<div id="orgf574799" class="figure">
<p><img src="images/exam-Network3.png" alt="exam-Network3.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 11: </span>學測成績預測模型#3</p>
</div>

<p>
圖<a href="#orge96060e">8</a>的實作程式如下：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">2: </span><span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">3: </span>
<span class="linenr">4: </span><span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">5: </span>model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr">6: </span>model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">7: </span>model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
</pre>
</div>

<p>
建好 model 後，要選擇一個損失函數和一個優化器，由於要處理的是二元分類問題，所以最好用 binary_crossentropy 損失函數，因為 crossentropy 主要就是用來測量機率分佈之間的距離(差異)。其實作如下：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr">2: </span>             loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr">3: </span>             metrics=[<span style="color: #98be65;">'accuracy'</span>])
</pre>
</div>

<p>
之所以能將 optimizer 和 loss function 以字串方式經由參數傳給 compile()，這是因為 rmsprop、binary_crossentropy 和 accuracy 均已事先在 Keras 套件中定義好了，若是要進一步自訂參數(如自訂學習率)，做法如下：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35519;&#25972;learning rate</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>model.<span style="color: #c678dd;">compile</span>(optimizer=optimizers.RMSprop(learning_rate=<span style="color: #da8548; font-weight: bold;">0.001</span>),
<span class="linenr"> 5: </span>              loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr"> 6: </span>              metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20351;&#29992;&#21478;&#22806;&#30340;&#35413;&#20272;&#20989;&#25976;</span>
<span class="linenr"> 9: </span><span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> losses
<span class="linenr">10: </span><span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> metrics
<span class="linenr">11: </span>
<span class="linenr">12: </span>model.<span style="color: #c678dd;">compile</span>(optimizer=optimizers.RMSprop(learning_rate=<span style="color: #da8548; font-weight: bold;">0.001</span>),
<span class="linenr">13: </span>              loss=losses.binary_crossentropy,
<span class="linenr">14: </span>              metrics=[metrics.binary_accuracy])
</pre>
</div>

<pre class="example">
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.
</pre>

<p>
若您使用的是M1/M2核心的Mac電腦，則可能會出現上述訊息，雖然不影響正執行結果，但你仍可以參考<a href="https://stackoverflow.com/questions/77222463/is-there-a-way-to-change-adam-to-legacy-when-using-mac-m1-m2-in-tensorflow">stackoverflow上的這篇文章</a>來解決這些惱人的訊息。<br />
</p>
</div>
</div>
<div id="outline-container-org4a07e6a" class="outline-4">
<h4 id="org4a07e6a"><span class="section-number-4">3.1.3.</span> 驗證神經網路的 model</h4>
<div class="outline-text-4" id="text-3-1-3">
<p>
為了在訓練期間監控 model 對新資料的準確度，可以從原始訓練資料中分離出 10000 個樣本來建立驗證資料集。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">x_val</span> = x_train[:<span style="color: #da8548; font-weight: bold;">10000</span>] <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21069;10000&#20491;&#36039;&#26009;&#28858;&#39511;&#35657;&#38598;</span>
<span class="linenr">2: </span><span style="color: #dcaeea;">partial_x_train</span> = x_train[<span style="color: #da8548; font-weight: bold;">10000</span>:] <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#31532;10000&#20491;&#20197;&#24460;&#28858;&#35347;&#32244;&#38598;</span>
<span class="linenr">3: </span>
<span class="linenr">4: </span><span style="color: #dcaeea;">y_val</span> = y_train[:<span style="color: #da8548; font-weight: bold;">10000</span>]
<span class="linenr">5: </span><span style="color: #dcaeea;">partial_y_train</span> = y_train[<span style="color: #da8548; font-weight: bold;">10000</span>:]
</pre>
</div>

<p>
接下來才是使用 fit()來訓練模型，進行 20 個訓練週期(epoch，即，把 x_train 和 y_train 張量中的所有訓練樣本進行 20 輪的訓練)，以 512 個小樣本的小批量(batch_size)進行訓練，<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">history</span> = model.fit(partial_x_train,
<span class="linenr">2: </span>                    partial_y_train,
<span class="linenr">3: </span>                    epochs=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr">4: </span>                    batch_size=<span style="color: #da8548; font-weight: bold;">512</span>,
<span class="linenr">5: </span>                    validation_data=(x_val, y_val))
</pre>
</div>

<pre class="example" id="orga32ec65">
Epoch 1/20
30/30 [==============================] - 2s 44ms/step - loss: 0.5010 - accuracy: 0.7841 - val_loss: 0.3645 - val_accuracy: 0.8758
Epoch 2/20
30/30 [==============================] - 1s 25ms/step - loss: 0.2951 - accuracy: 0.9032 - val_loss: 0.3066 - val_accuracy: 0.8823
...略...
Epoch 19/20
30/30 [==============================] - 1s 17ms/step - loss: 0.0089 - accuracy: 0.9987 - val_loss: 0.6555 - val_accuracy: 0.8679
Epoch 20/20
30/30 [==============================] - 0s 15ms/step - loss: 0.0036 - accuracy: 0.9999 - val_loss: 0.7007 - val_accuracy: 0.8644
</pre>

<p>
model.fit()會回傳一個 history 物件，這物件本身有一個 history 屬性，為一個包含有關訓練過程中相關數據的字典，這個字期包含有 4 個項目(val_loss, val_acc, loss, acc)，為訓練和驗證時監控的指標。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#31168;&#20986;history&#26550;&#27083;</span>
<span class="linenr"> 2: </span><span style="color: #dcaeea;">history_dict</span> = history.history
<span class="linenr"> 3: </span><span style="color: #c678dd;">print</span>(history_dict.keys())
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#22294;</span>
<span class="linenr"> 6: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 7: </span><span style="color: #dcaeea;">accuracy</span> = history.history[<span style="color: #98be65;">'accuracy'</span>]
<span class="linenr"> 8: </span><span style="color: #dcaeea;">val_accuracy</span> = history.history[<span style="color: #98be65;">'val_accuracy'</span>]
<span class="linenr"> 9: </span><span style="color: #dcaeea;">loss</span> = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr">10: </span><span style="color: #dcaeea;">val_loss</span> = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr">11: </span><span style="color: #dcaeea;">epochs</span> = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(accuracy) + <span style="color: #da8548; font-weight: bold;">1</span>)<span style="color: #5B6268;"># </span><span style="color: #5B6268;">"bo" is for "blue dot"</span>
<span class="linenr">12: </span>plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr">13: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">b is for "solid blue line"</span>
<span class="linenr">14: </span>plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr">15: </span>plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr">16: </span>plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">17: </span>plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr">18: </span>plt.legend()
<span class="linenr">19: </span>plt.plot()
<span class="linenr">20: </span>plt.savefig(<span style="color: #98be65;">"images/imdb-Keras-1.png"</span>)
<span class="linenr">21: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()plt.clf()   # clear figureplt.clf()</span>
<span class="linenr">22: </span><span style="color: #dcaeea;">acc_values</span> = history_dict[<span style="color: #98be65;">'accuracy'</span>]
<span class="linenr">23: </span><span style="color: #dcaeea;">val_acc_values</span> = history_dict[<span style="color: #98be65;">'val_accuracy'</span>]
<span class="linenr">24: </span>plt.plot(epochs, accuracy, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training acc'</span>)
<span class="linenr">25: </span>plt.plot(epochs, val_accuracy, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation acc'</span>)
<span class="linenr">26: </span>plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr">27: </span>plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">28: </span>plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr">29: </span>plt.legend()
<span class="linenr">30: </span>plt.plot()
<span class="linenr">31: </span>plt.savefig(<span style="color: #98be65;">"images/imdb-Keras-2.png"</span>)
<span class="linenr">32: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
</pre>
</div>

<pre class="example">
dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])
</pre>



<div id="org634c81b" class="figure">
<p><img src="images/imdb-Keras-1.png" alt="imdb-Keras-1.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 12: </span>IMDB-Keras-1</p>
</div>


<div id="org6bad141" class="figure">
<p><img src="images/imdb-Keras-2.png" alt="imdb-Keras-2.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 13: </span>IMDB-Keras-2</p>
</div>
</div>
</div>
<div id="outline-container-orge9ae3bc" class="outline-4">
<h4 id="orge9ae3bc"><span class="section-number-4">3.1.4.</span> 優化 model</h4>
<div class="outline-text-4" id="text-3-1-4">
<p>
由圖<a href="#orga4005f9">15</a>、<a href="#org47445d9">16</a>可以看出，上述 model 雖然在訓練階段的效能不錯，loss function 隨 epoch 下降、accuracy 也隨 epoch 升高，但在驗證階段的表現卻十分不理想，不僅 accuracy 隨 epoch 的增加呈緩降趨勢，loss function 甚至還往上急升。<br />
</p>

<p>
第二版的 model 做了以下改進:<br />
</p>
<ul class="org-ul">
<li>將資料向量化(vectorize_sequences())<br /></li>
<li>加入了兩層 layer 以及 dropout 層，其架構如圖<a href="#org211ea9b">14</a><br /></li>
</ul>

<div id="org211ea9b" class="figure">
<p><img src="images/nn3-6-2.png" alt="nn3-6-2.png" width="300" /><br />
</p>
<p><span class="figure-number">Figure 14: </span>IMDB model 架構#2</p>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21521;&#37327;&#21270;function</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">vectorize_sequences</span>(sequences, dimension=<span style="color: #da8548; font-weight: bold;">10000</span>):
<span class="linenr"> 3: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Create an all-zero matrix of shape (len(sequences), dimension)</span>
<span class="linenr"> 4: </span>    <span style="color: #dcaeea;">results</span> = np.zeros((<span style="color: #c678dd;">len</span>(sequences), dimension))
<span class="linenr"> 5: </span>    <span style="color: #51afef;">for</span> i, sequence <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(sequences):
<span class="linenr"> 6: </span>        <span style="color: #dcaeea;">results</span>[<span style="color: #dcaeea;">i</span>, <span style="color: #dcaeea;">sequence</span>] = <span style="color: #da8548; font-weight: bold;">1</span>.  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">set specific indices of results[i] to 1s</span>
<span class="linenr"> 7: </span>    <span style="color: #51afef;">return</span> results
<span class="linenr"> 8: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized training data</span>
<span class="linenr"> 9: </span><span style="color: #dcaeea;">x_train</span> = vectorize_sequences(train_data)
<span class="linenr">10: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized test data</span>
<span class="linenr">11: </span><span style="color: #dcaeea;">x_test</span> = vectorize_sequences(test_data)
<span class="linenr">12: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26368;&#24460;&#20877;&#23559;&#27161;&#31844;&#36039;&#26009;&#20063;&#21521;&#37327;&#21270;</span>
<span class="linenr">13: </span><span style="color: #dcaeea;">y_train</span> = np.asarray(train_labels).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">14: </span><span style="color: #dcaeea;">y_test</span> = np.asarray(test_labels).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">15: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24314;&#31435;model</span>
<span class="linenr">16: </span><span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">17: </span>model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr">18: </span>model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">19: </span>model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.25</span>))
<span class="linenr">20: </span>model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">21: </span>model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.25</span>))
<span class="linenr">22: </span>model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
<span class="linenr">23: </span>
<span class="linenr">24: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#21028;&#26039;&#20316;&#26989;&#31995;&#32113;&#39006;&#22411;&#65292;&#36984;&#25799;&#20778;&#21270;&#22120;</span>
<span class="linenr">25: </span><span style="color: #51afef;">import</span> platform
<span class="linenr">26: </span><span style="color: #51afef;">if</span> platform.system() == <span style="color: #98be65;">"Darwin"</span> <span style="color: #51afef;">and</span> platform.processor() == <span style="color: #98be65;">"arm"</span>:
<span class="linenr">27: </span>    <span style="color: #dcaeea;">opt</span> = optimizers.legacy.RMSprop(learning_rate=<span style="color: #da8548; font-weight: bold;">0.0001</span>)
<span class="linenr">28: </span><span style="color: #51afef;">else</span>:
<span class="linenr">29: </span>    <span style="color: #dcaeea;">opt</span> = optimizers.RMSprop(learning_rate=<span style="color: #da8548; font-weight: bold;">0.0001</span>)
<span class="linenr">30: </span>
<span class="linenr">31: </span>model.<span style="color: #c678dd;">compile</span>(optimizer=opt, loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr">32: </span>              metrics=[metrics.binary_accuracy])
<span class="linenr">33: </span>
<span class="linenr">34: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39511;&#35657;&#25976;&#25818;&#38598;</span>
<span class="linenr">35: </span><span style="color: #dcaeea;">x_val</span> = x_train[:<span style="color: #da8548; font-weight: bold;">10000</span>] <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21069;10000&#20491;&#36039;&#26009;&#28858;&#39511;&#35657;&#38598;</span>
<span class="linenr">36: </span><span style="color: #dcaeea;">partial_x_train</span> = x_train[<span style="color: #da8548; font-weight: bold;">10000</span>:] <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#31532;10000&#20491;&#20197;&#24460;&#28858;&#35347;&#32244;&#38598;</span>
<span class="linenr">37: </span><span style="color: #dcaeea;">y_val</span> = y_train[:<span style="color: #da8548; font-weight: bold;">10000</span>]
<span class="linenr">38: </span><span style="color: #dcaeea;">partial_y_train</span> = y_train[<span style="color: #da8548; font-weight: bold;">10000</span>:]
<span class="linenr">39: </span>
<span class="linenr">40: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;model</span>
<span class="linenr">41: </span><span style="color: #dcaeea;">history</span> = model.fit(partial_x_train, partial_y_train,
<span class="linenr">42: </span>                    epochs=<span style="color: #da8548; font-weight: bold;">20</span>, batch_size=<span style="color: #da8548; font-weight: bold;">512</span>,
<span class="linenr">43: </span>                    validation_data=(x_val, y_val), verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">44: </span>
<span class="linenr">45: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#31168;&#20986;history&#26550;&#27083;</span>
<span class="linenr">46: </span><span style="color: #dcaeea;">history_dict</span> = history.history
<span class="linenr">47: </span><span style="color: #c678dd;">print</span>(history_dict.keys())
<span class="linenr">48: </span>
<span class="linenr">49: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36914;&#34892;&#38928;&#28204;</span>
<span class="linenr">50: </span><span style="color: #dcaeea;">x</span> = model.predict(x_test)
<span class="linenr">51: </span><span style="color: #c678dd;">print</span>(x)
<span class="linenr">52: </span>
<span class="linenr">53: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#22294;</span>
<span class="linenr">54: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">55: </span>plt.clf()
<span class="linenr">56: </span><span style="color: #dcaeea;">binary_accuracy</span> = history.history[<span style="color: #98be65;">'binary_accuracy'</span>]
<span class="linenr">57: </span><span style="color: #dcaeea;">val_binary_accuracy</span> = history.history[<span style="color: #98be65;">'val_binary_accuracy'</span>]
<span class="linenr">58: </span><span style="color: #dcaeea;">loss</span> = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr">59: </span><span style="color: #dcaeea;">val_loss</span> = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr">60: </span><span style="color: #dcaeea;">epochs</span> = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(binary_accuracy) + <span style="color: #da8548; font-weight: bold;">1</span>)<span style="color: #5B6268;"># </span><span style="color: #5B6268;">"bo" is for "blue dot"</span>
<span class="linenr">61: </span>plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr">62: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">b is for "solid blue line"</span>
<span class="linenr">63: </span>plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr">64: </span>plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr">65: </span>plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">66: </span>plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr">67: </span>plt.legend()
<span class="linenr">68: </span>plt.plot()
<span class="linenr">69: </span>plt.savefig(<span style="color: #98be65;">"images/imdb-Keras-3.png"</span>)
<span class="linenr">70: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">71: </span>
<span class="linenr">72: </span>plt.clf()
<span class="linenr">73: </span><span style="color: #dcaeea;">acc_values</span> = history_dict[<span style="color: #98be65;">'binary_accuracy'</span>]
<span class="linenr">74: </span><span style="color: #dcaeea;">val_acc_values</span> = history_dict[<span style="color: #98be65;">'val_binary_accuracy'</span>]
<span class="linenr">75: </span>plt.plot(epochs, binary_accuracy, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training acc'</span>)
<span class="linenr">76: </span>plt.plot(epochs, val_binary_accuracy, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation acc'</span>)
<span class="linenr">77: </span>plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr">78: </span>plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">79: </span>plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr">80: </span>plt.legend()
<span class="linenr">81: </span>plt.plot()
<span class="linenr">82: </span>plt.savefig(<span style="color: #98be65;">"images/imdb-Keras-4.png"</span>)
<span class="linenr">83: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
</pre>
</div>

<pre class="example">
dict_keys(['loss', 'binary_accuracy', 'val_loss', 'val_binary_accuracy'])
782/782 [==============================] - 1s 670us/step
[[0.13477947]
 [0.99921584]
 [0.87466985]
 ...
 [0.04399278]
 [0.0864688 ]
 [0.42204612]]
</pre>



<div id="orga4005f9" class="figure">
<p><img src="images/imdb-Keras-1.png" alt="imdb-Keras-1.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 15: </span>IMDB-Keras-1</p>
</div>


<div id="org47445d9" class="figure">
<p><img src="images/imdb-Keras-2.png" alt="imdb-Keras-2.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 16: </span>IMDB-Keras-2</p>
</div>


<div id="orge62e8d4" class="figure">
<p><img src="images/imdb-Keras-3.png" alt="imdb-Keras-3.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 17: </span>IMDB-Keras-3</p>
</div>


<div id="org05b4f68" class="figure">
<p><img src="images/imdb-Keras-4.png" alt="imdb-Keras-4.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 18: </span>IMDB-Keras-4</p>
</div>

<p>
比較上述兩組結果，可以發現優化版的 model 在 loss function 以及 accuracy 的表現都有進步。<br />
</p>
</div>
</div>
</div>
<div id="outline-container-org504d94a" class="outline-3">
<h3 id="org504d94a"><span class="section-number-3">3.2.</span> 多類別分類：數位新聞</h3>
<div class="outline-text-3" id="text-3-2">
<p>
目標：將路透社(Reuters)的數位新聞專欄分成 46 個主題，這屬於多類別分類(multiclass classification)問題，每個資料點只會被歸入一個類別；如果每個資料點可能屬於多個類別，則屬於多標籤多類別(multilabel multiclass classification)問題。<br />
</p>
</div>
<div id="outline-container-org90cc679" class="outline-4">
<h4 id="org90cc679"><span class="section-number-4">3.2.1.</span> 資料集</h4>
<div class="outline-text-4" id="text-3-2-1">
<p>
和 MNIST、IMDB 一樣，這組由 Reuters 在 1986 年發布的簡短新聞主題資料集也內建在 Keras 中，這個資料集總共分為 46 個不同主題。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> reuters
<span class="linenr">2: </span>(train_data, train_labels), (<span style="color: #dcaeea;">test_data</span>, <span style="color: #dcaeea;">test_labels</span>) = reuters.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)
<span class="linenr">3: </span><span style="color: #c678dd;">print</span>(train_data[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">4: </span><span style="color: #c678dd;">print</span>(train_labels[<span style="color: #da8548; font-weight: bold;">0</span>])
</pre>
</div>

<pre class="example">
[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]
3
</pre>


<p>
將資料向量化有幾種方式：將 label list 轉為整數張量，或是用 one-hot 編碼。以下為使用 python 自訂的編碼程式：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">vectorize_sequences</span>(sequences, dimension=<span style="color: #da8548; font-weight: bold;">10000</span>):
<span class="linenr"> 4: </span>    <span style="color: #dcaeea;">results</span> = np.zeros((<span style="color: #c678dd;">len</span>(sequences), dimension))
<span class="linenr"> 5: </span>    <span style="color: #51afef;">for</span> i, sequence <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(sequences):
<span class="linenr"> 6: </span>        <span style="color: #dcaeea;">results</span>[<span style="color: #dcaeea;">i</span>, <span style="color: #dcaeea;">sequence</span>] = <span style="color: #da8548; font-weight: bold;">1</span>.
<span class="linenr"> 7: </span>    <span style="color: #51afef;">return</span> results
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized training data</span>
<span class="linenr">10: </span><span style="color: #dcaeea;">x_train</span> = vectorize_sequences(train_data)
<span class="linenr">11: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized test data</span>
<span class="linenr">12: </span><span style="color: #dcaeea;">x_test</span> = vectorize_sequences(test_data)
<span class="linenr">13: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#21407;&#22987;&#36039;&#26009;&#38598;&#32173;&#24230;:'</span>,train_data.shape)
<span class="linenr">14: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#21521;&#37327;&#21270;&#36039;&#26009;&#38598;&#32173;&#24230;:'</span>,x_train.shape)
<span class="linenr">15: </span><span style="color: #c678dd;">print</span>(x_train[<span style="color: #da8548; font-weight: bold;">0</span>])
</pre>
</div>

<pre class="example">
原始資料集維度: (8982,)
向量化資料集維度: (8982, 10000)
[0. 1. 1. ... 0. 0. 0.]
</pre>


<p>
另外，Keras 也有一個內建的函式可用：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> tensorflow.keras.utils <span style="color: #51afef;">import</span> to_categorical
<span class="linenr">2: </span>
<span class="linenr">3: </span><span style="color: #dcaeea;">one_hot_train_labels</span> = to_categorical(train_labels)
<span class="linenr">4: </span><span style="color: #dcaeea;">one_hot_test_labels</span> = to_categorical(test_labels)
<span class="linenr">5: </span><span style="color: #c678dd;">print</span>(one_hot_train_labels.shape)
<span class="linenr">6: </span><span style="color: #c678dd;">print</span>(one_hot_train_labels[<span style="color: #da8548; font-weight: bold;">0</span>])
</pre>
</div>

<pre class="example">
(8982, 46)
[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
</pre>
</div>
</div>
<div id="outline-container-orge887505" class="outline-4">
<h4 id="orge887505"><span class="section-number-4">3.2.2.</span> 建立神經網路</h4>
<div class="outline-text-4" id="text-3-2-2">
<p>
此次面臨的問題不似 IMDB 只分成兩類，而是共有 46 類，若每個 Dense layer 仍只使用16個維度，可能無法學會區分 46 個不同類別，故有需要將維度增加：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">2: </span><span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">3: </span>
<span class="linenr">4: </span><span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">5: </span>model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr">6: </span>model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">7: </span>model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">46</span>, activation=<span style="color: #98be65;">'softmax'</span>))
</pre>
</div>

<p>
另外，輸出層將啟動函數由 sigmoid 改為 softmax，以機率值來顯示預測的類別結果，配合這種情境，最適合的損失函數為 categorical_crossentropy，它可以測量兩個機率分佈間的差距（即神經網路輸出的預測機率分佈與真實分佈間的距離），透過最小化這兩個分佈間的距離來訓練神經網路，讓結果接近答案。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>, loss=<span style="color: #98be65;">'categorical_crossentropy'</span>,
<span class="linenr">2: </span>                metrics=[<span style="color: #98be65;">'accuracy'</span>])
</pre>
</div>

<p>
此處的metrics用來儲存後續評估(model.evaluate)模型的記錄<br />
</p>
</div>
</div>
<div id="outline-container-orgc39f941" class="outline-4">
<h4 id="orgc39f941"><span class="section-number-4">3.2.3.</span> 驗證數據集</h4>
<div class="outline-text-4" id="text-3-2-3">
<p>
由訓練集抽出 1000 個樣本來驗證：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">x_val</span> = x_train[:<span style="color: #da8548; font-weight: bold;">1000</span>]
<span class="linenr">2: </span><span style="color: #dcaeea;">partial_x_train</span> = x_train[<span style="color: #da8548; font-weight: bold;">1000</span>:]
<span class="linenr">3: </span>
<span class="linenr">4: </span><span style="color: #dcaeea;">y_val</span> = one_hot_train_labels[:<span style="color: #da8548; font-weight: bold;">1000</span>]
<span class="linenr">5: </span><span style="color: #dcaeea;">partial_y_train</span> = one_hot_train_labels[<span style="color: #da8548; font-weight: bold;">1000</span>:]
</pre>
</div>
</div>
</div>
<div id="outline-container-org9f1ad3e" class="outline-4">
<h4 id="org9f1ad3e"><span class="section-number-4">3.2.4.</span> 訓練模型</h4>
<div class="outline-text-4" id="text-3-2-4">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">history</span> = model.fit(partial_x_train,
<span class="linenr">2: </span>                    partial_y_train,
<span class="linenr">3: </span>                    epochs=<span style="color: #da8548; font-weight: bold;">9</span>,
<span class="linenr">4: </span>                    batch_size=<span style="color: #da8548; font-weight: bold;">512</span>,
<span class="linenr">5: </span>                    validation_data=(x_val, y_val),
<span class="linenr">6: </span>                    verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">7: </span><span style="color: #dcaeea;">history_dict</span> = history.history
<span class="linenr">8: </span><span style="color: #c678dd;">print</span>(history_dict.keys())
</pre>
</div>

<pre class="example">
dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])
</pre>
</div>
</div>
<div id="outline-container-orge43cc88" class="outline-4">
<h4 id="orge43cc88"><span class="section-number-4">3.2.5.</span> 評估模型</h4>
<div class="outline-text-4" id="text-3-2-5">
<p>
程式第<a href="#coderef-modelEvaluate" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-modelEvaluate');" onmouseout="CodeHighlightOff(this, 'coderef-modelEvaluate');">6</a>行的model.evaluate()會傳回兩個結果:<br />
</p>
<ul class="org-ul">
<li>loss value<br /></li>
<li>model.compile()時指定的metrics，這裡會記錄accuracy<br /></li>
</ul>


<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'loss:'</span>, history_dict[<span style="color: #98be65;">'loss'</span>])
<span class="linenr"> 2: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'accuracy:'</span>, history_dict[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr"> 3: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'val_accuracy:'</span>, history_dict[<span style="color: #98be65;">'val_accuracy'</span>])
<span class="linenr"> 4: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35413;&#20272;</span>
<span class="linenr"> 5: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Returns the loss value &amp; metrics values for the model in test mode.</span>
<span id="coderef-modelEvaluate" class="coderef-off"><span class="linenr"> 6: </span><span style="color: #dcaeea;">results</span> = model.evaluate(x_test, one_hot_test_labels)</span>
<span class="linenr"> 7: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#35413;&#20272;&#36039;&#26009;&#20839;&#23481;&#65306;"</span>,results)
<span class="linenr"> 8: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38928;&#28204;</span>
<span class="linenr"> 9: </span><span style="color: #dcaeea;">predictions</span> = model.predict(x_test)
<span class="linenr">10: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#38928;&#28204;&#36039;&#26009;&#26550;&#27083;&#65306;"</span>,predictions[<span style="color: #da8548; font-weight: bold;">0</span>].shape)
<span class="linenr">11: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#38928;&#28204;&#36039;&#26009;&#20839;&#23481;&#65306;"</span>,predictions[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">12: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#38928;&#28204;&#32080;&#26524;:"</span>,np.argmax(predictions[<span style="color: #da8548; font-weight: bold;">0</span>]))
<span class="linenr">13: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#31572;&#26696;:"</span>,one_hot_test_labels[<span style="color: #da8548; font-weight: bold;">0</span>])
</pre>
</div>

<pre class="example" id="org29ac941">
loss: [0.23970305919647217, 0.2028971165418625, 0.18050165474414825, 0.16348564624786377, 0.14964115619659424, 0.14013586938381195, 0.13095684349536896, 0.1236189678311348, 0.11970153450965881]
accuracy: [0.9445001482963562, 0.9502630829811096, 0.9521422982215881, 0.9515159130096436, 0.9556502103805542, 0.9548985362052917, 0.9553996324539185, 0.9576547145843506, 0.9574041366577148]
val_accuracy: [0.7990000247955322, 0.8220000267028809, 0.8209999799728394, 0.8169999718666077, 0.8149999976158142, 0.8100000023841858, 0.8080000281333923, 0.8119999766349792, 0.8149999976158142]
71/71 [==============================] - 0s 994us/step - loss: 1.1759 - accuracy: 0.7907
評估資料內容： [1.1759308576583862, 0.790739119052887]
71/71 [==============================] - 0s 747us/step
預測資料架構： (46,)
預測資料內容： [9.4937323e-06 3.6443867e-05 4.1897138e-07 8.9287710e-01 1.0305237e-01
 7.9761328e-08 3.7770697e-08 1.4734838e-05 1.3683739e-03 3.5000145e-08
 1.5104798e-06 2.8917767e-04 9.3399522e-06 1.2614631e-05 3.2949796e-05
 9.3838331e-08 1.3987279e-04 8.5348020e-06 3.8895523e-06 6.5101567e-04
 9.0811914e-04 2.6460900e-04 2.1848352e-06 7.0889015e-05 7.4386901e-07
 1.2730087e-06 1.2880573e-08 2.0513512e-06 1.9234722e-05 3.4746241e-05
 6.6688386e-05 6.4851406e-07 2.9292633e-07 1.9099436e-07 1.1117180e-05
 3.5746339e-07 7.1021852e-05 2.9590351e-06 2.5054899e-06 2.6526579e-05
 3.4896138e-07 5.1099514e-06 5.6957099e-08 1.9141594e-07 9.1464599e-08
 8.4113566e-08]
預測結果: 3
答案: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
</pre>
<p>
上述程式在經由 9 個 epoch 後精準度已近 80%(0.79)。<br />
</p>
</div>
</div>
<div id="outline-container-org2565482" class="outline-4">
<h4 id="org2565482"><span class="section-number-4">3.2.6.</span> 評估結果視覺化</h4>
<div class="outline-text-4" id="text-3-2-6">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#22294;</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">loss</span> = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr"> 5: </span><span style="color: #dcaeea;">val_loss</span> = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">epochs</span> = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(loss) + <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr">10: </span>plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr">11: </span>plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr">12: </span>plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">13: </span>plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr">14: </span>plt.axis([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">10</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">3</span>])
<span class="linenr">15: </span>plt.legend()
<span class="linenr">16: </span>plt.plot()
<span class="linenr">17: </span>plt.savefig(<span style="color: #98be65;">"images/reuters-1.png"</span>)
<span class="linenr">18: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">19: </span>
<span class="linenr">20: </span>plt.clf()   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">clear figure</span>
<span class="linenr">21: </span>
<span class="linenr">22: </span><span style="color: #dcaeea;">accuracy</span> = history.history[<span style="color: #98be65;">'accuracy'</span>]
<span class="linenr">23: </span><span style="color: #dcaeea;">val_accuracy</span> = history.history[<span style="color: #98be65;">'val_accuracy'</span>]
<span class="linenr">24: </span>
<span class="linenr">25: </span>plt.plot(epochs, accuracy, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training accuracy'</span>)
<span class="linenr">26: </span>plt.plot(epochs, val_accuracy, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation accuracy'</span>)
<span class="linenr">27: </span>plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr">28: </span>plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">29: </span>plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr">30: </span>plt.axis([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">10</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">31: </span>plt.legend()
<span class="linenr">32: </span>plt.plot()
<span class="linenr">33: </span>plt.savefig(<span style="color: #98be65;">"images/reuters-2.png"</span>)
<span class="linenr">34: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">plt.show()</span>
</pre>
</div>


<div id="orgb95c0cc" class="figure">
<p><img src="images/reuters-1.png" alt="reuters-1.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 19: </span>Reuters-1</p>
</div>


<div id="org8bbc76e" class="figure">
<p><img src="images/reuters-2.png" alt="reuters-2.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 20: </span>Reuters-2</p>
</div>
</div>
</div>
<div id="outline-container-org3646c0f" class="outline-4">
<h4 id="org3646c0f"><span class="section-number-4">3.2.7.</span> 優化 model</h4>
<div class="outline-text-4" id="text-3-2-7">
<p>
上例中的中間層若將神經元數(維度)降到 4，則其驗證準確率會降至 71%，主要原因是因為這樣會壓縮大量資訊到一個低維度的中間層表示空間，雖然神經網路能將大部份必要的資訊塞進這 4 維表示法中，但仍顯不足。若再提升維度、增加層數、加入 Dropout，結果似乎沒有顯著改善，為什麼？<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr"> 2: </span>model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr"> 3: </span>model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">128</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 4: </span>model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.25</span>))
<span class="linenr"> 5: </span>model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">256</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 6: </span>model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.3</span>))
<span class="linenr"> 7: </span>model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">512</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 8: </span>model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))
<span class="linenr"> 9: </span>model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">46</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr">10: </span>
<span class="linenr">11: </span>model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr">12: </span>              loss=<span style="color: #98be65;">'categorical_crossentropy'</span>,
<span id="coderef-metricsName" class="coderef-off"><span class="linenr">13: </span>              metrics=[<span style="color: #98be65;">'accuracy'</span>])</span>
<span class="linenr">14: </span>
<span class="linenr">15: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;</span>
<span class="linenr">16: </span><span style="color: #dcaeea;">history</span> = model.fit(partial_x_train,
<span class="linenr">17: </span>                    partial_y_train,
<span class="linenr">18: </span>                    epochs=<span style="color: #da8548; font-weight: bold;">9</span>,
<span class="linenr">19: </span>                    batch_size=<span style="color: #da8548; font-weight: bold;">512</span>,
<span class="linenr">20: </span>                    validation_data=(x_val, y_val),
<span class="linenr">21: </span>                    verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">22: </span>
<span class="linenr">23: </span><span style="color: #dcaeea;">history_dict</span> = history.history
<span class="linenr">24: </span><span style="color: #c678dd;">print</span>(history_dict.keys())
<span class="linenr">25: </span>
<span class="linenr">26: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35413;&#20272;</span>
<span class="linenr">27: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Returns the loss value &amp; metrics values for the model in test mode.</span>
<span id="coderef-modelEvaluate" class="coderef-off"><span class="linenr">28: </span><span style="color: #dcaeea;">results</span> = model.evaluate(x_test, one_hot_test_labels)</span>
<span class="linenr">29: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#35413;&#20272;&#36039;&#26009;&#20839;&#23481;&#65306;"</span>,results)
<span class="linenr">30: </span>
<span class="linenr">31: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38928;&#28204;</span>
<span class="linenr">32: </span><span style="color: #dcaeea;">predictions</span> = model.predict(x_test)
<span class="linenr">33: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#38928;&#28204;&#36039;&#26009;&#26550;&#27083;&#65306;"</span>,predictions[<span style="color: #da8548; font-weight: bold;">0</span>].shape)
<span class="linenr">34: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#38928;&#28204;&#36039;&#26009;&#20839;&#23481;&#65306;"</span>,predictions[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">35: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#38928;&#28204;&#32080;&#26524;:"</span>,np.argmax(predictions[<span style="color: #da8548; font-weight: bold;">0</span>]))
<span class="linenr">36: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#31572;&#26696;:"</span>,one_hot_test_labels[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">37: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#22294;</span>
<span class="linenr">38: </span>
<span class="linenr">39: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">40: </span>
<span class="linenr">41: </span><span style="color: #dcaeea;">loss</span> = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr">42: </span><span style="color: #dcaeea;">val_loss</span> = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr">43: </span>
<span class="linenr">44: </span><span style="color: #dcaeea;">epochs</span> = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(loss) + <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">45: </span>
<span class="linenr">46: </span>plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr">47: </span>plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr">48: </span>plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr">49: </span>plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">50: </span>plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr">51: </span>plt.axis([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">10</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">3</span>])
<span class="linenr">52: </span>plt.legend()
<span class="linenr">53: </span>plt.plot()
<span class="linenr">54: </span>plt.savefig(<span style="color: #98be65;">"images/reuters-3.png"</span>)
<span class="linenr">55: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">56: </span>
<span class="linenr">57: </span>plt.clf()   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">clear figure</span>
<span class="linenr">58: </span>
<span class="linenr">59: </span><span style="color: #dcaeea;">accuracy</span> = history.history[<span style="color: #98be65;">'accuracy'</span>]
<span class="linenr">60: </span><span style="color: #dcaeea;">val_accuracy</span> = history.history[<span style="color: #98be65;">'val_accuracy'</span>]
<span class="linenr">61: </span>
<span class="linenr">62: </span>plt.plot(epochs, accuracy, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training accuracy'</span>)
<span class="linenr">63: </span>plt.plot(epochs, val_accuracy, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation accuracy'</span>)
<span class="linenr">64: </span>plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr">65: </span>plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">66: </span>plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr">67: </span>plt.axis([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">10</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">68: </span>plt.legend()
<span class="linenr">69: </span>plt.plot()
<span class="linenr">70: </span>plt.savefig(<span style="color: #98be65;">"images/reuters-4.png"</span>)
<span class="linenr">71: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">plt.show()</span>
</pre>
</div>

<pre class="example" id="orgb83e074">
dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])
71/71 [==============================] - 0s 1ms/step - loss: 1.4258 - accuracy: 0.7458
評估資料內容： [1.4258323907852173, 0.745770275592804]
71/71 [==============================] - 0s 1ms/step
預測資料架構： (46,)
預測資料內容： [3.70623695e-07 3.30399143e-06 7.28266836e-10 9.81337667e-01
 1.33997165e-02 2.48406611e-08 5.62083142e-06 1.04829792e-06
 3.73544684e-03 1.55409850e-07 3.65617052e-06 4.48301253e-05
 2.77795170e-05 3.60696413e-06 1.45354832e-07 5.10505549e-09
 2.27808632e-04 4.35824631e-06 2.53213034e-06 2.67748692e-04
 8.94757453e-04 7.13192685e-06 1.96591565e-09 6.39728796e-06
 5.17120569e-09 4.76714968e-06 9.76192371e-09 9.12776486e-08
 2.97794315e-07 5.29521628e-07 1.64385278e-06 1.99777264e-06
 4.78679567e-07 8.98837271e-09 2.37750965e-06 3.36369732e-09
 2.53439634e-06 3.19595586e-08 8.00217848e-08 5.90340323e-06
 4.10031021e-07 4.74897615e-06 7.05325132e-09 5.10206952e-08
 1.30217703e-09 1.63158032e-09]
預測結果: 3
答案: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
</pre>


<div id="orgb61f25f" class="figure">
<p><img src="images/reuters-1.png" alt="reuters-1.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 21: </span>Rueter-1</p>
</div>


<div id="orga377534" class="figure">
<p><img src="images/reuters-2.png" alt="reuters-2.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 22: </span>Rueter-2</p>
</div>


<div id="orgc3bc815" class="figure">
<p><img src="images/reuters-3.png" alt="reuters-3.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 23: </span>Rueter-3</p>
</div>


<div id="org074e130" class="figure">
<p><img src="images/reuters-4.png" alt="reuters-4.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 24: </span>Rueter-4</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgbaacb9a" class="outline-3">
<h3 id="orgbaacb9a"><span class="section-number-3">3.3.</span> 以 Keras 解決迴歸問題：預測房價</h3>
<div class="outline-text-3" id="text-3-3">
</div>
<div id="outline-container-orgb4d265b" class="outline-4">
<h4 id="orgb4d265b"><span class="section-number-4">3.3.1.</span> 準備資料</h4>
<div class="outline-text-4" id="text-3-3-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> boston_housing
<span class="linenr">2: </span>
<span class="linenr">3: </span>(train_data, train_targets), (<span style="color: #dcaeea;">test_data</span>, <span style="color: #dcaeea;">test_targets</span>) =  boston_housing.load_data()
<span class="linenr">4: </span>
<span class="linenr">5: </span><span style="color: #c678dd;">print</span>(train_data.shape)
<span class="linenr">6: </span><span style="color: #c678dd;">print</span>(test_data.shape)
</pre>
</div>

<pre class="example">
(404, 13)
(102, 13)
</pre>
</div>
<div id="outline-container-org766f254" class="outline-5">
<h5 id="org766f254">資料集標準化</h5>
<div class="outline-text-5" id="text-org766f254">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">mean</span> = train_data.mean(axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">2: </span><span style="color: #dcaeea;">train_data</span> -= mean
<span class="linenr">3: </span><span style="color: #dcaeea;">std</span> = train_data.std(axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">4: </span><span style="color: #dcaeea;">train_data</span> /= std
<span class="linenr">5: </span>
<span class="linenr">6: </span><span style="color: #dcaeea;">test_data</span> -= mean
<span class="linenr">7: </span><span style="color: #dcaeea;">test_data</span> /= std
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orgec57982" class="outline-4">
<h4 id="orgec57982"><span class="section-number-4">3.3.2.</span> 建立神經網路</h4>
<div class="outline-text-4" id="text-3-3-2">
<p>
由於可用的樣本很少，所以使用一個較小的神經網路，一般來說，訓練資料集越少，過度配適的情況會越嚴重。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 2: </span><span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">build_model</span>():
<span class="linenr"> 5: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Because we will need to instantiate</span>
<span class="linenr"> 6: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">the same model multiple times,</span>
<span class="linenr"> 7: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">we use a function to construct it.</span>
<span class="linenr"> 8: </span>    <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr"> 9: </span>    model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>,
<span class="linenr">10: </span>                           input_shape=(train_data.shape[<span style="color: #da8548; font-weight: bold;">1</span>],)))
<span class="linenr">11: </span>    model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span id="coderef-OneUnitLayer" class="coderef-off"><span class="linenr">12: </span>    model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>))</span>
<span class="linenr">13: </span>    model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>, loss=<span style="color: #98be65;">'mse'</span>, metrics=[<span style="color: #98be65;">'mae'</span>])
<span class="linenr">14: </span>    <span style="color: #51afef;">return</span> model
</pre>
</div>

<p>
這裡以 1 unit 的神經網路結束而且沒有啟動函數(第<a href="#coderef-OneUnitLayer" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-OneUnitLayer');" onmouseout="CodeHighlightOff(this, 'coderef-OneUnitLayer');">12</a>行)，代表為線性轉換，這是純量迴歸的基本設定，會輸出一個浮點數型別的數值(即迴歸值)，如果使用啟動函數，則只會輸出 0~1 間的值。另，mse 也是迴歸常用的損失函數，在評量指標的選擇方面，則採用 mae(mean absolute error，即預測值與目標值間差異的絕對值)。<br />
</p>
</div>
</div>
<div id="outline-container-orgf98d9d0" class="outline-4">
<h4 id="orgf98d9d0"><span class="section-number-4">3.3.3.</span> 驗證</h4>
<div class="outline-text-4" id="text-3-3-3">
<p>
本例中由於資料點少，驗證集也只有 100 筆資料，故驗證分數可能會因驗證資料點或訓練資料點的選用而有很大的變化，因而阻礙評估 model 優劣的可靠性。在這種情況下，最好的方式是選用 K-fold corss validation，做法如圖<a href="#org124be19">25</a>，原理是將資料拆分為 K 個區域(通常 K=4 或 5)，每次取一個區域做為驗證資料集，最後求 K 次驗證分數的平均值。<br />
</p>


<div id="org124be19" class="figure">
<p><img src="images/k-fold-validation.png" alt="k-fold-validation.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 25: </span>K-fold 交叉驗證</p>
</div>

<p>
K-fold cross validation 的 python 實作程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span><span style="color: #dcaeea;">k</span> = <span style="color: #da8548; font-weight: bold;">4</span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">num_val_samples</span> = <span style="color: #c678dd;">len</span>(train_data) // k
<span class="linenr"> 5: </span><span style="color: #dcaeea;">num_epochs</span> = <span style="color: #da8548; font-weight: bold;">100</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">all_scores</span> = []
<span class="linenr"> 7: </span><span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(k):
<span class="linenr"> 8: </span>    <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'processing fold #'</span>, i)
<span class="linenr"> 9: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Prepare the validation data: data from partition # k</span>
<span class="linenr">10: </span>    <span style="color: #dcaeea;">val_data</span> = train_data[i * num_val_samples: (i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples]
<span class="linenr">11: </span>    <span style="color: #dcaeea;">val_targets</span> = train_targets[i * num_val_samples: (i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples]
<span class="linenr">12: </span>
<span class="linenr">13: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Prepare the training data: data from all other partitions</span>
<span class="linenr">14: </span>    <span style="color: #dcaeea;">partial_train_data</span> = np.concatenate([train_data[:i * num_val_samples],
<span class="linenr">15: </span>         train_data[(i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples:]], axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">16: </span>    <span style="color: #dcaeea;">partial_train_targets</span> = np.concatenate([train_targets[:i * num_val_samples],
<span class="linenr">17: </span>         train_targets[(i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples:]], axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">18: </span>
<span class="linenr">19: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Build the Keras model (already compiled)</span>
<span class="linenr">20: </span>    <span style="color: #dcaeea;">model</span> = build_model()
<span class="linenr">21: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Train the model (in silent mode, verbose=0)</span>
<span class="linenr">22: </span>    model.fit(partial_train_data, partial_train_targets,
<span class="linenr">23: </span>              epochs=num_epochs, batch_size=<span style="color: #da8548; font-weight: bold;">1</span>, verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">24: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Evaluate the model on the validation data</span>
<span class="linenr">25: </span>    <span style="color: #dcaeea;">val_mse</span>, <span style="color: #dcaeea;">val_mae</span> = model.evaluate(val_data, val_targets, verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">26: </span>    all_scores.append(val_mae)
</pre>
</div>

<pre class="example">
processing fold # 0
processing fold # 1
processing fold # 2
processing fold # 3
</pre>
</div>
</div>
<div id="outline-container-orgd6169aa" class="outline-4">
<h4 id="orgd6169aa"><span class="section-number-4">3.3.4.</span> 查看結果</h4>
<div class="outline-text-4" id="text-3-3-4">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #c678dd;">print</span>(all_scores)
<span class="linenr">2: </span><span style="color: #c678dd;">print</span>(np.mean(all_scores))
</pre>
</div>

<pre class="example">
[2.2767844200134277, 2.619281053543091, 2.72979474067688, 2.562032461166382]
2.546973168849945
</pre>


<p>
由上述結果看來，拆成 4 區的驗證分數自 2.28 到 2.73，總平均為 2.54，這個平均值是較為可靠的指標，因為當目標房價的數值很大時，2.28 到 2.73 會變成很大的誤差。<br />
</p>

<p>
可能是因為 MAC 與 Linux 版本的 Anaconda 相容性問題，或是 Keras 版本差異問題，MAC 版與 Linux 下的 history.history 架構略有差異：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Linux with Keras 2.2.5</span>
<span class="linenr">2: </span>dict_keys([<span style="color: #98be65;">'val_loss'</span>, <span style="color: #98be65;">'val_mean_absolute_error'</span>, <span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'mean_absolute_error'</span>])
<span class="linenr">3: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Mac with Keras 2.3.1</span>
<span class="linenr">4: </span>dict_keys([<span style="color: #98be65;">'val_loss'</span>, <span style="color: #98be65;">'val_mae'</span>, <span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'mae'</span>])
</pre>
</div>
</div>
<div id="outline-container-org75cf4eb" class="outline-5">
<h5 id="org75cf4eb">評估結果視覺化</h5>
<div class="outline-text-5" id="text-org75cf4eb">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Some memory clean-up</span>
<span class="linenr"> 2: </span><span style="color: #dcaeea;">k</span> = <span style="color: #da8548; font-weight: bold;">4</span>
<span class="linenr"> 3: </span><span style="color: #dcaeea;">num_val_samples</span> = <span style="color: #c678dd;">len</span>(train_data) // k
<span class="linenr"> 4: </span><span style="color: #dcaeea;">num_epochs</span> = <span style="color: #da8548; font-weight: bold;">500</span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">all_mae_histories</span> = []
<span class="linenr"> 6: </span><span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(k):
<span class="linenr"> 7: </span>    <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'processing fold #'</span>, i)
<span class="linenr"> 8: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Prepare the validation data: data from partition # k</span>
<span class="linenr"> 9: </span>    <span style="color: #dcaeea;">val_data</span> = train_data[i * num_val_samples: (i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples]
<span class="linenr">10: </span>    <span style="color: #dcaeea;">val_targets</span> = train_targets[i * num_val_samples: (i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples]
<span class="linenr">11: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Prepare the training data: data from all other partitions</span>
<span class="linenr">12: </span>    <span style="color: #dcaeea;">partial_train_data</span> = np.concatenate(
<span class="linenr">13: </span>        [train_data[:i * num_val_samples],
<span class="linenr">14: </span>         train_data[(i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples:]],
<span class="linenr">15: </span>        axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">16: </span>    <span style="color: #dcaeea;">partial_train_targets</span> = np.concatenate(
<span class="linenr">17: </span>        [train_targets[:i * num_val_samples],
<span class="linenr">18: </span>         train_targets[(i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples:]],
<span class="linenr">19: </span>        axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">20: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Build the Keras model (already compiled)</span>
<span class="linenr">21: </span>    <span style="color: #dcaeea;">model</span> = build_model()
<span class="linenr">22: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Train the model (in silent mode, verbose=0)</span>
<span class="linenr">23: </span>    <span style="color: #dcaeea;">history</span> = model.fit(partial_train_data, partial_train_targets,
<span class="linenr">24: </span>                        validation_data=(val_data, val_targets),
<span class="linenr">25: </span>                        epochs=num_epochs, batch_size=<span style="color: #da8548; font-weight: bold;">1</span>, verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">26: </span>    <span style="color: #dcaeea;">mae_history</span> = history.history[<span style="color: #98be65;">'val_mae'</span>]
<span class="linenr">27: </span>    all_mae_histories.append(mae_history)
<span class="linenr">28: </span>
<span class="linenr">29: </span><span style="color: #dcaeea;">average_mae_history</span> = [np.mean([x[i] <span style="color: #51afef;">for</span> x <span style="color: #51afef;">in</span> all_mae_histories]) <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(num_epochs)]
<span class="linenr">30: </span>
<span class="linenr">31: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">32: </span>plt.plot(<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(average_mae_history) + <span style="color: #da8548; font-weight: bold;">1</span>), average_mae_history)
<span class="linenr">33: </span>plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">34: </span>plt.ylabel(<span style="color: #98be65;">'Validation MAE'</span>)
<span class="linenr">35: </span>plt.plot()
<span class="linenr">36: </span>plt.savefig(<span style="color: #98be65;">"images/Boston-House-Price.png"</span>)
<span class="linenr">37: </span>
<span class="linenr">38: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25490;&#38500;&#27599;&#36913;&#26399;&#30340;&#21069;10&#20491;&#36039;&#26009;&#40670;</span>
<span class="linenr">39: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">smooth_curve</span>(points, factor=<span style="color: #da8548; font-weight: bold;">0.9</span>):
<span class="linenr">40: </span>  <span style="color: #dcaeea;">smoothed_points</span> = []
<span class="linenr">41: </span>  <span style="color: #51afef;">for</span> point <span style="color: #51afef;">in</span> points:
<span class="linenr">42: </span>    <span style="color: #51afef;">if</span> smoothed_points:
<span class="linenr">43: </span>      <span style="color: #dcaeea;">previous</span> = smoothed_points[-<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr">44: </span>      smoothed_points.append(previous * factor + point * (<span style="color: #da8548; font-weight: bold;">1</span> - factor))
<span class="linenr">45: </span>    <span style="color: #51afef;">else</span>:
<span class="linenr">46: </span>      smoothed_points.append(point)
<span class="linenr">47: </span>  <span style="color: #51afef;">return</span> smoothed_points
<span class="linenr">48: </span>
<span class="linenr">49: </span><span style="color: #dcaeea;">smooth_mae_history</span> = smooth_curve(average_mae_history[<span style="color: #da8548; font-weight: bold;">10</span>:])
<span class="linenr">50: </span>plt.clf()
<span class="linenr">51: </span>plt.plot(<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(smooth_mae_history) + <span style="color: #da8548; font-weight: bold;">1</span>), smooth_mae_history)
<span class="linenr">52: </span>plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">53: </span>plt.ylabel(<span style="color: #98be65;">'Validation MAE'</span>)
<span class="linenr">54: </span>plt.plot()
<span class="linenr">55: </span>plt.savefig(<span style="color: #98be65;">"images/Boston-House-Price-ex10.png"</span>)
</pre>
</div>

<pre class="example">
processing fold # 0
processing fold # 1
processing fold # 2
processing fold # 3
</pre>



<div id="orgb7ec17f" class="figure">
<p><img src="images/Boston-House-Price.png" alt="Boston-House-Price.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 26: </span>Boston House Price Training MAE</p>
</div>

<p>
圖<a href="#orgb7ec17f">26</a>是由每一訓練週期的平均 MAE 分數所繪出的折線圖，由於單位刻度與 y 軸刻度問題，此圖失去了部份重要細節，經由下列方式進行修正：<br />
</p>
<ul class="org-ul">
<li>省略前 10 個資料點，<br /></li>
<li>把每個資料點替換成前一點的指數移動平均值(exponential moving average, EMA)，讓誤差變平滑。<br /></li>
</ul>

<p>
EMA 常應用於各領域的資料分析中，其核心概念為：現在的資料會被過去的資料所影響，而時間點越近的資料影響越大，反之越小，如股票的漲幅，前 10 年的漲跌與前 10 日的漲跌，自然是後者對未來的影響更大。<br />
</p>

<p>
EMA 的數學函式如下：<br />
\( E_t = a \times V_t + (1-a) \times E_{t-1} \)，其中<br />
</p>
<ul class="org-ul">
<li>\(E_t\)為時間點\(t\)的指數移動平均值<br /></li>
<li>\(a\)為平滑係數，通常介於 0 到 1 之間<br /></li>
<li>\(V_t\)為時間點\(t\)的原始數值<br /></li>
<li>\(E_{t-1}\)為時間點\(t-1\)的指數移動平均值<br /></li>
</ul>

<p>
為什麼前例中前 10 筆數據的與其他數據差異如此巨大？我們以前 10 天的資料(一天一筆)來看，第 10 天的 EMA 為：<br />
\( E_{10} = aV_{10} + (1-a)E_9 \)<br />
展開第 9 天的\(E_9\)後<br />
\( E_{10} = aV_{10} + (1-a)[aV_9 + (1-a)E_8] \)<br />
整理後變成<br />
\( E_{10} = a(V_{10} + (1-a)V_9) + (1-a)^{2}E_8 \)<br />
若繼續展開所有天數，將得到<br />
\( E_{10} = a(V_{10} + (1-a)V_9) + (1-a)^{2}E_8+ \dots + (1-a)^{9}V_{1}) + (1-a)^{9}E_1 \)<br />
通常上式的最後一項會因為時間很長而變太小，故可忽略不計，而由此也可看出，\(E_{10}\)的值會被每天的原始資料\((V_{10} \dots V_{1}\))影響，每多一天，原始數值就會多乘(1-a)倍，成指數關係，故時間越久遠的事件，影響越小。<br />
</p>


<div id="org8c91589" class="figure">
<p><img src="images/Boston-House-Price-ex10.png" alt="Boston-House-Price-ex10.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 27: </span>Boston House Price Training MAE (排除前 10 個資料點)</p>
</div>

<p>
由圖<a href="#org8c91589">27</a>是可看出 MAE 在 80 個週期後已停止改善，然後開始往上升，即，過了這點就開始發生過度適配的情況。<br />
</p>
</div>
</div>
</div>
<div id="outline-container-org17df433" class="outline-4">
<h4 id="org17df433"><span class="section-number-4">3.3.5.</span> 小結</h4>
<div class="outline-text-4" id="text-3-3-5">
<p>
由此範例可知：<br />
</p>
<ul class="org-ul">
<li>進行迴歸分析時，常以 MSE 做為損失函數、以 MAE 做為評估指標(而非 accuracy).<br /></li>
<li>當輸入資料的特徵有不同刻度時，應先將每個特徵進行轉換。<br /></li>
<li>當可用資料很少時，使用 K-fold 驗證來評估模式。<br /></li>
<li>當可用資料很少時，最好使用隠藏層較少(較淺)的小型神經網路，如一個或兩個，以免產生過渡配適。<br /></li>
</ul>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">import</span> keras
<span class="linenr">2: </span>  <span style="color: #c678dd;">print</span>(keras.__version__)
</pre>
</div>

<pre class="example">
2.3.1
</pre>
</div>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://kknews.cc/zh-tw/tech/b4zkbom.html">主流的深度學習模型有哪些？</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://kknews.cc/zh-tw/tech/b4zkbom.html">主流的深度學習模型有哪些？</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
台科大計概<br />
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4" role="doc-backlink">4</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://www.baeldung.com/cs/cost-vs-loss-vs-objective-function">Difference Between the Cost, Loss, and the Objective Function</a><br />
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Yung-Chin Yen</p>
<p class="date">Created: 2024-02-25 Sun 15:51</p>
</div>
</body>
</html>
