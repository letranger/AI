:PROPERTIES:
:ID:       82e219c3-6ca0-43b0-bb11-e3a8454f089d
:END:
#+title: 資料預處理

#+INCLUDE: ../pdf.org
#+TAGS: AI, Machine Learning
#+OPTIONS: toc:2 ^:nil num:5
#+OPTIONS: H:4
#+PROPERTY: header-args :eval never-export
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../css/muse.css" />
#+HTML_HEAD_EXTRA: <script src="../css/copy_code.js"></script>
#+begin_export html
<a href="https://hits.sh/letranger.github.io/AI/20221023130936-資料預處理.html"><img alt="Hits" align="right" src="https://hits.sh/letranger.github.io/AI/20221023130936-資料預處理.html.svg"/></a>
#+end_export

#+latex:\newpage

進行數運模式運算之前，需要進行的數據預處理工作大致可分為以下幾點：
1. 數據遺漏值處理
1. 數據分類編碼
1. 數據訓練集與測試集之分割
1. 數據特徵選取

* 資料預處理
在收集到所需的資料後，常會遇到各種資料不全、缺失的情況，因此需要對資料進行整理，以便後續的分析。進行數運模式運算之前，需要進行的[[id:82e219c3-6ca0-43b0-bb11-e3a8454f089d][資料預處理]]工作大致可分為以下幾點：
1. 數據遺漏值處理
2. 數據分類編碼
3. 數據訓練集與測試集之分割
4. 數據特徵選取

以下的程式碼都是簡單的pandas應用，如果你看不懂，表示你自已要惡補一下Pandas的課程，可以參看:
- [[https://letranger.github.io/PythonCourse/Pandas.html][Pandas教學]]
- [[https://leemeng.tw/practical-pandas-tutorial-for-aspiring-data-scientists.html][資料科學家的 pandas實戰手冊:掌握40個實用數據技巧]]
- 其他的自己[[https://www.google.com/][Google]]

** 讀檔
- [[file:images/資料預處理/CatDog.csv][按這裡下載測試檔案]]
*** Colab
**** 檔案在/content裡
***** 先把下載的檔案拉到
#+CAPTION: 將檔案上傳至Colab雲端硬碟
#+LABEL:fig:Labl
#+name: fig:uploadFile
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 300
[[file:images/Colab編寫環境/2024-01-13_13-54-39_2024-01-13_13-54-06.png]]
***** 讀取檔案
#+begin_src python -r -n :results output :exports both
import pandas as pd

origdf = pd.read_csv("/content/CatDog.csv")

#+end_src
**** 檔案在你的Google Drive
先連結Google Drive
#+begin_src python -r -n :results output :exports both
import pandas as pd
# import Google Drive 套件
from google.colab import drive
# 將自己的雲端硬碟掛載上去
drive.mount('/content/gdrive')
# 透過 gdrive/My Drive/... 來存取檔案
test = pd.read_csv('gdrive/My Drive/CatDog.csv')
test
#+end_src
** 刪除遺漏值
現實世界中可能會因各種原因導致數據缺失或遺漏(如問卷被刻意留白)，這些部份通常會以「空白」、「NaN」或「NULL」來取代。
*** Emacs Baabel Virtual Environment for Python :noexport:
虛擬環境名稱設定: venv
#+BEGIN_SRC elisp :session venv
(pyvenv-activate "~/Dropbox/notes/roam/.venv")
(setq org-babel-python-command "~/Dropbox/notes/roam/.venv/bin/python")
(pyvenv-workon "venv")
#+END_SRC
*** 查看資料集內容
#+BEGIN_SRC python -r -n :async :results  output :exports both :session venv
csv_data ="""A,B,C,D,E
 1.0,,  2.0,  3.0, 4.0
 5.0,,  6.0,, 8.0
10.0,, 11.0, 12.0, 5.0
9.9,,8.0, 12.0"""

import sys
import pandas as pd
# python 2.7需進行unicode轉碼
if (sys.version_info < (3, 0)):
    csv_data = unicode(csv_data)
# 讀入程式檔中的csv資料
from io import StringIO
df = pd.read_csv(StringIO(csv_data))
print(df)
#+END_SRC

#+RESULTS:
:       A   B     C     D    E
: 0   1.0 NaN   2.0   3.0  4.0
: 1   5.0 NaN   6.0   NaN  8.0
: 2  10.0 NaN  11.0  12.0  5.0
: 3   9.9 NaN   8.0  12.0  NaN

雖然pd.read_csv是用來讀取網路上或本機端的csv檔，此處為了省去大家讀取d檔案的工作，我們以直接以字串模擬一個檔案出來，所以在讀取時要以以下的方式來讀：
#+begin_src python -r -n :results output :exports both
pd.read_csv)StringIO(字串變數名稱)
#+end_src
*** 遺漏值的識別
現在可以大概統計一下遺漏值
#+BEGIN_SRC python -r -n :async :results output :exports both :session venv
# 列出每行有的null個數
print(df.isnull().sum())
print(df.isnull().sum(axis=1))
#+END_SRC

#+RESULTS:
#+begin_example
A    0
B    4
C    0
D    1
E    1
dtype: int64
0    1
1    2
2    1
3    2
dtype: int64
#+end_example

*** 刪除有遺漏值的記錄
#+BEGIN_SRC python -r -n :async :results output :exports both :session venv
# 剛除有遺失值的資料列
print('====刪掉有遺失值的列:df.dropna(axis=1)====')
tmpDf = df.dropna(axis=0)
print(tmpDf)
# 剛除有遺失值的資料行
print('====刪掉有遺失值的欄:df.dropna(axis=1)====')
tmpDf = df.dropna(axis=1)
print(tmpDf)
# 剛除整列為NaN者
print('====剛除整欄為NaN者:df.dropna(how=\'all\')====')
tmpDf = df.dropna(axis=1, how='all')
print(tmpDf)
# 刪除有值個數低於thresh的列
print('====刪除有值個數低於thresh的列:df.dropna(thresh=4)====')
tmpDf = df.dropna(thresh=4)
print(tmpDf)
# 刪除特定行(如第C行)中有NaN之列
print('====刪除特定行(如第C行)中有NaN之列:df.dropna(columns=[\'C\'])====')
tmpDf=df.drop(columns=['C'])
print(tmpDf)
#+END_SRC

#+RESULTS:
#+begin_example
====刪掉有遺失值的列:df.dropna(axis=1)====
Empty DataFrame
Columns: [A, B, C, D, E]
Index: []
====刪掉有遺失值的欄:df.dropna(axis=1)====
      A     C
0   1.0   2.0
1   5.0   6.0
2  10.0  11.0
3   9.9   8.0
====剛除整欄為NaN者:df.dropna(how='all')====
      A     C     D    E
0   1.0   2.0   3.0  4.0
1   5.0   6.0   NaN  8.0
2  10.0  11.0  12.0  5.0
3   9.9   8.0  12.0  NaN
====刪除有值個數低於thresh的列:df.dropna(thresh=4)====
      A   B     C     D    E
0   1.0 NaN   2.0   3.0  4.0
2  10.0 NaN  11.0  12.0  5.0
====刪除特定行(如第C行)中有NaN之列:df.dropna(columns=['C'])====
      A   B     D    E
0   1.0 NaN   3.0  4.0
1   5.0 NaN   NaN  8.0
2  10.0 NaN  12.0  5.0
3   9.9 NaN  12.0  NaN
#+end_example
雖然刪除包含遺漏值的數據似乎是個方便的方法，但終究可能會刪除過多的樣本，導致分析的結果並不可靠；或是因為刪除了特徵的時候，卻失去了重要的資訊。
** 填補遺漏值
*** 直接填零
#+BEGIN_SRC python -r -n :async :results  output :exports both :session venv
csv_data ="""A,B,C,D,E
 1.0,7.7, 2.0,  3.0, 4.0
 5.0,,  6.0,, 8.0
10.0,, 11.0, 12.0, 5.0
9.9,8.8,8.0, 12.0"""

import sys
import pandas as pd
# python 2.7需進行unicode轉碼
if (sys.version_info < (3, 0)):
    csv_data = unicode(csv_data)
# 讀入程式檔中的csv資料
from io import StringIO
df = pd.read_csv(StringIO(csv_data))

tmpDf = df.fillna(0)
print(tmpDf)
#+end_src

#+RESULTS:
:       A    B     C     D    E
: 0   1.0  7.7   2.0   3.0  4.0
: 1   5.0  0.0   6.0   0.0  8.0
: 2  10.0  0.0  11.0  12.0  5.0
: 3   9.9  8.8   8.0  12.0  0.0
*** 以平均值填補
**** Python手動填補
#+BEGIN_SRC python -r -n :async :results  output :exports both :session venv
print(df)
df['D'] = df['D'].fillna(df['D'].mean())
print(df)
#+end_src

#+RESULTS:
#+begin_example
      A    B     C     D    E
0   1.0  7.7   2.0   3.0  4.0
1   5.0  NaN   6.0   NaN  8.0
2  10.0  NaN  11.0  12.0  5.0
3   9.9  8.8   8.0  12.0  NaN
      A    B     C     D    E
0   1.0  7.7   2.0   3.0  4.0
1   5.0  NaN   6.0   9.0  8.0
2  10.0  NaN  11.0  12.0  5.0
3   9.9  8.8   8.0  12.0  NaN
#+end_example
**** 以scikit-learn的impute填補
最常見的「插補技術」之一為「平均插補」(mean imputation)，即，以整個特徵行的平均值來代替遺漏值。
#+BEGIN_SRC python -r -n :async :results output :exports both :session venv
# impute missing values via the column mean
from sklearn.impute import SimpleImputer
import numpy as np

imr = SimpleImputer(missing_values=np.nan, strategy='mean')
imr = imr.fit(df.values)
imputed_data = imr.transform(df.values)
print(df)
print(imputed_data)
#+END_SRC

#+RESULTS:
:       A    B     C     D    E
: 0   1.0  7.7   2.0   3.0  4.0
: 1   5.0  NaN   6.0   9.0  8.0
: 2  10.0  NaN  11.0  12.0  5.0
: 3   9.9  8.8   8.0  12.0  NaN
: [[ 1.          7.7         2.          3.          4.        ]
:  [ 5.          8.25        6.          9.          8.        ]
:  [10.          8.25       11.         12.          5.        ]
:  [ 9.9         8.8         8.         12.          5.66666667]]

scikit-learn早期版本的填補類別為Imputer，屬於 transformer 類別，主要的工作是做「數據轉換」，這些 estimator 有兩種基本方法：fit 與 transform，fit 方法是用來進行參數學習。在目前的版本中，SimpleImputer 已經被拿來取代以前的 sklearn.preprocessing.Imputer，預設的strategy為mean，其他strategy請參關[[https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html][scikit-learn官網]]。

更詳細的使用教學請閱讀[[https://ithelp.ithome.com.tw/articles/10293386?sc=pt][[Day03] 拾起武器- Data Preprocessing(02)]]
** [作業]資料預處理 :TNFSH:
*** 題目
南一中網路書店即將開張，為了處理龐大的書單資料，資訊科教師們很無恥的把書籍資料登錄工作當成作業分派給一年級的修課學生，所謂團結力量大，一份不太可靠的[[file:Downloads/403books.csv][書目資料]]就這麼完成了。

這份[[file:Downloads/403books.csv][書目資料]]共計271,350筆，每筆資料有以下9個欄位
- 'ISBN'
- 'Book-Title'
- 'Book-Author'
- 'Year-Of-Publication'
- 'Publisher'
- 'Image-URL-S'
- 'Image-URL-M'
- 'Image-URL-L'
- 'Book-Price'

然而，大概是因為作者群都是被迫做白工的關係，這份資料有不少缺失值與錯誤資料，錯誤的類型大概有以下幾類：
1. 缺失: 就是該欄位完全沒有值
2. 價格錯誤: 書價為0，或是書價超過20000元
3. 出版年代錯誤: 年代為0或是超過2024年
*** 要求
請你透過colab來完成以下的任務：
**** 讀檔
你可以選擇用Pandas直接讀線上的檔案，也可以選擇將檔案上傳到Google的雲端硬碟後再利用Colab來讀取。
**** 預處理
要請你進行以下的資料預處理
1. 除所有有缺失值的記錄(只要有一欄有缺失值、該筆資料就整筆刪去)
1. 改變錯誤日期，超過2024的都改為2024
1. 改變錯誤日期，日期為0的都改為1900
1. 改變錯誤書價，超過2000的都改為1000
1. 改變錯誤書價，書價為0者改為100
**** 輸出
最後輸出以下內容
1. 列出原始資料筆數
1. 列出修正(刪除缺失值)後的資料筆數
1. 列出2000年出版的書籍數量
1. 列出作者中有Bruce的書籍數量
1. 列出 500<=書價<=800 的書籍數量
1. 列出平均書價
*** 參考答案
整份colab的程式碼要能一次執行並輸出以下結果(不能直接print我給的答案...)
#+begin_example
原始資料筆數 271350
可用資料數: 259397
2000年出版: 16438
作者群中有Bruce: 667
800<=書價<=1000: 58776
平均書價: 559.23
#+end_example
*** 友情提醒
- 資料量很大，相信我，你不會想用Excel或Numbers或Google試算表來打開它然後逐一處理...，我試過在一台8G的Macbook Air上用Numbers打開這個csv檔，大概花了 *八分鐘* 就開起來了...
- 你可以參考[[https://letranger.github.io/PythonCourse/Pandas.html][Python選修Pandas教材]]，不過這份教材只是概略描述基本功能，你可能還需要再自行Google相關的功能
*** 題目生成 :noexport:
#+begin_src python -r -n :results output :exports both
# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
from sklearn.utils import shuffle

df = pd.read_csv('/Users/letranger/Downloads/books_data/books.csv', delimiter=';', encoding='utf-8', on_bad_lines = 'skip', low_memory=False)

# 有錯誤的出版年份，這個要先剛掉才能做後面的工作
df = df.drop([df.index[221671], df.index[209531], df.index[220724]])
# 先把出版年份改成整數
df["Year-Of-Publication"] = df["Year-Of-Publication"].apply(pd.to_numeric)
# 加入錯誤年份 1/0000
df.loc[df.sample(frac=0.0001).index, 'Year-Of-Publication'] = np.random.randint(2030, 3599)
# 改回字串
df['Year-Of-Publication'] = df['Year-Of-Publication'].astype(str)

# 新增書價欄資訊
df['Book-Price'] = np.random.randint(120, 1000, df.shape[0])
# 加入錯誤的書價，超過30000的是錯的
df.loc[df.sample(frac=0.0001).index, 'Book-Price'] = np.random.randint(30000, 90000)
df.loc[df.sample(frac=0.0001).index, 'Book-Price'] = 0

# 隨機插入nan至各cell 1/10000
for col in df.columns:
    df.loc[df.sample(frac=0.005).index, col] = np.nan

print(df.columns)
#df = df.dropna()

# 寫入403books.csv
df.to_csv('/Users/letranger/Dropbox/notes/roam/Downloads/403books.csv', index=False)
#+end_src

#+RESULTS:
: Index(['ISBN', 'Book-Title', 'Book-Author', 'Year-Of-Publication', 'Publisher',
:        'Image-URL-S', 'Image-URL-M', 'Image-URL-L', 'Book-Price'],
:       dtype='object')
*** Solution :noexport:
#+begin_src python -r -n :results output :exports both
import pandas as pd
import numpy as np
from sklearn.utils import shuffle

df = pd.read_csv('/Users/letranger/Dropbox/notes/roam/Downloads/403books.csv')
#print(df.isnull().sum())
#print(df.shape)

# 原始資料
print('原始資料筆數', len(df))

# 刪除所有有缺失值的記錄
#print(df.dropna(how='any').shape[0])
df = df.dropna(how='any', axis=0)

# 先把出版年份改成整數
df['Year-Of-Publication'] = df['Year-Of-Publication'].astype(int)
# 改變錯誤日期，超過2024的都改為2024
m = (df['Year-Of-Publication'] > 2024)
df.loc[m, ['Year-Of-Publication']] = [2024]
# 改變錯誤日期，日期為0的都改為1900
m = (df['Year-Of-Publication'] == 0)
df.loc[m, ['Year-Of-Publication']] = [1900]

# 改變錯誤書價，超過2000的都改為1000
m = (df['Book-Price'] > 20000)
df.loc[m, ['Book-Price']] = [1000]
# 改變錯誤書價，書價為0者改為100
m = (df['Book-Price'] == 0)
df.loc[m, ['Book-Price']] = [100]

#print(df.describe())

# ==========作業輪出結果
# 1. 列出條正後的資料筆數
print('可用資料數:',len(df))
# 2. 查詢2000年出版了幾本書
m = (df['Year-Of-Publication'] == 2000)
print('2000年出版:',len(df.loc[m, ['ISBN', 'Year-Of-Publication']]))
# 3. 作者中有Bruce的資料數量
print('作者群中有Bruce:',len(df[df['Book-Author'].str.contains("Bruce")]))
# 4. 500<=書價<=800
m = (df['Book-Price'].between(800, 1000))
print('800<=書價<=1000):',len(df.loc[m, ['ISBN', 'Book-Price']]))
# 5. 平均書價
avg = df['Book-Price'].mean()
print(f'平均書價: {avg:.2f}')
print(df.describe())
#+end_src

#+RESULTS:
#+begin_example
原始資料筆數 271350
可用資料數: 259397
2000年出版: 16438
作者群中有Bruce: 667
800<=書價<=1000): 58776
平均書價: 559.23
       Year-Of-Publication     Book-Price
count        259397.000000  259397.000000
mean           1992.111108     559.226506
std              14.650505     254.106772
min            1376.000000     100.000000
25%            1989.000000     339.000000
50%            1995.000000     559.000000
75%            2000.000000     779.000000
max            2024.000000    1000.000000
#+end_example

* 資料間的關係
找出特徵值間的差異與相似性是機器學習中非常重要的工作。
** 兩點間的距離
兩種常見的特徵距離計算方式:曼哈頓距離及歐幾里得距離，以一個城市中的兩個地標為例(如圖[[fig:feature-distance]]中的兩個黑點)。

#+CAPTION: 城市中的兩個地標
#+LABEL:fig:Labl
#+name: fig:feature-distance
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/資料間的關係/2024-02-02_13-46-36_2024-02-02_13-20-54.png]]

*** 曼哈頓距離（Manhattan distance）
在如圖[[fig:feature-distance]]這樣的棋盤式街道的城式中，要由點(1,1)走到(7,7)的最簡單方式是先往上直走到(1,7)，再右轉直走到(7,7)，這便是曼哈頓距離，這段距離為 \(|1-7|+|1-7| \)，公式為
$$ \sum_{i=1}^n|x_i-y_i| $$

*** 歐幾里得距離（Euclidian distance）
#+CAPTION: Numbus 2000
#+LABEL:fig:Labl
#+name: fig:numbus2000
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 400
[[file:images/資料間的關係/2024-02-02_14-05-00_2024-02-02_14-03-31.png]]

如果我們肯花美金499[fn:4]買到[[https://cinereplicas.com/products/harry-potter-nimbus-2000-broom-2019-edition][Nimbus 2000]]，那我們就能實踐「直線是兩點間最短距離」這個法則，此時圖[[fig:feature-distance]]中由點(1,1)到點(7,7)的矩離就變成\( \sqrt{(1-7)^2+(1-7)^2} \)，公式為
$$ \sqrt{\sum_{i=1}^n(x_i-y_i)^2 } $$
*** [課堂練習]能力相似度 :TNFSH:
上述計算方式貌似沒什麼問題，但如果今天的問題不是二維地圖中的兩個點，而是：
- 計算兩個人身材的相似度:特徵值為身高、體重、體脂率[fn:5]
- 計算兩個學生程式設計能力的差異: 特徵值為APCS觀念題分數、APCS實作題分數[fn:6]、一年級程式設計學期分數

我們還能用同樣的方式來計算嗎？

以下是5位學生的程式設計能力資料(特徵值為APCS觀念題分數、APCS實作題分數、一年級程式設計學期分數)，請問那一位學生與你的能力(50, 300, 86)最為接近?
- A: 70, 340, 84
- B: 60, 310, 89
- C: 50, 280, 90
- D: 40, 320, 78
- E: 91, 310, 99
**** solution :noexport:
***** 原始資料
#+begin_src python -r -n :results output :exports both
import numpy as np
orig = [[70, 340, 84], [58, 318, 76], [54, 279, 85], [40, 320, 78], [91, 310, 99]]
sc = np.array(orig)
me = np.array([50, 300, 86])
for s in sc:
    dist = np.sqrt(np.sum(np.square(s - me)))
    print('ED:',dist)
for s in sc:
    dist = np.sqrt(np.sum(abs(s - me)))
    print('MD:',dist)
#+end_src

#+RESULTS:
#+begin_example
ED: 44.76605857119878
ED: 22.090722034374522
ED: 21.400934559032695
ED: 23.748684174075834
ED: 44.15880433163923
MD: 7.874007874011811
MD: 6.0
MD: 5.0990195135927845
MD: 6.164414002968976
MD: 8.0
#+end_example
***** 標準化
#+begin_src python -r -n :results output :exports both
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler

orig = [[70, 340, 84], [58, 318, 76], [54, 279, 85], [40, 320, 78], [91, 310, 99]]
me = [50, 300, 86]
orig.append(me)
scalsc = []
sc = np.array(orig)
stdsc = StandardScaler()
for i in range(len(sc[0])):
    scalsc.append(stdsc.fit_transform(sc)[:,i])
scalsc = np.array(scalsc).transpose()

for s in scalsc:
    dist = np.sqrt(np.sum(np.square(s - scalsc[5, :])))
    print(s, 'ED:',dist)
for s in scalsc:
    dist = np.sqrt(np.sum(abs(s - scalsc[5:,])))
    print('MD:',dist)
#+end_src

#+RESULTS:
#+begin_example
ED: 2.4716117475041277
ED: 1.729572721531174
ED: 1.1525315662800393
ED: 1.6376819924249435
ED: 3.1123821199274815
ED: 0.0
MD: 1.9040179292551567
MD: 1.6739662071123351
MD: 1.224091723377491
MD: 1.6614327764060297
MD: 2.1917020067767887
MD: 0.0
#+end_example

*** 進階閱讀
- [[https://kknews.cc/zh-tw/code/v3laxal.html][python 各類距離公式實現]]
- [[https://www.pinecone.io/learn/vector-similarity/][Vector Similarity Explained]]

** 兩個向量間的距離
以下為三個學生的國文、數學、英文三科成績，哪兩個學生的學業能力較為接近?
- James: 80, 90, 70
- Ruby: 90, 80, 60
- Vanessa: 90, 70, 90

為了回答上述問題，我們可以將學生的各科成績當成vector(如圖[[fig:3scores]])，我們的問題就變成：哪兩個vector更為相似? 三個學生的成績向量分佈如下：
#+begin_src python -r -n :results output :exports none
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS'] # 步驟一（替換系統中的字型，這裡用的是Mac OSX系統）
plt.rcParams['axes.unicode_minus'] = False  # 步驟二（解決座標軸負數的負號顯示問題）

def setup_3d_axes():
    ax = plt.axes(projection='3d')
    ax.view_init(azim=-105, elev=20)
    ax.set_xlabel('國文')
    ax.set_ylabel('英文')
    ax.set_zlabel('數學')
    ax.set_xlim(0, 100)
    ax.set_ylim(0, 100)
    ax.set_zlim(0, 100)
    return ax

ax = setup_3d_axes()

# plot the vector (3, 2, 5)
origin = np.zeros((3, 1))
point = np.array([[80, 90, 70]]).T
vector = np.hstack([origin, point])
ax.plot(*vector, color='r', label='James')
#ax.plot(*point, color='k', marker='o')

# project the vector onto the x,y plane and plot it
xy_projection_matrix = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 0]])
projected_point = xy_projection_matrix @ point
projected_vector = xy_projection_matrix @ vector

point = np.array([[90, 80, 60]]).T
vector = np.hstack([origin, point])
ax.plot(*vector, color='b', label='Ruby')
#ax.plot(*point, color='k', marker='o')

# project the vector onto the x,y plane and plot it
xy_projection_matrix = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 0]])
projected_point = xy_projection_matrix @ point
projected_vector = xy_projection_matrix @ vector

point = np.array([[90, 70, 90]]).T
vector = np.hstack([origin, point])
ax.plot(*vector, color='g', label='Vanessa')
#ax.plot(*point, color='k', marker='o')

# project the vector onto the x,y plane and plot it
xy_projection_matrix = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 0]])
projected_point = xy_projection_matrix @ point
projected_vector = xy_projection_matrix @ vector

ax.legend()
plt.legend()
plt.savefig('images/healthCondition.png', dpi=300)
#+end_src
#+RESULTS:
#+CAPTION: 三個學生的成績向量
#+LABEL:fig:Labl
#+name: fig:3scores
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/healthCondition.png]]

如何計算? 有以下三種方式：歐幾里得距離、餘弦相似度、向量內積(如圖[[fig:vecsim]])[fn:7]。
#+CAPTION: Vector Similarity
#+LABEL:fig:Labl
#+name: fig:vecsim
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/資料間的關係/2024-02-03_17-40-18_2024-02-03_17-39-29.png]]

*** 歐幾里得距離
#+CAPTION: 歐幾里得距離
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/資料間的關係/2024-02-03_17-48-03_2024-02-03_17-47-50.png]]

多維空間中兩個向量之間的直線距離，距離越近越相似。歐幾里得距離演算法的優點是可以反映向量的絕對距離，適用於需要考慮向量長度的相似性計算。例如推薦系統中，需要根據使用者的歷史行為來推薦相似的商品，這時就需要考慮使用者的歷史行為的數量，而不僅僅是使用者的歷史行為的相似度[fn:8]。

- \( a: (a_1, a_2, \dots , a_n)\)
- \( b: (b_1, b_2, \dots , b_n)\)
計算兩向量a, b歐幾里得距離的方式為: \( d(a,b) = \sqrt{(a_1-b_1)^2 + (a_2-b_2)^2 + \dots + (a_n-b_n)^2}\)
**** Python手刻 :noexport:
#+begin_src python -r -n :results output :exports both
import math

# 定義三個人的分數
James = [80, 90, 70]
Ruby = [90, 80, 60]
Vanessa = [90, 70, 90]

# 計算歐幾里得距離的函數
def euclidean_distance(a, b):
    #sum_squared_diff = sum((x - y) ** 2 for x, y in zip(a, b))
    sum_squared_diff = 0
    for x, y in zip(a, b):
        sum_squared_diff += (x - y) ** 2
    return math.sqrt(sum_squared_diff)

# 計算每對之間的距離
distance_James_Ruby = euclidean_distance(James, Ruby)
distance_James_Vanessa = euclidean_distance(James, Vanessa)
distance_Ruby_Vanessa = euclidean_distance(Ruby, Vanessa)

# 輸出結果
print(f"James 和 Ruby 之間的歐幾里得距離: {distance_James_Ruby}")
print(f"James 和 Vanessa 之間的歐幾里得距離: {distance_James_Vanessa}")
print(f"Ruby 和 Vanessa 之間的歐幾里得距離: {distance_Ruby_Vanessa}")
#+end_src

**** Numpy :noexport:
#+begin_src python -r -n :results output :exports both
import numpy as np

# 定義三個人的分數
James = np.array([80, 90, 70])
Ruby = np.array([90, 80, 60])
Vanessa = np.array([90, 70, 90])

# 計算歐幾里得距離的函數
def euclidean_distance(a, b):
    return np.sqrt(np.sum((a - b) ** 2))

# 計算每對之間的距離
distance_James_Ruby = euclidean_distance(James, Ruby)
distance_James_Vanessa = euclidean_distance(James, Vanessa)
distance_Ruby_Vanessa = euclidean_distance(Ruby, Vanessa)

# 輸出結果
print(f"James 和 Ruby 之間的歐幾里得距離: {distance_James_Ruby}")
print(f"James 和 Vanessa 之間的歐幾里得距離: {distance_James_Vanessa}")
print(f"Ruby 和 Vanessa 之間的歐幾里得距離: {distance_Ruby_Vanessa}")
#+end_src

*** 餘弦相似度(Cosine Similarity)
#+CAPTION: 餘弦相似度
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/資料間的關係/2024-02-03_17-50-17_2024-02-03_17-50-07.png]]

兩個向量的夾角越小越相似，比較兩個向量的餘弦值進行比較，夾角越小，餘弦值越大。餘弦相似度對向量的長度不敏感，只關注向量的方向，因此適用於高維向量的相似性計算。例如語義搜尋和文件分類[fn:8]。當兩個向量的方向重合時夾角餘弦取最大值1，當兩個向量的方向完全相反夾角餘弦取最小值-1。

- \( a: (a_1, a_2, \dots , a_n)\)
- \( b: (b_1, b_2, \dots , b_n)\)
計算兩向量a, b餘弦相似度的方式為: \( sim(a,b) = \frac{a \cdot b}{||a| \cdot |b||} = \frac{ \sum\limits_{i=1}^n a_i \times b_i}{\sqrt{\sum\limits_{i=1}^na_i^2} \times \sqrt{\sum\limits_{i=1}^nb_i^2}} \)
**** Dot Product
#+CAPTION: 向量內積
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/資料間的關係/2024-02-03_17-49-28_2024-02-03_17-49-09.png]]

一種計算向量之間相似度的度量演算法，它計算兩個向量之間的點積（內積），所得值越大越與搜尋值相似[fn:8]。

- \( a: (a_1, a_2, \dots , a_n)\)
- \( b: (b_1, b_2, \dots , b_n)\)
計算兩向量a, b內積的方式為: \( a \cdot b =  a_1b_1 + a_2b_2 + \dots + a_nb_n \)
或是
$$ a \cdot b = |a||b| \cos \alpha $$

*** 歐幾里得距離實作
有了公式就可以自己用你苦學了半年的Python+高一數學來手刻程式，或是呼叫其他函式庫來計算。
**** Python手刻
:PROPERTIES:
:ID:       24a2b357-9306-451d-a97a-683ed9e1b140
:END:
#+begin_src python -r -n :results output :exports both
import math

def cosine_similarity(v1,v2):
    sumxx, sumxy, sumyy = 0, 0, 0
    for i in range(len(v1)):
        x = v1[i]; y = v2[i]
        sumxx += x*x
        sumyy += y*y
        sumxy += x*y
    return sumxy/math.sqrt(sumxx*sumyy)

James = [80, 90, 70]
Ruby = [90, 80, 60]
Vanessa = [90, 70, 90]
JR_sim = cosine_similarity(James, Ruby)
RV_sim = cosine_similarity(Ruby, Vanessa)
JV_sim = cosine_similarity(James, Vanessa)

print('James v.s. Ruby:', JR_sim)
print('Ruby v.s. Vanessa:', RV_sim)
print('James v.s. Vanessa:', JV_sim)
#+end_src

#+RESULTS:
: James v.s. Ruby: 0.9925966195847843
: Ruby v.s. Vanessa: 0.977356154645257
: James v.s. Vanessa: 0.978640304032486

**** Numpy
#+begin_src python -r -n :results output :exports both
from numpy import dot
from numpy.linalg import norm

James = [80, 90, 70]
Ruby = [90, 80, 60]
Vanessa = [90, 70, 90]
JR_sim = dot(James, Ruby)/(norm(James)*norm(Ruby))
RV_sim = dot(Ruby, Vanessa)/(norm(Ruby)*norm(Vanessa))
JV_sim = dot(James, Vanessa)/(norm(James)*norm(Vanessa))

print('James v.s. Ruby:', JR_sim)
print('Ruby v.s. Vanessa:', RV_sim)
print('James v.s. Vanessa:', JV_sim)
#+end_src

#+RESULTS:
: James v.s. Ruby: 0.9925966195847841
: Ruby v.s. Vanessa: 0.9773561546452569
: James v.s. Vanessa: 0.9786403040324858
*** 餘弦相似度實作
**** SciPy
#+begin_src python -r -n :results output :exports both
from scipy import spatial

James = [80, 90, 70]
Ruby = [90, 80, 60]
Vanessa = [90, 70, 90]
JR_sim = 1 - spatial.distance.cosine(James, Ruby)
RV_sim = 1 - spatial.distance.cosine(Ruby, Vanessa)
JV_sim = 1 - spatial.distance.cosine(James, Vanessa)

print('James v.s. Ruby:', JR_sim)
print('Ruby v.s. Vanessa:', RV_sim)
print('James v.s. Vanessa:', JV_sim)
#+end_src

#+RESULTS:
: James v.s. Ruby: 0.9925966195847843
: Ruby v.s. Vanessa: 0.977356154645257
: James v.s. Vanessa: 0.978640304032486

** [課堂作業]曼哈頓距離
- 參考前節[歐幾里得距離實作]
- 自行Google曼哈頓距離的定義
- 計算以下三人兩兩間的距離，找出能力最接近的2人
#+begin_src python -r -n :results output :exports both
# 定義三個人的分數
James = [80, 90, 70]
Ruby = [90, 80, 60]
Vanessa = [90, 70, 90]
#+end_src
*** solution :noexport:
#+begin_src python -r -n :results output :exports both
import math

def cosine_similarity(v1,v2):
    sumxx, sumxy, sumyy = 0, 0, 0
    for i in range(len(v1)):
        x = v1[i]; y = v2[i]
        sumxx += x*x
        sumyy += y*y
        sumxy += x*y
    return sumxy/math.sqrt(sumxx*sumyy)

James = [80, 90, 70]
Ruby = [90, 80, 60]
Vanessa = [90, 70, 90]
JR_sim = cosine_similarity(James, Ruby)
RV_sim = cosine_similarity(Ruby, Vanessa)
JV_sim = cosine_similarity(James, Vanessa)

print('James v.s. Ruby:', JR_sim)
print('Ruby v.s. Vanessa:', RV_sim)
print('James v.s. Vanessa:', JV_sim)
#+end_src
** 其他應用
計算向量間的距離不僅可以比較上述以數值呈現的特徵，也能比較看起來並不是數值的特徵，例如：文字。例如，如何比較這幾個單字間的相似性：lion, dog, cat, love, apple, NYC[fn:9]。

如圖[[fig:nlpsim]][fn:9]，我們可以由三個角度(animalness, petness, cityness)來賦予每個單字一個數值，那麼就可以利用同樣的方式來比較不同單字間的相似性，這就是自然語言處理的基本原理。

#+CAPTION: 不同單字間的相似性
#+LABEL:fig:Labl
#+name: fig:nlpsim
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 400
[[file:images/資料間的關係/2024-02-03_19-07-07_2024-02-03_19-07-00.png]]


* 資料集分類特徵編碼
** 讀檔
真實世界的數據集往往包含各種「類別特徵」(categorical feature)，類別特徵可再分為
- nominal feature: 名義特徵
- ordinal feature: 次序特徵
#+BEGIN_SRC python -r -n :async :results output :exports both :session venv
import pandas as pd
df = pd.DataFrame([['東區',150,25.3,45.49,'華廈(10層含以下有電梯)',30,'四層/九層','住家用','文教區'],
['北區',321,16.8,78.66,'住宅大樓(11層含以上有電梯)',32,'十三層/十四層','商業用','行政區'],
['東區',900,29.2,30.87,'華廈(10層含以下有電梯)',28,'三層/九層','住商用','倉庫區'],
['新市區',460,13.4,34.35,'華廈(10層含以下有電梯)',43,'五層/五層','住家用','文教區'],
['安南區',350,32.00,11,'透天厝',57,'全/二層','住家用','農業區'],
['歸仁區',950,22.9,41.49,'住宅大樓(11層含以上有電梯)',29,'四層/二十層','住家用','保護區'],
['東區',390,24.7,56.38,'住宅大樓(11層含以上有電梯)',27,'十三層/十四層','住商用','行政區'],
['新化區',482,15.3,31.59,'公寓(5樓含以下無電梯)',42,'三層/五層','住家用','倉庫區']])
df.columns = ['地段', '總價', '單價', '總面積', '型態', '屋齡', '樓別', '用途', '地區別']
print(df)
#+END_SRC

#+RESULTS:
:     地段   總價    單價    總面積               型態  屋齡       樓別   用途  地區別
: 0   東區  150  25.3  45.49    華廈(10層含以下有電梯)  30    四層/九層  住家用  文教區
: 1   北區  321  16.8  78.66  住宅大樓(11層含以上有電梯)  32  十三層/十四層  商業用  行政區
: 2   東區  900  29.2  30.87    華廈(10層含以下有電梯)  28    三層/九層  住商用  倉庫區
: 3  新市區  460  13.4  34.35    華廈(10層含以下有電梯)  43    五層/五層  住家用  文教區
: 4  安南區  350  32.0  11.00              透天厝  57     全/二層  住家用  農業區
: 5  歸仁區  950  22.9  41.49  住宅大樓(11層含以上有電梯)  29   四層/二十層  住家用  保護區
: 6   東區  390  24.7  56.38  住宅大樓(11層含以上有電梯)  27  十三層/十四層  住商用  行政區
: 7  新化區  482  15.3  31.59     公寓(5樓含以下無電梯)  42    三層/五層  住家用  倉庫區
** 次序變項
目前的土地使用區分大致有行政區、文教區、倉庫區、風景區、農業區、河川區...等不同型態，這裡主觀的以上述順序做為土地價值順序，也就是把'土地區分'這個欄位視為ordinal feature。此處自定一個 mapping dictionary，即 land_mapping，然後將 land_mapping 對應到 land_mapping 中的鍵值(程式第[[(sizeMapping)]]行)。
#+BEGIN_SRC python -r -n :async :results output :exports both :session venv
### Mapping ordinal features
land_mapping = {
    '行政區': 7,
    '文教區': 6,
    '倉庫區': 5,
    '風景區': 4,
    '農業區': 3,
    '保護區': 2,
    '河川區': 1}
df['地區別'] = df['地區別'].map(land_mapping)   (ref:sizeMapping)
print(df)
#+END_SRC

#+RESULTS:
:     地段   總價    單價    總面積               型態  屋齡       樓別   用途  地區別
: 0   東區  150  25.3  45.49    華廈(10層含以下有電梯)  30    四層/九層  住家用    6
: 1   北區  321  16.8  78.66  住宅大樓(11層含以上有電梯)  32  十三層/十四層  商業用    7
: 2   東區  900  29.2  30.87    華廈(10層含以下有電梯)  28    三層/九層  住商用    5
: 3  新市區  460  13.4  34.35    華廈(10層含以下有電梯)  43    五層/五層  住家用    6
: 4  安南區  350  32.0  11.00              透天厝  57     全/二層  住家用    3
: 5  歸仁區  950  22.9  41.49  住宅大樓(11層含以上有電梯)  29   四層/二十層  住家用    2
: 6   東區  390  24.7  56.38  住宅大樓(11層含以上有電梯)  27  十三層/十四層  住商用    7
: 7  新化區  482  15.3  31.59     公寓(5樓含以下無電梯)  42    三層/五層  住家用    5
** 名義變項
接下來我們試著處理一個nominal feature: 用途
*** classlabel
許多機器學習的函式庫需要將「類別標籤」編碼為整數值。方法之一是以列舉方式為這些 nominal features 自 0 開始編號，先以 enumerate 方式建立一個 mapping dictionary: class_mapping(程式第[[(classMapping)]]行)，然後利用這個字典將類別特徵轉換為整數值。
#+BEGIN_SRC python -r -n :async :results output :exports both :session venv
class_mapping = {
    label: idx for idx, label in enumerate(np.unique(df['用途'])) (ref:classMapping)
}
print(class_mapping)
# 將類別特徵轉換為整數值
df['用途'] = df['用途'].map(class_mapping)
print(df)
#+end_src

#+RESULTS:
#+begin_example
{'住商用': 0, '住家用': 1, '商業用': 2}
    地段   總價    單價    總面積               型態  屋齡       樓別  用途  地區別
0   東區  150  25.3  45.49    華廈(10層含以下有電梯)  30    四層/九層   1    6
1   北區  321  16.8  78.66  住宅大樓(11層含以上有電梯)  32  十三層/十四層   2    7
2   東區  900  29.2  30.87    華廈(10層含以下有電梯)  28    三層/九層   0    5
3  新市區  460  13.4  34.35    華廈(10層含以下有電梯)  43    五層/五層   1    6
4  安南區  350  32.0  11.00              透天厝  57     全/二層   1    3
5  歸仁區  950  22.9  41.49  住宅大樓(11層含以上有電梯)  29   四層/二十層   1    2
6   東區  390  24.7  56.38  住宅大樓(11層含以上有電梯)  27  十三層/十四層   0    7
7  新化區  482  15.3  31.59     公寓(5樓含以下無電梯)  42    三層/五層   1    5
#+end_example

能將類別轉成整數，也要能將整數轉回類別。此處可以利用已產生的對應字典，藉由借調 key-value 來產生「反轉字典」(第[[(invClassMapping)]]行)，將對調產生的整數還原回原始類別特徵。

#+BEGIN_SRC python -r -n :async :results output :exports both :session venv
# 產生反轉字典，將整數還原至原始的類別標籤
inv_class_mapping = {v: k for k, v in class_mapping.items()} (ref:invClassMapping)
print(inv_class_mapping)
df['用途'] = df['用途'].map(inv_class_mapping)
print(df)
#+end_src

#+RESULTS:
#+begin_example
{0: '住商用', 1: '住家用', 2: '商業用'}
    地段   總價    單價    總面積               型態  屋齡       樓別   用途  地區別
0   東區  150  25.3  45.49    華廈(10層含以下有電梯)  30    四層/九層  住家用    6
1   北區  321  16.8  78.66  住宅大樓(11層含以上有電梯)  32  十三層/十四層  商業用    7
2   東區  900  29.2  30.87    華廈(10層含以下有電梯)  28    三層/九層  住商用    5
3  新市區  460  13.4  34.35    華廈(10層含以下有電梯)  43    五層/五層  住家用    6
4  安南區  350  32.0  11.00              透天厝  57     全/二層  住家用    3
5  歸仁區  950  22.9  41.49  住宅大樓(11層含以上有電梯)  29   四層/二十層  住家用    2
6   東區  390  24.7  56.38  住宅大樓(11層含以上有電梯)  27  十三層/十四層  住商用    7
7  新化區  482  15.3  31.59     公寓(5樓含以下無電梯)  42    三層/五層  住家用    5
#+end_example
*** scikit-learn LabelEncoder
事實上，scikit-learn 中有一個更為方便的 LabelEncoder 類別則可以直接完成上述工作(第[[(labelEncoder)]]行)。
#+BEGIN_SRC python -r -n :async :results output :exports both :session venv
# Label encoding with sklearn's LabelEncoder
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(df['用途'].values) (ref:labelEncoder)
print(y)
df['用途'] = y
print(df) # 類別與數字的對應不一定與自訂字典一致
#+END_SRC

#+RESULTS:
#+begin_example
[1 2 0 1 1 1 0 1]
    地段   總價    單價    總面積               型態  屋齡       樓別  用途  地區別
0   東區  150  25.3  45.49    華廈(10層含以下有電梯)  30    四層/九層   1    6
1   北區  321  16.8  78.66  住宅大樓(11層含以上有電梯)  32  十三層/十四層   2    7
2   東區  900  29.2  30.87    華廈(10層含以下有電梯)  28    三層/九層   0    5
3  新市區  460  13.4  34.35    華廈(10層含以下有電梯)  43    五層/五層   1    6
4  安南區  350  32.0  11.00              透天厝  57     全/二層   1    3
5  歸仁區  950  22.9  41.49  住宅大樓(11層含以上有電梯)  29   四層/二十層   1    2
6   東區  390  24.7  56.38  住宅大樓(11層含以上有電梯)  27  十三層/十四層   0    7
7  新化區  482  15.3  31.59     公寓(5樓含以下無電梯)  42    三層/五層   1    5
#+end_example

反向編碼
#+BEGIN_SRC python -r -n :async :results output :exports both :session venv
# 反向编碼，將數值編碼轉換回原始類別
original_labels = le.inverse_transform(y)
print("反轉編碼:", original_labels)

df['原來的用途欄'] = original_labels
print(df)
#+END_SRC

#+RESULTS:
#+begin_example
反轉编碼: ['住家用' '商業用' '住商用' '住家用' '住家用' '住家用' '住商用' '住家用']
    地段   總價    單價    總面積               型態  屋齡       樓別  用途  地區別 原來的用途欄
0   東區  150  25.3  45.49    華廈(10層含以下有電梯)  30    四層/九層   1    6    住家用
1   北區  321  16.8  78.66  住宅大樓(11層含以上有電梯)  32  十三層/十四層   2    7    商業用
2   東區  900  29.2  30.87    華廈(10層含以下有電梯)  28    三層/九層   0    5    住商用
3  新市區  460  13.4  34.35    華廈(10層含以下有電梯)  43    五層/五層   1    6    住家用
4  安南區  350  32.0  11.00              透天厝  57     全/二層   1    3    住家用
5  歸仁區  950  22.9  41.49  住宅大樓(11層含以上有電梯)  29   四層/二十層   1    2    住家用
6   東區  390  24.7  56.38  住宅大樓(11層含以上有電梯)  27  十三層/十四層   0    7    住商用
7  新化區  482  15.3  31.59     公寓(5樓含以下無電梯)  42    三層/五層   1    5    住家用
#+end_example

你有看出這樣轉換會有什麼問題嗎?
** One-Hot Encoding
在上面的例子中，我們以scikit-learn 的 LabelEncoder 類別將「類別特徵」編碼為整數值，但這樣會引發另一個問題:原本無序的類別變項就變成有序變項了。如果我們將上述資料中的 *地段* 特徵轉換為整數值，如下：
#+BEGIN_SRC python -r -n :async :results output :exports both :session venv
X = df[['地段']].values
# 以LabelEncoder轉換
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
print(X)
print(le.fit_transform(X[:,0]))
#+END_SRC

#+RESULTS:
: [['東區']
:  ['北區']
:  ['東區']
:  ['新市區']
:  ['安南區']
:  ['歸仁區']
:  ['東區']
:  ['新化區']]
: [4 0 4 3 1 5 4 2]

由輸出結果可以發現，經過類別編碼後的地段特徵，由原本不具次序的特徵變成存在大小關係(歸仁區>東區>新市區...)，這明顯會影響 model 運算的結果。

針對此一問題，常見的解決方案是 one-hot encoding(獨熱編碼--真是直白的翻譯啊啊啊....)，其原理是：對特徵值中的每個值，建立一個新的「虛擬特徵」(dummy feature)。
*** 以pandas get_dummies()進行One Hot Encoding
利用 Pandas 套件的 get_dummies 類別，直接將類別資料轉成二進位類型，即One-Hot encoding。這種轉換只有字串數據會被轉換，其他內容則否。
#+BEGIN_SRC python -r -n :async :results output :exports both :session venv
print('===原始資料===')
print(df)

OheDf = pd.get_dummies(df, columns=['地段'])
print('===轉換後資料===')
print(OheDf)
#+end_src

#+RESULTS:
#+begin_example
===原始資料===
    地段   總價    單價    總面積               型態  屋齡       樓別  用途  地區別 原來的用途欄
0   東區  150  25.3  45.49    華廈(10層含以下有電梯)  30    四層/九層   1    6    住家用
1   北區  321  16.8  78.66  住宅大樓(11層含以上有電梯)  32  十三層/十四層   2    7    商業用
2   東區  900  29.2  30.87    華廈(10層含以下有電梯)  28    三層/九層   0    5    住商用
3  新市區  460  13.4  34.35    華廈(10層含以下有電梯)  43    五層/五層   1    6    住家用
4  安南區  350  32.0  11.00              透天厝  57     全/二層   1    3    住家用
5  歸仁區  950  22.9  41.49  住宅大樓(11層含以上有電梯)  29   四層/二十層   1    2    住家用
6   東區  390  24.7  56.38  住宅大樓(11層含以上有電梯)  27  十三層/十四層   0    7    住商用
7  新化區  482  15.3  31.59     公寓(5樓含以下無電梯)  42    三層/五層   1    5    住家用
===轉換後資料===
    總價    單價    總面積               型態  屋齡  ... 地段_安南區  地段_新化區  地段_新市區  地段_東區  地段_歸仁區
0  150  25.3  45.49    華廈(10層含以下有電梯)  30  ...  False   False   False   True   False
1  321  16.8  78.66  住宅大樓(11層含以上有電梯)  32  ...  False   False   False  False   False
2  900  29.2  30.87    華廈(10層含以下有電梯)  28  ...  False   False   False   True   False
3  460  13.4  34.35    華廈(10層含以下有電梯)  43  ...  False   False    True  False   False
4  350  32.0  11.00              透天厝  57  ...   True   False   False  False   False
5  950  22.9  41.49  住宅大樓(11層含以上有電梯)  29  ...  False   False   False  False    True
6  390  24.7  56.38  住宅大樓(11層含以上有電梯)  27  ...  False   False   False   True   False
7  482  15.3  31.59     公寓(5樓含以下無電梯)  42  ...  False    True   False  False   False

[8 rows x 15 columns]
#+end_example
*** 以scikit-learn ColumnTransformer 進行One-Hot Encoding
利用 ColumnTransformer 函式庫的 ColumnTransformer 類別，將特徵值轉換 One-Hot Encoding 的對應矩陣，如程式第[[(FitTransform)]]行。

#+BEGIN_SRC python -r -n :async :results table output :exports both :session venv
from sklearn.preprocessing import OneHotEncoder
import pandas as pd

df = pd.DataFrame([['東區',150,25.3,45.49,'華廈(10層含以下有電梯)',30,'四層/九層','住家用','文教區'],
['北區',321,16.8,78.66,'住宅大樓(11層含以上有電梯)',32,'十三層/十四層','商業用','行政區'],
['東區',900,29.2,30.87,'華廈(10層含以下有電梯)',28,'三層/九層','住商用','倉庫區'],
['新市區',460,13.4,34.35,'華廈(10層含以下有電梯)',43,'五層/五層','住家用','文教區'],
['安南區',350,32.00,11,'透天厝',57,'全/二層','住家用','農業區'],
['歸仁區',950,22.9,41.49,'住宅大樓(11層含以上有電梯)',29,'四層/二十層','住家用','保護區'],
['東區',390,24.7,56.38,'住宅大樓(11層含以上有電梯)',27,'十三層/十四層','住商用','行政區'],
['新化區',482,15.3,31.59,'公寓(5樓含以下無電梯)',42,'三層/五層','住家用','倉庫區']])
df.columns = ['地段', '總價', '單價', '總面積', '型態', '屋齡', '樓別', '用途', '地區別']

print('===原始資料===')
print(df[['地段']])

from sklearn.compose import ColumnTransformer

X = df[['地段']].values
ct = ColumnTransformer(
    # The column numbers to be transformed (here is [0] but can be [0, 1, 3])
    # Leave the rest of the columns untouched
    [('OneHot', OneHotEncoder(), [0])], remainder='passthrough'
)
print('===轉換後的one-hot encoding資料===')
X_transformed = ct.fit_transform(X)
print(ct.fit_transform(X)) (ref:FitTransform)

# 將稀疏矩陣還原為密集矩陣(非必須，只是讓我們容易看一下結果)
X_dense = X_transformed.toarray()
# 轉為dataframe、加入column name
encoded_columns = ct.named_transformers_['OneHot'].get_feature_names_out(['地段'])
df_encoded = pd.DataFrame(X_dense, columns=encoded_columns)

print('===One-Hot结果===')
print(df_encoded)
#+END_SRC

#+RESULTS:
#+begin_example
===原始資料===
    地段
0   東區
1   北區
2   東區
3  新市區
4  安南區
5  歸仁區
6   東區
7  新化區
===轉換後的one-hot encoding資料===
  (0, 4)	1.0
  (1, 0)	1.0
  (2, 4)	1.0
  (3, 3)	1.0
  (4, 1)	1.0
  (5, 5)	1.0
  (6, 4)	1.0
  (7, 2)	1.0
===One-Hot结果===
   地段_北區  地段_安南區  地段_新化區  地段_新市區  地段_東區  地段_歸仁區
0    0.0     0.0     0.0     0.0    1.0     0.0
1    1.0     0.0     0.0     0.0    0.0     0.0
2    0.0     0.0     0.0     0.0    1.0     0.0
3    0.0     0.0     0.0     1.0    0.0     0.0
4    0.0     1.0     0.0     0.0    0.0     0.0
5    0.0     0.0     0.0     0.0    0.0     1.0
6    0.0     0.0     0.0     0.0    1.0     0.0
7    0.0     0.0     1.0     0.0    0.0     0.0
#+end_example
*** scikit learn OneHotEncoder()
#+BEGIN_SRC python -r -n :async :results output :exports both :session venv
# 初始化 OneHotEncoder
encoder = OneHotEncoder()
encoded_colors = encoder.fit_transform(df[['地段']])
encoded_df = pd.DataFrame(encoded_colors.toarray(), columns=encoder.get_feature_names_out(['地段']))

df_encoded = pd.concat([df.drop(columns=['地段']), encoded_df], axis=1)
print(df_encoded)
#+END_SRC

#+RESULTS:
#+begin_example
    總價    單價    總面積               型態  屋齡  ... 地段_安南區 地段_新化區 地段_新市區  地段_東區  地段_歸仁區
0  150  25.3  45.49    華廈(10層含以下有電梯)  30  ...    0.0    0.0    0.0    1.0     0.0
1  321  16.8  78.66  住宅大樓(11層含以上有電梯)  32  ...    0.0    0.0    0.0    0.0     0.0
2  900  29.2  30.87    華廈(10層含以下有電梯)  28  ...    0.0    0.0    0.0    1.0     0.0
3  460  13.4  34.35    華廈(10層含以下有電梯)  43  ...    0.0    0.0    1.0    0.0     0.0
4  350  32.0  11.00              透天厝  57  ...    1.0    0.0    0.0    0.0     0.0
5  950  22.9  41.49  住宅大樓(11層含以上有電梯)  29  ...    0.0    0.0    0.0    0.0     1.0
6  390  24.7  56.38  住宅大樓(11層含以上有電梯)  27  ...    0.0    0.0    0.0    1.0     0.0
7  482  15.3  31.59     公寓(5樓含以下無電梯)  42  ...    0.0    1.0    0.0    0.0     0.0

[8 rows x 14 columns]
#+end_example

* 特徵縮放(Feature scaling)
當我們在比較分析兩組數據資料時，可能會遭遇因單位的不同(例如：身高與體重)，或數字大小的代表性不同(例如：粉專1萬人與滿足感0.8)，造成各自變化的程度不一，進而影響統計分析的結果；為解決此類的問題，我們可利用資料的正規化(Normalization, 或譯為常態化)與標準化(Standardization)來進行數據的比較及分析[fn:2]。

「特徵縮放」(Feature scaling)是資料預處理的一個關鍵，「決策樹」和「隨機森林」是極少數無需進行 feature scaling 的分類技術；對多數機器學習演算法而言，若特徵值經過適當的縮放，都能有更佳成效。Feature scaling 的重要性可以以下例子看出，假設有兩個特徵值(a, b)，其中 a 的測量範圍為 1 到 10，b 的測量值範圍為 1 到 100000，以典型分類演算法的做法，一定是忙於最佳化特徵值 b；若以 KNN 的演算法，也會被特徵值 b 所支配。

在機器學習演算法中，將數值縮放到同一scale能帶給模型下面兩個好處：
1. 提升模型的收斂速度
   在建構機器學習模型時，我們會利用梯度下降法(Gradient Descent)來計算成本函數(Cost Function)的最佳解；假設我們現有兩個特徵值 x1 in [0,1] 與 x2 in [0,10000]，則在 x1-x2 平面上成本函數的等高線會呈窄長型，導致需較多的迭代步驟，另外也可能導致無法收斂的情況發生。因此，若將資料標準化，則能減少梯度下降法的收斂時間。
2. 提高模型的精準度
   將特徵值 x1 及 x2 餵入一些需計算樣本彼此的距離(例如:歐氏距離)分類器演算法中，則 x2 的影響很可能將遠大於 x1，若實際上 x1 的指標意義及重要性高於 x2，這將導致我們分析的結果失真。因此，資料的標準化是有必要的，可讓每個特徵值對結果做出相近程度的貢獻。

** 常態化(Normalization)
正規化的目的是將資料縮放到固定的範圍內，通常是 [0, 1] 或 [-1, 1]，這樣可以避免因為特徵值範圍過大或過小而影響模型的表現。這種方法特別適合那些依賴距離的模型，比如 k 最近鄰演算法（KNN）和神經網路。

以「將特徵值縮化為 0~1 間」為例，這是「最小最大縮放」(min-max scaling)的一個特例，做法如下：
$$x_{norm}^i = \frac{x^i-x_{min}}{x_{max}-x_{min}}$$
若以 scikit-learn 套件來完成實作，其程式碼如下：
#+BEGIN_SRC python -r -n :async :results output :exports both :session venv
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
df = pd.DataFrame([['東區',150,25.3,45.49,'華廈(10層含以下有電梯)',30,'四層/九層','住家用','文教區'],
['北區',321,16.8,78.66,'住宅大樓(11層含以上有電梯)',32,'十三層/十四層','商業用','行政區'],
['東區',900,29.2,30.87,'華廈(10層含以下有電梯)',28,'三層/九層','住商用','倉庫區'],
['新市區',460,13.4,34.35,'華廈(10層含以下有電梯)',43,'五層/五層','住家用','文教區'],
['安南區',350,32.00,11,'透天厝',57,'全/二層','住家用','農業區'],
['歸仁區',950,22.9,41.49,'住宅大樓(11層含以上有電梯)',29,'四層/二十層','住家用','保護區'],
['東區',390,24.7,56.38,'住宅大樓(11層含以上有電梯)',27,'十三層/十四層','住商用','行政區'],
['新化區',482,15.3,31.59,'公寓(5樓含以下無電梯)',42,'三層/五層','住家用','倉庫區']])
df.columns = ['地段', '總價', '單價', '總面積', '型態', '屋齡', '樓別', '用途', '地區別']
print(df)

mmScaler = MinMaxScaler()
print('===Normalization後的資料===')
df[['總價', '單價']] = mmScaler.fit_transform(df[['總價', '單價']])
print(df[['總價', '單價']])

# 將數據還原到原始範圍
print('===還原後的資料===')
df[['總價', '單價']] = mmScaler.inverse_transform(df[['總價', '單價']])
print(df[['總價', '單價']])
#+END_SRC

#+RESULTS:
#+begin_example
    地段   總價    單價    總面積               型態  屋齡       樓別   用途  地區別
0   東區  150  25.3  45.49    華廈(10層含以下有電梯)  30    四層/九層  住家用  文教區
1   北區  321  16.8  78.66  住宅大樓(11層含以上有電梯)  32  十三層/十四層  商業用  行政區
2   東區  900  29.2  30.87    華廈(10層含以下有電梯)  28    三層/九層  住商用  倉庫區
3  新市區  460  13.4  34.35    華廈(10層含以下有電梯)  43    五層/五層  住家用  文教區
4  安南區  350  32.0  11.00              透天厝  57     全/二層  住家用  農業區
5  歸仁區  950  22.9  41.49  住宅大樓(11層含以上有電梯)  29   四層/二十層  住家用  保護區
6   東區  390  24.7  56.38  住宅大樓(11層含以上有電梯)  27  十三層/十四層  住商用  行政區
7  新化區  482  15.3  31.59     公寓(5樓含以下無電梯)  42    三層/五層  住家用  倉庫區
===Normalization後的資料===
        總價        單價
0  0.00000  0.639785
1  0.21375  0.182796
2  0.93750  0.849462
3  0.38750  0.000000
4  0.25000  1.000000
5  1.00000  0.510753
6  0.30000  0.607527
7  0.41500  0.102151
===還原後的資料===
      總價    單價
0  150.0  25.3
1  321.0  16.8
2  900.0  29.2
3  460.0  13.4
4  350.0  32.0
5  950.0  22.9
6  390.0  24.7
7  482.0  15.3
#+end_example

** 標準化(Standardization)
標準化則是將資料轉換為具有零均值和單位方差(說人話就是平均數為0、標準差為1h)的分佈，這意味著數據被中心化並且具有相同的尺度。這種技術適合資料呈現常態分佈或近似常態分佈的情況，並且適合大多數機器學習模型（如線性回歸、支持向量機等）。

雖說常態化簡單實用，但對許多機器學習演算法來說(特別是梯度下降法的最佳化)，標準化則更為實際，我們可令標準化後的特徵值其平均數為 0、標準差為 1，這樣一來，特徵值會滿足常態分佈，進而使演算法對於離群值不那麼敏感。標準化的公式如下：
$$x_{std}^i = \frac{x^i-\mu_x}{\sigma_x}$$
若以 scikit-learn 套件來完成實作，其程式碼如下：
#+BEGIN_SRC python -r -n :async :results output :exports both :session venv
from sklearn.preprocessing import StandardScaler
sdScaler = StandardScaler()
df[['總價', '單價']] = sdScaler.fit_transform(df[['總價', '單價']])
print('===標準化後的資料===')
print(df[['總價', '單價']])

df[['總價', '單價']] = sdScaler.inverse_transform(df[['總價', '單價']])
print('===還原後的資料===')
print(df[['總價', '單價']])
#+END_SRC

#+RESULTS:
#+begin_example
===標準化後的資料===
         總價        單價
0 -1.331969  0.454115
1 -0.681904 -0.900263
2  1.519196  1.075535
3 -0.153488 -1.442014
4 -0.571659  1.521683
5  1.709274  0.071702
6 -0.419596  0.358512
7 -0.069854 -1.139270
===還原後的資料===
      總價    單價
0  150.0  25.3
1  321.0  16.8
2  900.0  29.2
3  460.0  13.4
4  350.0  32.0
5  950.0  22.9
6  390.0  24.7
7  482.0  15.3
#+end_example

* 資料擴增
資料擴增是一種在訓練機器學習模型特別是深度學習模型時非常重要的技術。它通過對訓練資料進行多種隨機變換，來生成更多的資料樣本，以增強模型的泛化能力並避免過擬合。資料擴增對於擁有有限數據的情況尤其有用，因為它能夠讓模型看到更多樣化的資料，從而提升模型的預測能力和準確度。
- 資料擴增的重要性
  資料擴增能夠模擬真實世界中數據的變化。例如，影像資料可能因為角度、光照、比例等不同而有所變化。透過對現有資料進行旋轉、翻轉、縮放、平移等變換，我們可以有效地提高模型的健壯性，使其能夠在不同場景下依然表現良好。
- 資料擴增的常見方法
  資料擴增的方式多種多樣，以下是幾種常見的影像資料擴增技術：
  1. 翻轉 (Flip)：將圖像在水平方向或垂直方向進行翻轉，模擬鏡像或反轉情況下的圖像。
  1. 旋轉 (Rotation)：對圖像進行不同角度的旋轉，例如 90 度、180 度、270 度等，模擬不同角度拍攝的圖像。
  1. 縮放 (Scaling)：將圖像進行放大或縮小，模擬不同距離下的拍攝效果。
  1. 裁剪 (Cropping)：隨機裁剪圖像的一部分，模擬局部的視角或遮擋的情況。
  1. 人工噪聲 (Noise Addition)：在圖像中增加隨機噪聲，模擬圖像在低光或其他干擾條件下的情況。
  1. 平移 (Translation)：將圖像沿著水平或垂直方向進行平移，模擬不同位置的圖像對象。
** 下載範例程式
- 底下的檔案可以透過以下方式下載(於終端機執行)
- 要記得修改main.py裡的資料夾路徑
#+begin_src shell -r -n :results output :exports both
git clone git@github.com:letranger/DataAugmentationDemo.git
#+end_src
** 範例
安裝opencv
#+begin_src shell -r -n :results output :exports both
pip3 install opencv-python
#+end_src
資料夾位置 ~/Downloads
資料夾架構
#+begin_src shell -r -n :results output :exports both
tree ~/Downloads/aug -d
#+end_src

#+RESULTS:
: /Users/letranger/Downloads/aug
: ├── augImages
: │   ├── cats
: │   └── dogs
: └── images
:     ├── cats
:     └── dogs
:
: 7 directories

#+begin_src python -r :async :results output :exports both :session venv
import cv2
import os
import numpy as np

def augment_image(image):
    """對輸入圖像進行數據增強並返回增強後的圖像列表"""
    augmented_images = []

    # 原圖
    augmented_images.append(image)

    # 翻轉圖像
    flip1 = cv2.flip(image, 0)  # 垂直翻轉
    flip2 = cv2.flip(image, 1)  # 水平翻轉
    flip3 = cv2.flip(image, -1) # 垂直和水平翻轉
    augmented_images.extend([flip1, flip2, flip3])

    # 旋轉圖像
    for angle in [90, 180, 270]:
        M = cv2.getRotationMatrix2D((image.shape[1] // 2, image.shape[0] // 2), angle, 1)
        rotated = cv2.warpAffine(image, M, (image.shape[1], image.shape[0]))
        augmented_images.append(rotated)

    # 縮放圖像
    for scale in [0.9, 1.1]:
        scaled = cv2.resize(image, None, fx=scale, fy=scale)
        augmented_images.append(scaled)

    return augmented_images

def process_directory(input_dir, output_dir):
    """處理輸入目錄中的所有圖像，並將增強後的圖像保存到輸出目錄"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    categories = ['dogs', 'cats']
    for category in categories:
        category_input_dir = os.path.join(input_dir, category)
        category_output_dir = os.path.join(output_dir, category)

        for filename in os.listdir(category_input_dir):
            image_path = os.path.join(category_input_dir, filename)
            image = cv2.imread(image_path)
            augmented_images = augment_image(image)

            for i, augmented_image in enumerate(augmented_images):
                output_filename = f"{os.path.splitext(filename)[0]}_aug_{i}.jpg"
                output_path = os.path.join(category_output_dir, output_filename)
                cv2.imwrite(output_path, augmented_image)

# 輸入和輸出目錄
input_directory = '/Users/letranger/Downloads/aug/images'
output_directory = '/Users/letranger/Downloads/aug/augImages'

# 處理圖像
process_directory(input_directory, output_directory)
#+end_src
** 其他擴增方法
其他資料擴增方式還有:濾波、銳化、去噪、旋轉、縮放、裁剪和增加人工噪聲....，至於資料擴增對於模型有何助益請[[https://blog.toright.com/posts/7144/keras-tutorial-cnn-data-augmentation-training][參閱這篇]]。其他資料擴增方式請看:
- [[https://cynthiachuang.github.io/Augmentor-Image-Augmentation-Library-in-Python/][Augmentor：影像資料增強工具庫]]
- [[https://chtseng.wordpress.com/2017/11/11/data-augmentation-%E8%B3%87%E6%96%99%E5%A2%9E%E5%BC%B7/][Data Augmentation 資料增強]]

* 資料集與資料分割
** 常用資料集
當你使用 Python 學習人工智慧（AI）和機器學習（ML）時，以下是一些常用的資料集及其簡單介紹：
*** MNIST
簡介：MNIST（Modified National Institute of Standards and Technology database）是一個大型手寫數字資料集，包含 0 到 9 的手寫數字圖像。
- 用途：常用於圖像分類和計算機視覺的入門練習。
- 特徵：包含 60,000 張訓練圖像和 10,000 張測試圖像，每張圖像大小為 28x28 像素。
- 來源：可以從 tensorflow 或 keras 中直接獲取。
#+begin_src python -r -n :async :results output :exports both :session venv
from tensorflow.keras.datasets import mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
#+end_src
*** Iris
- 簡介：Iris 資料集包含 3 種鰹魚花（Setosa、Versicolour 和 Virginica）的 150 個樣本，每個樣本有 4 個特徵（花萼長度、花萼寬度、花瓣長度、花瓣寬度）。
- 用途：常用於分類和聚類算法的入門練習。
- 特徵：每個樣本包含 4 個特徵和 1 個標籤。
- 來源：可以從 sklearn 中直接獲取。
#+begin_src python -r -n :results output :exports both
from sklearn.datasets import load_iris
iris = load_iris()
X, y = iris.data, iris.target
#+end_src
*** Boston 房價
- 簡介：Boston 房價資料集包含 506 個房屋的特徵和價格信息，用於回歸問題。
- 用途：常用於回歸算法的入門練習。
- 特徵：每個樣本包含 13 個特徵，如犯罪率、房間數、房產稅等。
- 來源：可以從 sklearn 中直接獲取。
#+begin_src python -r -n :results output :exports both
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import boston_housing

(train_x, train_y), (test_x, test_y) = boston_housing.load_data()
#+end_src
*** CIFAR-10
- 簡介：CIFAR-10 是一個影像資料集，包含 10 個類別的 60,000 張彩色圖片，每個類別有 6,000 張圖片。
- 用途：常用於圖像分類和深度學習的入門練習。
- 特徵：每張圖像大小為 32x32 像素。
- 來源：可以從 tensorflow 或 keras 中直接獲取。
#+begin_src python -r -n :results output :exports both
from tensorflow.keras.datasets import cifar10
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
#+end_src
***  Wine
- 簡介：Wine 資料集包含 178 個樣本，記錄了 3 種不同葡萄酒的 13 個化學成分。
- 用途：常用於分類問題。
- 特徵：每個樣本包含 13 個特徵和 1 個標籤。
- 來源：可以從 sklearn 中直接獲取。
#+begin_src python -r -n :results output :exports both
from sklearn.datasets import load_wine
wine = load_wine()
X, y = wine.data, wine.target
#+end_src
*** Breast Cancer Wisconsin
- 簡介：Breast Cancer Wisconsin 資料集包含 569 個乳腺癌樣本的特徵，目的是預測腫瘤是良性還是惡性。
- 用途：常用於二元分類問題。
- 特徵：每個樣本包含 30 個特徵。
- 來源：可以從 sklearn 中直接獲取。
#+begin_src python -r -n :results output :exports both
from sklearn.datasets import load_breast_cancer
breast_cancer = load_breast_cancer()
X, y = breast_cancer.data, breast_cancer.target
#+end_src
** 資料分割
#+begin_src plantuml :file images/datapreprocessing.png
@startuml
 left to right direction
 state 已訓練模型 {
 state 訓練集 #palegreen
 state 未訓練模型 #lightgray : 黑盒子
 訓練集 --> 未訓練模型
 }
 state 測試集 #palegreen
 測試集 --> 已訓練模型
 state 預測結果
 已訓練模型 --> 預測結果
@enduml
#+end_src

#+RESULTS:
[[file:images/datapreprocessing.png]]

*** 為什麼要分割資料
- 訓練集（training): 舉例來說就是上課學習。主要用在訓練階段，用於模型擬合，直接參與了模型參數調整的過程[fn:1]。
- 驗證集（validation）: 舉例來說就是模擬考，你會根據模擬考的成績繼續學習、或調整學習方式重新學習。在訓練過程中，用於評估模型的初步能力與超參數調整的依據。不過驗證集是非必需的，不像訓練集和測試集。如果不需要調整超參數，就可以不使用驗證集[fn:1]。
- 測試集（test）就像是學測，用來評估你最終的學習結果。用來評估模型最終的泛化能力。為了能評估模型真正的能力，測試集不應該為參數調整、選擇特徵等依據[fn:1]。
使用學測來比喻，是因為測試集不應該做為參數調整、選擇特徵等依據。這些選擇與調整可以想像成學習方式的調整，但學測已經考完，你不能時光倒轉回到最初調整學習方式[fn:1]。
*** 資料分割實作
訓練集與測試集的分割可以自行以Python進行分割，也可以直接呼叫函式進行分割
**** 手動分割
#+BEGIN_SRC python -r -n :results output :exports both
import pandas as pd
import numpy as np
import random

df_wine = pd.read_csv('https://archive.ics.uci.edu/'
                      'ml/machine-learning-databases/wine/wine.data',
                      header=None)

df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']

train_len = int(len(df_wine) * 0.7)

# 打亂資料集順序
idx = list(df_wine.index)
random.shuffle(idx)

# 分割資料集
TrainSet = df_wine.loc[idx[:train_len]]
TestSet = df_wine.loc[idx[train_len:]]
print(len(TrainSet))
print(len(TestSet))
X_train, y_train = TrainSet.iloc[:, 1:].values, TrainSet.iloc[:, 0].values
X_test, y_test = TestSet.iloc[:, 1:].values, TestSet.iloc[:, 0].values

print('==========訓練集==========')
print(X_train[:2])
print(y_train[:2])
print('==========測試集==========')
print(X_test[:2])
print(y_test[:2])
#+END_SRC

#+RESULTS:
#+begin_example
124
54
==========訓練集==========
[[1.229e+01 2.830e+00 2.220e+00 1.800e+01 8.800e+01 2.450e+00 2.250e+00
  2.500e-01 1.990e+00 2.150e+00 1.150e+00 3.300e+00 2.900e+02]
 [1.340e+01 4.600e+00 2.860e+00 2.500e+01 1.120e+02 1.980e+00 9.600e-01
  2.700e-01 1.110e+00 8.500e+00 6.700e-01 1.920e+00 6.300e+02]]
[2 3]
==========測試集==========
[[1.394e+01 1.730e+00 2.270e+00 1.740e+01 1.080e+02 2.880e+00 3.540e+00
  3.200e-01 2.080e+00 8.900e+00 1.120e+00 3.100e+00 1.260e+03]
 [1.402e+01 1.680e+00 2.210e+00 1.600e+01 9.600e+01 2.650e+00 2.330e+00
  2.600e-01 1.980e+00 4.700e+00 1.040e+00 3.590e+00 1.035e+03]]
[1 1]
#+end_example
**** 呼叫scikit learn的function
#+BEGIN_SRC python -r -n :results output :exports both
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

df_wine = pd.read_csv('https://archive.ics.uci.edu/'
                      'ml/machine-learning-databases/wine/wine.data',
                      header=None)

df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']

print('Class labels', np.unique(df_wine['Class label']))

X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y,
                     test_size=0.3, random_state=0, stratify=y)

print(len(X_train))
print(len(y_test))

print('==========訓練集==========')
print(X_train[:2])
print(y_train[:2])
print('==========測試集==========')
print(X_test[:2])
print(y_test[:2])
#+END_SRC

#+RESULTS:
#+begin_example
Class labels [1 2 3]
124
54
==========訓練集==========
[[1.362e+01 4.950e+00 2.350e+00 2.000e+01 9.200e+01 2.000e+00 8.000e-01
  4.700e-01 1.020e+00 4.400e+00 9.100e-01 2.050e+00 5.500e+02]
 [1.376e+01 1.530e+00 2.700e+00 1.950e+01 1.320e+02 2.950e+00 2.740e+00
  5.000e-01 1.350e+00 5.400e+00 1.250e+00 3.000e+00 1.235e+03]]
[3 1]
==========測試集==========
[[1.377e+01 1.900e+00 2.680e+00 1.710e+01 1.150e+02 3.000e+00 2.790e+00
  3.900e-01 1.680e+00 6.300e+00 1.130e+00 2.930e+00 1.375e+03]
 [1.217e+01 1.450e+00 2.530e+00 1.900e+01 1.040e+02 1.890e+00 1.750e+00
  4.500e-01 1.030e+00 2.950e+00 1.450e+00 2.230e+00 3.550e+02]]
[1 2]
#+end_example

* 選取有意義的特徵 :noexport:
overfitting 的產生原因是模型過度遷就於訓練數據，導致面對新數據(測試集)時成效不彰，我們稱這種模型具有較高變異性(high variance)，一般的解決策略有：
- 收集更多的訓練數據集
- 經由正規化，對於過度複雜的模型引進一個「懲罰」(penalty)
- 以較少的參數做出較簡單的模型(使用更簡單的模型)
- 減少數據維度

** L1L2 regularzation

一個典型的解釋[fn:10]如圖[[fig:OverFitting-1]]，"我們知道, 過擬合就是所謂的模型對可見的數據過度自信, 非常完美的擬合上了這些數據, 如果具備過擬合的能力, 那麼這個方程就可能是一個比較複雜的非線性方程 , 正是因為這裡的 $x^3$ 和 $x^2$ 使得這條虛線能夠被彎來彎去, 所以整個模型就會特別努力地去學習作用在 $x^3$ 和 $x^2$ 上的 $c$, $d$ 參數. 但是我們期望模型要學到的卻是 *這條藍色的曲線*. 因為它能更有效地概括數據.而且只需要一個 $y=a+bx$ 就能表達出數據的規律. 或者是說, 藍色的線最開始時, 和紅色線同樣也有 $c,d$ 兩個參數, 可是最終學出來時, $c$ 和 $d$ 都學成了 0, 雖然藍色方程的誤差要比紅色大, 但是概括起數據來還是藍色好. 那我們如何保證能學出來這樣的參數呢? 這就是 l1 l2 正規化出現的原因。"

#+CAPTION: 過擬合問題
#+LABEL:fig: OverFitting-1
#+name: fig:OverFitting-1
#+ATTR_LATEX: :width 300
[[file:images/L1l2regularization2.png]]

對於上述訓練出的兩個方程式，我們可以用\((y_{\theta}(x)-y)^2\)來計算模型預測值\(y(x)\)和真實數據\(y\)的誤差，而 L1, L2 就只是在這個誤差公式後加上一些式子來修正這個公式(如圖[[fig:OverFitting-2]])，其目的在於讓誤差的最佳化不僅取決於訓練數據擬合的優劣，同時也取決於參數值(如 $c,d$)的大小；L2 正規化以參數平方來做為計算方式，L1 正規化則是計算每個參數的絕對值。
#+CAPTION: L1,L2 正規化公式
#+LABEL:fig: OverFitting-2
#+name: fig:OverFitting-2
#+ATTR_LATEX: :width 300
[[file:images/L1l2regularization3.png]]

進一步以 Tensorflow Playground 的圖示來觀察 L1,L2 正規化的差異[fn:11]，如果把正規化(Regularization)設定為 L1，再執行訓練。可以看到很多權重都被設定為 0，特徵輸入與隱藏層的神經元被大大的減少，如圖[[fig:L1l2regularization4]]，整個模型的複雜度簡化很多。L1 正規化確實有助於將我們的複雜模型縮減為更小的泛化模型。添加正規化後，我們看到無用的功能全部變為零，並且連接線變得稀疏並顯示為灰色。倖存下來的唯一特徵是 $x_1$ 平方和 $x_2$ 平方，這是有道理的，因為這 2 個特徵加在一起就構成了一個圓的方程式。

#+CAPTION: L1 正規化
#+LABEL:fig: L1l2regularization4
#+name: fig:L1l2regularization4
#+ATTR_LATEX: :width 400
[[file:images/L1l2regularization4.png]]

反觀 L2 正規化，當我們訓練它時，每個權重與神經元都還是處於活動狀態，但是非常虛弱，如圖[[fig:OverFitting-3]]，L1 正規化使用其中一個特徵而將某些拋棄，而 L2 正規化將同時保留特徵並使權重值保持較小。因此，使用 L1，您可以得到一個較小的模型，但預測性可能較低。所以：

- L1 正規化：有可能導致零權重，因刪除更多特徵而使模型稀疏。
- L2 正規化：會對更大的權重值造成更大的影響，將使權重值保持較小。

#+CAPTION: L2 正規化
#+LABEL:fig: OverFitting-3
#+name: fig:OverFitting-3
#+ATTR_LATEX: :width 400
[[file:images/L1l2regularization5.png]]

* 處理數據中的分類特徵編碼問題 :noexport:
** categorical feature

真實世界的數據集往往包含各種「類別特徵」(categorical feature)，類別特徵可再分為
- nominal feature: 名義特徵
- ordinal feature: 次序特徵

#+BEGIN_SRC python -r -n :results output :exports both
  import pandas as pd
  df = pd.DataFrame([['green', 'M', 10.1, 'class2'],
                     ['red', 'L', 13.5, 'class1'],
                     ['blue', 'XL', 15.3, 'class2']])

  df.columns = ['color', 'size', 'price', 'classlabel']
  print(df)
#+END_SRC

#+RESULTS:
:    color size  price classlabel
: 0  green    M   10.1     class2
: 1    red    L   13.5     class1
: 2   blue   XL   15.3     class2

** 對應 ordinal feature

自定一個 mapping dictionary，即 size\under{}mapping，然後將 classlabel 對應到 size\under{}mapping 中的鍵值(程式第[[(sizeMapping)]]行)。

#+BEGIN_SRC python -r -n :results output :exports both
  import pandas as pd
  df = pd.DataFrame([['green', 'M', 10.1, 'class2'],
                     ['red', 'L', 13.5, 'class1'],
                     ['blue', 'XL', 15.3, 'class2']])

  df.columns = ['color', 'size', 'price', 'classlabel']
  ### Mapping ordinal features
  size_mapping = {'XL': 3,
                  'L': 2,
                  'M': 1}
  df['size'] = df['size'].map(size_mapping)   (ref:sizeMapping)
  print(df)
#+END_SRC

#+RESULTS:
:    color  size  price classlabel
: 0  green     1   10.1     class2
: 1    red     2   13.5     class1
: 2   blue     3   15.3     class2

** 對應 nominal feature

許多機器學習的函式庫需要將「類別標籤」編碼為整數值。方法之一是以列舉方式為這些 nominal features 自 0 開始編號，先以 enumerate 方式建立一個 mapping dictionary: class_mapping(程式第[[(classMapping)]]行)，然後利用這個字典將類別特徵轉換為整數值。

此外，也可以利用已產生的對應字典，藉由借調 key-value 來產生「反轉字典」(第[[(invClassMapping)]]行)，將對調產生的整數還原回原始類別特徵。

scikit-learn 中有一個更為方便的 LabelEncoder 類別則可以直接完成上述工作(第[[(labelEncoder)]]行)。

#+BEGIN_SRC python -r -n :results output :exports both
  import pandas as pd
  df = pd.DataFrame([['green', 'M', 10.1, 'class2'],
                     ['red', 'L', 13.5, 'class1'],
                     ['blue', 'XL', 15.3, 'class2']])

  df.columns = ['color', 'size', 'price', 'classlabel']
  # 建利對應字典
  import pandas as np
  class_mapping = {
      label: idx for idx, label in enumerate(np.unique(df['classlabel'])) (ref:classMapping)
  }
  print(class_mapping)
  # 將類別特徵轉換為整數值
  df['classlabel'] = df['classlabel'].map(class_mapping)
  print(df)

  # 產生反轉字典，將整數還原至原始的類別標籤
  inv_class_mapping = {v: k for k, v in class_mapping.items()} (ref:invClassMapping)
  df['classlabel'] = df['classlabel'].map(inv_class_mapping)
  print(df)

  # Label encoding with sklearn's LabelEncoder
  from sklearn.preprocessing import LabelEncoder
  class_le = LabelEncoder()
  y = class_le.fit_transform(df['classlabel'].values) (ref:labelEncoder)
  print(y)
  df['classlabel'] = y
  print(df) # 類別與數字的對應不一定與自訂字典一致

#+END_SRC

#+RESULTS:
#+begin_example
{'class2': 0, 'class1': 1}
   color size  price  classlabel
0  green    M   10.1           0
1    red    L   13.5           1
2   blue   XL   15.3           0
   color size  price classlabel
0  green    M   10.1     class2
1    red    L   13.5     class1
2   blue   XL   15.3     class2
[1 0 1]
   color size  price  classlabel
0  green    M   10.1           1
1    red    L   13.5           0
2   blue   XL   15.3           1
#+end_example

** 對 nominal feature 執行 one-hot encoding
將類別 (categorical)或是文字(text)的資料轉換成數字，而讓程式能夠更好的去理解及運算。理由:
1. 字串無法套入數學模型進行運算
2. 直接換成數字會造成誤解
*** pd.get_dummies
get_dummies 是利用pandas实现one hot encode的方式[fn:3]。
#+begin_src python -r -n :results output :exports both
import pandas as pd
df = pd.DataFrame([
    ['台中', '惠文高中'],
    ['台中', '中女中'],
    ['台南', '大灣高中'],
    ['台南', '台南一中'],
    ['高雄', '高雄女中']
])
df.columns = ['city', 'school']
print('---城市原值---')
print(df['city'].values)
print('---one hot encoding轉換---')
print(pd.get_dummies(df['city']))
#+end_src

#+RESULTS:
: ---城市原值---
: ['台中' '台中' '台南' '台南' '高雄']
: ---one hot encoding轉換---
:    台中  台南  高雄
: 0   1     0    0
: 1   1     0    0
: 2   0     1    0
: 3   0     1    0
: 4   0     0    1

*** scikit
scikit-learn 的 LabelENcoder 類別可以用來將「類別特徵」編碼為整數值，但這樣會引發另一個問題，如果我們將上述資料中的 color 特徵轉換為整數值，如下：

#+BEGIN_SRC python -r -n :results output :exports both
  import pandas as pd
  df = pd.DataFrame([['green', 'M', 10.1, 'class2'],
                     ['red', 'L', 13.5, 'class1'],
                     ['blue', 'XL', 15.3, 'class2']])

  df.columns = ['color', 'size', 'price', 'classlabel']

  X = df[['color', 'size', 'price', 'classlabel']].values

  # 以LabelEncoder轉換
  from sklearn.preprocessing import LabelEncoder
  color_le = LabelEncoder()
  print(X[:,0])
  X[:,0] = color_le.fit_transform(X[:,0])
  print(X[:,0])

#+END_SRC

#+RESULTS:
: ['green' 'red' 'blue']
: [1 2 0]

由輸出結果可以發現，經過類別編碼後的顏色特徵，由原本不具次序的特徵變成存在大小關係(red>green>blue)，這明顯會影響 model 運算的結果。針對此一問題，常見的解決方案是 one-hot encoding，其原理是：對特徵值中的每個值，建立一個新的「虛擬特徵」(dummy feature)。方法有二：
- 利用 ColumnTransformer 函式庫的 ColumnTransformer 類別，將特徵值轉換 One-Hot Encoding 的對應矩陣，如程式第[[(FitTransform)]]行。
- 利用 Pandas 套件的 get\under{}dummies 類別，一次將矩陣內指定之 column 轉換為 One-Hot encoding，如程式第[[(GetDummies)]]行。這種轉換只有字串數據會被轉換，其他內容則否。

#+BEGIN_SRC python -r -n :results output :exports both
  import pandas as pd
  df = pd.DataFrame([['green', 'M', 10.1, 'class2'],
                     ['red', 'L', 13.5, 'class1'],
                     ['blue', 'XL', 15.3, 'class2']])

  df.columns = ['color', 'size', 'price', 'classlabel']

  X = df[['color', 'size', 'price', 'classlabel']].values
  print(df)

  # one-hot encoding: ColumnTransformer / fit_transform
  from sklearn.preprocessing import LabelEncoder
  from sklearn.preprocessing import OneHotEncoder
  from sklearn.compose import ColumnTransformer
  import numpy as np

  X = df[['color', 'size', 'price']].values

  ct = ColumnTransformer(
      # The column numbers to be transformed (here is [0] but can be [0, 1, 3])
      # Leave the rest of the columns untouched
      [('OneHot', OneHotEncoder(), [0])], remainder='passthrough'
  )
  print(ct.fit_transform(X)) (ref:FitTransform)

  # on-hot encoding: pandas / get_dummies
  import pandas as pd
  print(pd.get_dummies(df[['price', 'color', 'size']])) (ref:GetDummies)
#+END_SRC

#+RESULTS:
#+begin_example
   color size  price classlabel
0  green    M   10.1     class2
1    red    L   13.5     class1
2   blue   XL   15.3     class2
[[0.0 1.0 0.0 'M' 10.1]
 [0.0 0.0 1.0 'L' 13.5]
 [1.0 0.0 0.0 'XL' 15.3]]
   price  color_blue  color_green  color_red  size_L  size_M  size_XL
0   10.1           0            1          0       0       1        0
1   13.5           0            0          1       1       0        0
2   15.3           1            0          0       0       0        1
#+end_example

應用 one-hot encoding 時，我們必須留意它所引入的「多元共線性」(multicollinearity)問題，這在某些狀況下(如要計算反矩陣)可能會產生一些問題，若特徵間有高度相關，則會難以計算反矩陣，導致數值不穩定的舘計。

* Footnotes

[fn:1] [[https://cynthiachuang.github.io/What-is-the-Difference-between-Training-Validation-and-Test-Dataset/][訓練集、驗證集、測試集的定義與劃分]]

[fn:2] [[https://aifreeblog.herokuapp.com/posts/54/data_science_203/][資料的正規化(Normalization)及標準化(Standardization)]]

[fn:3] [[https://blog.csdn.net/maymay_/article/details/80198468][pandas.get_dummies 的用法]]

[fn:4] [[https://cinereplicas.com/products/harry-potter-nimbus-2000-broom-2019-edition][Nimbus 2000]]

[fn:5] [[https://www.cw.com.tw/index.php/article/5124891?rec=es][體脂肪多少才標準？體脂機的原理是什麼？體脂率對照表一次看]]

[fn:6] [[https://apcs.csie.ntnu.edu.tw/index.php/info/grades/][APCS檢測資訊]]

[fn:7] [[https://www.pinecone.io/learn/vector-similarity/][Vector Similarity Explained]]

[fn:8] [[http://blog.itpub.net/70027826/viewspace-2970075/][向量資料庫與pgvector ]]

[fn:9] [[https://livebook.manning.com/concept/nlp/3d-vector][Natural Language Processing in Action: Understanding, analyzing, and generating text with Python]]

[fn:10][[https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-09-l1l2regularization/][L1 / L2 正規化]]

[fn:11][[https://ithelp.ithome.com.tw/articles/10219648?sc=rss.iron][Google ML課程筆記 - Overfitting 與 L1 /L2 Regularization ]]
