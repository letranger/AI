<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-09-22 Sun 15:50 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>分類</title>
<meta name="author" content="Yung-Chin Yen" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="../css/muse.css" />
<script src="../css/copy_code.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">分類</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org7e1807e">1. 關於分類</a></li>
<li><a href="#org6daf3d7">2. 最短距離分類器</a>
<ul>
<li><a href="#orga6d0271">2.1. 任務: 分辨貓狗</a></li>
<li><a href="#orge0e774a">2.2. 工作原理</a></li>
<li><a href="#orgc893d1c">2.3. 其他計算距離的方式</a></li>
<li><a href="#org8b0a882">2.4. [作業]手刻最短距離分類器&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></a></li>
</ul>
</li>
<li><a href="#IRIS-KNN">3. KNN 分類器:IRIS</a>
<ul>
<li><a href="#orgbb09716">3.1. [實作]鳶尾花分類&#xa0;&#xa0;&#xa0;<span class="tag"><span class="sklearn">sklearn</span></span></a></li>
<li><a href="#org40573c3">3.2. [作業]手刻 KNN 分類器&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></a></li>
<li><a href="#org709d1fe">3.3. [作業]分析 K 值對模型效能的影響&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></a></li>
</ul>
</li>
<li><a href="#orgc4d8600">4. 決策樹分類器</a>
<ul>
<li><a href="#orgfca22a1">4.1. 原理</a></li>
<li><a href="#orgf90af4c">4.2. 動手計算</a></li>
<li><a href="#orgcf7d783">4.3. Overfitting 問題</a></li>
<li><a href="#org0c79f5d">4.4. [實作]鳶尾花分類&#xa0;&#xa0;&#xa0;<span class="tag"><span class="sklearn">sklearn</span></span></a></li>
<li><a href="#org2319b00">4.5. [作業]以決策樹進行貸款核淮分析&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></a></li>
</ul>
</li>
</ul>
</div>
</div>
<a href="https://letranger.github.io/AI/20231204210821-分類.html"><img align="right" alt="Hits" src="https://hits.sh/letranger.github.io/AI/20231204210821-分類.html.svg"/></a>
<div id="outline-container-org7e1807e" class="outline-2">
<h2 id="org7e1807e"><span class="section-number-2">1.</span> 關於分類</h2>
<div class="outline-text-2" id="text-1">
<p>
分類(Cliasification): 藉由特徵值的蒐集，將資料依照某些規則加以整理，然後產生一套區分該物件的原則，藉此建構出分類器（Classifier），新的事物藉由分類器去判斷是屬於哪一個分類。
</p>

<p>
分類是<a href="20221023101626-監督式學習.html#ID-20221023T101626.420918">監督式學習</a>的方法之一(另一種為迴歸)，分類問題也稱為離散(discrete)預測問題，因為每個分類都是一個離散群組。In supervised learning, the training set you feed to the algorithm includes the desired solutions, called labels<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>.
</p>


<div id="orgc67c209" class="figure">
<p><img src="images/2022-04-30_10-38-58.jpg" alt="2022-04-30_10-38-58.jpg" width="500" />
</p>
<p><span class="figure-number">Figure 1: </span>典型的監督式學習：垃圾郵件分類</p>
</div>

<p>
分類可再細分為:
</p>
<ul class="org-ul">
<li>Binary classification</li>
<li>Multiclass classification</li>
</ul>
<p>
實務的演算法:
</p>
<ul class="org-ul">
<li>最短矩離分類器</li>
<li>KNN 分類器</li>
<li>決策樹 Decision Tree</li>
</ul>
</div>
</div>
<div id="outline-container-org6daf3d7" class="outline-2">
<h2 id="org6daf3d7"><span class="section-number-2">2.</span> 最短距離分類器</h2>
<div class="outline-text-2" id="text-2">
<p>
最短距離分類器是利用新進未知資料的特徵，與已知類別資料的特徵比較距離，看看新進資料與哪個類別的特徵最相似（即資料相似度最高），就將新進的資料預測為該類別。
</p>
<blockquote>
<p>
Birds of the same kind are often seen to flock and fly together.
</p>
</blockquote>
</div>
<div id="outline-container-orga6d0271" class="outline-3">
<h3 id="orga6d0271"><span class="section-number-3">2.1.</span> 任務: 分辨貓狗</h3>
<div class="outline-text-3" id="text-2-1">

<div id="orgb441ae3" class="figure">
<p><img src="images/最短距離分類器/2024-02-05_13-41-44_2024-02-05_13-41-30.png" alt="2024-02-05_13-41-44_2024-02-05_13-41-30.png" width="200" />
</p>
<p><span class="figure-number">Figure 2: </span>貓與狗的外觀比較</p>
</div>

<p>
貓與狗在外觀上較明顯的區別(特徵)為頭部大小及尾巴長度，其中:
</p>
<ul class="org-ul">
<li>頭部大小:計算圖中動物的頭部與身體比例，將比 例值分為 10 個類別，比例值越高，代表頭部所佔面積越大。</li>
<li>尾巴長度:計算圖中動物尾巴與身體長度比例，將比例值分為 10 個類別，比例值越高，代 表尾巴越長。</li>
</ul>

<p>
這個資料集裡有 8 張照片，其中兩張為：
</p>

<div id="org179f7d2" class="figure">
<p><img src="images/最短距離分類器/2024-02-05_13-44-20_2024-02-05_13-44-08.png" alt="2024-02-05_13-44-20_2024-02-05_13-44-08.png" width="500" />
</p>
<p><span class="figure-number">Figure 3: </span>資料集範例</p>
</div>

<p>
問題：如果出現了一張新動物的圖片，在測量出這張新照片中動物的特徵值後，我們如何利用現有的這 8 張圖的訓練資料來判斷新圖裡的動物是貓還是狗?
</p>
</div>
</div>
<div id="outline-container-orge0e774a" class="outline-3">
<h3 id="orge0e774a"><span class="section-number-3">2.2.</span> 工作原理</h3>
<div class="outline-text-3" id="text-2-2">
</div>
<div id="outline-container-org87c594c" class="outline-4">
<h4 id="org87c594c"><span class="section-number-4">2.2.1.</span> 找出中心點[中心點分類器]</h4>
<div class="outline-text-4" id="text-2-2-1">
<p>
首先我們找出圖<a href="#orgb8e7e3a">4</a>這兩群資料的中心點(各特徵值的平均值)
</p>

<div id="orgb8e7e3a" class="figure">
<p><img src="images/最短距離分類器/2024-02-05_13-54-53_2024-02-05_13-54-45.png" alt="2024-02-05_13-54-53_2024-02-05_13-54-45.png" width="500" />
</p>
<p><span class="figure-number">Figure 4: </span>兩群資料的中心點</p>
</div>

<p>
結果如圖<a href="#org1e81002">5</a>
</p>

<div id="org1e81002" class="figure">
<p><img src="images/最短距離分類器/2024-02-05_14-02-34_2024-02-05_14-02-27.png" alt="2024-02-05_14-02-34_2024-02-05_14-02-27.png" width="500" />
</p>
<p><span class="figure-number">Figure 5: </span>標題</p>
</div>
</div>
</div>
<div id="outline-container-orga046e39" class="outline-4">
<h4 id="orga046e39"><span class="section-number-4">2.2.2.</span> 對新資料預測</h4>
<div class="outline-text-4" id="text-2-2-2">
<p>
現在我們可以對新資料進行預測，如果有一張新圖被送進模型，其兩項特徵值為(頭部大小 5，尾巴長度 6)(如圖<a href="#org330f82c">6</a>中的<span style="color:red;">▲</span>)，那這張圖比較可能是貓還是狗呢?
</p>

<div id="org330f82c" class="figure">
<p><img src="images/最短距離分類器/2024-02-05_14-05-19_2024-02-05_14-05-00.png" alt="2024-02-05_14-05-19_2024-02-05_14-05-00.png" width="500" />
</p>
<p><span class="figure-number">Figure 6: </span>比較新資料與兩個中心點的距離</p>
</div>

<p>
最短距離法的判斷方式是:比較<span style="color:red;">▲</span>與兩個中心點(<span style="color:green;">★</span>與<span style="color:orange;">★</span>)的直線距離(歐幾里德距離)，也就是說，<span style="color:red;">▲</span>離哪個中心點直線距離較短就將之視為那種動物。計算方式如下:
</p>
<ul class="org-ul">
<li><span style="color:red;">▲</span>與<span style="color:green;">★</span>的直線距離為 \( \sqrt{(5 − 2.5)^2 + (6 − 7.0)^2 } \approx 2.69 \)</li>
<li><span style="color:red;">▲</span>與<span style="color:orange;">★</span>的直線距離為 \( \sqrt{(5 − 7.5)^2 + (6 − 5.25)^2} \approx 2.61 \)</li>
</ul>

<p>
根據上述計算結果，<span style="color:red;">▲</span>離<span style="color:orange;">★</span>較近，模型對這張新圖的預測結果為:狗。
上述範例是一個簡化的監督式學習模型工作原理，可預期當我們對於訓練資料掌握的特徵值越詳盡(如:對每張圖片建構出 1000 個特徵值)，則預測結果應該也會越精確，但隨著訓練資料的增加(如:有 100 萬張訓練圖片)，則整個運算量將會變得十分龐大。
</p>
</div>
</div>
</div>
<div id="outline-container-orgc893d1c" class="outline-3">
<h3 id="orgc893d1c"><span class="section-number-3">2.3.</span> 其他計算距離的方式</h3>
<div class="outline-text-3" id="text-2-3">
<p>
除了使用中心點做為距離判斷依據，也可以計算新進資料與 <b>所有</b> 資料集中各類資料點的距離，這裡的距離可以是曼哈頓距離或是歐幾里得距離。
</p>
</div>
</div>
<div id="outline-container-org8b0a882" class="outline-3">
<h3 id="org8b0a882"><span class="section-number-3">2.4.</span> [作業]手刻最短距離分類器&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></h3>
<div class="outline-text-3" id="text-2-4">
<ul class="org-ul">
<li><a href="https://moodle.tnfsh.tn.edu.tw/mod/vpl/view.php?id=5183">[VPL 3.1] Create your own classifier (最短距離分類器)</a></li>
</ul>
</div>
<div id="outline-container-org726a5bb" class="outline-4">
<h4 id="org726a5bb"><span class="section-number-4">2.4.1.</span> Numpy solution:</h4>
<div class="outline-text-4" id="text-2-4-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span><span style="color: #dcaeea;">a</span> = <span style="color: #c678dd;">list</span>(<span style="color: #c678dd;">map</span>(<span style="color: #c678dd;">float</span>, <span style="color: #c678dd;">input</span>().split()))
<span class="linenr"> 4: </span><span style="color: #dcaeea;">x</span>, <span style="color: #dcaeea;">y</span> = a[<span style="color: #da8548; font-weight: bold;">0</span>], a[<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">groupA</span> = np.array([[<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">6</span>], [<span style="color: #da8548; font-weight: bold;">5</span>, <span style="color: #da8548; font-weight: bold;">7</span>], [<span style="color: #da8548; font-weight: bold;">5</span>, <span style="color: #da8548; font-weight: bold;">8</span>], [<span style="color: #da8548; font-weight: bold;">5.8</span>, <span style="color: #da8548; font-weight: bold;">6</span>], [<span style="color: #da8548; font-weight: bold;">6</span>, <span style="color: #da8548; font-weight: bold;">6</span>], [<span style="color: #da8548; font-weight: bold;">6</span>, <span style="color: #da8548; font-weight: bold;">7</span>], [<span style="color: #da8548; font-weight: bold;">7</span>, <span style="color: #da8548; font-weight: bold;">5</span>], [<span style="color: #da8548; font-weight: bold;">7</span>, <span style="color: #da8548; font-weight: bold;">7</span>], [<span style="color: #da8548; font-weight: bold;">8</span>, <span style="color: #da8548; font-weight: bold;">4</span>], [<span style="color: #da8548; font-weight: bold;">9</span>, <span style="color: #da8548; font-weight: bold;">5</span>]])
<span class="linenr"> 7: </span><span style="color: #dcaeea;">groupB</span> = np.array([[<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>], [<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">2</span>], [<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">4</span>], [<span style="color: #da8548; font-weight: bold;">5</span>, <span style="color: #da8548; font-weight: bold;">4</span>], [<span style="color: #da8548; font-weight: bold;">5</span>, <span style="color: #da8548; font-weight: bold;">3</span>], [<span style="color: #da8548; font-weight: bold;">6</span>, <span style="color: #da8548; font-weight: bold;">2</span>]])
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35336;&#31639;&#20841;&#32068;&#30340;&#20013;&#24515;&#40670;</span>
<span class="linenr">10: </span><span style="color: #dcaeea;">xa</span>, <span style="color: #dcaeea;">ya</span> = np.mean(groupA, axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">11: </span><span style="color: #dcaeea;">xb</span>, <span style="color: #dcaeea;">yb</span> = np.mean(groupB, axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">12: </span>
<span class="linenr">13: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35336;&#31639;&#36664;&#20837;&#40670;&#21040;&#20841;&#32068;&#20013;&#24515;&#40670;&#30340;&#36317;&#38626;</span>
<span class="linenr">14: </span><span style="color: #dcaeea;">distanceA</span> = (xa - x) ** <span style="color: #da8548; font-weight: bold;">2</span> + (ya - y) ** <span style="color: #da8548; font-weight: bold;">2</span>
<span class="linenr">15: </span><span style="color: #dcaeea;">distanceB</span> = (xb - x) ** <span style="color: #da8548; font-weight: bold;">2</span> + (yb - y) ** <span style="color: #da8548; font-weight: bold;">2</span>
<span class="linenr">16: </span>
<span class="linenr">17: </span><span style="color: #51afef;">if</span> distanceA &gt; distanceB:
<span class="linenr">18: </span>    <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'B'</span>)
<span class="linenr">19: </span><span style="color: #51afef;">else</span>:
<span class="linenr">20: </span>    <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'A'</span>)
</pre>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-IRIS-KNN" class="outline-2">
<h2 id="IRIS-KNN"><span class="section-number-2">3.</span> KNN 分類器:IRIS</h2>
<div class="outline-text-2" id="text-IRIS-KNN">
<p>
K-Nearest Neighbor 分類算法由 Cover 和 Hart 在 1968 年提出，想法是：如果一個樣本在特徵空間中的 k 個最相鄰的樣本中的大多數屬於某一個類別，則該樣本也屬於這個類別，並具有這個類別上樣本的特性。步驟如下：
</p>
<ol class="org-ol">
<li>選定 k 的值和一個「距離度量」(distance metric)。</li>
<li>找出 k 個想要分類的、最相近的鄰近樣本。</li>
<li>以多數決的方式指定類別標籤。</li>
</ol>
</div>
<div id="outline-container-orgbb09716" class="outline-3">
<h3 id="orgbb09716"><span class="section-number-3">3.1.</span> [實作]鳶尾花分類&#xa0;&#xa0;&#xa0;<span class="tag"><span class="sklearn">sklearn</span></span></h3>
<div class="outline-text-3" id="text-3-1">
</div>
<div id="outline-container-orgab429b8" class="outline-4">
<h4 id="orgab429b8"><span class="section-number-4">3.1.1.</span> DataSet</h4>
<div class="outline-text-4" id="text-3-1-1">
<p>
收集了 3 種鳶尾花的四個特徵，分別是花萼(sepal)長度、寬度、花瓣(petal)長度、寬度，以及對應的鳶尾花種類。
</p>

<div id="org0f593d0" class="figure">
<p><img src="images/iris-1.png" alt="iris-1.png" width="300" />
</p>
<p><span class="figure-number">Figure 7: </span>鳶尾花的花萼與花瓣</p>
</div>
</div>
</div>
<div id="outline-container-org71a059a" class="outline-4">
<h4 id="org71a059a"><span class="section-number-4">3.1.2.</span> Mission</h4>
<div class="outline-text-4" id="text-3-1-2">
<p>
輸入花萼和花瓣資料後，推測所屬的鳶尾花類型。
</p>

<div id="org2a8c076" class="figure">
<p><img src="images/iris-2.png" alt="iris-2.png" width="600" />
</p>
<p><span class="figure-number">Figure 8: </span>三種鳶尾花</p>
</div>
</div>
</div>
<div id="outline-container-org8cf1dd6" class="outline-4">
<h4 id="org8cf1dd6"><span class="section-number-4">3.1.3.</span> 實作</h4>
<div class="outline-text-4" id="text-3-1-3">
</div>
<div id="outline-container-org830b302" class="outline-5">
<h5 id="org830b302"><span class="section-number-5">3.1.3.1.</span> 讀取資料集</h5>
<div class="outline-text-5" id="text-3-1-3-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> datasets
<span class="linenr">2: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#36039;&#26009;</span>
<span class="linenr">3: </span><span style="color: #dcaeea;">iris</span> = datasets.load_iris()
<span class="linenr">4: </span><span style="color: #c678dd;">print</span>(iris.DESCR)
</pre>
</div>
</div>
<ol class="org-ol">
<li><a id="org654537e"></a>資料集作者: Fisher<br />
<div class="outline-text-6" id="text-3-1-3-1-1">

<div id="orga927377" class="figure">
<p><img src="images/KNN分類器/2024-02-05_16-44-49_2024-02-05_16-44-34.png" alt="2024-02-05_16-44-49_2024-02-05_16-44-34.png" width="200" />
</p>
<p><span class="figure-number">Figure 9: </span>Sir R.A. Fisher</p>
</div>

<p>
羅納德·愛爾默·費雪爵士，FRS（英語：Sir Ronald Aylmer Fisher，1890 年 2 月 17 日—1962 年 7 月 29 日，英語發音[ˈɹɒnḷd ˈeɪlmə ˈfɪʃə]），英國統計學家、演化生物學家與遺傳學家。他是現代統計學與現代演化論的奠基者之一。安德斯·哈爾德稱他是「一位幾乎獨自建立現代統計科學的天才」[5]，理查·道金斯則認為他是「達爾文最偉大的繼承者」<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>。
</p>

<p>
The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fisher&rsquo;s paper. Note that it&rsquo;s the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points.
</p>
</div>
<ul class="org-ul">
<li><a id="org98af060"></a>Fisher 的學術成就<br />
<div class="outline-text-7" id="text-org98af060">
<ul class="org-ul">
<li>共變數分析</li>
<li>最大概似估計法</li>
<li>p 值的概念: 提出以 0.05（即 20 次實驗中有 1 次發生）為統計顯著性的臨界值，並應用於基於常態分布的雙尾檢定而形成日後所謂「兩個標準差法則」<sup><a id="fnr.2.100" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>。</li>
<li>實驗設計法</li>
<li>現代統計學之父</li>
</ul>
</div>
</li>
</ul>
</li>
<li><a id="org424e485"></a>紅茶與牛奶的沖泡順序<sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup><br />
<div class="outline-text-6" id="text-3-1-3-1-2">

<div id="orgee7497f" class="figure">
<p><img src="images/KNN分類器/2024-02-05_17-17-11_2024-02-05_17-16-52.png" alt="2024-02-05_17-17-11_2024-02-05_17-16-52.png" width="500" />
</p>
<p><span class="figure-number">Figure 10: </span>Tea time</p>
</div>
<ul class="org-ul">
<li>時間: 1920 年代的劍橋大學，某個風和日麗的夏天下午，一群人優閒地享受下午茶時光。</li>
<li><p>
起因: 有位女士(生物學家 Muriel Bristol<sup><a id="fnr.4" class="footref" href="#fn.4" role="doc-backlink">4</a></sup>)說：「沖泡的順序對於奶茶的風味影響很大，把茶加進牛奶裡和把牛奶加進茶裡，這兩種沖泡方式所泡出的奶茶口味截然不同。」，而且她有能嚐出二者差異的超能力。Fisher 說: “林北聽妳在唬爛(That’s impossible.)&ldquo;
</p>

<div id="org76dc922" class="figure">
<p><img src="images/KNN分類器/2024-02-05_17-05-52_2024-02-05_17-05-32.png" alt="2024-02-05_17-05-52_2024-02-05_17-05-32.png" width="200" />
</p>
<p><span class="figure-number">Figure 11: </span>Muriel Bristol</p>
</div></li>
<li>推手: 化學家 William Roach 為了拍 Muriel Bristol 馬屁，建議做個實驗</li>
<li>結果: 現代統計學誕生</li>
<li>問題: 如果你是 Fisher，你要怎麼證明 Bristol 是對的或錯的? 實際泡茶? 怎麼泡? 要泡幾杯、讓 Bristol 猜幾杯才能確定 Bristol 的超能力。</li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org0121919" class="outline-5">
<h5 id="org0121919"><span class="section-number-5">3.1.3.2.</span> 取出特徵與標籤</h5>
<div class="outline-text-5" id="text-3-1-3-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> datasets
<span class="linenr">2: </span>
<span class="linenr">3: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#36039;&#26009;</span>
<span class="linenr">4: </span><span style="color: #dcaeea;">iris</span> = datasets.load_iris()
<span class="linenr">5: </span>
<span class="linenr">6: </span><span style="color: #dcaeea;">x</span> = iris.data
<span class="linenr">7: </span><span style="color: #dcaeea;">y</span> = iris.target
<span class="linenr">8: </span><span style="color: #c678dd;">print</span>(x[:<span style="color: #da8548; font-weight: bold;">5</span>])
<span class="linenr">9: </span><span style="color: #c678dd;">print</span>(y[:<span style="color: #da8548; font-weight: bold;">5</span>])
</pre>
</div>

<pre class="example">
[[5.1 3.5 1.4 0.2]
 [4.9 3.  1.4 0.2]
 [4.7 3.2 1.3 0.2]
 [4.6 3.1 1.5 0.2]
 [5.  3.6 1.4 0.2]]
[0 0 0 0 0]
</pre>
</div>
</div>
<div id="outline-container-org34783c7" class="outline-5">
<h5 id="org34783c7"><span class="section-number-5">3.1.3.3.</span> 資料觀察</h5>
<div class="outline-text-5" id="text-3-1-3-3">
<p>
先將資料視覺化，有助於我們對於資料分佈與預測結果先有一個概念。為了方便以二維圖形呈現結果，我們先隨意挑兩個特徵值來觀察一下。
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 3: </span><span style="color: #51afef;">import</span> seaborn <span style="color: #51afef;">as</span> sns
<span class="linenr"> 4: </span><span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> datasets
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#36039;&#26009;</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">iris</span> = datasets.load_iris()
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span><span style="color: #dcaeea;">x</span> = iris.data
<span class="linenr">10: </span><span style="color: #dcaeea;">y</span> = iris.target
<span class="linenr">11: </span>
<span class="linenr">12: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#25226;nupmy ndarray&#36681;&#28858;pandas dataFrame,&#21152;&#19978;columns title</span>
<span class="linenr">13: </span><span style="color: #dcaeea;">npx</span> = pd.DataFrame(x, columns=[<span style="color: #98be65;">'fac1'</span>,<span style="color: #98be65;">'fac2'</span>,<span style="color: #98be65;">'fac3'</span>,<span style="color: #98be65;">'fac4'</span>])
<span class="linenr">14: </span><span style="color: #dcaeea;">npy</span> = pd.DataFrame(y.astype(<span style="color: #c678dd;">int</span>), columns=[<span style="color: #98be65;">'category'</span>])
<span class="linenr">15: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#21512;&#20341;</span>
<span class="linenr">16: </span><span style="color: #dcaeea;">dataPD</span> = pd.concat([npx, npy], axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">17: </span><span style="color: #c678dd;">print</span>(dataPD)
<span class="linenr">18: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#22294;</span>
<span class="linenr">19: </span>sns.lmplot(x=<span style="color: #98be65;">'fac1'</span>, y=<span style="color: #98be65;">'fac2'</span>, data=dataPD, hue=<span style="color: #98be65;">'category'</span>, fit_reg=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">20: </span>plt.savefig(<span style="color: #98be65;">'images/irisdemo.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>

<pre class="example" id="org286fe2b">
     fac1  fac2  fac3  fac4  category
0     5.1   3.5   1.4   0.2         0
1     4.9   3.0   1.4   0.2         0
2     4.7   3.2   1.3   0.2         0
3     4.6   3.1   1.5   0.2         0
4     5.0   3.6   1.4   0.2         0
..    ...   ...   ...   ...       ...
145   6.7   3.0   5.2   2.3         2
146   6.3   2.5   5.0   1.9         2
147   6.5   3.0   5.2   2.0         2
148   6.2   3.4   5.4   2.3         2
149   5.9   3.0   5.1   1.8         2

[150 rows x 5 columns]
</pre>

<div id="orgab3c8f0" class="figure">
<p><img src="images/irisdemo.png" alt="irisdemo.png" width="500" />
</p>
<p><span class="figure-number">Figure 12: </span>Iris 不同特徵值與其類別的關係</p>
</div>
</div>
</div>
<div id="outline-container-orga117792" class="outline-5">
<h5 id="orga117792"><span class="section-number-5">3.1.3.4.</span> 分割資料集</h5>
<div class="outline-text-5" id="text-3-1-3-4">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr">2: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21123;&#20998;&#36039;&#26009;&#38598;</span>
<span class="linenr">3: </span><span style="color: #dcaeea;">x_train</span>, <span style="color: #dcaeea;">x_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(iris.data, iris.target, random_state=<span style="color: #da8548; font-weight: bold;">6</span>)
</pre>
</div>
</div>
<ol class="org-ol">
<li><a id="orgde116ed"></a>train_test_split()<br />
<div class="outline-text-6" id="text-3-1-3-4-1">
<p>
接受三個參數：原始的資料、Seed、比例
</p>
<ol class="org-ol">
<li>原始的資料：就如同上方的 data 一般，是我們打算切成 Training data 以及 Test data 的原始資料</li>
<li>Seed： 亂數種子，可以固定我們切割資料的結果</li>
<li>比例：可以設定 train_size 或 test_size，只要設定一邊即可，範圍在 [0-1] 之間</li>
</ol>
</div>
</li>
<li><a id="org6208cb2"></a>scikit-learn.org: sklearn.model_selection.train_test_split<br />
<div class="outline-text-6" id="text-3-1-3-4-2">
<ul class="org-ul">
<li>官網: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">sklearn.model_selection.train_test_split</a></li>
<li>Split arrays or matrices into random train and test subsets. Quick utility that wraps input validation and next(ShuffleSplit().split(X, y)) and application to input data into a single call for splitting (and optionally subsampling) data in a oneliner<sup><a id="fnr.5" class="footref" href="#fn.5" role="doc-backlink">5</a></sup>.</li>
<li>test_size: If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If train_size is also None, it will be set to 0.25<sup><a id="fnr.5.100" class="footref" href="#fn.5" role="doc-backlink">5</a></sup>.</li>
</ul>
<div class="org-src-container">
<pre class="src src-python">sklearn.model_selection.train_test_split(*arrays, test_size=<span style="color: #a9a1e1;">None</span>, train_size=<span style="color: #a9a1e1;">None</span>, random_state=<span style="color: #a9a1e1;">None</span>, shuffle=<span style="color: #a9a1e1;">True</span>, stratify=<span style="color: #a9a1e1;">None</span>)
</pre>
</div>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgd653f7f" class="outline-5">
<h5 id="orgd653f7f"><span class="section-number-5">3.1.3.5.</span> 資料標準化</h5>
<div class="outline-text-5" id="text-3-1-3-5">
<p>
利用 preprocessing 模組裡的 StandardScaler 類別將資料標準化
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr">2: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21033;&#29992;fit&#26041;&#27861;&#65292;&#23565;X_train&#20013;&#27599;&#20491;&#29305;&#24501;&#20540;&#20272;&#24179;&#22343;&#25976;&#21644;&#27161;&#28310;&#24046;</span>
<span class="linenr">3: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28982;&#24460;&#23565;&#27599;&#20491;&#29305;&#24501;&#20540;&#36914;&#34892;&#27161;&#28310;&#21270;(train&#21644;test&#37117;&#35201;&#20570;)</span>
<span class="linenr">4: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29305;&#24501;&#24037;&#31243;&#65306;&#27161;&#28310;&#21270;</span>
<span class="linenr">5: </span><span style="color: #dcaeea;">transfer</span> = StandardScaler()
<span class="linenr">6: </span><span style="color: #dcaeea;">x_train</span> = transfer.fit_transform(x_train)
<span class="linenr">7: </span><span style="color: #dcaeea;">x_test</span> = transfer.fit_transform(x_test)
</pre>
</div>
</div>
</div>
<div id="outline-container-orge42028a" class="outline-5">
<h5 id="orge42028a"><span class="section-number-5">3.1.3.6.</span> 訓練、評估模型效能</h5>
<div class="outline-text-5" id="text-3-1-3-6">
<ul class="org-ul">
<li>測試一下 k=2 的效能</li>
<li>以訓練集來訓練模型，然後以測試集來評估模型的準確性</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">from</span> sklearn.neighbors <span style="color: #51afef;">import</span> KNeighborsClassifier
<span class="linenr"> 2: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">KNN &#20998;&#39006;&#22120;</span>
<span class="linenr"> 3: </span><span style="color: #dcaeea;">estimator</span> = KNeighborsClassifier(n_neighbors=<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr"> 4: </span>estimator.fit(x_train, y_train)
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27169;&#22411;&#35413;&#20272;</span>
<span class="linenr"> 7: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26041;&#27861;&#19968;&#65306;&#30452;&#25509;&#23565;&#27604;&#30495;&#23526;&#20540;&#21644;&#38928;&#28204;&#20540;</span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">y_predict</span> = estimator.predict(x_test)
<span class="linenr"> 9: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'y_predict&#65306;</span><span style="color: #a9a1e1;">\n</span><span style="color: #98be65;">'</span>, y_predict)
<span class="linenr">10: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#30452;&#25509;&#23565;&#27604;&#30495;&#23526;&#20540;&#21644;&#38928;&#28204;&#20540;:</span><span style="color: #a9a1e1;">\n</span><span style="color: #98be65;">'</span>, y_test == y_predict)
<span class="linenr">11: </span>
<span class="linenr">12: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26041;&#27861;&#20108;&#65306;&#35336;&#31639;&#28310;&#30906;&#29575;</span>
<span class="linenr">13: </span><span style="color: #dcaeea;">score</span> = estimator.score(x_test, y_test)
<span class="linenr">14: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#28310;&#30906;&#29575;:</span><span style="color: #a9a1e1;">\n</span><span style="color: #98be65;">'</span>, score)
</pre>
</div>

<pre class="example" id="org8f1d2ff">
y_predict：
 [0 2 0 0 2 1 1 0 2 1 1 1 2 2 1 1 2 1 1 0 0 2 0 0 1 1 1 2 0 1 0 1 0 0 1 2 1
 2]
直接對比真實值和預測值:
 [ True  True  True  True  True  True False  True  True  True False  True
  True  True  True False  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True False  True
  True  True]
準確率:
 0.8947368421052632
</pre>
</div>
</div>
</div>
<div id="outline-container-org5226bf0" class="outline-4">
<h4 id="org5226bf0"><span class="section-number-4">3.1.4.</span> 關於得分</h4>
<div class="outline-text-4" id="text-3-1-4">
<ul class="org-ul">
<li>estimator.score()是用來計算模型在測試數據上的準確率，表示模型預測正確的比例，取值範圍在 0 到 1 之間。</li>
<li>一般來說，0.7（70%） 以上的準確率就算是不錯，但具體標準取決於任務難度和數據集特性。</li>
<li>如果數據集不平衡（例如，某個類別佔了大多數），即使高於 0.7 也可能不一定是好結果，這時需要結合其他指標如精確率（Precision）或召回率（Recall）來評估模型。</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org40573c3" class="outline-3">
<h3 id="org40573c3"><span class="section-number-3">3.2.</span> [作業]手刻 KNN 分類器&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></h3>
<div class="outline-text-3" id="text-3-2">
<ul class="org-ul">
<li><a href="https://moodle.tnfsh.tn.edu.tw/mod/vpl/view.php?id=4394">[VPL 3.2] Create your own classifier (KNN)</a></li>
</ul>
</div>
</div>
<div id="outline-container-org709d1fe" class="outline-3">
<h3 id="org709d1fe"><span class="section-number-3">3.3.</span> [作業]分析 K 值對模型效能的影響&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></h3>
<div class="outline-text-3" id="text-3-3">
<p>
修改KNN分類器:IRIS程式碼,完成以下任務
</p>
<ol class="org-ol">
<li>以不同的K值進行KNN預測</li>
<li>以折線圖表示K值與KNN預測準確度間的關係</li>
<li>哪一種K值的預測準確度最高?請自行想辦法在圖上標示出準確度最高的K值</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-orgc4d8600" class="outline-2">
<h2 id="orgc4d8600"><span class="section-number-2">4.</span> 決策樹分類器</h2>
<div class="outline-text-2" id="text-4">

<div id="orgc386ae2" class="figure">
<p><img src="images/SBQNWUA1dDtFsMHv.png" alt="SBQNWUA1dDtFsMHv.png" width="600" />
</p>
<p><span class="figure-number">Figure 13: </span>一棵複雜的決策樹</p>
</div>
</div>
<div id="outline-container-orgfca22a1" class="outline-3">
<h3 id="orgfca22a1"><span class="section-number-3">4.1.</span> 原理</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li>「決策樹」是一種條件式的分類器，可視為專門處理分類問題的樹狀結構。</li>
<li>依現有資料進行建構，通常採用「由上而下」的方式，將整群資料從某個特徵開始，根據該特徵的值分為數個子群，各個子群再根據某個特徵將子群再分為更小的子群，直到子群內的資料都是同一個類別為止。</li>
</ul>
</div>
<div id="outline-container-org0a3db24" class="outline-4">
<h4 id="org0a3db24"><span class="section-number-4">4.1.1.</span> XXX, 出來打球</h4>
<div class="outline-text-4" id="text-4-1-1">

<div id="org11721e0" class="figure">
<p><img src="images/決策樹分類器/2024-02-06_15-20-16_2024-02-06_15-18-05.png" alt="2024-02-06_15-20-16_2024-02-06_15-18-05.png" width="500" />
</p>
<p><span class="figure-number">Figure 14: </span>影響打球的天候因素</p>
</div>

<p>
圖<a href="#org11721e0">14</a>為某間高爾夫俱樂部決定是否開放球場的因素，包括：
</p>
<ul class="org-ul">
<li>outlook：天象</li>
<li>overcast：陰天</li>
<li>humidity：濕度</li>
<li>windy：颳風</li>
</ul>

<p>
如何建立一個決策系統來協助業者自動判斷？我們可以建立一棵如下的決策樹：
</p>


<div id="org01ad81f" class="figure">
<p><img src="images/決策樹分類器/2024-02-06_15-23-47_2024-02-06_15-23-13.png" alt="2024-02-06_15-23-47_2024-02-06_15-23-13.png" width="500" />
</p>
<p><span class="figure-number">Figure 15: </span>目標決策樹</p>
</div>
</div>
</div>
<div id="outline-container-orgc26c2b9" class="outline-4">
<h4 id="orgc26c2b9"><span class="section-number-4">4.1.2.</span> 誰會買筆電: 一個較簡單的例子</h4>
<div class="outline-text-4" id="text-4-1-2">
<p>
現在就讓我們化身為 3C 賣場的員工，針對消費者是否購買筆記型電腦的消費記錄來建構出一棵決策樹吧！
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-center">年紀</th>
<th scope="col" class="org-center">收入</th>
<th scope="col" class="org-center">學生與否</th>
<th scope="col" class="org-center">購買筆電與否</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-center">&lt;=30</td>
<td class="org-center">高</td>
<td class="org-center">否</td>
<td class="org-center">否</td>
</tr>

<tr>
<td class="org-center">31…40</td>
<td class="org-center">高</td>
<td class="org-center">否</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">&gt;40</td>
<td class="org-center">中</td>
<td class="org-center">否</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">&gt;40</td>
<td class="org-center">低</td>
<td class="org-center">是</td>
<td class="org-center">否</td>
</tr>

<tr>
<td class="org-center">31…40</td>
<td class="org-center">低</td>
<td class="org-center">是</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">&lt;=30</td>
<td class="org-center">中</td>
<td class="org-center">否</td>
<td class="org-center">否</td>
</tr>

<tr>
<td class="org-center">&lt;=30</td>
<td class="org-center">低</td>
<td class="org-center">是</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">&lt;=30</td>
<td class="org-center">中</td>
<td class="org-center">是</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">31…40</td>
<td class="org-center">中</td>
<td class="org-center">否</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">31…40</td>
<td class="org-center">高</td>
<td class="org-center">是</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">&gt;40</td>
<td class="org-center">中</td>
<td class="org-center">是</td>
<td class="org-center">否</td>
</tr>
</tbody>
</table>
</div>
<div id="outline-container-orgfd260ce" class="outline-5">
<h5 id="orgfd260ce"><span class="section-number-5">4.1.2.1.</span> 第一步：決定要先用哪一項特徵值</h5>
<div class="outline-text-5" id="text-4-1-2-1">
<p>
依據「是否購買筆記型電腦」的結果，到底要用哪一個特徵值來當成樹根?
</p>
<ul class="org-ul">
<li>年紀：&lt;=30、30…40、&gt;=40。</li>
<li>收入：低、中、高。</li>
<li>學生與否：是、否。</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org485feba"></a>作法 1: 暴力解決<br />
<div class="outline-text-6" id="text-4-1-2-1-1">
<p>
將每個特徵值都當作分類的條件一一去建構決策樹，彷彿以列舉的方式將所有的特徵值排列組合出很多的決策樹，再一一分析哪一種可以有「最好的分類結果」。
</p>
</div>
</li>
<li><a id="org2b7fd88"></a>作法 2: 優雅策略<br />
<div class="outline-text-6" id="text-4-1-2-1-2">
<p>
希望越優先選擇的條件，越能有效率地將訓練資料較為明顯地分類成幾個類別，如此便能減少判斷條件的次數，也能讓決策樹較為精簡有效率。
數學上常用來定義分類優劣程度的指標
</p>
<ul class="org-ul">
<li>Gini index</li>
<li>Information Gain: Entropy</li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgd175572" class="outline-5">
<h5 id="orgd175572"><span class="section-number-5">4.1.2.2.</span> Gini Index</h5>
<div class="outline-text-5" id="text-4-1-2-2">
<p>
Gini Index 的實作演算法是 CART tree。CART 是 Classification And Regression Tree 的縮寫，因此兼具分類與迴歸兩種功能。
</p>

<p>
Gini Index 與 Information Gain 的差別：
</p>
<ul class="org-ul">
<li>Information Gain:一次可產生多個不同節點，</li>
<li>Gini Index:一次僅能產生兩個，即 True 或 False 的 Binary 的二元分類樹。</li>
</ul>

<p>
如下圖我們想從 30 位學生中找出有打板球 15 位學生，左圖用性別做區分右圖用班級做區分。很合理的猜測男生一定是比較喜歡運動用性別應該有不錯效果，而用班級區分除非有體育班否則兩分類結果應該雷同。
</p>


<div id="org9dd4cc4" class="figure">
<p><img src="images/決策樹分類器/2024-02-06_15-45-00_2024-02-06_15-44-47.png" alt="2024-02-06_15-45-00_2024-02-06_15-44-47.png" width="500" />
</p>
<p><span class="figure-number">Figure 16: </span>Gini Index</p>
</div>

<p>
這兩種分法哪一種比較好?我們可以來算一算每一種分法的 Gini index。
</p>
<ul class="org-ul">
<li>Gini 公式: \( Gini(S) = \sum\limits_{j=1}^{n}p^2_j \)</li>
<li>若依特徵值 A 分割資料集合 S 為 S1 與 S2，則計算式為: \( Gini_A(S) = \frac{|S_1|}{|S|}Gini(S_1)+\frac{|S_2|}{|S|}Gini(S_2) \)</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org9c7834e"></a>用性別分類(圖<a href="#org9dd4cc4">16</a>左)<br />
<div class="outline-text-6" id="text-4-1-2-2-1">
<ul class="org-ul">
<li>Female 節點：10 位女性，其中有 2 位打板球 8 位不打，Gini 係數為 \((\frac{2}{10})^2+(\frac{8}{10})^2=0.68\)</li>
<li>Male 節點：20 位男性，其中有 13 位打板球 7 位不打，Gini 係數為 \((\frac{13}{20})^2+(\frac{7}{20})^2=0.55\)</li>
<li>因此以性別分類的 Gini 係數加權後為：\(\frac{10}{30}*0.68+\frac{20}{30}*0.55=0.59\)</li>
</ul>
</div>
</li>
<li><a id="org8b2b615"></a>用班級分類<br />
<div class="outline-text-6" id="text-4-1-2-2-2">
<ul class="org-ul">
<li>Class IX 節點：此班 14 位同學，其中 6 位打板球 8 位不打，因此 Gini 係數為 \((\frac{6}{14})^2+(\frac{8}{14})^2=0.51\)</li>
<li>Class X 節點：此班 16 位同學，其中 9 位打板球 7 位不打，因此 Gini 係數為\((\frac{9}{16})^2+(\frac{7}{16})^2=0.51\)</li>
</ul>
<p>
因此以班級分類的決策樹，其 Gini 係數加權結果: \(\frac{14}{30}*0.51+\frac{16}{30}*0.51=0.51\)
</p>
</div>
</li>
<li><a id="orge3678e3"></a>結果<br />
<div class="outline-text-6" id="text-4-1-2-2-3">
<p>
兩樹相互比較(性別:0.59/班級:0.51)分類，因此系統會採用性別來進行節點的分類。
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgbb11e85" class="outline-5">
<h5 id="orgbb11e85"><span class="section-number-5">4.1.2.3.</span> Entropy</h5>
<div class="outline-text-5" id="text-4-1-2-3">
<p>
熵(entropy)原本是物理學概念，代表事物的混亂程度：熵愈高、事物愈混亂。資訊理論(Information Theory)之父夏農(Claude Shannon)於 1948 年將熵引入電腦科學，成為代表資訊量的量度<sup><a id="fnr.6" class="footref" href="#fn.6" role="doc-backlink">6</a></sup>，用來衡量一組資料的不確定性(uncertainty)，因此又名為夏農熵(Shannon)。
</p>


<div id="orgb305489" class="figure">
<p><img src="images/決策樹分類器/2024-02-06_22-20-46_2024-02-06_22-20-36.png" alt="2024-02-06_22-20-46_2024-02-06_22-20-36.png" width="500" />
</p>
<p><span class="figure-number">Figure 17: </span>幾種不同的熵值</p>
</div>

<p>
資訊熵(information entropy)的概念也很簡單，就是熵愈高，資訊愈多。也就是愈混亂，資訊愈多<sup><a id="fnr.6.100" class="footref" href="#fn.6" role="doc-backlink">6</a></sup>。
</p>

<p>
當所有資料的類型都是一致時，得到的熵為 0(如圖<a href="#orgb305489">17</a>右)，但如果資料類型各自對半呈現差異時，熵為 1(如圖<a href="#orgb305489">17</a>左)。
</p>

<p>
另一種解讀 Entropy 的方式為將其視為度量資訊量的單位：
</p>
<ul class="org-ul">
<li>度量距離: cm/meter</li>
<li>度量時間: sec/min</li>
<li>度量資訊: 資訊量，資訊量指的是「如果我們要度量一個未知事物，那麼我們需要查詢的資訊有多少」，單位是位元。</li>
</ul>

<p>
再以丟硬幣為例：
</p>
<ul class="org-ul">
<li>1 枚硬幣 2 面都是人頭，結果確定，無不確定性，熵值為 0 。</li>
<li>1 枚正常硬幣，有 50%的機率猜中投擲結果，熵值為 1。</li>
<li>2 枚正常硬幣，有 33%的機率猜中其投擲結果(正正、反反、正反)，不確定性&gt;1 枚硬幣，其熵值約為 1.58 。</li>
</ul>
<p>
隨著不確定性增加，熵值也會增加。
</p>
</div>
<ol class="org-ol">
<li><a id="org9178fba"></a>賭馬<br />
<div class="outline-text-6" id="text-4-1-2-3-1">
<p>
我們再來看一個更複雜點的例子(例子取自<a href="https://www.twblogs.net/a/5b8ed2a52b71771883480414">白話信息熵</a>)，這是一場有 8 匹賽馬的比賽，馬匹編號為 1~8 號，每匹馬贏的概率都一樣，那麼你需要獲得多少資訊（也就是猜測多少次）才可以知道哪匹馬獲勝？<sup><a id="fnr.7" class="footref" href="#fn.7" role="doc-backlink">7</a></sup>
</p>
</div>
<ul class="org-ul">
<li><a id="orgdef362a"></a>機率相同<br />
<div class="outline-text-7" id="text-orgdef362a">

<div id="org5e8014c" class="figure">
<p><img src="images/決策樹分類器/2024-02-06_22-30-12_2024-02-06_22-30-05.png" alt="2024-02-06_22-30-12_2024-02-06_22-30-05.png" width="500" />
</p>
<p><span class="figure-number">Figure 18: </span>機率相等下的熵</p>
</div>

<p>
我們先假設每匹馬獲勝的機率都一樣，如圖<a href="#org5e8014c">18</a>利用二分法，那麼只要猜測三次就可以找到所需資訊，需要的資訊量: 3 位元。
</p>
</div>
</li>
<li><a id="orgfdeec96"></a>機率不同<br />
<div class="outline-text-7" id="text-orgfdeec96">

<div id="org9df8921" class="figure">
<p><img src="images/決策樹分類器/2024-02-07_10-32-08_2024-02-07_10-32-02.png" alt="2024-02-07_10-32-08_2024-02-07_10-32-02.png" width="500" />
</p>
<p><span class="figure-number">Figure 19: </span>機率相等下的熵</p>
</div>

<p>
如果每匹馬的實力都不一樣，各自的獲勝機率為\(\frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{16}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64}\)，那需要多少資訊量呢？
</p>
</div>
<ul class="org-ul">
<li><a id="org2db429c"></a>猜測 i 號獲勝所需信息量<br />
<div class="outline-text-8" id="text-org2db429c">
<ul class="org-ul">
<li>假設第 i 號馬獲勝的概率為\(\frac{1}{p_i}\)</li>
<li>猜測 i 號獲勝所需訊息量: \(I_i=\log_2(\frac{1}{p_i})=-\log_2p_i\)</li>
</ul>
</div>
</li>
<li><a id="org9f0739b"></a>猜測 6 號獲勝所需信息量<br />
<div class="outline-text-8" id="text-org9f0739b">
<ul class="org-ul">
<li>6 號獲勝的概率為\(\frac{1}{64}\)</li>
<li>所需訊息量: \(I_6=-\log_2(p_6)=\log_264=6 bits (2^6=64) \)</li>
</ul>
</div>
</li>
</ul>
</li>
<li><a id="orge1bc504"></a>熵<br />
<div class="outline-text-7" id="text-orge1bc504">
<p>
可將之視為對所需訊息量的期望值
</p>
<ul class="org-ul">
<li>1~8 號獲勝的概率分別為\(\frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{16}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64}\)</li>
<li>猜測 1~8 號馬匹獲勝至少要猜測的次數分別為 1、2、3、4、6、6、6、6</li>
<li>\(\frac{1}{2}\times1 + \frac{1}{4}\times2 + \frac{1}{8}\times3 + \frac{1}{16}\times4 + \frac{1}{64}\times6 + \frac{1}{64}\times6 + \frac{1}{64}\times6 + \frac{1}{64}\times6 = 2\)</li>
<li>也就是猜測出獲勝的馬匹，平均需要 2 次。</li>
</ul>
</div>
</li>
</ul>
</li>
<li><a id="org25dac82"></a>熵的計算公式<br />
<div class="outline-text-6" id="text-4-1-2-3-2">
<p>
熵: 資訊量的期望值 \(H(X)=-\sum_{i=1}^{n}P(x_i)\times\log_2 P(x_i) \)
參考上例，以 X 為賽馬編號，則 x 為
</p>
\begin{align*}
H(X) &= -\frac{1}{2}\times\log_2{\frac{1}{2}} -\frac{1}{4}\times\log_2{\frac{1}{4}} \\
       &--\frac{1}{8}\times\log_2{\frac{1}{8}} -\frac{1}{16}\times\log_2{\frac{1}{16}} \\
       &-\frac{1}{64}\times\log_2{\frac{1}{64}} -\frac{1}{64}\times\log_2{\frac{1}{64}}\\
       &-\frac{1}{64}\times\log_2{\frac{1}{64}}-\frac{1}{64}\times\log_2{\frac{1}{64}}\\
     &=2
\end{align*}
<p>
也就是說：
\( Info(D) = -\sum\limits_{i=1}^{m}p_i\times\log_2 p_i \)，其中
</p>
<ul class="org-ul">
<li>D 代表某一個資料集，而這個特徵值會有 1 到 m 種類別，p就是某個類別在這個特徵值中出現的機率。</li>
<li>另外在取對數時一般會以 2 為底，源自於資訊的編碼大多是以 0/1 二進位的方式編碼。</li>
<li>\(p_i\times\log_2 p_i\)的數學意義：某資料出現的機率越低，在 log 的加成下該數值會提高，該數值可以代表人們看到這個資料的驚訝(surprise)程度。</li>
<li>例如 1 組數據平常數值都是 1 或 2，但是某一天卻異常出現 99 時，我們就會感到驚訝。</li>
<li>當越多令人驚訝的情況出現，整體的不確定性/混亂程度就會提高。</li>
</ul>
</div>
</li>
<li><a id="orgfc923c3"></a>文字的亂度<br />
<div class="outline-text-6" id="text-4-1-2-3-3">
<p>
底下這兩個句子：
</p>
<blockquote>
<p>
the quick brown fox jumps over the lazy dog
</p>
</blockquote>
<p>
與
</p>
<blockquote>
<p>
don&rsquo;t trouble trouble till trouble troubles you
</p>
</blockquote>
<p>
哪一個的熵值更高？
</p>

<p>
其資訊熵的計算方式就是：將每個字母的概率與其概率之自然對數相乘，再將每個字母的結果相加，相加之和的負數。可以 python 計算如下<sup><a id="fnr.8" class="footref" href="#fn.8" role="doc-backlink">8</a></sup>：
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> math
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> string
<span class="linenr"> 3: </span><span style="color: #51afef;">import</span> sys
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">shannon_entropy</span>(data):
<span class="linenr"> 6: </span>    <span style="color: #83898d;">"""</span>
<span class="linenr"> 7: </span><span style="color: #83898d;">    Adapted from http://blog.dkbza.org/2007/05/scanning-data-for-entropy-anomalies.html</span>
<span class="linenr"> 8: </span><span style="color: #83898d;">    by way of truffleHog (https://github.com/dxa4481/truffleHog)</span>
<span class="linenr"> 9: </span><span style="color: #83898d;">    """</span>
<span class="linenr">10: </span>    <span style="color: #51afef;">if</span> <span style="color: #51afef;">not</span> data:
<span class="linenr">11: </span>        <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">12: </span>    <span style="color: #dcaeea;">entropy</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">13: </span>    <span style="color: #51afef;">for</span> x <span style="color: #51afef;">in</span> string.printable:
<span class="linenr">14: </span>        <span style="color: #dcaeea;">p_x</span> = <span style="color: #c678dd;">float</span>(data.count(x)) / <span style="color: #c678dd;">len</span>(data)
<span class="linenr">15: </span>        <span style="color: #51afef;">if</span> p_x &gt; <span style="color: #da8548; font-weight: bold;">0</span>:
<span class="linenr">16: </span>            <span style="color: #dcaeea;">entropy</span> += - p_x * math.log(p_x, <span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">17: </span>    <span style="color: #51afef;">return</span> entropy
<span class="linenr">18: </span>
<span class="linenr">19: </span><span style="color: #dcaeea;">sentence1</span> = <span style="color: #98be65;">'the quick brown fox jumps over the lazy dog'</span>
<span class="linenr">20: </span><span style="color: #dcaeea;">sentence2</span> = <span style="color: #98be65;">"don't trouble trouble till trouble troubles you"</span>
<span class="linenr">21: </span><span style="color: #c678dd;">print</span>(shannon_entropy(sentence1))
<span class="linenr">22: </span><span style="color: #c678dd;">print</span>(shannon_entropy(sentence2))
</pre>
</div>

<pre class="example">
4.385453417442482
3.47695607525754
</pre>
</div>
</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-orgf90af4c" class="outline-3">
<h3 id="orgf90af4c"><span class="section-number-3">4.2.</span> 動手計算</h3>
<div class="outline-text-3" id="text-4-2">
<p>
讓我們再回到「誰會買筆電」的例子，以 Entropy 來決定還沒完成的第一步：決定要先用哪一項特徵值。這就需要逐一來計算了，其中要用到這個表的資訊，用它來計算幾個熵值。
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-center">編號</th>
<th scope="col" class="org-center">年紀</th>
<th scope="col" class="org-center">收入</th>
<th scope="col" class="org-center">學生與否</th>
<th scope="col" class="org-center">購買筆電與否</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-center">1</td>
<td class="org-center">&lt;=30</td>
<td class="org-center">高</td>
<td class="org-center">否</td>
<td class="org-center">否</td>
</tr>

<tr>
<td class="org-center">2</td>
<td class="org-center">31…40</td>
<td class="org-center">高</td>
<td class="org-center">否</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">3</td>
<td class="org-center">&gt;40</td>
<td class="org-center">中</td>
<td class="org-center">否</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">4</td>
<td class="org-center">&gt;40</td>
<td class="org-center">低</td>
<td class="org-center">是</td>
<td class="org-center">否</td>
</tr>

<tr>
<td class="org-center">5</td>
<td class="org-center">31…40</td>
<td class="org-center">低</td>
<td class="org-center">是</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">6</td>
<td class="org-center">&lt;=30</td>
<td class="org-center">中</td>
<td class="org-center">否</td>
<td class="org-center">否</td>
</tr>

<tr>
<td class="org-center">7</td>
<td class="org-center">&lt;=30</td>
<td class="org-center">低</td>
<td class="org-center">是</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">8</td>
<td class="org-center">&lt;=30</td>
<td class="org-center">中</td>
<td class="org-center">是</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">9</td>
<td class="org-center">31…40</td>
<td class="org-center">中</td>
<td class="org-center">否</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">10</td>
<td class="org-center">31…40</td>
<td class="org-center">高</td>
<td class="org-center">是</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">11</td>
<td class="org-center">&gt;40</td>
<td class="org-center">中</td>
<td class="org-center">是</td>
<td class="org-center">否</td>
</tr>
</tbody>
</table>
</div>
<div id="outline-container-orgc71e4d7" class="outline-4">
<h4 id="orgc71e4d7"><span class="section-number-4">4.2.1.</span> 決定第一層</h4>
<div class="outline-text-4" id="text-4-2-1">
</div>
<div id="outline-container-orgd0952e3" class="outline-5">
<h5 id="orgd0952e3"><span class="section-number-5">4.2.1.1.</span> 計算購買筆電與否的熵值</h5>
<div class="outline-text-5" id="text-4-2-1-1">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-center">購買筆電與否</th>
<th scope="col" class="org-center">出現次數</th>
<th scope="col" class="org-center">\(p_i\)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-center">是</td>
<td class="org-center">7</td>
<td class="org-center">\(\frac{7}{11}\)</td>
</tr>

<tr>
<td class="org-center">否</td>
<td class="org-center">4</td>
<td class="org-center">\(\frac{4}{11}\)</td>
</tr>
</tbody>
</table>
<p>
根據\( Info(D) = -\sum\limits_{i=1}^{m}p_i\times\log_2 p_i \)，是否購買(Buy)的熵值為：
</p>
\begin{align*}
Info(Buy) &= I(7,4) \\
          &= -\frac{7}{11}\log_2\frac{7}{11}-\frac{4}{11}\log_2\frac{4}{11} \\
          &= 0.425+0.531 \\
          &=0.956
\end{align*}
</div>
</div>
<div id="outline-container-org5b8203a" class="outline-5">
<h5 id="org5b8203a"><span class="section-number-5">4.2.1.2.</span> 計算不同特徵值的熵值</h5>
<div class="outline-text-5" id="text-4-2-1-2">
<p>
現在我們可以來比較選擇不同特徵值當成樹根的優劣了，我們有以下三種特徵值可以來判斷「購買筆電與否」這個結果：
</p>
<ul class="org-ul">
<li>年紀 v.s. 購買筆電與否</li>
<li>收入 v.s. 購買筆電與否</li>
<li>學生與否 v.s. 購買筆電與否</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org84236c9"></a>年紀<br />
<div class="outline-text-6" id="text-4-2-1-2-1">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-center">年紀</th>
<th scope="col" class="org-center">購買筆電</th>
<th scope="col" class="org-center">未購買筆電</th>
<th scope="col" class="org-center">人數</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-center">&lt;=30</td>
<td class="org-center">2</td>
<td class="org-center">2</td>
<td class="org-center">4</td>
</tr>

<tr>
<td class="org-center">30~40</td>
<td class="org-center">4</td>
<td class="org-center">0</td>
<td class="org-center">4</td>
</tr>

<tr>
<td class="org-center">&gt;40</td>
<td class="org-center">1</td>
<td class="org-center">2</td>
<td class="org-center">3</td>
</tr>
</tbody>
</table>
<p>
不同年紀特徵值(Age)是否購買(Buy)的熵值為：
</p>
\begin{align*}
Info_{Age}(Buy) &= \frac{4}{11}I(2,2) + \frac{4}{11}I(4,0) + \frac{3}{11}I(1,2) \\
               &= \frac{4}{11}(-\frac{2}{4}\log_2\frac{2}{4}-\frac{2}{4}\log_2\frac{2}{4} \\
               &+ \frac{4}{11}(-\frac{4}{4}\log_2\frac{4}{4}-\frac{0}{4}\log_2\frac{0}{4} \\
               &+ \frac{3}{11}(-\frac{1}{3}\log_2\frac{1}{3}-\frac{2}{3}\log_2\frac{2}{3} \\
               &= 0.364 + 0 + 0.250 = 0.614
\end{align*}
</div>
</li>
<li><a id="orgcb44133"></a>收入<br />
<div class="outline-text-6" id="text-4-2-1-2-2">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-center">收入</th>
<th scope="col" class="org-center">購買筆電</th>
<th scope="col" class="org-center">未購買筆電</th>
<th scope="col" class="org-center">人數</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-center">高</td>
<td class="org-center">2</td>
<td class="org-center">1</td>
<td class="org-center">3</td>
</tr>

<tr>
<td class="org-center">中</td>
<td class="org-center">3</td>
<td class="org-center">2</td>
<td class="org-center">5</td>
</tr>

<tr>
<td class="org-center">低</td>
<td class="org-center">2</td>
<td class="org-center">1</td>
<td class="org-center">3</td>
</tr>
</tbody>
</table>
<p>
不同收特徵值(Income)是否購買(Buy)的熵值為：
</p>
\begin{align*}
Info_{Income}(Buy) &= \frac{3}{11}I(2,1) + \frac{5}{11}I(3,2) + \frac{3}{11}I(2,1) \\
               &= \frac{3}{11}(-\frac{2}{3}\log_2\frac{2}{3}-\frac{1}{3}\log_2\frac{1}{3} \\
               &+ \frac{5}{11}(-\frac{3}{5}\log_2\frac{3}{5}-\frac{2}{5}\log_2\frac{2}{5} \\
               &+ \frac{3}{11}(-\frac{2}{3}\log_2\frac{2}{3}-\frac{1}{3}\log_2\frac{1}{3} \\
               &= 0.250 + 0.441 + 0.250 = 0.941
\end{align*}
</div>
</li>
<li><a id="org5559d00"></a>學生與否<br />
<div class="outline-text-6" id="text-4-2-1-2-3">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-center">學生</th>
<th scope="col" class="org-center">購買筆電</th>
<th scope="col" class="org-center">未購買筆電</th>
<th scope="col" class="org-center">人數</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-center">是</td>
<td class="org-center">4</td>
<td class="org-center">1</td>
<td class="org-center">5</td>
</tr>

<tr>
<td class="org-center">否</td>
<td class="org-center">3</td>
<td class="org-center">3</td>
<td class="org-center">6</td>
</tr>
</tbody>
</table>
<p>
不同學生特徵值(Student)是否購買(Buy)的熵值為：
</p>
\begin{align*}
Info_{Student}(Buy) &= \frac{5}{11}I(4,1) + \frac{6}{11}I(3,3) \\
               &= \frac{5}{11}(-\frac{4}{5}\log_2\frac{4}{5}-\frac{1}{5}\log_2\frac{1}{5} \\
               &+ \frac{6}{11}(-\frac{3}{6}\log_2\frac{3}{6}-\frac{3}{6}\log_2\frac{3}{6} \\
               &= 0.498 + 0.545 = 0.953
\end{align*}
</div>
</li>
</ol>
</div>
<div id="outline-container-org5582569" class="outline-5">
<h5 id="org5582569"><span class="section-number-5">4.2.1.3.</span> 以資訊獲利評估合適的第一層特徵值</h5>
<div class="outline-text-5" id="text-4-2-1-3">
<p>
計算結果：
</p>
<ul class="org-ul">
<li>年紀 v.s. 購買筆電與否, 熵=0.614</li>
<li>收入 v.s. 購買筆電與否, 熵=0.941</li>
<li>學生與否 v.s. 購買筆電與否, 熵=0.953</li>
</ul>
<p>
資訊獲利(information gain)就是用來衡量特徵值於分類資料的能力。依此範例，各項特徵值的資訊獲利計算方式為：
「購買筆電與否的熵」-「某個特徵值下購買筆電與否的熵」。
</p>
<ul class="org-ul">
<li>特徵值「年紀」的資訊獲利: 0.956－0.614=0.342</li>
<li>特徵值「收入」的資訊獲利: 0.956－0.941=0.015</li>
<li>特徵值「學生與否」的資訊獲利: 0.956－0.953=0.003</li>
</ul>
<p>
根據不同特徵值得到的<span style="color:green;">資訊獲利越高</span>，表示該特徵值內資料的<span style="color:green;">凌亂程度越小</span>，用來分類資料效果越佳；反之，若資訊獲利越低，表示該特徵值內資料的凌亂程度越大，用來分類資料效果較差。也就是說：我們應該選<span style="color:red;">年紀</span>來做為第一個決策樹分類值，結果如下圖。
</p>

<div id="org1ecd200" class="figure">
<p><img src="images/tree-1.png" alt="tree-1.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-org7a107c3" class="outline-5">
<h5 id="org7a107c3"><span class="section-number-5">4.2.1.4.</span> 第一次計算成果</h5>
<div class="outline-text-5" id="text-4-2-1-4">
<p>
在以年紀為第一個決策特徵值的前提下，我們來看看不同年紀下的購買狀況：
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-center">年紀</th>
<th scope="col" class="org-center">收入</th>
<th scope="col" class="org-center">學生與否</th>
<th scope="col" class="org-center">購買筆電與否</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-center">&lt;=30</td>
<td class="org-center">高</td>
<td class="org-center">否</td>
<td class="org-center">否</td>
</tr>

<tr>
<td class="org-center">&lt;=30</td>
<td class="org-center">中</td>
<td class="org-center">否</td>
<td class="org-center">否</td>
</tr>

<tr>
<td class="org-center">&lt;=30</td>
<td class="org-center">低</td>
<td class="org-center">是</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">&lt;=30</td>
<td class="org-center">中</td>
<td class="org-center">是</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">31…40</td>
<td class="org-center">高</td>
<td class="org-center">否</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">31…40</td>
<td class="org-center">低</td>
<td class="org-center">是</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">31…40</td>
<td class="org-center">中</td>
<td class="org-center">否</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">31…40</td>
<td class="org-center">高</td>
<td class="org-center">是</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">&gt;40</td>
<td class="org-center">中</td>
<td class="org-center">否</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">&gt;40</td>
<td class="org-center">低</td>
<td class="org-center">是</td>
<td class="org-center">否</td>
</tr>

<tr>
<td class="org-center">&gt;40</td>
<td class="org-center">中</td>
<td class="org-center">是</td>
<td class="org-center">否</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="outline-container-org5e6faca" class="outline-4">
<h4 id="org5e6faca"><span class="section-number-4">4.2.2.</span> 決定第二層</h4>
<div class="outline-text-4" id="text-4-2-2">
<p>
決定好以年紀做為第一層決策樹特徵值後，接下來就只要考慮
</p>
<ul class="org-ul">
<li>收入</li>
<li>學生與否</li>
</ul>
<p>
我們來看看針對年紀的三種條件各要以哪一個特徵當成接下來的決策樹特徵?
</p>
</div>
<div id="outline-container-org969b647" class="outline-5">
<h5 id="org969b647"><span class="section-number-5">4.2.2.1.</span> 年紀&lt;=30</h5>
<div class="outline-text-5" id="text-4-2-2-1">
<p>
先考慮年紀&lt;=30 的狀況
</p>

<div id="orgc331aaa" class="figure">
<p><img src="images/tree-2.png" alt="tree-2.png" />
</p>
</div>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-center">年紀</th>
<th scope="col" class="org-center">收入</th>
<th scope="col" class="org-center">學生與否</th>
<th scope="col" class="org-center">購買筆電與否</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-center">&lt;=30</td>
<td class="org-center">高</td>
<td class="org-center">否</td>
<td class="org-center">否</td>
</tr>

<tr>
<td class="org-center">&lt;=30</td>
<td class="org-center">中</td>
<td class="org-center">否</td>
<td class="org-center">否</td>
</tr>

<tr>
<td class="org-center">&lt;=30</td>
<td class="org-center">低</td>
<td class="org-center">是</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">&lt;=30</td>
<td class="org-center">中</td>
<td class="org-center">是</td>
<td class="org-center">是</td>
</tr>
</tbody>
</table>
</div>
<ol class="org-ol">
<li><a id="org3a83e42"></a>計算年紀&lt;=30 購買筆電與否的熵值<br />
<div class="outline-text-6" id="text-4-2-2-1-1">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-center">購買筆電與否</th>
<th scope="col" class="org-center">出現次數</th>
<th scope="col" class="org-center">出現機率</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-center">是</td>
<td class="org-center">2</td>
<td class="org-center">\(\frac{2}{4}\)</td>
</tr>

<tr>
<td class="org-center">否</td>
<td class="org-center">2</td>
<td class="org-center">\(\frac{2}{4}\)</td>
</tr>
</tbody>
</table>
<p>
根據\( Info(D) = -\sum\limits_{i=1}^{m}p_i\times\log_2 p_i \)，是否購買(Buy)的熵值為：
</p>
\begin{align*}
Info(Buy) &= I(2,2) \\
          &= -\frac{2}{4}\log_2\frac{2}{4}-\frac{2}{4}\log_2\frac{2}{f} \\
          &= 0.5+0.5 \\
          &=1
\end{align*}
</div>
</li>
<li><a id="orgb58c4fe"></a>計算不同特徵值的熵值<br />
<div class="outline-text-6" id="text-4-2-2-1-2">
<p>
目前有以下兩種特徵值可以來判斷「購買筆電與否」這個結果：
</p>
<ul class="org-ul">
<li>收入 v.s. 購買筆電與否</li>
<li>學生與否 v.s. 購買筆電與否</li>
</ul>
</div>
<ul class="org-ul">
<li><a id="org3b8bba3"></a>收入<br />
<div class="outline-text-7" id="text-org3b8bba3">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-center">收入</th>
<th scope="col" class="org-center">購買筆電</th>
<th scope="col" class="org-center">未購買筆電</th>
<th scope="col" class="org-center">人數</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-center">高</td>
<td class="org-center">0</td>
<td class="org-center">1</td>
<td class="org-center">1</td>
</tr>

<tr>
<td class="org-center">中</td>
<td class="org-center">1</td>
<td class="org-center">1</td>
<td class="org-center">2</td>
</tr>

<tr>
<td class="org-center">低</td>
<td class="org-center">1</td>
<td class="org-center">0</td>
<td class="org-center">1</td>
</tr>
</tbody>
</table>
<p>
不同收入特徵值(Income)是否購買(Buy)的熵值為：
</p>
\begin{align*}
Info_{Income}(Buy) &= \frac{1}{4}I(0,1) + \frac{2}{4}I(1,1) + \frac{1}{4}I(1,0) \\
               &= \frac{1}{4}(-\frac{0}{1}\log_2\frac{0}{1}-\frac{1}{1}\log_2\frac{1}{1} \\
               &+ \frac{2}{4}(-\frac{1}{2}\log_2\frac{1}{2}-\frac{1}{2}\log_2\frac{1}{2} \\
               &+ \frac{1}{4}(-\frac{1}{1}\log_2\frac{1}{1}-\frac{0}{1}\log_2\frac{0}{1} \\
               &= 0 + 0.5 + 0 = 0.5
\end{align*}
</div>
</li>
<li><a id="org7a4f8de"></a>學生與否<br />
<div class="outline-text-7" id="text-org7a4f8de">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-center">學生</th>
<th scope="col" class="org-center">購買筆電</th>
<th scope="col" class="org-center">未購買筆電</th>
<th scope="col" class="org-center">人數</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-center">是</td>
<td class="org-center">2</td>
<td class="org-center">0</td>
<td class="org-center">2</td>
</tr>

<tr>
<td class="org-center">否</td>
<td class="org-center">0</td>
<td class="org-center">2</td>
<td class="org-center">2</td>
</tr>
</tbody>
</table>
<p>
不同收入特徵值(Income)是否購買(Buy)的熵值為：
</p>
\begin{align*}
Info_{Income}(Buy) &= \frac{2}{4}I(2,0) + \frac{2}{4}I(0,2) \\
               &= \frac{2}{4}(-\frac{2}{2}\log_2\frac{2}{2}-\frac{0}{2}\log_2\frac{0}{2} \\
               &+ \frac{2}{4}(-\frac{0}{2}\log_2\frac{0}{2}-\frac{2}{2}\log_2\frac{2}{2} \\
               &= 0 + 0 = 0
\end{align*}
</div>
</li>
</ul>
</li>
<li><a id="org7e3e436"></a>以資訊獲利評估合適的第二層特徵值<br />
<div class="outline-text-6" id="text-4-2-2-1-3">
<p>
計算結果：
</p>
<ul class="org-ul">
<li>收入 v.s. 購買筆電與否, 熵=0.5</li>
<li>學生與否 v.s. 購買筆電與否, 熵=0</li>
</ul>
<p>
資訊獲利
</p>
<ul class="org-ul">
<li>特徵值「收入」的資訊獲利: 1－0.5 = 0.05</li>
<li>特徵值「學生身份」的資訊獲利: 1－0 = 1</li>
</ul>

<div id="orgfbc6464" class="figure">
<p><img src="images/tree-3.png" alt="tree-3.png" />
</p>
</div>
</div>
</li>
<li><a id="orge423019"></a>需要再分下去嗎?<br />
<div class="outline-text-6" id="text-4-2-2-1-4">
<p>
在鎖定了年紀小於 30、學生身份這兩個特徵值後，我們會發現剩下的狀況為：
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-center">年紀</th>
<th scope="col" class="org-center">學生與否</th>
<th scope="col" class="org-center">購買筆電與否</th>
<th scope="col" class="org-center">收入</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-center">&lt;=30</td>
<td class="org-center">否</td>
<td class="org-center">否</td>
<td class="org-center">高</td>
</tr>

<tr>
<td class="org-center">&lt;=30</td>
<td class="org-center">否</td>
<td class="org-center">否</td>
<td class="org-center">中</td>
</tr>

<tr>
<td class="org-center">&lt;=30</td>
<td class="org-center">是</td>
<td class="org-center">是</td>
<td class="org-center">低</td>
</tr>

<tr>
<td class="org-center">&lt;=30</td>
<td class="org-center">是</td>
<td class="org-center">是</td>
<td class="org-center">中</td>
</tr>
</tbody>
</table>
<p>
也就是說，此時的收入已無考慮價值，因為我們已能確定該名顧客是否會購買筆電，即如下圖：
</p>

<div id="org5e15cac" class="figure">
<p><img src="images/tree-4.png" alt="tree-4.png" />
</p>
</div>
</div>
</li>
</ol>
</div>
<div id="outline-container-org2c2430f" class="outline-5">
<h5 id="org2c2430f"><span class="section-number-5">4.2.2.2.</span> 31&lt;=年紀&lt;40</h5>
<div class="outline-text-5" id="text-4-2-2-2">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-center">年紀</th>
<th scope="col" class="org-center">收入</th>
<th scope="col" class="org-center">購買筆電與否</th>
<th scope="col" class="org-center">學生與否</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-center">31…40</td>
<td class="org-center">高</td>
<td class="org-center">是</td>
<td class="org-center">否</td>
</tr>

<tr>
<td class="org-center">31…40</td>
<td class="org-center">低</td>
<td class="org-center">是</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">31…40</td>
<td class="org-center">中</td>
<td class="org-center">是</td>
<td class="org-center">否</td>
</tr>

<tr>
<td class="org-center">31…40</td>
<td class="org-center">高</td>
<td class="org-center">是</td>
<td class="org-center">是</td>
</tr>
</tbody>
</table>
<p>
在你急著依照上例手動計算之前，不妨先用力看清楚上面的表，你就會發現其實所有介於這個年齡層的顧客都會購買筆電，所以這就不用再算了&#x2026;我們會得到底下的部份決策樹:
</p>

<div id="org42d57f9" class="figure">
<p><img src="images/tree-5.png" alt="tree-5.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-orgb55025a" class="outline-5">
<h5 id="orgb55025a"><span class="section-number-5">4.2.2.3.</span> [課堂任務]年紀&gt;40&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></h5>
<div class="outline-text-5" id="text-4-2-2-3">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-center">年紀</th>
<th scope="col" class="org-center">收入</th>
<th scope="col" class="org-center">學生與否</th>
<th scope="col" class="org-center">購買筆電與否</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-center">&gt;40</td>
<td class="org-center">中</td>
<td class="org-center">否</td>
<td class="org-center">是</td>
</tr>

<tr>
<td class="org-center">&gt;40</td>
<td class="org-center">低</td>
<td class="org-center">是</td>
<td class="org-center">否</td>
</tr>

<tr>
<td class="org-center">&gt;40</td>
<td class="org-center">中</td>
<td class="org-center">是</td>
<td class="org-center">否</td>
</tr>
</tbody>
</table>
<p>
你不會還在等我把剩下的做完吧？請你和你的組員一起把這棵決策樹完成吧。你可以用 Python、用 Excel、用 Numbers、用計算機、用手算、用心算，請你們利用下課 10 分鐘完作這個作業，並將學習單交到講台。
</p>

<div id="orgae3192b" class="figure">
<p><img src="images/tree-6.png" alt="tree-6.png" />
</p>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-orgcf7d783" class="outline-3">
<h3 id="orgcf7d783"><span class="section-number-3">4.3.</span> Overfitting 問題</h3>
<div class="outline-text-3" id="text-4-3">
<ul class="org-ul">
<li>決策樹很容易產生 Overfitting，如果不限制它，它可以一直長下去分得過細。所以有以下常用的幾種方法來設限</li>
<li>Minimum samples for a node split：資料數目不得小於多少才能再產生新節點。</li>
<li>Minimum samples for a terminal node (leaf)：要成為葉節點，最少需要多少資料。</li>
<li>Maximum depth of tree (vertical depth)：限制樹的高度最多幾層。</li>
<li>Maximum number of terminal nodes：限制最終葉節點的數目</li>
<li>Maximum features to consider for split：在分離節點時，最多考慮幾種特徵值。</li>
</ul>
</div>
</div>
<div id="outline-container-org0c79f5d" class="outline-3">
<h3 id="org0c79f5d"><span class="section-number-3">4.4.</span> [實作]鳶尾花分類&#xa0;&#xa0;&#xa0;<span class="tag"><span class="sklearn">sklearn</span></span></h3>
<div class="outline-text-3" id="text-4-4">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">from</span> sklearn.datasets <span style="color: #51afef;">import</span> load_iris
<span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> tree
<span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">Load in our dataset</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;"># &#35712;&#20837;&#40182;&#23614;&#33457;&#36039;&#26009;</span>
<span style="color: #dcaeea;">iris</span> = load_iris()
<span style="color: #dcaeea;">iris_x</span> = iris.data
<span style="color: #dcaeea;">iris_y</span> = iris.target

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20999;&#20998;&#35347;&#32244;&#33287;&#28204;&#35430;&#36039;&#26009;</span>
<span style="color: #dcaeea;">train_x</span>, <span style="color: #dcaeea;">test_x</span>, <span style="color: #dcaeea;">train_y</span>, <span style="color: #dcaeea;">test_y</span> = train_test_split(iris_x, iris_y, test_size = <span style="color: #da8548; font-weight: bold;">0.3</span>)

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24314;&#31435;&#20998;&#39006;&#22120;</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">Initialize our decision tree object</span>
<span style="color: #dcaeea;">classification_tree</span> = tree.DecisionTreeClassifier(criterion = <span style="color: #98be65;">"entropy"</span>)

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">Train our decision tree (tree induction + pruning)</span>
<span style="color: #dcaeea;">classification_tree</span> = classification_tree.fit(train_x, train_y)

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38928;&#28204;</span>
<span style="color: #dcaeea;">test_y_predicted</span> = classification_tree.predict(test_x)
<span style="color: #c678dd;">print</span>(test_y_predicted)

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27161;&#28310;&#31572;&#26696;</span>
<span style="color: #c678dd;">print</span>(test_y)


<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#24471;&#20998;:'</span>,classification_tree.score(test_x, test_y))
<span style="color: #51afef;">import</span> graphviz

<span style="color: #51afef;">import</span> pydot
<span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.clf()</span>
<span style="color: #dcaeea;">dot_data</span> = tree.export_graphviz(classification_tree, out_file=<span style="color: #a9a1e1;">None</span>,
                     feature_names=iris.feature_names,
                     class_names=iris.target_names,
                     filled=<span style="color: #a9a1e1;">True</span>, rounded=<span style="color: #a9a1e1;">True</span>,
                     special_characters=<span style="color: #a9a1e1;">True</span>)
<span style="color: #dcaeea;">graph</span> = graphviz.Source(dot_data)
graph <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36889;&#34892;&#21482;&#36969;&#21512;for colab&#29872;&#22659;</span>
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">graph.render("images/DecisionTree.png", view=True)</span>
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">graph.format = 'png'</span>
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">graph.render('images/DecisionTree')</span>
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.savefig('images/DecisionTree.png', dpi=300)</span>

</pre>
</div>

<pre class="example">
[0 0 0 0 1 2 0 1 0 2 2 0 2 2 2 2 2 1 1 1 0 2 1 1 2 1 2 2 0 2 0 1 0 2 0 2 2
 0 1 1 1 2 2 0 0]
[0 0 0 0 1 2 0 1 0 2 2 0 2 2 2 2 2 1 1 1 0 2 1 1 2 1 2 2 0 2 0 1 0 2 0 2 2
 0 1 1 1 2 2 0 0]
得分: 1.0
</pre>



<div id="org35e4677" class="figure">
<p><img src="images/DecisionTree.png" alt="DecisionTree.png" width="800" />
</p>
<p><span class="figure-number">Figure 20: </span>Decision Tree</p>
</div>
</div>
<div id="outline-container-org0e6be14" class="outline-4">
<h4 id="org0e6be14"><span class="section-number-4">4.4.1.</span> 關於classification_tree.score()</h4>
<div class="outline-text-4" id="text-4-4-1">
<ul class="org-ul">
<li>classification_tree.score(test_x, test_y) 是用來計算模型在測試數據上的準確率，表示模型預測正確的比例，取值範圍在 0 到 1 之間。</li>
<li>一般來說，0.7（70%） 以上的準確率就算是不錯，但具體標準取決於任務難度和數據集特性。</li>
<li>如果數據集不平衡（例如，某個類別佔了大多數），即使高於 0.7 也可能不一定是好結果，這時需要結合其他指標如精確率（Precision）或召回率（Recall）來評估模型。</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org2319b00" class="outline-3">
<h3 id="org2319b00"><span class="section-number-3">4.5.</span> [作業]以決策樹進行貸款核淮分析&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></h3>
<div class="outline-text-3" id="text-4-5">
<p>
參考上述程式碼,完成以下任務
</p>
<ol class="org-ol">
<li>以 Pandas 讀取線上 csv[<a href="https://letranger.github.io/AI/Downloads/loantree.csv">https://letranger.github.io/AI/Downloads/loantree.csv</a>]</li>
<li>移除有缺失值的記錄</li>
<li>將 YES/NO、Male/Female 等分類值改為 0/1 值</li>
<li>依 Gd,Md,Dd,Ed,SE 這五個特徵值來決定是否核淮貸款申請</li>
<li>最終是否核淮貸款的欄位為 LS</li>
<li>分別以 Gini index, Entropy 兩種策略來進行分類，比較效能</li>
<li>觀察其他特徵值，你有其他的想法可以提高效能嗎？</li>
</ol>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Hands-On Machine Learning with Scikit-Learn: Aurelien Geron
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://zh.wikipedia.org/zh-tw/%E7%BE%85%E7%B4%8D%E5%BE%B7%C2%B7%E6%84%9B%E7%88%BE%E9%BB%98%C2%B7%E8%B2%BB%E9%9B%AA">羅納德·愛爾默·費雪</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://www.amazon.com/Understanding-Psychology-Science-Introduction-Statistical/dp/023054231X">Understanding Psychology as a Science: An Introduction to Scientific and Statistical Inference</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4" role="doc-backlink">4</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://www.sciencehistory.org/stories/magazine/ronald-fisher-a-bad-cup-of-tea-and-the-birth-of-modern-statistics/">Ronald Fisher, a Bad Cup of Tea, and the Birth of Modern Statistics Science History Institute</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.5" class="footnum" href="#fnr.5" role="doc-backlink">5</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">sklearn.model_selection.train_test_split</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.6" class="footnum" href="#fnr.6" role="doc-backlink">6</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://tecky.io/en/blog/_%E9%87%8F%E5%BA%A6%E8%B3%87%E8%A8%8A/">資訊熵</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.7" class="footnum" href="#fnr.7" role="doc-backlink">7</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://www.twblogs.net/a/5b8ed2a52b71771883480414">白話信息熵</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.8" class="footnum" href="#fnr.8" role="doc-backlink">8</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://gist.github.com/nstarke/bc662d2858756f4812d74f7fb3eab28a">nstarke/find-entropy.py</a>
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Yung-Chin Yen</p>
<p class="date">Created: 2024-09-22 Sun 15:50</p>
</div>
</body>
</html>
