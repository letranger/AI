:PROPERTIES:
:ID:       6ae7fb7a-0b38-4448-b19f-073d262513f2
:ROAM_ALIASES: Regression
:END:

#+TITLE: 迴歸
# -*- org-export-babel-evaluate: nil -*-
#+TAGS: AI
#+OPTIONS: toc:2 ^:nil num:5
#+PROPERTY: header-args :eval never-export
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../css/muse.css" />
#+EXCLUDE_TAGS: noexport
#+latex:\newpage
#+begin_export html
<a href="https://letranger.github.io/AI/20221023154410-regression.html"><img align="right" alt="Hits" src="https://hits.sh/letranger.github.io/AI/20221023154410-regression.html.svg"/></a>
#+end_export


即，根據一組預測特徵（predictor，如里程數、車齡、品牌）來預測目標數值（如二手車車價）[fn:1]，這個目標數值也是label。

有些迴歸演算法也可以用來分類，例如Logistic，它可以輸出一個數值，以這個數值來表示對應到特定類別的機率，例如，某封email為垃圾郵件的機率為20%、某張圖片為狗的機率為70%。

迴歸問題可再細分為兩類：
- Linear regression:
  * 假設輸入變量(x)與單一輸出變量(y)間存在線性關係，並以此建立模型。
  * 優點: 簡單、容易解釋
  * 缺點: 輸入與輸出變量關係為線性時會導致低度擬合
  * 例: 身高與體重間的關係
- Logistic regression
  * 也是線性方法，但使用logist function轉換輸出的預測結果，其輸出結果為類別機率(class probabilities)
  * 優點: 簡單、容易解釋
  * 缺點: 輸入與輸出變量關係為線性時無法處理分類問題

典型迴歸案例: Boston Housing Data

* 迴歸原理
** Step 1
1. Model: $y = w*x+b$
2. Data: 找一堆現成的資料
** Step 2: Goodness of Function
1. Training Data
2. Loss function L: 越小越好
   input: a function / output: how bad it is
3. Pick the "Best: Function
   $f* = arg min L(f)$
   上述可以微分來求最佳解，即求 function L 的最小值
4. 數值最佳解: Gradient Descent(找拋物面最低點)

* 迴歸預測流程(以波士頓房價預測為例) :noexport:
1. Import the required module
1. Load and configure the Boston housing data set
1. Chekc the relation between the variable, using pairplot and correlation graph
1. Descriptive statistics: central tendency and dispersion
1. Select the required columns
1. Train the test split
1. Normalize the data
1. Build the input pipeline for the TensorFlow model
1. Model tranining
1. Predictions
1. Validation

* 簡單線性迴歸 :noexport:
** Pizza
Let's assume that you have recorded the diameters and prices of pizzas that you have previously eaten in your pizza journal. These observations comprise our training data:
|--------------------+------------------|
| Diameter in inches | Price in dollars |
|--------------------+------------------|
|                  6 |                7 |
|                  8 |                9 |
|                 10 |               13 |
|                 14 |             17.5 |
|                 18 |               18 |
|--------------------+------------------|
*** 觀察數據
We can visualize our training data by plotting it on a graph using matplotlib:
#+begin_src python -r -n :results output :exports both
import numpy as np
# "np" and "plt" are common aliases for NumPy and Matplotlib, respectively.
import matplotlib.pyplot as plt

# X represents the features of our training data, the diameters of the pizzas.
# A scikit–learn convention is to name the matrix of feature vectors X.
# Uppercase letters indicate matrices, and lowercase letters indicate vectors.
X = np.array([[6], [8], [10], [14], [18]]).reshape(-1, 1)

y = [7, 9, 13, 17.5 , 18]
# y is a vector representing the prices of the pizzas.

#plt.figure()
plt.title('Pizza price plotted against diameter')
plt.xlabel('Diameter in inches')
plt.ylabel('Price in dollars')
plt.plot(X, y, 'k.')
plt.axis([0, 25, 0, 25])
plt.grid(True)
plt.savefig('images/pizza-1.png', dpi=300)
#+end_src

#+RESULTS:
#+CAPTION: Pizza Regression #1
#+LABEL:fig:Pizza-Reg-1
#+name: fig:Pizza-Reg-1
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/pizza-1.png]]
We can see from the plot of the training data that there is a positive relationship between the diameter of a pizza and its price, which should be corroborated by our own pizza-eating experience.
*** 建模: LinearRegression
The following pizza price predictor program models this relationship using simple linear regression.
#+begin_src python -r -n :results output :exports both
import numpy as np
X = np.array([[6], [8], [10], [14], [18]]).reshape(-1, 1)
y = [7, 9, 13, 17.5 , 18]

print(X.shape)

from sklearn.linear_model import LinearRegression
model = LinearRegression()
# Create an instance of the estimator
model.fit(X, y)
# Fit the model on the training data

# Predict the price of a pizza with a diameter that has never been seen before
test_pizza = np.array([[12]])
predicted_price = model.predict(test_pizza)[0]
print('A 12" pizza should cost: $%.2f' % predicted_price)
#+end_src

#+RESULTS:
: (5, 1)
: A 12" pizza should cost: $13.68

- The LinearRegression class is an *estimator*. Estimators predict a value based on observed data.
- In scikit-learn, all estimators implement the fit methods and predict.
- The fit method of LinearRegression learns the parameters of the following model for simple linear regression:$$y=\alpha+\beta x$$
- $y$ is the predicted value of the response variable; in this example, it is the predicted price of the pizza.
- $x$ is the explanatory variable.
- The intercept term $\alpha$ and the coefficient $\beta$ are parameters of the model that are learned by the learning algorithm.
- The hyperplane plotted in the following figure models the relationship between the size of a pizza and its price.
- Using training data to learn the values of the parameters for simple linear regression that produce the best fitting model is called ordinary least squares (OLS) or linear least squares.

#+begin_src python -r -n :results output :exports both
import numpy as np
X = np.array([[6], [8], [10], [14], [18]]).reshape(-1, 1)
y = [7, 9, 13, 17.5 , 18]

from sklearn.linear_model import LinearRegression
model = LinearRegression()
# Create an instance of the estimator
model.fit(X, y)
# Fit the model on the training data

from matplotlib import pyplot as plt
plt.scatter(X, y, color = 'k')
plt.plot(X, model.predict(X), color='g')
plt.title('Pizza price plotted against diameter')
plt.xlabel('Diameter in inches')
plt.ylabel('Price in dollars')
plt.savefig('images/pizza-2.png', dpi=300)
#+end_src

  #+RESULTS:

#+CAPTION: Pizza regression 2
#+LABEL:fig:Pizza-reg-2
#+name: fig:Pizza-reg-2
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/pizza-2.png]]
*** Evaluating the fitness of the model with a cost function
Regression lines produced by several sets of parameter values are plotted in the following figure. How can we assess which parameters produced the best-fitting regression line?
#+begin_src python -r -n :results output :exports none
import numpy as np
X = np.array([[6], [8], [10], [14], [18]]).reshape(-1, 1)
y = [7, 9, 13, 17.5 , 18]
print()
print(y)
#from sklearn.linear_model import LinearRegression
#model = LinearRegression()
## Create an instance of the estimator
#model.fit(X, y)
## Fit the model on the training data
#
#from matplotlib import pyplot as plt
#plt.scatter(X, y, color = 'k')
#plt.plot(X, model.predict(X), color='g')
#plt.plot(X, model.predict(X)+.5, color='c', linestyle='--')
#plt.plot(X, model.predict(X)*.9, color='m', linestyle='-.')
#plt.title('Pizza price plotted against diameter')
#plt.xlabel('Diameter in inches')
#plt.ylabel('Price in dollars')
#plt.savefig('images/pizza-3.png', dpi=300)

#+end_src

#+RESULTS:

#+CAPTION: Pizza regression 3
#+LABEL:fig:Pizza-reg-3
#+name: fig:Pizza-reg-3
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/pizza-3.png]]
**** cost function
A cost function, also called a loss function, is used to define and measure the error of a model. The differences between the prices predicted by the model and the observed prices of the pizzas in the training set are called residuals, or training errors. The differences between the predicted and observed values in the test data are called prediction errors, or test errors.
#+begin_src python -r -n :results output :exports none
import numpy as np
X = np.array([[6], [8], [10], [14], [18]]).reshape(-1, 1)
y = [7, 9, 13, 17.5 , 18]

from sklearn.linear_model import LinearRegression
model = LinearRegression()
# Create an instance of the estimator
model.fit(X, y)
# Fit the model on the training data

from matplotlib import pyplot as plt

dy = (model.predict(X)-y)/2
for x, y1, y2 in zip(X, y, model.predict(X)):
    xs = [x, x]
    ys = [y1, y2]
    plt.plot(xs, ys, color='orange')
plt.scatter(X, y, color = 'k')
plt.plot(X, model.predict(X), color='g')
#plt.errorbar(X, model.predict(X)-dy, yerr=dy, fmt='.')
plt.title('Pizza price plotted against diameter')
plt.xlabel('Diameter in inches')
plt.ylabel('Price in dollars')
plt.savefig('images/pizza-4.png', dpi=300)
#+end_src
#+RESULTS:
#+CAPTION: Pizza regression 4
#+LABEL:fig:Pizza-reg-4
#+name: fig:Pizza-reg-4
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/pizza-4.png]]

This measure of the model's fitness is called the residual sum of squares (RSS) cost function. Formally, this function assesses the fitness of a model by summing the squared residuals for all of our training examples. The RSS is calculated with the formula in the following equation, where \(y_i\) is the observed value and \(f(x_i)\) is the predicted value:$$SS_{res}=\sum_{i=1}^{n}(y_i-f(x_i))^2$$



#+begin_src emacs-lisp
(add-to-list 'package-archives '("melpa" . "https://melpa.org/packages/"))
(setq python-shell-interpreter "/usr/bin/python3")
(setq python-shell-interpreter-arg "-i")
(setq py-use-current-dir-when-execute-p t)
(setq python-shell-prompt-detect-enabled nil)
(setq python-shell-interpreter "ipython")
(setq python-shell-interpreter-interactive-args "-i --simple-prompt")
#+end_src

#+RESULTS:
: -i --simple-prompt


#+begin_src emacs-lisp
(add-to-list 'package-archives '("melpa" . "https://melpa.org/packages/"))
#+end_src

#+RESULTS:
: ((gnu . https://elpa.gnu.org/packages/) (melpa . https://melpa.org/packages/) (org . https://orgmode.org/elpa/))

#+begin_src jupyter-python :session py :async yes :kernel python :results scalar both raw drawer :display text/html :exports both
import numpy as np
import pandas as pd

a = 3
print(a)
data = [[1,2], [3,4]]
pd.DataFrame(data, columns=["Foo", "Bar"])
#+end_src

#+RESULTS:
:results:
# Out[5]:
#+BEGIN_EXAMPLE
  Foo  Bar
  0    1    2
  1    3    4
#+END_EXAMPLE
:end:

#+begin_src jupyter-python :session py :async yes :kernel python3 :results scalar both raw drawer :exports both
from ipywidgets import  interact, interactive, fixed, interact_manual
import ipywidgets as widgets
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
print(data)
def f(x):
    plt.plot(np.arange(0, 10), x*np.arange(0, 10))
    plt.ylim(-30, 30)
#interact(f, x=10)
f(10)
#+end_src

#+RESULTS:
:results:
# Out[6]:
[[file:.ob-ipython-resrcnqInND.png]]
:end:

:end:
:end:
:end:

#+BEGIN_SRC emacs-lisp
(require 'jupyter)
#+END_SRC

#+RESULTS:
: jupyter

* 線性迴歸:年齡身高預測
** 資料生成
這是當初上帝創造人類時決定人類身高的規則，我們也可以將之視為這組資料的模型，這個規則或模型是很神祕的，等一下我們要假裝我們不知道這個模型的存在，而迴歸的目的就在於想辦法猜出這個規則或模型。
#+begin_src ipython -r :results output :exports both :session test
import numpy as np
import matplotlib.pyplot as plt

n = 10                               # 資料筆數
year = 5 + 25 * np.random.rand(n)  # 年紀
height = 170 - 108 * np.exp(-0.2 * year) + 4 * np.random.randn(n)
print(year)
print(height)
#+end_src
: [13.3, 16.2, 10.9, 28.7, 19.8, 14.2, 11.7, 26.6, 22.4, 18.3, 19.4]
: [163.61, 168.53, 155.06, 171.3 , 166.69, 160.98, 158.23, 165.27, 170.83,  161.31, 163.58]

** 查看資料
對於平凡的人類而言，他們只能看到身邊的人們隨著年齡增長而出現身高的變化，也就是由神袐模型所生成的數字：年齡和身高(如圖[[fig:yearHeight]])。
#+begin_src python -r :results output :exports none
import numpy as np
import matplotlib.pyplot as plt

year = np.array([13.3, 16.2, 10.9, 28.7, 14.2, 11.7, 26.6, 22.4, 18.3, 20.4])
height = np.array([163.61, 168.53, 155.06, 168.3 ,158.98, 158.23, 165.27, 170.83,  161.31, 163.58])

plt.figure(figsize=(5, 5))
plt.plot(year, height, marker='o', linestyle='None',
         markeredgecolor='black', color='cornflowerblue')

plt.xlim(5, 35)
plt.ylim(150,175)

plt.grid(True)
plt.ylabel('Height')
plt.xlabel('Year')
plt.savefig("images/yearHeight.png")
#+end_src

#+RESULTS:

#+CAPTION: 年齡與身高的資料分佈
#+name: fig:yearHeight
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/yearHeight.png]]

但那些一身反骨的數學家則不甘於當平凡人，他們想透過統計、分析、思考、通靈等方式對這個既有現象進行逆向工程，去推估這個現象背後的神祕規則，藉此窺探上帝的意志。
這些規則也許是如圖[[fig:yearHeightModel]]中的各種線段。一但找到了規則，我們就能根據這些規則進行 *預測* ，例如，由某人的年齡來合理推估他的身高。
#+begin_src python -r :results output :exports none
import numpy as np
import matplotlib.pyplot as plt

year = np.array([13.3, 16.2, 10.9, 28.7, 14.2, 11.7, 26.6, 22.4, 18.3, 20.4])
height = np.array([163.61, 168.53, 155.06, 168.3 ,158.98, 158.23, 165.27, 170.83,  161.31, 163.58])

plt.figure(figsize=(5, 5))
plt.plot(year, height, marker='o', linestyle='None',
         markeredgecolor='black', color='cornflowerblue')
plt.xlim(5, 35)
plt.ylim(150,175)
plt.grid(True)
plt.plot((10, 30), (153, 173), color='r', linestyle='-')
plt.plot((10, 30), (160, 169), color='b', linestyle='--')
plt.plot((10, 30), (157, 171), color='g', linestyle='--')

plt.ylabel('Height')
plt.xlabel('Year')
plt.savefig("images/yearHeightModel.png")
#+end_src

#+RESULTS:
: [13.3 16.2 10.9 28.7 19.8 14.2 11.7 26.6 22.4 18.3 19.4]
: [163.61 168.53 155.06 171.3  166.69 160.98 158.23 165.27 170.83 161.31
:  163.58]

#+CAPTION: 隱藏在年齡與身高資料背後的規則(模型)
#+name: fig:yearHeightModel
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/yearHeightModel.png]]

** 直線模型
我們可以在圖[[fig:yearHeightModel]]中畫上無數條線，但，最能代表年齡和身高關係的線應該只有一條，我們要如何找出這條線？

首先，既然我們想以 *直線* 來表示我們想找的模型或規則，那我們就先把這條直線以下列數學示表示出來:
$$y=ax+b$$ 或 $$f(x)=ax+b$$
這樣的直線 $y$ 或函數 $f(x)$ 有無限多個，迴歸的目的就是要為函數 $f(x)$ 找出一組最好的參數 $a,b$，或是為直線 $y$ 找到最適合的斜率 $a$ 和截距 $b$。這也是現今許多AI模型的基本精神：找到一組最好的參數，或者說：從無數個可能的模型中挑出最好的一個。

為了從無限多個備選模型中找出最佳的，我們需要有一個評估機制。

** 損失函數
#+begin_src python -r :results output :exports none
import numpy as np
import matplotlib.pyplot as plt

year = np.array([13.3, 16.2, 10.9, 28.7, 14.2, 11.7, 26.6, 22.4, 18.3, 20.4]).reshape([-1, 1])
height = np.array([163.61, 168.53, 155.06, 168.3 ,158.98, 158.23, 165.27, 170.83,  161.31, 163.58])

from sklearn.linear_model import LinearRegression
model = LinearRegression()
# Create an instance of the estimator
model.fit(year, height)

# Fit the model on the training data
plt.figure(figsize=(5, 5))
plt.grid(True)
plt.xlim(5, 35)
plt.ylim(150,175)
plt.scatter(year, height, color = 'k')
i = 0
for x, y in zip(year, height):
    plt.text(x+0.3, y-0.5, rf'$y_{i}$', color='red')
    i += 1

dy = (model.predict(year)-height)/2
i = 0
for x, y1, y2 in zip(year, height, model.predict(year)):
    xs = [x, x]
    ys = [y1, y2]
    plt.plot(xs, ys, color='orange')
    plt.text(x-0.4, y2+0.5, rf'$\hat y_{i}$', color='blue')
    i += 1
#====================
#plt.scatter(X, y, color = 'k')
#trueA = (model.predict(year)[1] - model.predict(year)[2])/(year[1][0]- year[2][0])
#trueB = model.predict(year)[1] - trueA * year[1][0]
#print(f'{trueA}') #0.7426437155683577
#print(f'{trueB}') #149.72040800429542
#====================
plt.plot(year, model.predict(year), color='g')


plt.plot(year, model.predict(year), color='g')
plt.xlabel('Year')
plt.ylabel('Height')
plt.savefig('images/yearHeightLoss.png', dpi=300)
#+end_src

#+RESULTS:

損失函數(loss function)也稱為成本函數(cost function)，就是最常用來定義、衡量模型誤差的方法。以圖[[fig:yearHeightLoss]]為例，我們可以計算所有原始資料$(x_0, y_0) \dots (x_9, y_9)$ 離這條預測線的距離，這些距離的總和越小，表示預測線離每一點越近，也就是說這個模型越準確。

#+RESULTS:
#+CAPTION: 直線模型的均方誤差
#+name: fig:yearHeightLoss
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 600
[[file:images/yearHeightLoss.png]]

圖[[fig:yearHeightLoss]]中的 $y_i$ 為實際資料 $x_i$ 對應的結果， 而 $\hat{y_i}$ 則是將每個實際資料 $x_i$ 丟入模型後的預測結果，計算 $y_i$ 與 $\hat{y_i}$ 誤差的方法稱為 *殘差平方和* (Residual Sum of Squares, RSS)，計算公式為
$$ RSS = \sum_{i=1}^{n}(\hat{y_i}-y_i)^2 $$
把RSS再除以n就或是 *均方差* (Mean Square Error, MSE)，即
$$ MSE = \frac{1}{n}\sum_{i=1}^{n}(\hat{y_i}-y_i)^2 $$
迴歸的任務就是把RSS或MSE最小化。

如何讓RSS/MSE最小化呢？

** 窮舉所有的可能性
#+begin_src python -r :results output :exports none
import numpy as np
import matplotlib.pyplot as plt

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS'] # 步驟一
plt.rcParams['axes.unicode_minus'] = False  # 步驟二（解決座標軸

year = np.array([13.3, 16.2, 10.9, 28.7, 14.2, 11.7, 26.6, 22.4, 18.3, 20.4]).reshape([-1, 1])
height = np.array([163.61, 168.53, 155.06, 168.3 ,158.98, 158.23, 165.27, 170.83,  161.31, 163.58])

from sklearn.linear_model import LinearRegression
model = LinearRegression()
# 均方誤差函數 ------------------------------
def mse_line(x, t, w):
    y = w[0] * x + w[1]
    mse = np.mean((y - t)**2)
    return mse

xn = 100    #等高線解析度
b0, b1 = -40, 41
a0, a1 = 40, 161
# Fit the model on the training data
w0 = np.linspace(b0, b1, xn)
w1 = np.linspace(a0, a1, xn)
ww0, ww1 = np.meshgrid(w0, w1)
J = np.zeros((len(w0), len(w1)))
for i0 in range(len(w0)):
    for i1 in range(len(w1)):
        J[i1, i0] = mse_line(year, height, (w0[i0], w1[i1]))

# 顯示 --------------------------------------
plt.figure(figsize=(5, 5))
from matplotlib import ticker

formatter = ticker.ScalarFormatter()
formatter.set_scientific(False)
# A
ax = plt.subplot(1, 1, 1, projection='3d')
ax.yaxis.set_major_formatter(formatter)
ax.plot_surface(ww0, ww1, J, rstride=20, cstride=20, alpha=0.3,
                color='blue', edgecolor='black')
ax.set_xticks(np.arange(b0, b1, 20))
ax.set_yticks(np.arange(a0, a1, 20))

ax.set_xlabel('a')
ax.set_ylabel('b')
ax.set_zlabel('SSE')
ax.view_init(20, -60)
plt.savefig('images/SSELossA.png', dpi=300)
#+end_src

#+RESULTS:

為了找出哪一組參數 $a,b$ 可以讓模型 $y=ax+b$ 的預測誤差達到最小，我們可以將一些合理的a,b值可能組合都列出來，如圖[[fig:SSELossA]]，我們列出了由參數a(-40~40)、參數b(40~160)的所有可能模型，圖中的z軸代表每一種模型產生的誤差。由圖[[fig:SSELossA]]可以看出兩件事:
1. 參數a對模型誤差的影響遠大於參數b
2. 當參數a的值接近0時，所生成的模型會有較低的MSE，也就是模型預測能力較好

#+CAPTION: 不同a,b情況下的均方差
#+name: fig:SSELossA
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 400
#+ATTR_HTML: :width 500
[[file:images/SSELossA.png]]

讓我們回憶一下等高線這個東西，如果我們把圖[[fig:SSELossA]]當成某個山谷的地形圖(z軸為高度)，那我們就可以畫出這個區域的等高線圖[[fig:SSELossB]](先別管我是怎麼畫出來的)，從等高線圖[[fig:SSELossB]]就能大概看出來當a的值約等於0、b的值約等於150時會有最低的SSE(如圖[[fig:SSELossB]]中的灰點，這是我透過觀落音得到的訊息)。

#+begin_src python -r :results output :exports none
import numpy as np
import matplotlib.pyplot as plt

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS'] # 步驟一（替換系統中的字型，這裡用的是Mac OSX系統）
plt.rcParams['axes.unicode_minus'] = False  # 步驟二（解決座標軸負數的負號顯示問題）
year = np.array([13.3, 16.2, 10.9, 28.7, 14.2, 11.7, 26.6, 22.4, 18.3, 20.4]).reshape([-1, 1])
height = np.array([163.61, 168.53, 155.06, 168.3 ,158.98, 158.23, 165.27, 170.83,  161.31, 163.58])

from sklearn.linear_model import LinearRegression
model = LinearRegression()
# 均方誤差函數 ------------------------------
def mse_line(x, t, w):
    y = w[0] * x + w[1]
    mse = np.mean((y - t)**2)
    return mse

xn = 100    #等高線解析度
b0, b1 = -40, 41
a0, a1 = 40, 161
# Fit the model on the training data
w0 = np.linspace(b0, b1, xn)
w1 = np.linspace(a0, a1, xn)
ww0, ww1 = np.meshgrid(w0, w1)
J = np.zeros((len(w0), len(w1)))
for i0 in range(len(w0)):
    for i1 in range(len(w1)):
        J[i1, i0] = mse_line(year, height, (w0[i0], w1[i1]))

# 顯示 --------------------------------------
plt.figure(figsize=(5, 5))
from matplotlib import ticker

formatter = ticker.ScalarFormatter()
formatter.set_scientific(False)
# B
cont = plt.contour(ww0, ww1, J, 30, colors='green',
                   levels=[0, 100, 1000, 10000, 100000, 1000000], linewidths=0.5)
cont.clabel(fmt='%d', fontsize=6, colors='r')
plt.scatter(0.74, 150.72, color = 'gray')
plt.xlabel("a")

plt.ylabel("b")
plt.grid(True)
plt.savefig('images/SSELossB.png', dpi=300)
#+end_src

#+RESULTS:

#+CAPTION: 不同a,b情況下的MSE(俯視/等高線)
#+name: fig:SSELossB
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 400
#+ATTR_HTML: :width 500
[[file:images/SSELossB.png]]

總之，看起來是有辦法找到最佳的模型的，只是有點麻煩...，這個方法稱為梯度下降，在這裡我們先知道有這麼個方法、知道這個方法可以找出最佳模型就好，至於深入探討這個方法是如何運作這件事，等我搞清楚了再說吧(或是等你們上大學再自己去研究)...

** 快速求出最佳解
雖然從無數組 $(a,b)$ 中找出最好的一組看似困難，不過其實許多現成的相關模組已經有了這些功能，例如[[https://scikit-learn.org/stable/][scikit-learn]]。以底下的程式為例：
#+begin_src python -r -n :results output :exports both
import numpy as np
from sklearn.linear_model import LinearRegression

year = np.array([13.3, 16.2, 10.9, 28.7, 14.2, 11.7, 26.6, 22.4, 18.3, 20.4]).reshape([-1, 1])
height = np.array([163.61, 168.53, 155.06, 168.3 ,158.98, 158.23, 165.27, 170.83,  161.31, 163.58])

model = LinearRegression() (ref:modelRegression)
model.fit(year, height) (ref:modelFit)

slope = model.coef_
intercept = model.intercept_
heightHat = year * slope + intercept

print('斜率/Slope:', slope)
print('截距/Intercept:', intercept)
#+end_src

#+RESULTS:
: 斜率/Slope: [0.58182444]
: 截距/Intercept: 152.74006747354875

在上述程式碼中，真正與計算迴歸有關的只有第[[(modelRegression)]]行(利用scikit-learn建立一個線性迴歸模型)與第[[(modelFit)]]行(把手上的10組 $(a,b)$ 資料丟進模型訓練)，
夠簡單吧，這樣我們就能畫出一條斜率約為0.58、截距約為152.74的最佳迴歸線(如圖[[fig:bestRegressionLine]]):
#+begin_src python -r -n :results output :exports none
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS'] # 步驟一（替換系統中的字型，這裡用的是Mac OSX系統）
plt.rcParams['axes.unicode_minus'] = False  # 步驟二（解決座標軸負數的負號顯示問題）

year = np.array([13.3, 16.2, 10.9, 28.7, 14.2, 11.7, 26.6, 22.4, 18.3, 20.4]).reshape([-1, 1])
height = np.array([163.61, 168.53, 155.06, 168.3 ,158.98, 158.23, 165.27, 170.83,  161.31, 163.58])

model = LinearRegression()
model.fit(year, height)

plt.figure(figsize=(5, 5))
plt.plot(year, height, marker='o', linestyle='None',
         markeredgecolor='black', color='cornflowerblue')
plt.xlim(5, 35)
plt.ylim(150,175)
plt.grid(True)
plt.plot(year, heightHat, color='r', linestyle='-')

plt.ylabel('Height')
plt.xlabel('Year')
plt.savefig("images/yearHeightModelHat.png")
#+end_src
#+CAPTION: 線性迴歸求解
#+name: fig:bestRegressionLine
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/yearHeightModelHat.png]]

** 逐步找出最佳解
#+begin_src python -r :results output :exports none
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS'] # 步驟一（替換系統中的字型，這裡用的是Mac OSX系統）
plt.rcParams['axes.unicode_minus'] = False  # 步驟二（解決座標軸負數的負號顯示問題）
year = np.array([13.3, 16.2, 10.9, 28.7, 14.2, 11.7, 26.6, 22.4, 18.3, 20.4]).reshape([-1, 1])
height = np.array([163.61, 168.53, 155.06, 168.3 ,158.98, 158.23, 165.27, 170.83,  161.31, 163.58])

from sklearn.linear_model import LinearRegression
model = LinearRegression()
# 均方誤差函數 ------------------------------
def mse_line(x, t, w):
    y = w[0] * x + w[1]
    mse = np.mean((y - t)**2)
    return mse

xn = 100    #等高線解析度
b0, b1 = -40, 41
a0, a1 = 40, 161
# Fit the model on the training data
w0 = np.linspace(b0, b1, xn)
w1 = np.linspace(a0, a1, xn)
ww0, ww1 = np.meshgrid(w0, w1)
J = np.zeros((len(w0), len(w1)))
for i0 in range(len(w0)):
    for i1 in range(len(w1)):
        J[i1, i0] = mse_line(year, height, (w0[i0], w1[i1]))

# 顯示 --------------------------------------
plt.figure(figsize=(5, 5))
from matplotlib import ticker
from mpl_toolkits.mplot3d import Axes3D

formatter = ticker.ScalarFormatter()
formatter.set_scientific(False)
# A
ax = plt.subplot(1, 1, 1, projection='3d')
ax.yaxis.set_major_formatter(formatter)
ax.plot_surface(ww0, ww1, J, rstride=20, cstride=20, alpha=0.3,
                color='blue', edgecolor='black')
ax.set_xticks(np.arange(b0, b1, 20))
ax.set_yticks(np.arange(a0, a1, 20))
# scatter

#ax.quiver([-20],[80],[300000],[-20],[80],[300000], colors='b')
#ax.quiver([0],[0],[0],[1],[b],[a], colors='r')
#ax.quiver([0],[0],[0],[10],[60],[0], colors='r')
ax.quiver(-20, 70, 300000, -10, 16, 10,  color='blue', arrow_length_ratio = 0.1)
ax.quiver(-20, 70, 300000, 10, -16, 10,  color='red', arrow_length_ratio = 0.5, pivot='tail', length=1)
ax.scatter(-20, 70, 300000, color='green')
#ax.quiver(0,0,0,10,10,10,color='b',arrow_length_ratio = 0.1)

ax.set_xlabel('a')
ax.set_ylabel('b')
ax.set_zlabel('SSE')
ax.view_init(20, -60)
plt.savefig('images/SSELossC.png', dpi=300)
#+end_src
雖然我們可以快速的利用如[[https://scikit-learn.org/][scikit-learn]]這類第三方模組求出最佳解，但是相信對於有志投入AI領域的你來說，光知道如何快速求解顯然遠遠不夠，讓我們來搞清楚這到底是怎麼完成的。

*** 隨機的力量
:PROPERTIES:
:ID:       7cd4a142-4cd9-46b6-b9a4-2ad750ae622f
:END:
萬事起頭難，要找出最佳的參數組合 $(a,b)$ ，最合理的方式就是我們 *閉上眼睛* 在圖[[fig:SSELossA]]中隨意點圈出一個點b $(a_0, b_0)$，這就是我們的第一步，其結果就如圖[[fig:SSELossC]]所示。有了這個開頭，我們接下來要做的事就是：
1. 找出 *一個方法* 來判斷要由點 $(a_0, b_0)$ 點沿著這個曲面的 *哪一個方向* 前進 *多遠* ，來到下一點 $(a_1, b_1)$。也許是沿著曲面往上移一小段(如圖[[fig:SSELossC]]中的藍色線段)、也許是沿著曲面往下移一小段(如圖[[fig:SSELossC]]中的紅色線段)。
3. 利用 *同一個方法* 來判斷接下來要由點 $(a_1, b_1)$ 點沿著這個曲面的 *哪一個方向* 繼續前進 *多遠* ，來到下一點 $(a_2, b_2)$
4. 重複同樣的步驟，直到找到最佳的點 $(a_n, b_n)$ ，也就是這一點 $(a_n, b_n)$ 能使整個模型的SSE來到最小，讓模型具備最佳的預測效能。
#+CAPTION: 找出最佳a,b組合的方法
#+name: fig:SSELossC
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 400
#+ATTR_HTML: :width 500
[[file:images/SSELossC.png]]

*** 何去何從
發現了嗎？其實AI的本質就是在解數學問題，我們在求某個方程式的最小值。

到這裡我想你一定會發現上面那個方法的幾個漏洞：
- 我怎麼知道要往哪個方向移呢？
- 我怎麼知道要移動多長的距離呢?
- 我怎麼知道移動後的新位置比原來的位置好呢？

好吧，我也不知道。不如我們先跳過這個看起來太複雜的問題，先換個簡單點的來強化自信。

* 保持距離以測安全
讓我們先來看一個更簡單的例子。

這是一組從[[https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html][R語言資料集]]偷來的資料，這個資料集有七百多組教學用的資料集，其中有一組簡單的資料集cars，裡面有50筆資料，每筆資料只有兩個欄位：
- speed: 車速
- dist: 所需剎車距離
資料分佈如圖[[fig:carsScatter]]所示
#+begin_src python -r -n :results output :exports none
# use library pydatset to get data
from pydataset import data
import matplotlib.pyplot as plt

#print("\n\n","There are %s" %len(data()), "datasets in pydataset library")
## access data
#cars = data('cars')
#print(len(cars))
#print(cars.head(5))
plt.scatter(cars['speed'], cars['dist'])
plt.savefig("images/carsScatter.png", dpi=300)
#+end_src

#+RESULTS:
#+CAPTION: 車速與剎車距離關係分佈圖
#+name: fig:carsScatter
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/carsScatter.png]]

我們為了這組簡單的資料分佈建了一個如下的模型
$$dist=w*speed$$
建立這組模型的最終目的當然是希望輸入 *車速* ($x$) 後就能得到 *預測的剎車距離* ( $y_{predicted}$ ) ，即
$$ y_{predicted} = w * x $$
或是更常見的寫法：
$$ \hat{y} = w * x $$
同時，為了評估不同 $w$ 值下模型的優劣，我們當也要提出相對應的損失函數：
$$ Loss = \frac{1}{n}\sum_{1}^{n}(y_{predicted} - y_{actual})^2 $$
例如：
- 當 $w$ 為-15時，Loss值為85113.26
- 當 $w$ 為-10時，Loss值為44346.86
- 當 $w$ 為 -5時，Loss值為16808.46
- 當 $w$ 為  0時，Loss為2498.06
- 當 $w$ 為  5時，Loss為1415.66
- 當 $w$ 為 10時，Loss為17152.28
** 模型的目的
顯然，解出方程式(或是說找到最佳模型) $dist=w*speed$ 在於找到一個最佳的參數 $w$，一開始當然沒啥頭緒，那，不如就暴力一點吧，弄個窮舉法：試試從-20 try到+20吧，觀察一下損失函數Loss的變化：
#+begin_src python -r -n :results output :exports none
from pydataset import data
import matplotlib.pyplot as plt
import numpy as np

cars = data('cars')
speed = np.array(cars['speed'])
dist = np.array(cars['dist'])
w = np.arange(-20,21,0.1)
loss = []
for i in w:
    yHat =  i * speed
    loss.append(np.mean((yHat - dist)**2))
#    if int(i % 5) == 0:
#        print(f'{i:.2f}: {loss[-1]:.2f}')

plt.figure(figsize=(7, 5))
plt.plot(w, loss)
plt.xlabel(r'w')
plt.ylabel(r'Loss')
plt.savefig("images/carsLoss.png", dpi=300)
#+end_src

#+CAPTION: 不同參數w下的損失函數Loss分佈圖
#+name: fig:carLoss
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 400
#+ATTR_HTML: :width 500
[[file:images/carsLoss.png]]

現在我們可以想辦法找出最好的 $w$ 在哪裡了。

** 沿著曲線上下爬
雖然我們從圖[[fig:carLoss]]大概可以看出來模型大概在參數 $w$ 介於0和5之間會有最小的Loss，也就是模型會最準確，但身為嚴謹的學術研究者，我們不能這樣蠻幹，這是土匪的行為，我們要用最科學的方法：既然不知道從哪裡著手，就閉著眼睛隨意給個 $w$ 好了，例如：-15，如圖[[fig:carLoss1]]。

你看，我們這不就邁出成功的第一步了?

隨機就是這麼美而有力!!

#+begin_src python -r -n :results output :exports none
from pydataset import data
import matplotlib.pyplot as plt
import numpy as np

cars = data('cars')
speed = np.array(cars['speed'])
dist = np.array(cars['dist'])
w = np.arange(-20,21,0.5)
loss = []
for i in w:
    yHat =  i * speed
    loss.append(np.mean((yHat - dist)**2))

# 畫圖====================
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
fig, ax = plt.subplots()

arrowprop1 = dict(arrowstyle="->",color='red')
arrowprop2 = dict(arrowstyle="->",color='blue')
ax.plot(w, loss)
x, y = -15, loss[np.where(w == -15)[0][0]]
ax.scatter(x, y, color='r')
ax.plot([-15, -15], [0, loss[np.where(w == -15)[0][0]]], 'g--')
ax.text(x+1, y, '隨意指定w得到的Loss', color='black')
ax.set_xlabel(r'w')
ax.set_ylabel(r'Loss')
plt.savefig("images/carsLoss1.png", dpi=300)
#+end_src
#+CAPTION: 先隨機假設一個數(-15)為最佳參數w的值
#+name: fig:carLoss1
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/carsLoss1.png]]

有了出發點(我們估且稱之為 $w_0$ 好了，如圖[[fig:carLoss1]])，接下來就只要決定下一個「較好的下一個 $w_1$ 」是在 $w_0$ 的左邊還是右邊(根據Loss值來判斷)，然後繼續往左或往右移(如圖[[fig:carLoss2]])。接下來我們只要決定以下兩個因素，就可以利用python把模型的最佳參數 $w$ 找出來了。
1. 每次要往左或往右移多少距離?
2. 這樣的修正動作要重複幾次？（或，結束的條件為何？)

#+begin_src python -r -n :results output :exports none
from pydataset import data
import matplotlib.pyplot as plt
import numpy as np

cars = data('cars')
speed = np.array(cars['speed'])
dist = np.array(cars['dist'])
w = np.arange(-20,21,0.5)
loss = []
for i in w:
    yHat =  i * speed
    loss.append(np.mean((yHat - dist)**2))

# 畫圖====================
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
fig, ax = plt.subplots()

arrowprop1 = dict(arrowstyle="->",color='red')
arrowprop2 = dict(arrowstyle="->",color='blue')
ax.plot(w, loss)
x, y = -15, loss[np.where(w == -15)[0][0]]
ax.scatter(x, y, color='r')
x1, y1 = -20, loss[np.where(w == -20)[0][0]]
ax.scatter(x1, y1, color='r')
x2, y2 = -10, loss[np.where(w == -10)[0][0]]
ax.scatter(x2, y2, color='r')
ax.plot([-15, -15], [0, loss[np.where(w == -15)[0][0]]], 'g--')
ax.plot([-20, -20], [0, loss[np.where(w == -20)[0][0]]], 'g--')
ax.plot([-10, -10], [0, loss[np.where(w == -10)[0][0]]], 'g--')
ax.set_xlabel(r'w')
ax.set_ylabel(r'Loss')
ax.annotate('',xy=(x-5, 200), xytext=(x, 0),arrowprops=arrowprop1)
ax.annotate('',xy=(x+5, 200), xytext=(x, 0),arrowprops=arrowprop2)
ax.text(x+1, y, rf'$w_0$', color='green')
ax.text(x1+1, y1, rf'$w_1$', color='green')
ax.text(x2+1, y2, rf'$w_1$', color='green')
ax.text(x-4, 5000, '往左移', color='red')
ax.text(x+1, 5000, '往右移', color='blue')
plt.savefig("images/carsLoss2.png", dpi=300)
#+end_src
#+CAPTION: 決定w應往哪個方向移動
#+name: fig:carLoss2
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 400
#+ATTR_HTML: :width 500
[[file:images/carsLoss2.png]]

** 確定方向
BUT，相信學過幾何學的你一定有想到另一種策略：切線。既然 $w$ 與 Loss的關係是如圖[[fig:carLoss2]]的曲線，我們應該可以找出 $w_0$ 這個點的 *切線* 。如果我們從點 $w_0$ 各向左、右移動一段很小的距離(例如0.0000001)，所連接的這條線(如圖[[fig:carLoss3]])就很接近點 $w_0$ 沿曲線的切線了。

根據斜率的計算公式(或是以肉眼觀察這條切線)，我們發現這條切線的斜率是負的。這表示
1. 曲線的最低點應該是出現在點 $w_0$ 的右側
2. 下一點 $w_1$ 要往右邊找

#+begin_src python -r -n :results output :exports none
from pydataset import data
import matplotlib.pyplot as plt
import numpy as np

cars = data('cars')
speed = np.array(cars['speed'])
dist = np.array(cars['dist'])
w = np.arange(-20,21,0.5)
loss = []
for i in w:
    yHat =  i * speed
    loss.append(np.mean((yHat - dist)**2))

# 畫圖====================
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
fig, ax = plt.subplots()

arrowprop1 = dict(arrowstyle="->",color='red')
arrowprop2 = dict(arrowstyle="->",color='blue')
ax.plot(w, loss, 'y--')
x, y = -15, loss[np.where(w == -15)[0][0]]
ax.scatter(x, y, color='r')

# 計算斜率用
xdist = 0.5
x1, y1 = -15+xdist, loss[np.where(w == -15+xdist)[0][0]]
x2, y2 = -15-xdist, loss[np.where(w == -15-xdist)[0][0]]
m = (y1-y2)/(x1-x2)
#print('斜率:',m)
ax.text(x1+4, y1, rf'$w_0+0.0000001$', color='blue')
ax.plot([x1+3, x1], [y1, y1], 'b--')

ax.text(x2+4, y2, rf'$w_0-0.0000001$', color='blue')
ax.plot([x2+3, x2], [y2, y2], 'b--')

ax.scatter(x1, y1, color='b')
ax.scatter(x2, y2, color='b')
ax.plot([-15, -15], [0, loss[np.where(w == -15)[0][0]]], 'g--')
xdist = 0.5
x1, y1 = -15+xdist, loss[np.where(w == -15+xdist)[0][0]]
x2, y2 = -15-xdist, loss[np.where(w == -15-xdist)[0][0]]
#---???
m = (y1-y2)/(x1-x2)
x1, x2 = -15 - 5, -15 + 5
y1, y2 = y-5*m, y+5*m
ax.plot([x1, x2], [y1, y2], c='black')
#---???
ax.set_xlabel(r'w')
ax.set_ylabel(r'Loss')
ax.annotate('',xy=(x-5, 200), xytext=(x, 0),arrowprops=arrowprop1)
ax.annotate('',xy=(x+5, 200), xytext=(x, 0),arrowprops=arrowprop2)

ax.text(x-5, y, rf'$w_0$', color='red')
ax.plot([x-3, x], [y, y], 'r--')
ax.text(x-4, 5000, '往左移', color='red')
ax.text(x+1, 5000, '往右移', color='blue')
plt.savefig("images/carsLoss3.png", dpi=300)
#+end_src
#+CAPTION: 決定w應往哪個方向移動
#+name: fig:carLoss3
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 400
#+ATTR_HTML: :width 500
[[file:images/carsLoss3.png]]

問題是：該往右邊移動多少距離呢？要移動幾次？

** 確定移動距離與重複次數
由圖[[fig:carLoss3]]的w與Loss分佈，不難發現可以逐步往右移動w，Loss的值就會慢慢降下來，所以我們可以先這麼計畫：
- 每次往右邊加0.5、直到Loss不再變小。
或換另一種說法：
- 每次往右邊加0.5、直到Loss開始變大(因為越過了曲線最低點)。

上述的Python實作程式碼如下：
#+begin_src python -r -n :results output :exports both
from pydataset import data
import numpy as np

# 取得資料集
cars = data('cars')
speed = np.array(cars['speed'])
dist = np.array(cars['dist'])

# 計算Loss用的function
def loss_func(y_true, y_predict):
    return y_true - y_predict

w = np.arange(-20,21,0.5)
loss = []
for i in w:
    yHat =  i * speed
    loss.append(np.mean(loss_func(dist, yHat)**2))

# 找最佳w值, 這裡以x代表w值
x, y = -15, loss[np.where(w == -15)[0][0]]
while True:
    loss = np.mean(loss_func(dist, x*speed)**2)

    x += 0.5
    newLoss = np.mean(loss_func(dist, x*speed)**2)
    if newLoss >= loss:
        print(f'STOP: w值:{x-0.5}, Loss:{loss}')
        print(f'NEXT: w值:{x}, Loss:{newLoss}')
        break
    if int(x) % 5 == 0:
        print(f'w值:{x}, Loss:{newLoss}')
#+end_src
由執行結果可發現隨著 $w$ 值的增加，Loss值也隨之減少，直到 $w$ 值為3時可以得到最低的Loss值(261.26)，過了這一點，Loss值便又開始增加。圖[[fig:carLoss4]]為w值持續修正的模擬結果。
#+RESULTS:
: w值:-10.5, Loss:47828.24
: w值:-10.0, Loss:44346.86
: w值:-5.5, Loss:18967.04
: w值:-5.0, Loss:16808.46
: w值:-0.5, Loss:3333.84
: w值:0.0, Loss:2498.06
: w值:0.5, Loss:1794.56
: STOP: w值:3.0, Loss:261.26
: NEXT: w值:3.5, Loss:351.44

#+begin_src python -r -n :results output :exports none
from pydataset import data
import matplotlib.pyplot as plt
import numpy as np

# 取得資料集
cars = data('cars')
speed = np.array(cars['speed'])
dist = np.array(cars['dist'])

# 計算Loss用的function
def loss_func(y_true, y_predict):
    return y_true - y_predict

# 畫圖====================
w = np.arange(-20,21,0.5)
loss = []
for i in w:
    yHat =  i * speed
    loss.append(np.mean(loss_func(dist, yHat)**2))
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
fig, ax = plt.subplots()

ax.plot(w, loss, 'y--')
# 找最佳w值, 這裡以x代表w值
x, y = -15, loss[np.where(w == -15)[0][0]]
i = 0
ax.scatter(x, y, color='r')
ax.text(x+1, y, rf'$w_{i}$', color='blue')
while True:
    loss = np.mean(loss_func(dist, x*speed)**2)

    x += 0.5
    i += 1
    newLoss = np.mean(loss_func(dist, x*speed)**2)
    if newLoss >= loss:
        print('STOP:', x-0.5, loss)
        break
    ax.scatter(x, newLoss, color='r')
    if i < 10:
        ax.text(x+1, newLoss, rf'$w_{i}$', color='blue')
    else:
        ax.text(x+1, newLoss, rf'...', color='blue')

ax.set_xlabel(r'w')
ax.set_ylabel(r'Loss')

'''

arrowprop1 = dict(arrowstyle="->",color='red')
arrowprop2 = dict(arrowstyle="->",color='blue')

ax.scatter(x, y, color='r')

# 計算斜率用
xdist = 0.5
m = (y1-y2)/(x1-x2)

ax.text(x1+4, y1, rf'$w_0+0.0000001$', color='blue')
ax.plot([x1+3, x1], [y1, y1], 'b--')

ax.text(x2+4, y2, rf'$w_0-0.0000001$', color='blue')
ax.plot([x2+3, x2], [y2, y2], 'b--')

ax.scatter(x1, y1, color='b')
ax.scatter(x2, y2, color='b')
ax.plot([-15, -15], [0, loss[np.where(w == -15)[0][0]]], 'g--')
xdist = 0.5
x1, y1 = -15+xdist, loss[np.where(w == -15+xdist)[0][0]]
x2, y2 = -15-xdist, loss[np.where(w == -15-xdist)[0][0]]
#---???
m = (y1-y2)/(x1-x2)
x1, x2 = -15 - 5, -15 + 5
y1, y2 = y-5*m, y+5*m
ax.plot([x1, x2], [y1, y2], c='black')
#---???
ax.annotate('',xy=(x-5, 200), xytext=(x, 0),arrowprops=arrowprop1)
ax.annotate('',xy=(x+5, 200), xytext=(x, 0),arrowprops=arrowprop2)

ax.text(x-5, y, rf'$w_0$', color='red')
ax.plot([x-3, x], [y, y], 'r--')

'''
plt.savefig("images/carsLoss4.png", dpi=300)

#+end_src

#+CAPTION: 決定w應往哪個方向移動
#+name: fig:carLoss4
#+ATTR_LATEX: :width 500
#+ATTR_ORG: :width 500
#+ATTR_HTML: :width 500
[[file:images/carsLoss4.png]]

* 線性迴歸[fn:2]
[[https://tree.rocks/deep-learning-from-scratch-by-linear-regression-e42f5dcdb024][手刻 Deep Learning — 第零章 — 線性回歸]]
原始資料:
#+CAPTION: Caption
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/Xyh-1.png]]

#+begin_src python -r -n :results output :exports both
import numpy as np
import matplotlib.pyplot as plt

def gen_data(X, a, b):
    return X * a + b

## 重新產生X, y，較合理，而非已存在一條線
X = np.array(range(1, 10))
y = np.array([27, 35, 40, 50, 66, 60, 76, 88, 90])
#y = gen_data(X, a=8, b=20)

plt.scatter(X, y, color='black')
plt.plot(X, 1 * X + 1)
plt.plot(X, 4 * X + 10)
plt.plot(X, 4 * X + 10)
plt.plot(X, 8 * X + 30)
plt.ylim(0, 121)
plt.legend(['Raw Data', 'Line 1', 'Line 2', 'Line 3'])
plt.savefig("images/Xyh-1.png", dpi=300)

a = 1
b = 1
yh = a * X + b #y hat


plt.plot(X, yh)
#plt.savefig("images/Xyh.png", dpi=300)

def loss_func(y_true, y_predict):
    return y_true - y_predict

def optimizer(d, loss):
    return np.mean(d * loss * 0.01)

N = 1000
for i in range(N):
    p_y = a * X + b
    loss = loss_func(y, p_y)
    a -= optimizer(-2 * X, loss)
    b -= optimizer(-2, loss)
    if i % int(N/10) == 0:
        print('誤差: {:.2f}'.format(np.mean(loss)), '目前 a: {:.2f}, b: {:.2f}'.format(a, b))

yh = a * X + b #y hat
#plt.plot(X, yh)
#plt.legend(['Target', 'Initialization', 'Optimization'])
#plt.savefig("images/Xyh.png", dpi=300)
#+end_src

#+RESULTS:
#+begin_example
[[1]
 [2]
 [3]
 [4]
 [5]
 [6]
 [7]
 [8]
 [9]]
誤差: 53.11 目前 a: 7.27, b: 2.06
誤差: 2.11 目前 a: 9.84, b: 7.81
誤差: 1.40 目前 a: 9.29, b: 11.26
誤差: 0.93 目前 a: 8.93, b: 13.54
誤差: 0.61 目前 a: 8.69, b: 15.05
誤差: 0.41 目前 a: 8.53, b: 16.06
誤差: 0.27 目前 a: 8.42, b: 16.72
誤差: 0.18 目前 a: 8.35, b: 17.16
誤差: 0.12 目前 a: 8.31, b: 17.45
誤差: 0.08 目前 a: 8.28, b: 17.65
#+end_example


#+CAPTION: Caption
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/Xyh.png]]

開始 Linear Regression (線性回歸)

練習投藍的時後，我們需要知道籃筐位置，誤差多少，做出丟球的修正；做 Machine Learning 也是一樣道理，我們需要 :
1. 找出誤差
2. 做出修正

所以我們這邊帶入兩個觀念:
1. loss function (誤差計算，找出誤差)
2. optimizer (最佳化方法，做出修正)

我們用程式碼來看
loss function: 其中 loss_func 的 y_true 表示商店的真實價格，y_predict 是我們預測的價格，我們這邊採用 真實價格 減去 預測價格，就是預測的誤差
#+begin_src python -r -n :results output :exports both
def loss_func(y_true, y_predict):
    return y_true - y_predict
#+end_src
optimizer: 這邊有個參數叫做 d ，其實他是 partial derivative ，這是微積分的概念。optimizer的修正並非最佳，可以自行修正找出最佳參數
#+begin_src python -r -n :results output :exports both
def optimizer(d, loss):
    return np.mean(d * loss * 0.01)
#+end_src

上面就是我們的訓練用程式碼，跑 1000 次訓練，每 100 次 ( N/10 ) 我們印出一次誤差讓我們看看過程
其中：
a -= optimizer(-2 * X, loss)
b -= optimizer(-2, loss)
這邊就是每次的訓練我們都在調整 a 與 b，就像是我們投籃丟歪球了，每次練習都在調整力道

各位可以試看看將 a 與 b 改成任意數值 ( 不要太過極端以免 overflow )，在這個訓練過程中，不管 a, b 初始是多少，都會逐漸往我們正確答案靠近，為什麼會這樣呢？

    這就是微積分的力量

大多的 Machine Learning 也是類似這種方法，不停的 Training ( 訓練 ) 找到答案，微積分這部分日後有空再來解說 XD

微分: https://tree.rocks/deep-learning-from-scratch-introduce-differential-91f5b4400d1a

** sklear版solution
#+begin_src python -r -n :results output :exports both
import numpy as np
X = np.arange(1, 10).reshape(-1, 1) #轉換矩陣形狀以符合sklearn要求
y = [27, 35, 40, 50, 66, 60, 76, 88, 90]

from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X, y)

print('Slope:', model.coef_)
print('Intercept:', model.intercept_)
#+end_src

#+RESULTS:
: Slope: [8.21666667]
: Intercept: 18.02777777777777


* BOOK
- Title: Mastering Machine Learning with scikit-learn
- Author: Gavin Hackeling

* Footnotes
[fn:2] [[https://tree.rocks/deep-learning-from-scratch-by-linear-regression-e42f5dcdb024][手刻 Deep Learning — 第零章 — 線性回歸]]

[fn:1] Hands-On Machine Learning with Scikit-Learn: Aurelien Geron
