:PROPERTIES:
:ID:       6ae7fb7a-0b38-4448-b19f-073d262513f2
:ROAM_ALIASES: Regression
:END:

#+TITLE: 迴歸
# -*- org-export-babel-evaluate: nil -*-
#+TAGS: AI
#+OPTIONS: toc:2 ^:nil num:5
#+PROPERTY: header-args :eval never-export
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../css/muse.css" />
#+EXCLUDE_TAGS: noexport
#+latex:\newpage
#+begin_export html
<a href="https://letranger.github.io/AI/20221023154410-regression.html"><img align="right" alt="Hits" src="https://hits.sh/letranger.github.io/AI/20221023154410-regression.html.svg"/></a>
#+end_export

* 簡介
** 迴歸類型
即，根據一組預測特徵（predictor，如里程數、車齡、品牌）來預測目標數值（如二手車車價）[fn:1]，這個目標數值也叫label。

部份迴歸演算法也可以用來分類，例如Logistic，它可以輸出一個數值，以這個數值來表示對應到特定類別的機率，例如，某封email為垃圾郵件的機率為20%、某張圖片為狗的機率為70%。

迴歸問題可分為兩類：
- Linear regression:
  * 假設輸入變量(x)與單一輸出變量(y)間存在線性關係，並以此建立模型。
  * 優點: 簡單、容易解釋
  * 缺點: 輸入與輸出變量關係為線性時會導致低度擬合
  * 例: 身高與體重間的關係
- Logistic regression
  * 也是線性方法，但使用logist function轉換輸出的預測結果，其輸出結果為類別機率(class probabilities)
  * 優點: 簡單、容易解釋
  * 缺點: 輸入與輸出變量關係為線性時無法處理分類問題

典型迴歸案例: Boston Housing Data

** 迴歸原理
*** Step 1
1. Model: $y = w*x+b$
2. Data: 找一堆現成的資料
*** Step 2: Goodness of Function
1. Training Data
2. Loss function L: 越小越好
   input: a function / output: how bad it is
3. Pick the *Best Function* :
   $f* = arg min L(f)$
   上述可以微分來求最佳解，即求 function L 的最小值
4. 數值最佳解: Gradient Descent(找拋物線/面最低點)

** 迴歸預測流程(以波士頓房價預測為例)
1. Import the required module
1. Load and configure the Boston housing data set
1. Chekc the relation between the variable, using pairplot and correlation graph
1. Descriptive statistics: central tendency and dispersion
1. Select the required columns
1. Train the test split
1. Normalize the data
1. Build the input pipeline for the TensorFlow model
1. Model tranining
1. Predictions
1. Validation

* 線性迴歸:年齡身高預測#1
<<datagen>>
** 資料生成
這是當初上帝創造人類時決定人類身高的規則，我們也可以將之視為這組資料的模型，這個規則或模型是很神祕的，等一下我們要假裝我們不知道這個模型的存在，而迴歸的目的就在於想辦法猜出這個規則或模型。
#+begin_src ipython -r :results output :exports both :session test
import numpy as np
import matplotlib.pyplot as plt

n = 10                               # 資料筆數
year = 5 + 25 * np.random.rand(n)  # 年紀
height = 170 - 108 * np.exp(-0.2 * year) + 4 * np.random.randn(n)
print(year)
print(height)
#+end_src
: [13.3, 16.2, 10.9, 28.7, 19.8, 14.2, 11.7, 26.6, 22.4, 18.3, 19.4]
: [163.61, 168.53, 155.06, 171.3 , 166.69, 160.98, 158.23, 165.27, 170.83,  161.31, 163.58]

** 查看資料
對於平凡的人類而言，他們只能看到身邊的人們隨著年齡增長而出現身高的變化，也就是由神袐模型所生成的數字：年齡和身高(如圖[[fig:yearHeight]])。
#+begin_src python -r :results output :exports none
import numpy as np
import matplotlib.pyplot as plt

year = np.array([13.3, 16.2, 10.9, 28.7, 14.2, 11.7, 26.6, 22.4, 18.3, 20.4])
height = np.array([163.61, 168.53, 155.06, 168.3 ,158.98, 158.23, 165.27, 170.83,  161.31, 163.58])

plt.figure(figsize=(5, 5))
plt.plot(year, height, marker='o', linestyle='None',
         markeredgecolor='black', color='cornflowerblue')

plt.xlim(5, 35)
plt.ylim(150,175)

plt.grid(True)
plt.ylabel('Height')
plt.xlabel('Year')
plt.savefig("images/yearHeight.png")
#+end_src

#+RESULTS:

#+CAPTION: 年齡與身高的資料分佈
#+name: fig:yearHeight
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/yearHeight.png]]

但那些一身反骨的數學家則不甘於當平凡人，他們想透過統計、分析、思考、通靈等方式對這個既有現象進行逆向工程，去推估這個現象背後的神祕規則，藉此窺探上帝的意志。
這些規則也許是如圖[[fig:yearHeightModel]]中的各種線段。一但找到了規則，我們就能根據這些規則進行 *預測* ，例如，由某人的年齡來合理推估他的身高。
#+begin_src python -r :results output :exports none
import numpy as np
import matplotlib.pyplot as plt

year = np.array([13.3, 16.2, 10.9, 28.7, 14.2, 11.7, 26.6, 22.4, 18.3, 20.4])
height = np.array([163.61, 168.53, 155.06, 168.3 ,158.98, 158.23, 165.27, 170.83,  161.31, 163.58])

plt.figure(figsize=(5, 5))
plt.plot(year, height, marker='o', linestyle='None',
         markeredgecolor='black', color='cornflowerblue')
plt.xlim(5, 35)
plt.ylim(150,175)
plt.grid(True)
plt.plot((10, 30), (153, 173), color='r', linestyle='-')
plt.plot((10, 30), (160, 169), color='b', linestyle='--')
plt.plot((10, 30), (157, 171), color='g', linestyle='--')

plt.ylabel('Height')
plt.xlabel('Year')
plt.savefig("images/yearHeightModel.png")
#+end_src

#+RESULTS:
: [13.3 16.2 10.9 28.7 19.8 14.2 11.7 26.6 22.4 18.3 19.4]
: [163.61 168.53 155.06 171.3  166.69 160.98 158.23 165.27 170.83 161.31
:  163.58]

#+CAPTION: 隱藏在年齡與身高資料背後的規則(模型)
#+name: fig:yearHeightModel
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/yearHeightModel.png]]

** 直線模型
我們可以在圖[[fig:yearHeightModel]]中畫上無數條線，但，最能代表年齡和身高關係的線應該只有一條，我們要如何找出這條線？

首先，既然我們想以 *直線* 來表示我們想找的模型或規則，那我們就先把這條直線以下列數學示表示出來:
$$y=ax+b$$ 或 $$f(x)=ax+b$$
這樣的直線 $y$ 或函數 $f(x)$ 有無限多個，迴歸的目的就是要為函數 $f(x)$ 找出一組最好的參數 $a,b$，或是為直線 $y$ 找到最適合的斜率 $a$ 和截距 $b$。這也是現今許多AI模型的基本精神：找到一組最好的參數，或者說：從無數個可能的模型中挑出最好的一個。

為了從無限多個備選模型中找出最佳的，我們需要有一個評估機制。

** 損失函數
#+begin_src python -r :results output :exports none
import numpy as np
import matplotlib.pyplot as plt

year = np.array([13.3, 16.2, 10.9, 28.7, 14.2, 11.7, 26.6, 22.4, 18.3, 20.4]).reshape([-1, 1])
height = np.array([163.61, 168.53, 155.06, 168.3 ,158.98, 158.23, 165.27, 170.83,  161.31, 163.58])

from sklearn.linear_model import LinearRegression
model = LinearRegression()
# Create an instance of the estimator
model.fit(year, height)

# Fit the model on the training data
plt.figure(figsize=(5, 5))
plt.grid(True)
plt.xlim(5, 35)
plt.ylim(150,175)
plt.scatter(year, height, color = 'k')
i = 0
for x, y in zip(year, height):
    plt.text(x+0.3, y-0.5, rf'$y_{i}$', color='red')
    i += 1

dy = (model.predict(year)-height)/2
i = 0
for x, y1, y2 in zip(year, height, model.predict(year)):
    xs = [x, x]
    ys = [y1, y2]
    plt.plot(xs, ys, color='orange')
    plt.text(x-0.4, y2+0.5, rf'$\hat y_{i}$', color='blue')
    i += 1
#====================
#plt.scatter(X, y, color = 'k')
#trueA = (model.predict(year)[1] - model.predict(year)[2])/(year[1][0]- year[2][0])
#trueB = model.predict(year)[1] - trueA * year[1][0]
#print(f'{trueA}') #0.7426437155683577
#print(f'{trueB}') #149.72040800429542
#====================
plt.plot(year, model.predict(year), color='g')


plt.plot(year, model.predict(year), color='g')
plt.xlabel('Year')
plt.ylabel('Height')
plt.savefig('images/yearHeightLoss.png', dpi=300)
#+end_src

#+RESULTS:

損失函數(loss function)也稱為成本函數(cost function)，就是最常用來定義、衡量模型誤差的方法。以圖[[fig:yearHeightLoss]]為例，我們可以計算所有原始資料$(x_0, y_0) \dots (x_9, y_9)$ 離這條預測線的距離(預測結果為 $\hat{y_0} \dots \hat{y_9}$)，這些距離( $y_0 - \hat{y_0} \dots y_9 - \hat{y_9}$ )的總和越小，表示預測線離每一點越近，也就是說這個模型越準確。

#+RESULTS:
#+CAPTION: 直線模型的均方誤差
#+name: fig:yearHeightLoss
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 600
[[file:images/yearHeightLoss.png]]

圖[[fig:yearHeightLoss]]中的 $y_i$ 為實際資料 $x_i$ 對應的結果， 而 $\hat{y_i}$ 則是將每個實際資料 $x_i$ 丟入模型後的預測結果，計算 $y_i$ 與 $\hat{y_i}$ 誤差的方法稱為 *殘差平方和* (Residual Sum of Squares, RSS)，計算公式為
$$ RSS = \sum_{i=1}^{n}(\hat{y_i}-y_i)^2 $$
把RSS再除以n就或是 *均方差* (Mean Square Error, MSE)，即
$$ MSE = \frac{1}{n}\sum_{i=1}^{n}(\hat{y_i}-y_i)^2 $$
迴歸的任務就是把RSS或MSE最小化。

如何讓RSS/MSE最小化呢？

** 窮舉所有的可能性
#+begin_src python -r :results output :exports none
import numpy as np
import matplotlib.pyplot as plt

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS'] # 步驟一
plt.rcParams['axes.unicode_minus'] = False  # 步驟二（解決座標軸

year = np.array([13.3, 16.2, 10.9, 28.7, 14.2, 11.7, 26.6, 22.4, 18.3, 20.4]).reshape([-1, 1])
height = np.array([163.61, 168.53, 155.06, 168.3 ,158.98, 158.23, 165.27, 170.83,  161.31, 163.58])

from sklearn.linear_model import LinearRegression
model = LinearRegression()
# 均方誤差函數 ------------------------------
def mse_line(x, t, w):
    y = w[0] * x + w[1]
    mse = np.mean((y - t)**2)
    return mse

xn = 100    #等高線解析度
b0, b1 = -40, 41
a0, a1 = 40, 161
# Fit the model on the training data
w0 = np.linspace(b0, b1, xn)
w1 = np.linspace(a0, a1, xn)
ww0, ww1 = np.meshgrid(w0, w1)
J = np.zeros((len(w0), len(w1)))
for i0 in range(len(w0)):
    for i1 in range(len(w1)):
        J[i1, i0] = mse_line(year, height, (w0[i0], w1[i1]))

# 顯示 --------------------------------------
plt.figure(figsize=(5, 5))
from matplotlib import ticker

formatter = ticker.ScalarFormatter()
formatter.set_scientific(False)
# A
ax = plt.subplot(1, 1, 1, projection='3d')
ax.yaxis.set_major_formatter(formatter)
ax.plot_surface(ww0, ww1, J, rstride=20, cstride=20, alpha=0.3,
                color='blue', edgecolor='black')
ax.set_xticks(np.arange(b0, b1, 20))
ax.set_yticks(np.arange(a0, a1, 20))
ax.tick_params(axis='x', labelsize=8)
ax.tick_params(axis='y', labelsize=8)
ax.tick_params(axis='z', labelsize=8)

ax.set_xlabel('a')
ax.set_ylabel('b')
ax.set_zlabel('RSS')
ax.view_init(20, -60)
plt.savefig('images/SSELossA.png', dpi=300)
#+end_src

#+RESULTS:

為了找出哪一組參數 $a,b$ 可以讓模型 $y=ax+b$ 的預測誤差達到最小，我們可以將一些合理的a,b值可能組合都列出來，如圖[[fig:SSELossA]]，我們列出了由參數 $a$ (-40~40)、參數 $b$ (40~160)的所有可能模型，圖中的 $z$ 軸代表每一種模型產生的誤差(RSS)。由圖[[fig:SSELossA]]可以看出兩件事:
1. 參數 $a$ 對模型誤差的影響遠大於參數 $b$
2. 當參數 $a$ 的值接近0時，所生成的模型會有較低的MSE，也就是模型預測能力較好

#+CAPTION: 不同a,b情況下的均方差
#+name: fig:SSELossA
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 400
#+ATTR_HTML: :width 500
[[file:images/SSELossA.png]]

讓我們回憶一下等高線這個東西，如果我們把圖[[fig:SSELossA]]當成某個山谷的地形圖(z軸為高度)，那我們就可以畫出這個區域的等高線圖[[fig:SSELossB]](先別管我是怎麼畫出來的)，從等高線圖[[fig:SSELossB]]就能大概看出來當a的值約等於0、b的值約等於150時會有最低的SSE(如圖[[fig:SSELossB]]中的紅點，這是我透過觀落陰得到的訊息)。

#+begin_src python -r :results output :exports none
import numpy as np
import matplotlib.pyplot as plt

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS'] # 步驟一（替換系統中的字型，這裡用的是Mac OSX系統）
plt.rcParams['axes.unicode_minus'] = False  # 步驟二（解決座標軸負數的負號顯示問題）
year = np.array([13.3, 16.2, 10.9, 28.7, 14.2, 11.7, 26.6, 22.4, 18.3, 20.4]).reshape([-1, 1])
height = np.array([163.61, 168.53, 155.06, 168.3 ,158.98, 158.23, 165.27, 170.83,  161.31, 163.58])

from sklearn.linear_model import LinearRegression
model = LinearRegression()
# 均方誤差函數 ------------------------------
def mse_line(x, t, w):
    y = w[0] * x + w[1]
    mse = np.mean((y - t)**2)
    return mse

xn = 100    #等高線解析度
b0, b1 = -40, 41
a0, a1 = 40, 161
# Fit the model on the training data
w0 = np.linspace(b0, b1, xn)
w1 = np.linspace(a0, a1, xn)
ww0, ww1 = np.meshgrid(w0, w1)
J = np.zeros((len(w0), len(w1)))
for i0 in range(len(w0)):
    for i1 in range(len(w1)):
        J[i1, i0] = mse_line(year, height, (w0[i0], w1[i1]))

# 顯示 --------------------------------------
plt.figure(figsize=(5, 5))
from matplotlib import ticker

formatter = ticker.ScalarFormatter()
formatter.set_scientific(False)
# B
cont = plt.contour(ww0, ww1, J, 30, colors='green',
                   levels=[0, 100, 1000, 10000, 100000, 1000000], linewidths=0.5)
cont.clabel(fmt='%d', fontsize=6, colors='r')
plt.scatter(0.74, 150.72, color = 'red')
plt.xlabel("a")

plt.ylabel("b")
plt.grid(True)
plt.savefig('images/SSELossB.png', dpi=300)
#+end_src

#+RESULTS:

#+CAPTION: 不同a,b情況下的MSE(俯視/等高線)
#+name: fig:SSELossB
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 400
#+ATTR_HTML: :width 500
[[file:images/SSELossB.png]]

總之，看起來是有辦法找到最佳的模型的(只是有點麻煩)，這個方法稱為梯度下降，在這裡我們先知道有這麼個方法、知道這個方法可以找出最佳模型就好，至於深入探討這個方法是如何運作這件事，等我搞清楚了再說吧(或是等你們上大學再自己去研究)...

** 快速求出最佳解
雖然從無數組 $(a,b)$ 中找出最好的一組看似困難，不過其實許多現成的相關模組已經有了這些功能，例如[[https://scikit-learn.org/stable/][scikit-learn]]。以底下的程式為例：
#+begin_src python -r -n :results output :exports both
import numpy as np
from sklearn.linear_model import LinearRegression

year = np.array([13.3, 16.2, 10.9, 28.7, 14.2, 11.7, 26.6, 22.4, 18.3, 20.4]).reshape([-1, 1])
height = np.array([163.61, 168.53, 155.06, 168.3 ,158.98, 158.23, 165.27, 170.83,  161.31, 163.58])

model = LinearRegression() (ref:modelRegression)
model.fit(year, height) (ref:modelFit)

slope = model.coef_
intercept = model.intercept_
heightHat = year * slope + intercept

print('斜率/Slope:', slope)
print('截距/Intercept:', intercept)
#+end_src

#+RESULTS:
: 斜率/Slope: [0.58182444]
: 截距/Intercept: 152.74006747354875

在上述程式碼中，真正與計算迴歸有關的只有第[[(modelRegression)]]行(利用scikit-learn建立一個線性迴歸模型)與第[[(modelFit)]]行(把手上的10組 $(a,b)$ 資料丟進模型訓練)，
夠簡單吧，這樣我們就能畫出一條斜率約為0.58、截距約為152.74的最佳迴歸線(如圖[[fig:bestRegressionLine]]):
#+begin_src python -r -n :results output :exports none
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS'] # 步驟一（替換系統中的字型，這裡用的是Mac OSX系統）
plt.rcParams['axes.unicode_minus'] = False  # 步驟二（解決座標軸負數的負號顯示問題）

year = np.array([13.3, 16.2, 10.9, 28.7, 14.2, 11.7, 26.6, 22.4, 18.3, 20.4]).reshape([-1, 1])
height = np.array([163.61, 168.53, 155.06, 168.3 ,158.98, 158.23, 165.27, 170.83,  161.31, 163.58])

model = LinearRegression()
model.fit(year, height)

plt.figure(figsize=(5, 5))
plt.plot(year, height, marker='o', linestyle='None',
         markeredgecolor='black', color='cornflowerblue')
plt.xlim(5, 35)
plt.ylim(150,175)
plt.grid(True)
plt.plot(year, heightHat, color='r', linestyle='-')

plt.ylabel('Height')
plt.xlabel('Year')
plt.savefig("images/yearHeightModelHat.png")
#+end_src
#+CAPTION: 線性迴歸求解
#+name: fig:bestRegressionLine
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/yearHeightModelHat.png]]

** 逐步找出最佳解
<<sec:sbs>>
#+begin_src python -r :results output :exports none
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS'] # 步驟一（替換系統中的字型，這裡用的是Mac OSX系統）
plt.rcParams['axes.unicode_minus'] = False  # 步驟二（解決座標軸負數的負號顯示問題）
year = np.array([13.3, 16.2, 10.9, 28.7, 14.2, 11.7, 26.6, 22.4, 18.3, 20.4]).reshape([-1, 1])
height = np.array([163.61, 168.53, 155.06, 168.3 ,158.98, 158.23, 165.27, 170.83,  161.31, 163.58])

from sklearn.linear_model import LinearRegression
model = LinearRegression()
# 均方誤差函數 ------------------------------
def mse_line(x, t, w):
    y = w[0] * x + w[1]
    mse = np.mean((y - t)**2)
    return mse

xn = 100    #等高線解析度
b0, b1 = -40, 41
a0, a1 = 40, 161
# Fit the model on the training data
w0 = np.linspace(b0, b1, xn)
w1 = np.linspace(a0, a1, xn)
ww0, ww1 = np.meshgrid(w0, w1)
J = np.zeros((len(w0), len(w1)))
for i0 in range(len(w0)):
    for i1 in range(len(w1)):
        J[i1, i0] = mse_line(year, height, (w0[i0], w1[i1]))

# 顯示 --------------------------------------
plt.figure(figsize=(5, 5))
from matplotlib import ticker
from mpl_toolkits.mplot3d import Axes3D

formatter = ticker.ScalarFormatter()
formatter.set_scientific(False)
# A
ax = plt.subplot(1, 1, 1, projection='3d')
ax.yaxis.set_major_formatter(formatter)
ax.plot_surface(ww0, ww1, J, rstride=20, cstride=20, alpha=0.3,
                color='blue', edgecolor='black')
ax.set_xticks(np.arange(b0, b1, 20))
ax.set_yticks(np.arange(a0, a1, 20))
# scatter

#ax.quiver([-20],[80],[300000],[-20],[80],[300000], colors='b')
#ax.quiver([0],[0],[0],[1],[b],[a], colors='r')
#ax.quiver([0],[0],[0],[10],[60],[0], colors='r')
ax.quiver(-20, 70, 300000, -10, 16, 10,  color='blue', arrow_length_ratio = 0.1)
ax.quiver(-20, 70, 300000, 10, -16, 10,  color='red', arrow_length_ratio = 0.5, pivot='tail', length=1)
ax.scatter(-20, 70, 300000, color='green')
#ax.quiver(0,0,0,10,10,10,color='b',arrow_length_ratio = 0.1)

ax.set_xlabel('a')
ax.set_ylabel('b')
ax.set_zlabel('SSE')
ax.view_init(20, -60)
plt.savefig('images/SSELossC.png', dpi=300)
#+end_src
雖然我們可以快速的利用如[[https://scikit-learn.org/][scikit-learn]]這類第三方模組求出最佳解，但是相信對於有志投入AI領域的你來說，光知道如何快速求解顯然遠遠不夠，讓我們來搞清楚這到底是怎麼完成的。

*** 隨機的力量
:PROPERTIES:
:ID:       7cd4a142-4cd9-46b6-b9a4-2ad750ae622f
:END:
萬事起頭難，要找出最佳的參數組合 $(a,b)$ ，最合理的方式就是我們 *閉上眼睛* 在圖[[fig:SSELossA]]中隨意點圈出一個點 $(a_0, b_0)$，這就是我們的第一步，其結果就如圖[[fig:SSELossC]]所示。有了這個開頭，我們接下來要做的事就是：
1. 找出 *一個方法* 來判斷要由點 $(a_0, b_0)$ 點沿著這個曲面的 *哪一個方向* 前進 *多遠* ，來到下一點 $(a_1, b_1)$。也許是沿著曲面往上移一小段(如圖[[fig:SSELossC]]中的藍色線段)、也許是沿著曲面往下移一小段(如圖[[fig:SSELossC]]中的紅色線段)。
3. 利用 *同一個方法* 來判斷接下來要由點 $(a_1, b_1)$ 點沿著這個曲面的 *哪一個方向* 繼續前進 *多遠* ，來到下一點 $(a_2, b_2)$
4. 重複同樣的步驟，直到找到最佳的點 $(a_n, b_n)$ ，也就是這一點 $(a_n, b_n)$ 能使整個模型的SSE來到最小，讓模型具備最佳的預測效能。
#+CAPTION: 找出最佳a,b組合的方法
#+name: fig:SSELossC
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 400
#+ATTR_HTML: :width 500
[[file:images/SSELossC.png]]

*** 何去何從
發現了嗎？其實我們就只是在求某個方程式的最小值。

到這裡我想你一定會發現上面那個方法的幾個漏洞：
- 我怎麼知道要往哪個方向移呢？
- 我怎麼知道要移動多長的距離呢?
- 我怎麼知道移動後的新位置比原來的位置好呢？

好吧，我也不知道。不如我們先跳過這個看起來太複雜的問題，先換個簡單點的來強化自信。

* 保持距離以測安全
讓我們先來看一個更簡單的例子。

這是一組從[[https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html][R語言資料集]]偷來的資料，這個資料集有七百多組教學用的資料集，其中有一組簡單的資料集cars，裡面有50筆資料，每筆資料只有兩個欄位：
- speed: 車速
- dist: 所需剎車距離
資料分佈如圖[[fig:carsScatter]]所示
#+begin_src python -r -n :results output :exports none
# use library pydatset to get data
from pydataset import data
import matplotlib.pyplot as plt

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS'] # 步驟一（替換系統中的字型，這裡用的是Ma
plt.rcParams['axes.unicode_minus'] = False  # 步驟二（解決座標軸負數的負號顯示問題）

#print("\n\n","There are %s" %len(data()), "datasets in pydataset library")
## access data
cars = data('cars')
#print(len(cars))
#print(cars.head(5))
plt.xlabel('speed:車速',fontsize=12)
plt.ylabel('dist:所需剎車距離',fontsize=12)
plt.scatter(cars['speed'], cars['dist'])
plt.savefig("images/carsScatter.png", dpi=300)
#+end_src

#+RESULTS:
#+CAPTION: 車速與剎車距離關係分佈圖
#+name: fig:carsScatter
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/carsScatter.png]]

我們為了這組簡單的資料分佈建了一個如下的模型
$$dist=w*speed$$
建立這組模型的最終目的當然是希望輸入 *車速* ($speed$) 後就能得到 *預測的所需剎車距離* ( $dist$ ) 。我們也可以用常見的數學表示法( $x$ 為車速、$y_{predicted}$ 為預測的剎車距離)：
$$ y_{predicted} = w * x $$
或是更常見的寫法( $\hat{y}$ 為預測的剎車距離)：
$$ \hat{y} = w * x $$
我們的任務就是找到一個最佳的 $w$ 值，也就是這個模型的參數。同時，為了評估不同 $w$ 值下模型的優劣，我們當也要提出相對應的損失函數( $\hat{y}$ 為模型預測的剎車距離、 $y_i$ 為實際資料的剎車距離，$n$ 為資料筆數，共有50筆資料)：
$$ Loss = \frac{1}{n}\sum_{i=1}^{n}(\hat{y} - y_i)^2 $$
例如：
- 當 $w$ 為-15時，Loss值為85113.26
- 當 $w$ 為-10時，Loss值為44346.86
- 當 $w$ 為 -5時，Loss值為16808.46
- 當 $w$ 為  0時，Loss為2498.06
- 當 $w$ 為  5時，Loss為1415.66
- 當 $w$ 為 10時，Loss為17152.28

在這個例子中，我們的任務就變成：提出一個假設模型 $f(x)=w*x$，然後找出最理想的參數 $w$，讓這個模型可以俱備最好的預測能力(Loss值最小)。

** 模型的目的
顯然，對於如何解出方程式(或是說找到最佳模型) ，一開始當然沒啥頭緒，那，不如就暴力一點吧，弄個窮舉法：試試從 $w=-20$ try到 $w=+20$ 吧，觀察一下損失函數Loss的變化：
#+begin_src python -r -n :results output :exports none
from pydataset import data
import matplotlib.pyplot as plt
import numpy as np

cars = data('cars')
speed = np.array(cars['speed'])
dist = np.array(cars['dist'])
w = np.arange(-20,21,0.1)
loss = []
for i in w:
    yHat =  i * speed
    loss.append(np.mean((yHat - dist)**2))
#    if int(i % 5) == 0:
#        print(f'{i:.2f}: {loss[-1]:.2f}')

plt.figure(figsize=(7, 5))
plt.plot(w, loss)
plt.xlabel(r'w')
plt.ylabel(r'Loss')
plt.savefig("images/carsLoss.png", dpi=300)
#+end_src

#+CAPTION: 不同參數w下的損失函數Loss分佈圖
#+name: fig:carLoss
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 400
#+ATTR_HTML: :width 500
[[file:images/carsLoss.png]]

現在我們來想辦法找出最好的 $w$ 在哪裡。

** 沿著曲線上下爬
雖然我們從圖[[fig:carLoss]]大概可以看出來模型大概在參數 $w$ 介於0和5之間會有最小的Loss，也就是模型會最準確，但身為嚴謹的學術研究者，我們不能這樣蠻幹，這是土匪的行為，我們要用最科學的方法：既然不知道從哪裡著手，就閉著眼睛隨意給個 $w$ 好了，例如：-15，如圖[[fig:carLoss1]]。

你看，我們這不就邁出成功的第一步了?

隨機就是這麼美而有力!!

#+begin_src python -r -n :results output :exports none
from pydataset import data
import matplotlib.pyplot as plt
import numpy as np

cars = data('cars')
speed = np.array(cars['speed'])
dist = np.array(cars['dist'])
w = np.arange(-20,21,0.5)
loss = []
for i in w:
    yHat =  i * speed
    loss.append(np.mean((yHat - dist)**2))

# 畫圖====================
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
fig, ax = plt.subplots()

arrowprop1 = dict(arrowstyle="->",color='red')
arrowprop2 = dict(arrowstyle="->",color='blue')
ax.plot(w, loss)
x, y = -15, loss[np.where(w == -15)[0][0]]
ax.scatter(x, y, color='r')
ax.plot([-15, -15], [0, loss[np.where(w == -15)[0][0]]], 'g--')
ax.text(x+1, y, '隨意指定w得到的Loss', color='black')
ax.set_xlabel(r'w')
ax.set_ylabel(r'Loss')
plt.savefig("images/carsLoss1.png", dpi=300)
#+end_src
#+CAPTION: 先隨機假設一個數(-15)為最佳參數w的值
#+name: fig:carLoss1
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/carsLoss1.png]]

有了出發點(我們估且稱之為 $w_0$ 好了，如圖[[fig:carLoss1]])，接下來就只要決定下一個「較好的下一個 $w$ 」是在 $w_0$ 的左邊還是右邊(根據Loss值來判斷)，然後繼續往左或往右移(如圖[[fig:carLoss2]]。

總之，我們只要決定以下兩個因素，就可以利用[[https://letranger.github.io/PythonCourse/][Python]]把模型的最佳參數 $w$ 找出來了。
1. 每次要往左或往右移多少距離?
2. 這樣的修正動作要重複幾次？或者說，程式結束的條件為何？

#+begin_src python -r -n :results output :exports none
from pydataset import data
import matplotlib.pyplot as plt
import numpy as np

cars = data('cars')
speed = np.array(cars['speed'])
dist = np.array(cars['dist'])
w = np.arange(-20,21,0.5)
loss = []
for i in w:
    yHat =  i * speed
    loss.append(np.mean((yHat - dist)**2))

# 畫圖====================
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
fig, ax = plt.subplots()

arrowprop1 = dict(arrowstyle="->",color='red')
arrowprop2 = dict(arrowstyle="->",color='blue')
ax.plot(w, loss)
x, y = -15, loss[np.where(w == -15)[0][0]]
ax.scatter(x, y, color='r')
x1, y1 = -20, loss[np.where(w == -20)[0][0]]
ax.scatter(x1, y1, color='r')
x2, y2 = -10, loss[np.where(w == -10)[0][0]]
ax.scatter(x2, y2, color='r')
ax.plot([-15, -15], [0, loss[np.where(w == -15)[0][0]]], 'g--')
ax.plot([-20, -20], [0, loss[np.where(w == -20)[0][0]]], 'g--')
ax.plot([-10, -10], [0, loss[np.where(w == -10)[0][0]]], 'g--')
ax.set_xlabel(r'w')
ax.set_ylabel(r'Loss')
ax.annotate('',xy=(x-5, 200), xytext=(x, 0),arrowprops=arrowprop1)
ax.annotate('',xy=(x+5, 200), xytext=(x, 0),arrowprops=arrowprop2)
ax.text(x+1, y, rf'$w_0$', color='green')
ax.text(x1+1, y1, rf'$w_1$', color='green')
ax.text(x2+1, y2, rf'$w_2$', color='green')
ax.text(x-4, 5000, '往左移', color='red')
ax.text(x+1, 5000, '往右移', color='blue')
plt.savefig("images/carsLoss2.png", dpi=300)
#+end_src

#+RESULTS:

#+CAPTION: 決定w應往哪個方向移動
#+name: fig:carLoss2
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 400
#+ATTR_HTML: :width 500
[[file:images/carsLoss2.png]]

** 確定方向
相信學過幾何學的你一定有想到一種策略：切線。既然 $w$ 與  $Loss$ 的關係是如圖[[fig:carLoss2]]的曲線，我們應該可以找出 $w_0$ 這個點的 *切線* ，根據這條切線的斜率(也就是點 $w_0$ 的斜率)為正或負來判斷要往哪個方向移動，如果切線是負的，也就是一條左上右下的線，那我們就知道曲線的最低點應該是在這個點的右側。

讓我們從點 $w_0$ 的 $x$ 軸向左移動一段距離(例如5)，就會在曲線上找到 $w_1$ 對應的 $Loss_1$ ，然後連接點$(w_0, Loss_0)$ 、點$(w_1, Loss_1)$ ，就會得到一條經過點$(w_0, Loss_0)$ 的割線。

#+begin_src python -r -n :results output :exports none
from pydataset import data
import matplotlib.pyplot as plt
import numpy as np

cars = data('cars')
speed = np.array(cars['speed'])
dist = np.array(cars['dist'])
w = np.arange(-20,21,0.5)
loss = []
for i in w:
    yHat =  i * speed
    loss.append(np.mean((yHat - dist)**2))

# 畫圖====================
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
fig, ax = plt.subplots()

arrowprop1 = dict(arrowstyle="->",color='red')
arrowprop2 = dict(arrowstyle="->",color='blue')
ax.plot(w, loss, 'y--')
x0, y0 = -15, loss[np.where(w == -15)[0][0]]

ax.annotate('',xy=(x0-5, 200), xytext=(x0, 0),arrowprops=arrowprop1)
ax.annotate('',xy=(x0+5, 200), xytext=(x0, 0),arrowprops=arrowprop2)
ax.plot([x0, x0], [0, y0], 'y--')
ax.text(x0-4, 5000, '往左移', color='red')
ax.text(x0+1, 5000, '往右移', color='blue')
# 計算斜率用
xdist = 5
x2, y2 = x0+xdist, loss[np.where(w == x0+xdist)[0][0]]
x1, y1 = x0-xdist, loss[np.where(w == x0-xdist)[0][0]]
m = (y1-y2)/(x1-x2)
ax.plot([x1, x0], [y0, y0], 'c-.')
ax.plot([x1, x1], [y1, y0], 'c-.')
ax.text(x0-3, y0-5000, rf'$\Delta x$', color='c')
ax.text(-19.7, (y0+y1)/2, rf'$\Delta y$', color='c')

# w0
ax.scatter(x0, y0, color='k')
ax.text(x0+3, y0, rf'$w_0, Loss_0$', color='black')
ax.plot([x0+3, x0], [y0, y0], 'k--')
# w1
ax.scatter(x1, y1, color='r')
ax.text(x1+3, y1, rf'$w_1, Loss_1$', color='red')
ax.plot([x1, x1+3], [y1, y1], c='red')
ax.plot([x1, x0], [y1, y0], c='red')
#---???
ax.set_xlabel(r'w')
ax.set_ylabel(r'Loss')

plt.savefig("images/carsLoss30.png", dpi=300)
#+end_src

#+RESULTS:

#+CAPTION: 決定w應往哪個方向移動#1
#+name: fig:carLoss30
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 400
#+ATTR_HTML: :width 500
[[file:images/carsLoss30.png]]

這條割線的斜率計算方式為 $$ Slope = \frac{\Delta Loss}{\Delta x} = \frac{Loss_0-Loss_1}{x_0-x_1} $$
讓我們進一步把 *移動距離* 縮到無限小，也就是把割線斜率中的 $\Delta x$ 逼近於0，就能得到一條點 $(w_0, Loss_0)$ 的在曲線上的切線，其斜率計算方式為：
$$f'(w_0)=lim_{w_1 \rightarrow w_0}\frac{f(w_1)-f(w_0)}{w_1-w_0}$$

#+begin_src python -r -n :results output :exports none
from pydataset import data
import matplotlib.pyplot as plt
import numpy as np

cars = data('cars')
speed = np.array(cars['speed'])
dist = np.array(cars['dist'])
w = np.arange(-20,21,0.5)
loss = []
for i in w:
    yHat =  i * speed
    loss.append(np.mean((yHat - dist)**2))

# 畫圖====================
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
fig, ax = plt.subplots()

arrowprop1 = dict(arrowstyle="->",color='red')
arrowprop2 = dict(arrowstyle="->",color='blue')
ax.plot(w, loss, 'y--')
x0, y0 = -15, loss[np.where(w == -15)[0][0]]


ax.annotate('',xy=(x0-2, 200), xytext=(x0, 0),arrowprops=arrowprop1)
ax.plot([x0, x0], [0, y0], 'y--')

ax.text(x0-1, 5000, '往左移', color='red')

# 計算斜率用
xdist = 0.5
x2, y2 = x0+xdist, loss[np.where(w == x0+xdist)[0][0]]
x1, y1 = x0-xdist, loss[np.where(w == x0-xdist)[0][0]]
# w0
ax.scatter(x0, y0, color='k')
ax.text(x0+3, y0, rf'$w_0, Loss_0$', color='black')
ax.plot([x0+3, x0], [y0, y0], 'k--')
# w1
ax.scatter(x1, y1, color='r')
ax.text(x1+3, y1, rf'$w_1, Loss_1$', color='red')
ax.plot([x1, x1+3], [y1, y1], c='red')
ax.plot([x1, x0], [y1, y0], c='red')
#---???
ax.set_xlabel(r'w')
ax.set_ylabel(r'Loss')

plt.savefig("images/carsLoss31.png", dpi=300)
#+end_src

#+RESULTS:

#+CAPTION: 決定w應往哪個方向移動#2
#+name: fig:carLoss31
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 400
#+ATTR_HTML: :width 500
[[file:images/carsLoss31.png]]

以程式計算斜率的方式也很簡單，甚至不需要懂微分，所謂把 *移動距離* 縮到無限小，我們可以用作弊的方式，直接將 $\Delta x$ 設為一個很小的值，例如 0.5。
#+begin_src python -r -n :results output :exports both
from pydataset import data
import numpy as np

# 取得資料集
cars = data('cars')
speed = np.array(cars['speed'])
dist = np.array(cars['dist'])

# 計算Loss用的function
def loss_func(y_true, y_predict):
    return y_true - y_predict

w = np.arange(-20,21,0.5)
loss = []
for i in w:
    yHat =  i * speed
    loss.append(np.mean(loss_func(dist, yHat)**2))


x, y = -5, loss[np.where(w == -5)[0][0]]
x1, y1 = -4.5, loss[np.where(w == -4.5)[0][0]]
# 計算切線斜率
print((y-y1)/(x-x1))
#+end_src

#+RESULTS:
: -4052.5999999999985

計算結果 $Slope<0$ ，表示這是條左上右下的切線，顯然接下來該往右側去找到最低點。問題是：該往右邊移動多少距離呢？要移動幾次？

** 確定移動距離與重複次數
<<sec:move>>
由圖[[fig:carLoss31]]中點 $(w_0, Loss_0)$ 的斜率可知應逐步往右移動w，Loss的值就會慢慢降下來，所以我們可以先這麼計畫：
- 每次由 $w0$ 的 $x$ 軸往右邊加0.5、直到Loss不再變小。
或換另一種說法：
- 每次往右邊加0.5、直到Loss開始變大(因為越過了曲線最低點)。

上述的Python實作程式碼如下：
#+begin_src python -r -n :results output :exports both
from pydataset import data
import numpy as np

# 取得資料集
cars = data('cars')
speed = np.array(cars['speed'])
dist = np.array(cars['dist'])

# 計算Loss用的function
def loss_func(y_true, y_predict):
    return y_true - y_predict

w = np.arange(-20,21,0.5)
loss = []
for i in w:
    yHat =  i * speed
    loss.append(np.mean(loss_func(dist, yHat)**2))

# 找最佳w值, 這裡以x代表w值
x, y = -15, loss[np.where(w == -15)[0][0]]
while True: (ref:keepIncrease)
    loss = np.mean(loss_func(dist, x*speed)**2)
    x += 0.5 (ref:increaseX)
    newLoss = np.mean(loss_func(dist, x*speed)**2)
    if newLoss >= loss: (ref:breakWhile)
        print(f'STOP: w值:{x-0.5}, Loss:{loss}')
        print(f'NEXT: w值:{x}, Loss:{newLoss}')
        break
    if int(x) % 5 == 0: (ref:monitorW)
        print(f'w值:{x}, Loss:{newLoss}')
#+end_src

逐步右移的控制主要由第[[(keepIncrease)]]行的while負責，$w$ 每次右移0.5，直到Loss值不再變小就停止(第[[(breakWhile)]]行)，為簡化輸出，每移動10次 $w$ 我們就把對應的 $w$ 和Loss輸出來觀察一下（第[[(monitorW)]]行)。輸出結果如下：

#+RESULTS:
: w值:-10.5, Loss:47828.24
: w值:-10.0, Loss:44346.86
: w值:-5.5, Loss:18967.04
: w值:-5.0, Loss:16808.46
: w值:-0.5, Loss:3333.84
: w值:0.0, Loss:2498.06
: w值:0.5, Loss:1794.56
: STOP: w值:3.0, Loss:261.26
: NEXT: w值:3.5, Loss:351.44

由執行結果可發現隨著 $w$ 值的增加，Loss值也隨之減少，直到 $w$ 值為3時可以得到最低的Loss值(261.26)，過了這一點，Loss值便又開始增加。圖[[fig:carLoss4]]為w值持續修正的模擬結果，圖[[fig:carLine]]則為假設$w$ 值為3所畫出的預測線(模型)。

#+begin_src shell -r -n :results output :exports none
pip3 install matplotlib
#+end_src

#+begin_src python -r -n :results output :exports none
# use library pydatset to get data
from pydataset import data
import matplotlib.pyplot as plt

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS'] # 步驟一（替換系統中的字型，這裡用的是Ma
plt.rcParams['axes.unicode_minus'] = False  # 步驟二（解決座標軸負數的負號顯示問題）

#print("\n\n","There are %s" %len(data()), "datasets in pydataset library")
## access data
cars = data('cars')
#print(len(cars))
#print(cars.head(5))
plt.xlabel('speed:車速',fontsize=12)
plt.ylabel('dist:所需剎車距離',fontsize=12)
plt.scatter(cars['speed'], cars['dist'])
plt.plot(cars['speed'], cars['speed']*3, 'r')
plt.savefig("images/carsLine.png", dpi=300)
#+end_src

#+RESULTS:
#+CAPTION: 車速與剎車距離關係分佈及預測模型
#+name: fig:carLine
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/carsLine.png]]


然而，這個 $w=3$ 的模型就是最佳模型嗎？你有什麼可以更快找到更精確的「使Loss最低的 $w$ 值」的修正方案嗎？

#+begin_src python -r -n :results output :exports none
from pydataset import data
import matplotlib.pyplot as plt
import numpy as np

# 取得資料集
cars = data('cars')
speed = np.array(cars['speed'])
dist = np.array(cars['dist'])

# 計算Loss用的function
def loss_func(y_true, y_predict):
    return y_true - y_predict

# 畫圖====================
w = np.arange(-20,21,0.5)
loss = []
for i in w:
    yHat =  i * speed
    loss.append(np.mean(loss_func(dist, yHat)**2))
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
fig, ax = plt.subplots()

ax.plot(w, loss, 'y--')
# 找最佳w值, 這裡以x代表w值
x, y = -15, loss[np.where(w == -15)[0][0]]
i = 0
ax.scatter(x, y, color='r')
ax.text(x+1, y, rf'$w_{i}$', color='blue')
while True:
    loss = np.mean(loss_func(dist, x*speed)**2)

    x += 0.5
    i += 1
    newLoss = np.mean(loss_func(dist, x*speed)**2)
    if newLoss >= loss:
        print('STOP:', x-0.5, loss)
        break
    ax.scatter(x, newLoss, color='r')
    if i < 10:
        ax.text(x+1, newLoss, rf'$w_{i}$', color='blue')
    else:
        ax.text(x+1, newLoss, rf'...', color='blue')

ax.set_xlabel(r'w')
ax.set_ylabel(r'Loss')

plt.savefig("images/carsLoss4.png", dpi=300)
#+end_src

#+CAPTION: 決定w應往哪個方向移動
#+name: fig:carLoss4
#+ATTR_LATEX: :width 500
#+ATTR_ORG: :width 500
#+ATTR_HTML: :width 500
[[file:images/carsLoss4.png]]

** 斜率與微分
:PROPERTIES:
:ID:       5dc9ce02-13b4-4a1c-b5fd-813820407adb
:END:
為什麼 *找出曲線上各點的斜率* 這件事如此重要呢？原因有二
- 在第[[sec:move]]中，我們據以移動 $w$ 值的依據是 " *每次往右邊加0.5、直到Loss開始變大(因為越過了曲線最低點)*" ，其實我們有另一種更方便的判斷方式：在曲線最低點左側所有點的斜率均為負、在曲線最低點右側所有點的斜率均為正，知道了某點的斜率為正或負，我們就知道該往左側或右側去移動，找出最低點的位置。理解這件事，我們可以用程式來求出接近的最低點。
- 曲線最低點的斜率為0，我們只要能找到斜率為0的這一點，就能找到模型的最佳參數。理解這件事，我們就能用數學的方式求出最低點的位置。 事實上，如果我們知道曲線 $y=f(x)$ 的實際內容，我們就能用[[id:3ce884c1-cd16-4310-b757-37cdd1ddcdef][微分]]來找到曲線上的最低點。

*** 頂點公式求函數解
如假設損失函數L為 $$ Loss = MSE = \frac{1}{n}\sum_{i=1}^{n}(\hat{y_i}-y_i)^2 $$
其中
- \(n=50\)(實際有50筆資料)
- \(\hat{y_i}\)為用模型\(y=wx\)所預測出來的剎車距離
- \(y_i\)為當實測車速為\(x_i\)時所對應的剎車距離
故
\begin{align}
Loss=\frac{1}{n}\sum_{i=1}^{n}(\hat{y_i}-y_i)^2 \\
=\frac{1}{n}\sum_{i=1}^{n}(wx_i - y_i)^2 \\
=\frac{1}{n}\sum_{i=1}^{n}(w^2x_i^2 -2wx_iy_i + y_i^2) \\
=\frac{1}{n}(w^2\sum_{i=1}^{n}x_i^2 -2w\sum_{i=1}^{n}x_iy_i + ny_i^2) \\
\end{align}
雖然可以用微分來解，但是這個一元二次函數也可以用頂點公式來找出曲線頂點(模型最小值)，求出的最小Loss會出現在
$$ w=\frac{\sum_{i=1}^{n}x_iy_i}{\sum_{i=1}^{n}x_i^2} $$
因為當 $a>0$
$$ ax^2+bx+c = a(x+\frac{b}{2a})+\frac{4ac-b^2}{4a} $$
故 函數在 $x=\frac{-b}{2a}$ 時有最小值

以程式驗證如下：
#+begin_src python -r -n :results output :exports both
from pydataset import data
import numpy as np

cars = data('cars')
speed = np.array(cars['speed'])
dist = np.array(cars['dist'])

print('最小Loss的參數w值:',sum(speed*dist) / sum(speed*speed))
#+end_src

#+RESULTS:
: 最小Loss的參數w值: 2.909132143937103
*** 微分求函數頂點
當 $a>0$
\begin{align}
f(x)=ax^2+bx+c \\
f'(x)=2ax+b
\end{align}
找曲線最低點、令函式為0
$$2ax+b=0$$
故 $x=\frac{-b}{2a}$ 時有最小值

* 線性迴歸:年齡身高預測#2
回到第[[sec:sbs]]節[[datagen][以年齡預測身高]]的例子，我們提及要找出最佳的參數組合 $(a,b)$ ，最合理的方式是在圖[[fig:SSELossA]]中隨意點圈出一個點 $(a_0, b_0)$ (如圖[[fig:SSELossC]])。接下來：
1. 找出 *一個方法* 來判斷要由點 $(a_0, b_0)$ 點沿著這個曲面的 *哪一個方向* 前進 *多遠* ，來到下一點 $(a_1, b_1)$。也許是沿著曲面往上移一小段(如圖[[fig:SSELossC]]中的藍色線段)、也許是沿著曲面往下移一小段(如圖[[fig:SSELossC]]中的紅色線段)。
3. 利用 *同一個方法* 來判斷接下來要由點 $(a_1, b_1)$ 點沿著這個曲面的 *哪一個方向* 繼續前進 *多遠* ，來到下一點 $(a_2, b_2)$
4. 重複同樣的步驟，直到找到最佳的點 $(a_n, b_n)$ ，也就是這一點 $(a_n, b_n)$ 能使整個模型的SSE來到最小，讓模型具備最佳的預測效能。

假設損失函數L為 $$ L = MSE = \frac{1}{n}\sum_{i=1}^{n}(\hat{y_i}-y_i)^2 $$
因為 $\hat{y_i}$ 代表直線模型 $y=ax+b$ 的預測結果。

實際的做法就變成：
1. 計算點 $(a_0, b_0)$ 的斜率，然後朝著 *使Loss(MSE)減小得最快的方向* 稍微移動 $a, b$
2. 重複步驟1
** 梯度(Gradient)
為了符合機器學習的表達習慣，我們稍微修正一下上面的式子，將 $a,b$ 改為 $w_0, w_1$ ，因為對機器學習來說， $a,b$ 都是模型的權重(weight) ，訓練模型的目的就是找出最佳的權重，讓模型的預測最準確，所以上述式子就變成了 $y=w_0x+w_1$ 。

假設我們就站在下圖(圖[[fig:SSELossD]])中的點 $(w_0, w_1)$ 上，環顧四周，由這點往上的方向可以用 $L$ 對 $w_0,w_1$ 的偏導數向量
\(
\begin{bmatrix}
\frac{\delta L}{\delta w_0} \frac{\delta L}{\delta w_1}
\end{bmatrix}^T
\)
，也稱之為 *梯度(gradient)* ，以符號 $\nabla_wL$ 表示。為了使 $L$ 最小，我們要朝著著 $L$ 梯度的反方向前進，也就是
\(
-\nabla_wL =
-\begin{bmatrix}
\frac{\delta L}{\delta w_0} \frac{\delta L}{\delta w_1}
\end{bmatrix}^T
\)

#+begin_src python -r :results output :exports none
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS'] # 步驟一（替換系統中的字型，這裡用的是Mac OSX系統）
plt.rcParams['axes.unicode_minus'] = False  # 步驟二（解決座標軸負數的負號顯示問題）
year = np.array([13.3, 16.2, 10.9, 28.7, 14.2, 11.7, 26.6, 22.4, 18.3, 20.4]).reshape([-1, 1])
height = np.array([163.61, 168.53, 155.06, 168.3 ,158.98, 158.23, 165.27, 170.83,  161.31, 163.58])

from sklearn.linear_model import LinearRegression
model = LinearRegression()
# 均方誤差函數 ------------------------------
def mse_line(x, t, w):
    y = w[0] * x + w[1]
    mse = np.mean((y - t)**2)
    return mse

xn = 100    #等高線解析度
b0, b1 = -40, 41
a0, a1 = 40, 161
# Fit the model on the training data
w0 = np.linspace(b0, b1, xn)
w1 = np.linspace(a0, a1, xn)
ww0, ww1 = np.meshgrid(w0, w1)
J = np.zeros((len(w0), len(w1)))
for i0 in range(len(w0)):
    for i1 in range(len(w1)):
        J[i1, i0] = mse_line(year, height, (w0[i0], w1[i1]))

# 顯示 --------------------------------------
plt.figure(figsize=(5, 5))
from matplotlib import ticker
from mpl_toolkits.mplot3d import Axes3D

formatter = ticker.ScalarFormatter()
formatter.set_scientific(False)
# A
ax = plt.subplot(1, 1, 1, projection='3d')
ax.yaxis.set_major_formatter(formatter)
ax.plot_surface(ww0, ww1, J, rstride=20, cstride=20, alpha=0.3,
                color='blue', edgecolor='black')
ax.set_xticks(np.arange(b0, b1, 20))
ax.set_yticks(np.arange(a0, a1, 20))

ax.quiver(-20, 70, 300000, -10, 16, 10,  color='blue', arrow_length_ratio = 0.1)
ax.quiver(-20, 70, 300000, 10, -16, 10,  color='red', arrow_length_ratio = 0.5, pivot='tail', length=1)
ax.scatter(-20, 70, 300000, color='green')

ax.quiver(-20, 70, 300000, -10, 16, 10,  color='blue', arrow_length_ratio = 0.1)
ax.text(-18, 75, 350000, rf'隨意指定一點$(w_0,w_1)$', color='blue')

ax.set_xlabel(rf'$w_0$')
ax.set_ylabel(rf'$w_1$')
ax.set_zlabel('SSE')
ax.view_init(20, -60)
plt.savefig('images/SSELossD.png', dpi=300)
#+end_src

#+RESULTS:

#+CAPTION: 隨意於曲面指定一點(a,b)
#+LABEL:fig:Labl
#+name: fig:SSELossD
#+ATTR_LATEX: :width 500
#+ATTR_ORG: :width 500
#+ATTR_HTML: :width 500
[[file:images/SSELossD.png]]

確定移動方向後，我們就要來研究出一個適當的移動距離。我們先定義幾個表達式：
- $\nabla_wL$ 為 $w$ 的函數
- $w(\tau)$ 代表目前的 $(w_0, w_1)$ 所在位置
- $w(\tau+1)$ 代表下一次移動的新 $(w_0, w_1)$ 所在位置
- $\alpha$ 為一個大於0的數，稱為學習率，用來控制調整權重的步幅，其值越大，調整的幅度就越大。典型的值可能為 $0.001$
- $y_i$ 為實際由第i個年齡 *對應* 到的身高
- $\hat y_i$ 為模型由第i個年齡 *預測出來* 的身高
兩個權重的下一步的調整步幅為：
- $w_0(\tau+1) = w_0(\tau)-\alpha\frac{\partial L}{\partial w_0}$
- $w_1(\tau+1) = w_0(\tau)-\alpha\frac{\partial L}{\partial w_1}$
因為 $$L=\frac{1}{n}\sum\limits_{i=1}^{n}(\hat y_i - y_i)^2 = \frac{1}{n}\sum\limits_{i=1}^{n}(w_0x_i + w_1 - y_i)^2$$
對 $w_0$ 求偏導數：
$$\frac{\partial L}{\partial w_0} = \frac{2}{n}\sum\limits_{i=1}^{n}(w_0x_i + w_1 - y_i)x_i = \frac{2}{n}\sum\limits_{i=1}^{n}(\hat y_i - y_i)x_i$$
對 $w_1$ 求偏導數：
$$\frac{\partial L}{\partial w_1} = \frac{2}{n}\sum\limits_{i=1}^{n}(w_0x_i + w_1 - y_i)= \frac{2}{n}\sum\limits_{i=1}^{n}(\hat y_i - y_i)$$
即 $(w_0, w_1)$ 的下一次移動距離為：
- $w_0(\tau+1) = w_0(\tau)-\alpha\frac{\partial L}{\partial w_0} = \frac{2}{n}\sum\limits_{i=1}^{n}(\hat y_i - y_i)x_i$
- $w_1(\tau+1) = w_0(\tau)-\alpha\frac{\partial L}{\partial w_1} = \frac{2}{n}\sum\limits_{i=1}^{n}(\hat y_i - y_i)$
上面數學式中貌似有很複雜的計算，但是如果我們以Numpy的array來儲存 $x_i, y_i, \hat y_i$，就能利用Numpy的矩陣運算np.mean()輕易求出結果，例如當 $(w_0, w_1) = (-20, 70)$ (如圖[[fig:SSELossD]])，則下一個點梯度的計算方式為
#+begin_src python -r -n :results output :exports both
import numpy as np
year = np.array([13.3, 16.2, 10.9, 28.7, 14.2, 11.7, 26.6, 22.4, 18.3, 20.4]).reshape([-1, 1])
height = np.array([163.61, 168.53, 155.06, 168.3 ,158.98, 158.23, 165.27, 170.83,  161.31, 163.58])
# 均方差的梯度
def dw(x, y, w):
    yhat = w[0]*x + w[1]
    d_w0 = 2*np.mean((yhat-y)*x)
    d_w1 = 2*np.mean(yhat-y)
    return d_w0, d_w1
w = dw(year, height, [-20, 70])
print(w)
#+end_src

#+RESULTS:
: (-18134.659799999998, -917.54)
結果分別是 $w_0, w_1$ 方向的斜率，可以看出斜率都非常小，且 $w_0$ 斜率比 $w_1$ 方向的斜度更大，這與由圖[[fig:SSELossD]]觀察結果一致。

** 實作
#+begin_src python -r -n :results output :exports both
import numpy as np
year = np.array([13.3, 16.2, 10.9, 28.7, 14.2, 11.7, 26.6, 22.4, 18.3, 20.4]).reshape([-1, 1])
height = np.array([163.61, 168.53, 155.06, 168.3 ,158.98, 158.23, 165.27, 170.83,  161.31, 163.58])

def mse(x, t, w):
    y = w[0] * x + w[1]
    mse = np.mean((y - t)**2)
    return mse

# 均方差的梯度
def dw(x, y, w):
    yhat = w[0]*x + w[1]
    d_w0 = 2*np.mean((yhat-y)*x)
    d_w1 = 2*np.mean(yhat-y)
    return d_w0, d_w1

# 梯度法 ------------------------------------
def fit_gd(x, y):
    w_init = [-20, 70]  # 初始參數
    alpha = 0.001  # 學習率
    tau_max = 100000  # 重複的最大次數
    eps = 0.1  # 停止重複的梯度絕對值的閥值
    w_hist = np.zeros([tau_max, 2])
    w_hist[0, :] = w_init
    for tau in range(1, tau_max):
        dmse = dw(x, y, w_hist[tau - 1])
        w_hist[tau, 0] = w_hist[tau - 1, 0] - alpha * dmse[0]
        w_hist[tau, 1] = w_hist[tau - 1, 1] - alpha * dmse[1]
        if max(np.absolute(dmse)) < eps: # 結束判斷
            break
    w0 = w_hist[tau, 0]
    w1 = w_hist[tau, 1]
    w_hist = w_hist[:tau, :]
    return w0, w1, dmse, w_hist

# 調用梯度法
W0, W1, dMSE, W_history = fit_gd(year, height)
print('重複次數 {0}'.format(W_history.shape[0]))
print('W=[{0:.6f}, {1:.6f}]'.format(W0, W1))
print('dMSE=[{0:.6f}, {1:.6f}]'.format(dMSE[0], dMSE[1]))
print('MSE={0:.6f}'.format(mse(year, height, [W0, W1])))
#+end_src

#+RESULTS:
: 重複次數 27678
: W=[0.026711, 162.832008]
: dMSE=[0.004964, -0.099983]
: MSE=22.955476

* 線性迴歸實作: 波士頓房價預測
- 本例中部份程式碼及文字來自[[https://medium.com/li-ting-liao-tiffany/python-%E5%BF%AB%E9%80%9F%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-boston-housing%E6%B3%A2%E5%A3%AB%E9%A0%93%E6%88%BF%E5%83%B9-9c535fb7ceb7][What impacts Boston Housing Prices]]
- 本例使用資料集為 1970 年中期 Boston 郊區資料，包含犯罪率、當地財產稅等，用以預測某郊區房價中位數，本例有 506 筆資料，分為 404 個訓練樣本和 102 個測試樣本，但每個 feature 的單位不同，故須先進行資料預調整。
** 下載資料
#+begin_src python -r -n :async :results output :exports both :session boston
import pandas as pd

housing = pd.read_csv('https://raw.githubusercontent.com/letranger/AI/gh-pages/Downloads/boston_housing.csv')
#+end_src

#+RESULTS:

也可以用tensorflow的load_data()直接下載，但這組沒有column title
#+begin_src python -r -n :async :results output :exports both :session boston
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import boston_housing

(train_x, train_y), (test_x, test_y) = boston_housing.load_data()
#+end_src

** 大概觀察一下資料集
#+begin_src python -r -n :async :results output :exports both :session boston
print(type(housing))
print(housing.shape)
print(housing.iloc[0])
#+end_src

#+RESULTS:
#+begin_example
<class 'pandas.core.frame.DataFrame'>
(506, 14)
crim         0.00632
zn          18.00000
indus        2.31000
chas         0.00000
nox          0.53800
rm           6.57500
age         65.20000
dis          4.09000
rad          1.00000
tax        296.00000
ptratio     15.30000
b          396.90000
lstat        4.98000
medv        24.00000
Name: 0, dtype: float64
#+end_example
這個資料集共有506筆資料，前13個為特徵值，最後一個medv為房價。其他特徵值分別代表:
- CRIM: per capita crime rate by town
- ZN: proportion of residential land zoned for lots over 25,000 sq.ft.
- INDUS: proportion of non-retail business acres per town
- CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
- NOX: nitric oxides concentration (parts per 10 million)
- RM: average number of rooms per dwelling
- AGE: proportion of owner-occupied units built prior to 1940
- DIS: weighted distances to five Boston employment centres
- RAD: index of accessibility to radial highways
- TAX: full-value property-tax rate per $10,000
- PTRATIO: pupil-teacher ratio by town
- B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
- LSTAT: % lower status of the population
- MEDV: Median value of owner-occupied homes in $1000's

** 資料預處理
*** 處理缺漏值
快速檢查是否有缺漏值
#+begin_src python -r -n :async :results output :exports both :session boston
print(housing.isnull().sum())
#+end_src
#+RESULTS:
#+begin_example
crim        0
zn          0
indus       0
chas        0
nox         0
rm          0
age         0
dis         0
rad         0
tax         0
ptratio     0
b           0
lstat       0
medv       54
dtype: int64
#+end_example
刪掉有缺失值的資料
#+begin_src python -r -n :async :results output :exports both :session boston
housing.dropna(axis=0, inplace=True)
print(housing.isnull().sum())
print(housing.shape)
#+end_src

#+RESULTS:
#+begin_example
crim       0
zn         0
indus      0
chas       0
nox        0
rm         0
age        0
dis        0
rad        0
tax        0
ptratio    0
b          0
lstat      0
medv       0
dtype: int64
(452, 14)
#+end_example
*** 資料標準化
由第一筆訓練資料特徵housing.iloc[0]可以看出，每項特徵值的差異甚大，我們可以先對這些資料特徵進行標準化：
#+begin_src python -r -n :async :results output :exports both :session boston
print(housing.iloc[0,:-1])

mean = housing.iloc[:,:-1].mean(axis=0)
housing.iloc[:,:-1] -= mean
std = housing.iloc[:,:-1].std(axis=0)
housing.iloc[:,:-1] /= std

print(housing.iloc[0,:-1])
#+end_src

#+RESULTS:
#+begin_example
crim         0.00632
zn          18.00000
indus        2.31000
chas         0.00000
nox          0.53800
rm           6.57500
age         65.20000
dis          4.09000
rad          1.00000
tax        296.00000
ptratio     15.30000
b          396.90000
lstat        4.98000
Name: 0, dtype: float64
<string>:5: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0     -6.823009
1     -5.823009
2     -5.823009
3     -4.823009
4     -4.823009
         ...
501   -6.823009
502   -6.823009
503   -6.823009
504   -6.823009
505   -6.823009
Name: rad, Length: 452, dtype: float64' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.
crim      -0.566733
zn         0.217000
indus     -1.176220
chas      -0.289391
nox       -0.024739
rm         0.347120
age       -0.012727
dis        0.022210
rad       -0.904489
tax       -0.538187
ptratio   -1.339563
b          0.394920
lstat     -1.049614
Name: 0, dtype: float64
#+end_example

** 觀察資料
*** 初步看一下房價的分佈
#+begin_src python -r -n :async :results output :exports both :session boston
import matplotlib.pyplot as plt
import seaborn as sns

sns.histplot(housing['medv'])
plt.savefig("images/housing-price.png", dpi=300)
#+end_src
#+CAPTION: 房價分佈概況
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/housing-price.png]]
*** 各特徵值間的關係
#+begin_src python -r -n :async :results output :exports both :session boston
correlation_matrix = housing.corr().round(2)
# annot = True 讓我們可以把數字標進每個格子裡
sns.heatmap(data=correlation_matrix, annot = True)
plt.savefig("images/housing-corr.png", dpi=300)
#+end_src
#+CAPTION: 特徵值間的闗係
#+LABEL:fig:Labl
#+name: fig:boston-cor
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/housing-corr.png]]

由圖[[fig:boston-cor]]可以看出：
- 跟MEDV（房價）高度相關的是LSTAT（中低收入戶佔當地居住人口的比例）和RM（房子有幾間房間）這兩個變數。
- 此外也看到DIS（到波士頓商業中心的距離）和AGE（屋齡），INDUS（非零售業土地使用比例）和ZN（居住使用土地比例）這兩組變數有多元共線性問題，所以未來如果要做其他模型，避免同時使用這兩組中的變數。

所以目前可以用LSTAT和RM來做出預測MEDV的模型。再次把這兩個變數跟房價變數的關係畫出來，可以看到兩者和房價變數都接近線性關係：
#+begin_src python -r -n :async :results output :exports both :session boston
# 設定整張圖的長寬
plt.figure(figsize=(20, 10))
features = ['lstat', 'rm']
target = housing['medv']
for i, col in enumerate(features):
    # 排版1 row, 2 columns, nth plot：在jupyter notebook上兩張並排
    plt.subplot(1, len(features) , i+1)
    # add data column into plot
    x = housing[col]
    y = target
    plt.scatter(x, y, marker='o')
    plt.title(col)
    plt.xlabel(col)
    plt.ylabel('medv')
plt.savefig('images/housing-2var.png', dpi=300)
#+end_src

#+RESULTS:

#+CAPTION: Caption
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 600
[[file:images/housing-2var.png]]
*** 準備訓練用的資料
先拿兩項特徵值來試一下水溫: lstat和rm
#+begin_src python -r -n :async :results output :exports both :session boston
import numpy as np
X = housing[['lstat', 'rm']]
Y = housing['medv']
print(X)
print(Y)
#+end_src

#+RESULTS:
#+begin_example
        lstat        rm
0   -1.049614  0.347120
1   -0.373898  0.116169
2   -1.203924  1.261927
3   -1.380974  0.981486
4   -0.992763  1.204939
..        ...       ...
501 -0.287809  0.374115
502 -0.383644 -0.335236
503 -0.942409  0.948493
504 -0.805966  0.675551
505 -0.578562 -0.470207

[452 rows x 2 columns]
0      24.0
1      21.6
2      34.7
3      33.4
4      36.2
       ...
501    22.4
502    20.6
503    23.9
504    22.0
505    11.9
Name: medv, Length: 452, dtype: float64
#+end_example

** 分割訓練集與測試集
訓練集佔80%、測試集佔20%
#+begin_src python -r -n :async :results output :exports both :session boston
# train_test_split
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=5)
# 再用.shape看切出來的資料的長相（列, 欄）
print(X_train.shape)
print(X_test.shape)
print(Y_train.shape)
print(Y_test.shape)
#+end_src

#+RESULTS:
: (361, 2)
: (91, 2)
: (361,)
: (91,)

** 建立模型
new出一個LinearRegression的物件後，用特徵變數的訓練資料和目標變數的訓練資料產生一個模型。接著將特徵變數的測試資料倒進這個新產生的模型當中，得到預測的目標變數資料[fn:2]。
#+begin_src python -r -n :async :results output :exports both :session boston
# Modeling
from sklearn.linear_model import LinearRegression
reg = LinearRegression()# 學習/訓練Fitting linear model
reg.fit(X_train,Y_train)
#+end_src

#+RESULTS:

** 測試效能
將這個預測的目標變數資料（預測結果）和目標變數的測試資料（真實結果）做R2-score：
#+begin_src python -r -n :async :results output :exports both :session boston
# 預測結果Predicting using the linear model
reg.predict(X_test)# 真實結果：Y_test# 測試準確度：
print('R2:', reg.score(X_test, Y_test))
#+end_src

#+RESULTS:
: R2: 0.6048366146231109
得到的這個R2-score讓我們可以知道特徵變數對於目標變數的解釋程度為何，而越接近1代表越準確。這裡大約是66%，解釋程度算是相當好的[fn:2]。
*** 模型效能視覺化
把剛剛的預測的目標變數資料和測試的目標變數資料畫成散佈圖
#+begin_src python -r -n :async :results output :exports both :session boston
# plotting the y_test vs y_pred
Y_pred = reg.predict(X_test)
plt.cla()
plt.tight_layout()
plt.figure(figsize=(10,8))

plt.scatter(Y_pred, Y_test)
plt.xlabel('Y_pred')
plt.ylabel('Y_test')
plt.savefig('images/boston-perf.png', dpi=300)
#+end_src

#+RESULTS:

#+CAPTION: Caption
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/boston-perf.png]]

** 找出線性模型
由LinearRegression()找出線性模型的intercept和coefficient
#+begin_src python -r -n :async :results output :exports both :session boston
print('intercept:',reg.intercept_)
print('coefficient::',reg.coef_)
print('lstat:',reg.coef_[0])
print('rm:',reg.coef_[1])
#+end_src

#+RESULTS:
: intercept: 23.662167506495486
: coefficient:: [-3.2284783   4.66331239]
: lstat: -3.228478297753095
: rm: 4.663312387946355
線性模型為：\(medv=23.66 + -3.22 \times lstat + 4.66 \times rm + error\)

* [作業]依據期中考成績預測期末考成績 :TNFSH:
** Data
- 線上資料: [[https://letranger.github.io/AI/2023A-CS-Data.csv][https://letranger.github.io/AI/PythonScores.csv]]
- 資料中有424筆記錄，每筆記錄分別為學生的
  1. id: 學號
  2. class: 平時成績
  3. task: 作業成績
  4. mid: 期中考成績
  5. final: 期末考成績

** Task
你的任務是建立一個模型，輸入一個或多個特徵值(class, task, mid)來預測期末考成績(final)，其他相關任務包括:
1. 部份學生的期中、期末考有缺考行為，請將這些缺考記錄填入0分
2. 畫出所有特徵資料的分佈狀況(直方圖)
3. 將所有分數間的相關以視覺化方式表現出來
4. 將資料集分割為訓練集(70%)及測試集(30%)
5. 請自行決定你要用多少個特徵值來預測，並以測試集來評估模型效能，輸出分數(R2-score)
6. 列出你找出的模型方程式

** Solution :noexport:TNFSH:
#+begin_src python -r -n :results output :exports both
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS'] # 步驟一（替換系統中的字型，這裡用的是Ma
plt.rcParams['axes.unicode_minus'] = False  # 步驟二（解決座標軸負數的負號顯示問題）

from sklearn.linear_model import LinearRegression
import seaborn as sns

# 讀資料
docURL = 'https://letranger.github.io/AI/Downloads/PythonScores.csv'
df = pd.read_csv(docURL)

# 1. 處理缺失值
print(df.isnull().sum())
df = df.fillna(0)
print(df.isnull().sum())
print(df.describe())
# 2. 畫出所有特徵值分佈
plt.cla()
plt.figure(figsize=(20, 10))
features = ['class', 'task', 'mid', 'final']
for i, col in enumerate(features):
    # 排版1 row, 2 columns, nth plot：在jupyter notebook上兩張並排
    plt.subplot(2, 2, i+1)
    sns.histplot(df[col])
    plt.title(col)
    plt.xlabel(col)
    plt.ylabel('Density')
plt.savefig('images/taskPythonScoreDist.png', dpi=300)
# 3. 特徵值間的闗係
plt.clf()
plt.figure(figsize=(12, 10))
correlation_matrix = df[features].corr().round(2)
# 4. 分割訓練集與資料集
X = df[['class', 'task', 'mid']]
y = df['final']
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=5)
# 5. 建模、預測
# annot = True 讓我們可以把數字標進每個格子裡
sns.heatmap(data=correlation_matrix, annot = True)
plt.savefig("images/takPythonScore-corr.png", dpi=300)
from sklearn.linear_model import LinearRegression
reg = LinearRegression()# 學習/訓練Fitting linear model
reg.fit(X_train,y_train)
reg.predict(X_test)# 真實結果：Y_test# 測試準確度：

# 5. 方程式
print('R2:', reg.score(X_test, y_test))
print('intercept:',reg.intercept_)
print('coefficient::',reg.coef_)
print(f'方程式: final={reg.intercept_:.2f}+{reg.coef_[0]:.2f}*class+{reg.coef_[1]:.2f}*task+{reg.coef_[2]:.2f}*mid+error')
#+end_src

#+RESULTS:
#+begin_example
id        0
class     0
task      2
mid      34
final    38
dtype: int64
id       0
class    0
task     0
mid      0
final    0
dtype: int64
            class        task         mid      final
count  424.000000  424.000000  424.000000  424.00000
mean    81.500000   91.613208   49.813679   52.28066
std     10.073724   24.310665   33.377144   37.37800
min     30.000000    0.000000    0.000000    0.00000
25%     77.000000   84.000000   25.500000   20.00000
50%     80.000000   99.000000   43.000000   50.00000
75%     88.000000  105.000000   70.000000   82.00000
max    120.000000  120.000000  124.000000  120.00000
R2: 0.6019870084729806
intercept: -29.2033592850823
coefficient:: [0.43379714 0.06700382 0.80966573]
方程式: final=-29.20+0.43*class+0.07*task+0.81*mid+error
#+end_example
#+CAPTION: 迴歸作業2
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/taskPythonScoreDist.png]]
#+CAPTION: 迴歸作業3
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/takPythonScore-corr.png]]

方程式: \(final=-29.20+0.43*class+0.07*task+0.81*mid+error\)

* [作業]依據期中考成績預測期末考成績:1->1 :TNFSH:noexport:
** Data
- 線上資料: [[https://letranger.github.io/AI/2023A-CS-Data.csv][https://letranger.github.io/AI/2023A-CS-Data.csv]]
- 資料中有208筆記錄，每筆記錄分別為學生 *期中考成績, 期末考成績*

** Task
1. 畫出所有來源資料的分佈狀況
2. 畫出預測迴歸線
3. 依據期中考成績預測以下學生的學期成績
   - 38
   - 87
   - 92
4. 列出你找出的模型方程式

** Solution :noexport:
#+begin_src python -r -n :results output :exports both
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS'] # 步驟一（替換系統中的字型，這裡用的是Ma
plt.rcParams['axes.unicode_minus'] = False  # 步驟二（解決座標軸負數的負號顯示問題）

from sklearn.linear_model import LinearRegression

docURL = 'https://letranger.github.io/AI/2023A-CS-Data.csv'
df = pd.read_csv(docURL)
df = df.to_numpy()

mid, final = df[:,0].reshape([-1,1]), df[:,1]
#print(mid.shape)
#print(final.shape)
#print(final)
model = LinearRegression()
model.fit(mid, final)
#print(model.coef_)

# Plot
plt.figure(figsize=(7, 5))
plt.xlabel('期中考成績',fontsize=12)
plt.ylabel('期末考成績',fontsize=12)
plt.scatter(mid, final)
plt.plot(mid, model.predict(mid), color='r')
plt.savefig("images/regression-task-1.png", dpi=300)
pred = np.array([
    [38], [87], [92]
])
print(f'期中考:{pred[0]}, 學期成績:{model.predict(pred)[[0]]}')
print(f'期中考:{pred[1]}, 學期成績:{model.predict(pred)[[1]]}')
print(f'期中考:{pred[2]}, 學期成績:{model.predict(pred)[[2]]}')
#+end_src

#+RESULTS:
: 期中考:[38], 學期成績:[62.10283135]
: 期中考:[87], 學期成績:[92.65879543]
: 期中考:[92], 學期成績:[95.77675095]
#+CAPTION: 迴歸作業1
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/regression-task-1.png]]

* 新的資料集 :noexport:
#+begin_src python -r -n :async :results output :exports both :session boston
#+begin_src python -r -n :async :results output :exports both :session boston
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

from sklearn.datasets import fetch_openml
housing = fetch_openml(name="house_prices", as_frame=True)

print(type(housing))
print(housing.keys())
#+end_src
根據[[https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html][scikit官網]]的描述，bunch是一種類似字典的資料型別，其中的幾個key:
- data： 每個房子的資訊
- target： 每個房子的價格
- feature_names： 每個房子的特徵
- DESCR： 這個資料集的描述
如果想知道每個欄位詳細的描述，可以用：
#+begin_src python -r -n :async :results output :exports both :session boston
print(housing.DESCR)
#+end_src
#+RESULTS:
#+begin_example
        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  ... PoolQC  Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition
0        1          60       RL         65.0     8450   Pave   NaN      Reg  ...    NaN    NaN         NaN       0      2   2008       WD        Normal
1        2          20       RL         80.0     9600   Pave   NaN      Reg  ...    NaN    NaN         NaN       0      5   2007       WD        Normal
2        3          60       RL         68.0    11250   Pave   NaN      IR1  ...    NaN    NaN         NaN       0      9   2008       WD        Normal
3        4          70       RL         60.0     9550   Pave   NaN      IR1  ...    NaN    NaN         NaN       0      2   2006       WD       Abnorml
4        5          60       RL         84.0    14260   Pave   NaN      IR1  ...    NaN    NaN         NaN       0     12   2008       WD        Normal
...    ...         ...      ...          ...      ...    ...   ...      ...  ...    ...    ...         ...     ...    ...    ...      ...           ...
1455  1456          60       RL         62.0     7917   Pave   NaN      Reg  ...    NaN    NaN         NaN       0      8   2007       WD        Normal
1456  1457          20       RL         85.0    13175   Pave   NaN      Reg  ...    NaN  MnPrv         NaN       0      2   2010       WD        Normal
1457  1458          70       RL         66.0     9042   Pave   NaN      Reg  ...    NaN  GdPrv        Shed    2500      5   2010       WD        Normal
1458  1459          20       RL         68.0     9717   Pave   NaN      Reg  ...    NaN    NaN         NaN       0      4   2010       WD        Normal
1459  1460          20       RL         75.0     9937   Pave   NaN      Reg  ...    NaN    NaN         NaN       0      6   2008       WD        Normal

[1460 rows x 80 columns]
#+end_example
#+begin_src python -r -n :async :results output :exports both :session boston
print(housing.DESCR)
#+end_src

#+RESULTS:
#+begin_example
Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.

With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.

MSSubClass: Identifies the type of dwelling involved in the sale.

        20	1-STORY 1946 & NEWER ALL STYLES
        30	1-STORY 1945 & OLDER
        40	1-STORY W/FINISHED ATTIC ALL AGES
        45	1-1/2 STORY - UNFINISHED ALL AGES
        50	1-1/2 STORY FINISHED ALL AGES
        60	2-STORY 1946 & NEWER
        70	2-STORY 1945 & OLDER
        75	2-1/2 STORY ALL AGES
        80	SPLIT OR MULTI-LEVEL
        85	SPLIT FOYER
        90	DUPLEX - ALL STYLES AND AGES
       120	1-STORY PUD (Planned Unit Development) - 1946 & NEWER
       150	1-1/2 STORY PUD - ALL AGES
       160	2-STORY PUD - 1946 & NEWER
       180	PUD - MULTILEVEL - INCL SPLIT LEV/FOYER
       190	2 FAMILY CONVERSION - ALL STYLES AND AGES

MSZoning: Identifies the general zoning classification of the sale.

       A	Agriculture
       C	Commercial
       FV	Floating Village Residential
       I	Industrial
       RH	Residential High Density
       RL	Residential Low Density
       RP	Residential Low Density Park
       RM	Residential Medium Density

LotFrontage: Linear feet of street connected to property

LotArea: Lot size in square feet

Street: Type of road access to property

       Grvl	Gravel
       Pave	Paved

Alley: Type of alley access to property

       Grvl	Gravel
       Pave	Paved
       NA 	No alley access

LotShape: General shape of property

       Reg	Regular
       IR1	Slightly irregular
       IR2	Moderately Irregular
       IR3	Irregular

LandContour: Flatness of the property

       Lvl	Near Flat/Level
       Bnk	Banked - Quick and significant rise from street grade to building
       HLS	Hillside - Significant slope from side to side
       Low	Depression

Utilities: Type of utilities available

       AllPub	All public Utilities (E,G,W,& S)
       NoSewr	Electricity, Gas, and Water (Septic Tank)
       NoSeWa	Electricity and Gas Only
       ELO	Electricity only

LotConfig: Lot configuration

       Inside	Inside lot
       Corner	Corner lot
       CulDSac	Cul-de-sac
       FR2	Frontage on 2 sides of property
       FR3	Frontage on 3 sides of property

LandSlope: Slope of property

       Gtl	Gentle slope
       Mod	Moderate Slope
       Sev	Severe Slope

Neighborhood: Physical locations within Ames city limits

       Blmngtn	Bloomington Heights
       Blueste	Bluestem
       BrDale	Briardale
       BrkSide	Brookside
       ClearCr	Clear Creek
       CollgCr	College Creek
       Crawfor	Crawford
       Edwards	Edwards
       Gilbert	Gilbert
       IDOTRR	Iowa DOT and Rail Road
       MeadowV	Meadow Village
       Mitchel	Mitchell
       Names	North Ames
       NoRidge	Northridge
       NPkVill	Northpark Villa
       NridgHt	Northridge Heights
       NWAmes	Northwest Ames
       OldTown	Old Town
       SWISU	South & West of Iowa State University
       Sawyer	Sawyer
       SawyerW	Sawyer West
       Somerst	Somerset
       StoneBr	Stone Brook
       Timber	Timberland
       Veenker	Veenker

Condition1: Proximity to various conditions

       Artery	Adjacent to arterial street
       Feedr	Adjacent to feeder street
       Norm	Normal
       RRNn	Within 200' of North-South Railroad
       RRAn	Adjacent to North-South Railroad
       PosN	Near positive off-site feature--park, greenbelt, etc.
       PosA	Adjacent to postive off-site feature
       RRNe	Within 200' of East-West Railroad
       RRAe	Adjacent to East-West Railroad

Condition2: Proximity to various conditions (if more than one is present)

       Artery	Adjacent to arterial street
       Feedr	Adjacent to feeder street
       Norm	Normal
       RRNn	Within 200' of North-South Railroad
       RRAn	Adjacent to North-South Railroad
       PosN	Near positive off-site feature--park, greenbelt, etc.
       PosA	Adjacent to postive off-site feature
       RRNe	Within 200' of East-West Railroad
       RRAe	Adjacent to East-West Railroad

BldgType: Type of dwelling

       1Fam	Single-family Detached
       2FmCon	Two-family Conversion; originally built as one-family dwelling
       Duplx	Duplex
       TwnhsE	Townhouse End Unit
       TwnhsI	Townhouse Inside Unit

HouseStyle: Style of dwelling

       1Story	One story
       1.5Fin	One and one-half story: 2nd level finished
       1.5Unf	One and one-half story: 2nd level unfinished
       2Story	Two story
       2.5Fin	Two and one-half story: 2nd level finished
       2.5Unf	Two and one-half story: 2nd level unfinished
       SFoyer	Split Foyer
       SLvl	Split Level

OverallQual: Rates the overall material and finish of the house

       10	Very Excellent
       9	Excellent
       8	Very Good
       7	Good
       6	Above Average
       5	Average
       4	Below Average
       3	Fair
       2	Poor
       1	Very Poor

OverallCond: Rates the overall condition of the house

       10	Very Excellent
       9	Excellent
       8	Very Good
       7	Good
       6	Above Average
       5	Average
       4	Below Average
       3	Fair
       2	Poor
       1	Very Poor

YearBuilt: Original construction date

YearRemodAdd: Remodel date (same as construction date if no remodeling or additions)

RoofStyle: Type of roof

       Flat	Flat
       Gable	Gable
       Gambrel	Gabrel (Barn)
       Hip	Hip
       Mansard	Mansard
       Shed	Shed

RoofMatl: Roof material

       ClyTile	Clay or Tile
       CompShg	Standard (Composite) Shingle
       Membran	Membrane
       Metal	Metal
       Roll	Roll
       Tar&Grv	Gravel & Tar
       WdShake	Wood Shakes
       WdShngl	Wood Shingles

Exterior1st: Exterior covering on house

       AsbShng	Asbestos Shingles
       AsphShn	Asphalt Shingles
       BrkComm	Brick Common
       BrkFace	Brick Face
       CBlock	Cinder Block
       CemntBd	Cement Board
       HdBoard	Hard Board
       ImStucc	Imitation Stucco
       MetalSd	Metal Siding
       Other	Other
       Plywood	Plywood
       PreCast	PreCast
       Stone	Stone
       Stucco	Stucco
       VinylSd	Vinyl Siding
       Wd Sdng	Wood Siding
       WdShing	Wood Shingles

Exterior2nd: Exterior covering on house (if more than one material)

       AsbShng	Asbestos Shingles
       AsphShn	Asphalt Shingles
       BrkComm	Brick Common
       BrkFace	Brick Face
       CBlock	Cinder Block
       CemntBd	Cement Board
       HdBoard	Hard Board
       ImStucc	Imitation Stucco
       MetalSd	Metal Siding
       Other	Other
       Plywood	Plywood
       PreCast	PreCast
       Stone	Stone
       Stucco	Stucco
       VinylSd	Vinyl Siding
       Wd Sdng	Wood Siding
       WdShing	Wood Shingles

MasVnrType: Masonry veneer type

       BrkCmn	Brick Common
       BrkFace	Brick Face
       CBlock	Cinder Block
       None	None
       Stone	Stone

MasVnrArea: Masonry veneer area in square feet

ExterQual: Evaluates the quality of the material on the exterior

       Ex	Excellent
       Gd	Good
       TA	Average/Typical
       Fa	Fair
       Po	Poor

ExterCond: Evaluates the present condition of the material on the exterior

       Ex	Excellent
       Gd	Good
       TA	Average/Typical
       Fa	Fair
       Po	Poor

Foundation: Type of foundation

       BrkTil	Brick & Tile
       CBlock	Cinder Block
       PConc	Poured Contrete
       Slab	Slab
       Stone	Stone
       Wood	Wood

BsmtQual: Evaluates the height of the basement

       Ex	Excellent (100+ inches)
       Gd	Good (90-99 inches)
       TA	Typical (80-89 inches)
       Fa	Fair (70-79 inches)
       Po	Poor (<70 inches
       NA	No Basement

BsmtCond: Evaluates the general condition of the basement

       Ex	Excellent
       Gd	Good
       TA	Typical - slight dampness allowed
       Fa	Fair - dampness or some cracking or settling
       Po	Poor - Severe cracking, settling, or wetness
       NA	No Basement

BsmtExposure: Refers to walkout or garden level walls

       Gd	Good Exposure
       Av	Average Exposure (split levels or foyers typically score average or above)
       Mn	Mimimum Exposure
       No	No Exposure
       NA	No Basement

BsmtFinType1: Rating of basement finished area

       GLQ	Good Living Quarters
       ALQ	Average Living Quarters
       BLQ	Below Average Living Quarters
       Rec	Average Rec Room
       LwQ	Low Quality
       Unf	Unfinshed
       NA	No Basement

BsmtFinSF1: Type 1 finished square feet

BsmtFinType2: Rating of basement finished area (if multiple types)

       GLQ	Good Living Quarters
       ALQ	Average Living Quarters
       BLQ	Below Average Living Quarters
       Rec	Average Rec Room
       LwQ	Low Quality
       Unf	Unfinshed
       NA	No Basement

BsmtFinSF2: Type 2 finished square feet

BsmtUnfSF: Unfinished square feet of basement area

TotalBsmtSF: Total square feet of basement area

Heating: Type of heating

       Floor	Floor Furnace
       GasA	Gas forced warm air furnace
       GasW	Gas hot water or steam heat
       Grav	Gravity furnace
       OthW	Hot water or steam heat other than gas
       Wall	Wall furnace

HeatingQC: Heating quality and condition

       Ex	Excellent
       Gd	Good
       TA	Average/Typical
       Fa	Fair
       Po	Poor

CentralAir: Central air conditioning

       N	No
       Y	Yes

Electrical: Electrical system

       SBrkr	Standard Circuit Breakers & Romex
       FuseA	Fuse Box over 60 AMP and all Romex wiring (Average)
       FuseF	60 AMP Fuse Box and mostly Romex wiring (Fair)
       FuseP	60 AMP Fuse Box and mostly knob & tube wiring (poor)
       Mix	Mixed

1stFlrSF: First Floor square feet

2ndFlrSF: Second floor square feet

LowQualFinSF: Low quality finished square feet (all floors)

GrLivArea: Above grade (ground) living area square feet

BsmtFullBath: Basement full bathrooms

BsmtHalfBath: Basement half bathrooms

FullBath: Full bathrooms above grade

HalfBath: Half baths above grade

Bedroom: Bedrooms above grade (does NOT include basement bedrooms)

Kitchen: Kitchens above grade

KitchenQual: Kitchen quality

       Ex	Excellent
       Gd	Good
       TA	Typical/Average
       Fa	Fair
       Po	Poor

TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)

Functional: Home functionality (Assume typical unless deductions are warranted)

       Typ	Typical Functionality
       Min1	Minor Deductions 1
       Min2	Minor Deductions 2
       Mod	Moderate Deductions
       Maj1	Major Deductions 1
       Maj2	Major Deductions 2
       Sev	Severely Damaged
       Sal	Salvage only

Fireplaces: Number of fireplaces

FireplaceQu: Fireplace quality

       Ex	Excellent - Exceptional Masonry Fireplace
       Gd	Good - Masonry Fireplace in main level
       TA	Average - Prefabricated Fireplace in main living area or Masonry Fireplace in basement
       Fa	Fair - Prefabricated Fireplace in basement
       Po	Poor - Ben Franklin Stove
       NA	No Fireplace

GarageType: Garage location

       2Types	More than one type of garage
       Attchd	Attached to home
       Basment	Basement Garage
       BuiltIn	Built-In (Garage part of house - typically has room above garage)
       CarPort	Car Port
       Detchd	Detached from home
       NA	No Garage

GarageYrBlt: Year garage was built

GarageFinish: Interior finish of the garage

       Fin	Finished
       RFn	Rough Finished
       Unf	Unfinished
       NA	No Garage

GarageCars: Size of garage in car capacity

GarageArea: Size of garage in square feet

GarageQual: Garage quality

       Ex	Excellent
       Gd	Good
       TA	Typical/Average
       Fa	Fair
       Po	Poor
       NA	No Garage

GarageCond: Garage condition

       Ex	Excellent
       Gd	Good
       TA	Typical/Average
       Fa	Fair
       Po	Poor
       NA	No Garage

PavedDrive: Paved driveway

       Y	Paved
       P	Partial Pavement
       N	Dirt/Gravel

WoodDeckSF: Wood deck area in square feet

OpenPorchSF: Open porch area in square feet

EnclosedPorch: Enclosed porch area in square feet

3SsnPorch: Three season porch area in square feet

ScreenPorch: Screen porch area in square feet

PoolArea: Pool area in square feet

PoolQC: Pool quality

       Ex	Excellent
       Gd	Good
       TA	Average/Typical
       Fa	Fair
       NA	No Pool

Fence: Fence quality

       GdPrv	Good Privacy
       MnPrv	Minimum Privacy
       GdWo	Good Wood
       MnWw	Minimum Wood/Wire
       NA	No Fence

MiscFeature: Miscellaneous feature not covered in other categories

       Elev	Elevator
       Gar2	2nd Garage (if not described in garage section)
       Othr	Other
       Shed	Shed (over 100 SF)
       TenC	Tennis Court
       NA	None

MiscVal: $Value of miscellaneous feature

MoSold: Month Sold (MM)

YrSold: Year Sold (YYYY)

SaleType: Type of sale

       WD 	Warranty Deed - Conventional
       CWD	Warranty Deed - Cash
       VWD	Warranty Deed - VA Loan
       New	Home just constructed and sold
       COD	Court Officer Deed/Estate
       Con	Contract 15% Down payment regular terms
       ConLw	Contract Low Down payment and low interest
       ConLI	Contract Low Interest
       ConLD	Contract Low Down
       Oth	Other

SaleCondition: Condition of sale

       Normal	Normal Sale
       Abnorml	Abnormal Sale -  trade, foreclosure, short sale
       AdjLand	Adjoining Land Purchase
       Alloca	Allocation - two linked properties with separate deeds, typically condo with a garage unit
       Family	Sale between family members
       Partial	Home was not completed when last assessed (associated with New Homes)

Downloaded from openml.org.
#+end_example

#+begin_src python -r -n :async :results output :exports both :session boston
#+end_src
#+begin_src python -r -n :async :results output :exports both :session boston
#+end_src

* 簡單線性迴歸建模：Pizza :noexport:
Let's assume that you have recorded the diameters and prices of pizzas that you have previously eaten in your pizza journal. These observations comprise our training data:
|--------------------+------------------|
|        <c>         |       <c>        |
| Diameter in inches | Price in dollars |
|--------------------+------------------|
|         6          |        7         |
|         8          |        9         |
|         10         |        13        |
|         14         |       17.5       |
|         18         |        18        |
|--------------------+------------------|
** 觀察數據
We can visualize our training data by plotting it on a graph using matplotlib:
#+begin_src python -r -n :results output :exports both
import numpy as np
# "np" and "plt" are common aliases for NumPy and Matplotlib, respectively.
import matplotlib.pyplot as plt

# X represents the features of our training data, the diameters of the pizzas.
# A scikit–learn convention is to name the matrix of feature vectors X.
# Uppercase letters indicate matrices, and lowercase letters indicate vectors.
X = np.array([[6], [8], [10], [14], [18]]).reshape(-1, 1)

y = [7, 9, 13, 17.5 , 18]
# y is a vector representing the prices of the pizzas.

#plt.figure()
plt.title('Pizza price plotted against diameter')
plt.xlabel('Diameter in inches')
plt.ylabel('Price in dollars')
plt.plot(X, y, 'k.')
plt.axis([0, 25, 0, 25])
plt.grid(True)
plt.savefig('images/pizza-1.png', dpi=300)
#+end_src

#+RESULTS:
#+CAPTION: Pizza Regression #1
#+LABEL:fig:Pizza-Reg-1
#+name: fig:Pizza-Reg-1
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/pizza-1.png]]
We can see from the plot of the training data that there is a positive relationship between the diameter of a pizza and its price, which should be corroborated by our own pizza-eating experience.
** 建模: LinearRegression
The following pizza price predictor program models this relationship using simple linear regression.
#+begin_src python -r -n :results output :exports both
import numpy as np
X = np.array([[6], [8], [10], [14], [18]]).reshape(-1, 1)
y = [7, 9, 13, 17.5 , 18]

print(X.shape)

from sklearn.linear_model import LinearRegression
model = LinearRegression()
# Create an instance of the estimator
model.fit(X, y)
# Fit the model on the training data

# Predict the price of a pizza with a diameter that has never been seen before
test_pizza = np.array([[12]])
predicted_price = model.predict(test_pizza)[0]
print('A 12" pizza should cost: $%.2f' % predicted_price)
#+end_src

#+RESULTS:
: (5, 1)
: A 12" pizza should cost: $13.68

- The LinearRegression class is an *estimator*. Estimators predict a value based on observed data.
- In scikit-learn, all estimators implement the fit methods and predict.
- The fit method of LinearRegression learns the parameters of the following model for simple linear regression:$$y=\alpha+\beta x$$
- $y$ is the predicted value of the response variable; in this example, it is the predicted price of the pizza.
- $x$ is the explanatory variable.
- The intercept term $\alpha$ and the coefficient $\beta$ are parameters of the model that are learned by the learning algorithm.
- The hyperplane plotted in the following figure models the relationship between the size of a pizza and its price.
- Using training data to learn the values of the parameters for simple linear regression that produce the best fitting model is called ordinary least squares (OLS) or linear least squares.

#+begin_src python -r -n :results output :exports both
import numpy as np
X = np.array([[6], [8], [10], [14], [18]]).reshape(-1, 1)
y = [7, 9, 13, 17.5 , 18]

from sklearn.linear_model import LinearRegression
model = LinearRegression()
# Create an instance of the estimator
model.fit(X, y)
# Fit the model on the training data

from matplotlib import pyplot as plt
plt.scatter(X, y, color = 'k')
plt.plot(X, model.predict(X), color='g')
plt.title('Pizza price plotted against diameter')
plt.xlabel('Diameter in inches')
plt.ylabel('Price in dollars')
plt.savefig('images/pizza-2.png', dpi=300)
#+end_src

  #+RESULTS:

#+CAPTION: Pizza regression 2
#+LABEL:fig:Pizza-reg-2
#+name: fig:Pizza-reg-2
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/pizza-2.png]]
** Evaluating the fitness of the model with a cost function
Regression lines produced by several sets of parameter values are plotted in the following figure. How can we assess which parameters produced the best-fitting regression line?
#+begin_src python -r -n :results output :exports none
import numpy as np
X = np.array([[6], [8], [10], [14], [18]]).reshape(-1, 1)
y = [7, 9, 13, 17.5 , 18]
print()
print(y)
#from sklearn.linear_model import LinearRegression
#model = LinearRegression()
## Create an instance of the estimator
#model.fit(X, y)
## Fit the model on the training data
#
#from matplotlib import pyplot as plt
#plt.scatter(X, y, color = 'k')
#plt.plot(X, model.predict(X), color='g')
#plt.plot(X, model.predict(X)+.5, color='c', linestyle='--')
#plt.plot(X, model.predict(X)*.9, color='m', linestyle='-.')
#plt.title('Pizza price plotted against diameter')
#plt.xlabel('Diameter in inches')
#plt.ylabel('Price in dollars')
#plt.savefig('images/pizza-3.png', dpi=300)

#+end_src

#+RESULTS:

#+CAPTION: Pizza regression 3
#+LABEL:fig:Pizza-reg-3
#+name: fig:Pizza-reg-3
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/pizza-3.png]]
*** cost function
A cost function, also called a loss function, is used to define and measure the error of a model. The differences between the prices predicted by the model and the observed prices of the pizzas in the training set are called residuals, or training errors. The differences between the predicted and observed values in the test data are called prediction errors, or test errors.
#+begin_src python -r -n :results output :exports none
import numpy as np
X = np.array([[6], [8], [10], [14], [18]]).reshape(-1, 1)
y = [7, 9, 13, 17.5 , 18]

from sklearn.linear_model import LinearRegression
model = LinearRegression()
# Create an instance of the estimator
model.fit(X, y)
# Fit the model on the training data

from matplotlib import pyplot as plt

dy = (model.predict(X)-y)/2
for x, y1, y2 in zip(X, y, model.predict(X)):
    xs = [x, x]
    ys = [y1, y2]
    plt.plot(xs, ys, color='orange')
plt.scatter(X, y, color = 'k')
plt.plot(X, model.predict(X), color='g')
#plt.errorbar(X, model.predict(X)-dy, yerr=dy, fmt='.')
plt.title('Pizza price plotted against diameter')
plt.xlabel('Diameter in inches')
plt.ylabel('Price in dollars')
plt.savefig('images/pizza-4.png', dpi=300)
#+end_src
#+RESULTS:
#+CAPTION: Pizza regression 4
#+LABEL:fig:Pizza-reg-4
#+name: fig:Pizza-reg-4
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/pizza-4.png]]

This measure of the model's fitness is called the residual sum of squares (RSS) cost function. Formally, this function assesses the fitness of a model by summing the squared residuals for all of our training examples. The RSS is calculated with the formula in the following equation, where \(y_i\) is the observed value and \(f(x_i)\) is the predicted value:$$SS_{res}=\sum_{i=1}^{n}(y_i-f(x_i))^2$$

#+begin_src emacs-lisp :exports none
(add-to-list 'package-archives '("melpa" . "https://melpa.org/packages/"))
(setq python-shell-interpreter "/usr/bin/python3")
(setq python-shell-interpreter-arg "-i")
(setq py-use-current-dir-when-execute-p t)
(setq python-shell-prompt-detect-enabled nil)
(setq python-shell-interpreter "ipython")
(setq python-shell-interpreter-interactive-args "-i --simple-prompt")
#+end_src

#+RESULTS:
: -i --simple-prompt

#+begin_src emacs-lisp :exports none
(add-to-list 'package-archives '("melpa" . "https://melpa.org/packages/"))
#+end_src

#+RESULTS:
: ((gnu . https://elpa.gnu.org/packages/) (melpa . https://melpa.org/packages/) (org . https://orgmode.org/elpa/))

* TNFSH作業

- 線上資料: [[https://letranger.github.io/AI/2023A-CS-Data.csv][https://letranger.github.io/AI/2023A-CS-Data.csv]]

** Solution
#+begin_src python -r -n :results output :exports both
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

docURL = 'https://letranger.github.io/AI/2023A-CS-Data.csv'
df = pd.read_csv(docURL)
print(type(df))
df = df.to_numpy()
print(type(df))
print(df[:5])
#+end_src

#+RESULTS:
: <class 'pandas.core.frame.DataFrame'>
: <class 'numpy.ndarray'>
: [['期中考' '期末成績']
:  ['105.0' '99']
:  ['33.0' '66']
:  ['24.0' '75']
:  ['40.0' '67']]

* Footnotes

[fn:1] Hands-On Machine Learning with Scikit-Learn: Aurelien Geron
