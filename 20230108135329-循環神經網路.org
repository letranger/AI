:PROPERTIES:
:ID:       263cb433-d0eb-4400-a373-35175c000c01
:ROAM_ALIASES: RNN
:ROAM_ALIASES: LSTM
:ROAM_ALIASES: GRU
:END:
#+title: 時間序列的預言者：如何通過 RNN、LSTM 和 GRU 預測未來
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../css/muse.css" />
#+TAGS: AI, RNN
#+INCLUDE: ../pdf.org
#+OPTIONS: toc:2     ^:nil num:5
#+PROPERTY: header-args :eval never-export
#+EXCLUDE_TAGS: noexport
#+latex:\newpage

* CNN的限制
在[[id:20221023T101414.457264][卷積神經網路]]中，我們提過CNN的想法源自於對人類大腦認知方式的模仿，當我們辨識一個圖像，會先注意到顏色鮮明的點、線、面，之後將它們構成一個個不同的形狀(眼睛、鼻子、嘴巴 ...)，這種抽象化的過程就是 CNN 演算法建立模型的方式。其過程如下圖[fn:1]。

#+CAPTION: CNN 概念
#+name: fig:CNN-1
#+ATTR_LATEX: :width 500
#+ATTR_ORG: :width 500
#+ATTR_HTML: :width 500
#+RESULTS:
[[file:images/CNN-1.png]]

至於圖片中的每一個特徵則是利用卷積核來取得(如下圖)，換言之，CNN其實是在模擬人類的眼睛。
#+CAPTION: CNN原理
#+LABEL:fig:CNN-arch
#+name: fig:CNN-arch
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/CNN_卷積神經網路/2024-04-17_09-18-53_2024-04-17_09-11-28.png]]

萬一我們所要處理的資料並不是一張張的圖、而是一系列連續性、有時間順序的資料呢？例如：
- 一篇文章: 也許我們想生成這篇文章的摘要
- 一段時間內蒐集一的某地PM 2.5數值: 也許我們想預測該地下週的PM 2.5
- 一段演講錄音: 也許我們想生成逐字稿

你會發現，這類資料其實不太適合用眼睛，可能更適合用耳朵，所以拿CNN來分析這類資料大概是用錯了工具(相信經過[[id:0d76c861-2338-4fff-942a-47b6e02e86e3][理財達人競賽]]的你應該深有同感)。

那麼，哪一種模型比較適合模擬出人類的耳朵功能？這是本節的討論重點。

* 遞迴神經網路(Recurrent Neural Networks, RNNs)
遞迴神經網絡（RNN）是一種專門設計來處理序列數據的人工神經網絡。序列數據指的是那些隨時間連續出現的數據，比如語言（單詞組成的句子）、影片（一連串的影像畫面），或者是音樂（一連串的音符）。

想像你利用每天晚上睡前花30分鐘追劇，每當新的一集開始時，你通常還會記得上一集發生了什麼。RNN也是這樣工作的：它在處理資料（例如一句話中的每個單詞）時，會記得之前的資訊，並利用這些資訊來幫助理解或預測下一步會發生什麼。

那RNN是如何做到這點的呢?這種“記憶”是通過網絡中的循環連接實現的。這些連接使得訊息可以在模型的一層之間前後流動，就像你在看連續劇時保持對劇情的記憶一樣。我們先來看圖[[fig:rnnvscnn]]，右邊是我們熟悉的神經網路(例如[[id:20221023T101414.457264][CNN]]、DNN)，資料一律由模型的左側layer往右側傳送；而左邊的RNN則有點不同，每一層的神經元在將資料往右傳遞的同時，還偷偷留了一份給 **自己** ，這裡說的自己不是真正的自己，而是 **下一個回合的自己** 。

#+CAPTION: CNN v.s. RNN
#+LABEL:fig:rnnvscnn
#+name: fig:rnnvscnn
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/遞迴神經網路(Recurrent_Neural_Networks,_RNNs)/2024-05-10_08-43-10_RNN-03.png]]

聽起來好像有點抽象，沒關係，我們現在把圖[[fig:rnnvscnn]]中左側RNN的某一個神經元單獨抽出來分解它的內部動作，我們把圖[[fig:rnnvscnn]]中的那個循環的箭頭拆解成如圖[[fig:rnn-cell-1]]。

#+CAPTION: RNN典型結構
#+LABEL:fig:rnn-cell-1
#+name: fig:rnn-cell-1
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/遞迴神經網路(Recurrent_Neural_Networks,_RNNs)/2024-05-10_09-01-35_RNN-Cell-1.png]]

要看懂圖[[fig:rnn-cell-1]]，你只要搞清楚三件事:
1. RNN不像[[id:20221023T101414.457264][CNN]]那樣每次讀入一整張圖，而是分批讀入序列資料，例如，第1次(也就是第1個時間點($t_0$)讀入$X_0$、第2次(也就是第2個時間點($t_1$)讀入$X_1$...
2. 圖中右側「展開後」的三神經元其實是同一個，分別代表不同時間點的神經元，我們可以由 $h_{t-1}, h_{t}, h_{t+1}$ 和 $X_{t-1}, X_{t}, X_{t+1}$ 觀察出同樣的意思。
3. 原本常見的資料在模型中傳遞方向是由左而右，在圖[[fig:rnn-cell-1]]中則是由下而上，也就是輸入資料是底下的$X_t$、輸出為上面的$h_t$。

圖[[fig:rnn-cell-1]]右側代表的意思是：
1. 在第1個時間點($t-1$)取得輸入($X_{t-1}$)後，神經元會針對 $X_{t-1}$ 進行運算，更新自己的「狀態」(這個就是會影響「下一個自己」的關鍵)然後輸出結果$h_{t-1}$
2. 在第2個時間點($t$)取得輸入($X_{t}$)後，利用剛才(時間點{$t-1$)更新的「狀態」來運算$X_t$(這就是神經元受到上一個自己影響的來源，也被稱為「記憶」)，然後再次更新自己的狀態並輸出結果$h_t$
3. 最後，在第3個時間點($t+1$)取得輸入($X_{t+1}$)後，利用剛才(時間點{$t$)更新的「狀態」來運算$X_{t+1}$，然後再次更新自己的狀態並輸出結果$h_{t+1}$

整個資料讀取、處理、傳遞的流程大致如下圖所示：

#+CAPTION: RNN的運作流程
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/遞迴神經網路(Recurrent_Neural_Networks,_RNNs)/2024-05-10_13-00-27_Fully_connected_Recurrent_Neural_Network.webp]]

用個具體一點的例子，假設我們假設剛剛的序列 X 實際上是一個內容如下的英文問句：
#+begin_src python -r -n :results output :exports both
X = [ What, time, is, it, ? ]
#+end_src
而且 RNN 已經處理完前兩個元素 What 和 time 了。

則接下來 RNN 會這樣處理剩下的句子：
#+CAPTION: RNN如何處理自然語言
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/rnn-animate.gif]]

如同我們由左到右逐字閱讀這段文字同時不斷地更新你腦中的記憶狀態，RNN也是以相同的原理在做這件事。RNN的這種設計使它特別適合於像語言翻譯、語音識別或任何需要考慮過去資訊以更好地理解當前情境的任務。例如，在翻譯句子時，理解前面的詞可以幫助更準確地翻譯後面的詞。

上面提及RNN的「記憶」能力是由神經元的「狀態」實作出來，這種狀態以一個隱藏向量(hidden vector)的形式存在於神經元中，如圖[[fig:rnn-cell-2]]中的$h_{t}$)。

#+CAPTION: RNN神經元
#+LABEL:fig:rnn-cell-2
#+name: fig:rnn-cell-2
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/遞迴神經網路(Recurrent_Neural_Networks,_RNNs)/2024-05-10_15-39-00_RNN-Cell-2.png]]

這個神經元在時間點t的輸出 $h_t$ 由以下公式計算:
$$ h_t = f(W_x \cdot X_t + W_h \cdot h_{t-1} + b) $$
其中，
- $X_t$: 這是在每個時間點($t$)輸入給神經元給 RNN 的資料，例如句子中的單字、圖像中的像素或時間序列中的資料點。
- $h_{t-1}$: 先前的隱藏狀態($h_{t-1}$)，可以把它看成前一個時間點($t-1$)的網路記憶，就是它封裝了重要的歷史訊息，以舊有的記憶協助 RNN 理解當前的資料。
- 權重矩陣($W_t, W_h$ ): 這些矩陣是模型訓練的目的，可以將其視為模型的「知識」。它們決定了應該對當前輸入($X_t$) 和過去記憶($h_{t-1}$ ) 的重視程度。
- 偏移值($b$): 偏差項可作為模型的微調器，確保激活函數與資料的固有特徵協調運作。
- 激活函數($f$): RNN常用的激活函數有tanh 或 ReLU，讓RNN具備非線性的特徵，以捕捉線性模型可能忽略的複雜資料模式。

但是，RNN也有一些限制，比如它們很難處理很長的序列，因為過長時間的記憶會逐漸消失。這就像如果你試圖回憶幾個月前看的某集連續劇的細節，可能會比較困難。這個問題在後來被一種叫做LSTM的更進階版本的RNN解決。

總之，RNN是一種強大的工具，專門用於處理和預測序列數據中的模式，就像我們用記憶來理解和預測日常生活中的事件一樣。

* RNN實作
** 概念
#+begin_src python -r -n :results output :exports both
state_t = 0
for input_t in input_sequence:
    output_t = f(input_t, state_t)
    state_t = output_t
#+end_src
在 RNN 每次讀入任何新的序列數據前，細胞 A 中的記憶狀態 state_t 都會被初始化為 0。

接著在每個時間點 t，RNN 會重複以下步驟：

- 讀入 input_sequence 序列中的一個新元素 input_t
- 利用 f 函式將當前細胞的狀態 state_t 以及輸入 input_t 做些處理產生 output_t
- 輸出 output_t 並同時更新自己的狀態 state_t

在 Keras 裏頭只要 2 行就可以建立一個 RNN layer：
#+begin_src python -r -n :results output :exports both
from keras import layers
rnn = layers.SimpleRNN()
#+end_src
#+CAPTION: RNN示例
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/nn-layers.jpg]]

* LSTM
為了加強這種RNN的「記憶能力」，人們開發各種各樣的變形體，如非常著名的Long Short-term Memory(LSTM)，用於解決「長期及遠距離的依賴關係」。如下圖所示，左邊的小圖是最簡單版本的循環網絡，而右邊是人們為了增強記憶能力而開發的LSTM。
  #+CAPTION: LSTM
  #+LABEL:fig:LSTM-1
  #+name: fig:LSTM-1
  #+ATTR_LATEX: :width 300
  #+ATTR_ORG: :width 300
  #+ATTR_HTML: :width 500
  [[file:images/3r5o0000r126proo7o8q.jpg]]

面對一個如下的簡易RNN，要如何將神經元當下的記憶 state_t 與輸入 input_t 結合，才能產生最有意義的輸出 output_t 呢？
#+begin_src python -r -n :results output :exports both
state_t = 0
# 細胞 A 會重複執行以下處理
for input_t in input_sequence:
    output_t = f(input_t, state_t)
    state_t = output_t
#+end_src

前節提及，RNN神經元在時間點t的輸出 $h_t$ 由以下公式計算:
$$ h_t = f(W_x \cdot X_t + W_h \cdot h_{t-1} + b) $$
在 SimpleRNN 的神經元中，這個函數 $f$ 的實作很簡單，這也導致了其記憶狀態 state_t 沒辦法很好地「記住」前面處理過的序列元素，因而造成 RNN 在處理後來的元素時，就已經把前面重要的資訊給忘記了，也就是只有短期記憶，沒有長期記憶。

長短期記憶（Long Short-Term Memory, 後簡稱 LSTM）就是被設計來解決 RNN 的這個問題。

** LSTM的運作原理
想象你有一個書包（LSTM的內部結構），你可以決定在上課前放入什麼書籍、何時取出某本書，或者甚至決定更新裡面的某些書，你每天上學就利用書包裡的書來學習新的知識。LSTM也有類似的機制來處理信息，這些機制就是一個個的閘門(Gate)。
1. 遺忘閘（Forget Gate）：這就像是你決定從書包中拿掉不再需要的書。在LSTM中，遺忘閘會查看新的輸入信息和當前的記憶，然後決定保留哪些記憶（有用的）或者遺忘哪些（不再重要的）。
2. 輸入閘（Input Gate）：這是決定將哪些新書放入書包。LSTM會評估當前的輸入（例如新的單詞或數據點），並決定應該添加哪些信息到記憶中，這有助於更新記憶內容。
3. 輸出閘（Output Gate）：決定從書包中拿出哪本書來使用。根據需要的話題或任務，LSTM會決定哪些記憶是目前有用的，然後基於這些記憶提供輸出信息。

#+CAPTION: LSTM架構
#+LABEL:fig:LSTM-arch-0
#+name: fig:LSTM-arch-0
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/LSTM/2024-05-10_23-22-45_LSTM-arch-00.jpeg]]

典型的LSTM架構如圖[[fig:LSTM-arch-0]]所示，可以看出除了原本的資料輸入(input)，LSTM還多了三個輸入，分別是input(模型輸入），forget gate(遺忘門)，input gate(輸入門)，以及output gate(輸出門)。因此相比普通的神經網路，LSTM的參數量是它們的4倍。這3個門訊號都是處於0～1之間的實數，1代表完全打開，0代表關閉。

- 遺忘閘(Forget Gate)：該閘決定在特定時間點(timestamp, 例如圖[[fig:LSTM-arch-1]]中的$t$ )，前一個時間點($t-1$) 的模型記憶(也就是狀態, state)是否會被記住保留參與這個時間點的運算，或是直接被遺忘。當遺忘門打開時，前一刻的記憶會被保留，當遺忘門關閉時，前一刻的記憶就會被清空。換句話說，就讓模型具備選擇性遺忘部份訊息的能力，這個機制可以由激活函數sigmoid來實作，其中0代表完全忘記，1代表完全記住。
- 輸入閘(Input Gate): 決定目前這個時間點有哪些神經元的輸入($X$)中有哪些是足夠重要到可以保留下來加入「目前狀態」中，因為在序列輸入中，並不是每個時刻的輸入的資訊都是同等重要的，當輸入完全沒有用時，輸入門關閉，也就是此時刻的輸入資訊被丟棄了。這個機制同樣也可以由sigmoid 激活函數來實作，sigmoid產生的值介於0到1之間，可以被看作是一個閘控信號，這個閘控信號​和tanh函數生成的候選隱藏狀態相乘，確定了從候選狀態中將多少資訊添加到當前的單元狀態​中。
- 輸出閘(Output Gate): 決定目前神經元的狀態中有哪一部分可以輸出(流向下一個狀態)，同樣由激活函數來sigmoid來決定，這個輸出會通過tanh函數來調整，因為Tanh能夠將單元狀態的值正規化到-1到1之間，這有助於控制神經網絡的激活範圍。再由Tanh來提供輸出權重。

#+CAPTION: LSTM運作原理
#+LABEL:fig:LSTM-arch-1
#+name: fig:LSTM-arch-1
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/LSTM/2024-05-10_21-14-18_LSTM-arch-01.png]]

因為這樣的機制，讓 LSTM 即使面對很長的序列數據也能有效處理，不遺忘以前的記憶。因為效果卓越，LSTM 非常廣泛地被使用。事實上，當有人跟你說他用 RNN 做了什麼 NLP 專案時，有 9 成機率他是使用 LSTM 或是 GRU（LSTM 的改良版，只使用 2 個閘門） 來實作，而不是使用最簡單的 SimpleRNN。

** 實作: 以AI預測股價-隔日漲跌
[[https://colab.research.google.com/drive/1IehBuskagMTm6RK6WB-dsf3NPZKTNVqs?usp=sharing][當AI遇上股票-LSTM.ipynb]]
*** 安裝相關套件
#+begin_src shell -r -n :results output :exports both
pip install yfinance
#+end_src
*** 下載股價資訊
#+begin_src python -r -n :results output :exports both
import yfinance as yf

df = yf.Ticker('2330.TW').history(period='10y')
print(type(df))
#+end_src
**** 查看下載的資料集
#+begin_src python -r -n :results output :exports both :session stock :async
df
#print(df[:5])
#+end_src
**** 取出需要的特徵值
此次將成交量納入考慮
#+begin_src python -r -n :results output :exports both
data = df.filter(['Close'])
data
#+end_src
*** 觀察原始資料/日K圖
#+begin_src python -r -n :results output :exports both
import matplotlib.pyplot as plt
plt.clf()
plt.plot(data.Close)
plt.show()
#+end_src
*** 將資料標準化
#+begin_src python -r -n :results output :exports both
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
sc_data = scaler.fit_transform(data.values)

sc_data #變成numpy array
#+end_src
*** 建立、分割資料
**** 建立資料集及標籤
#+begin_src python -r -n :results output :exports both
import numpy as np

featureDays = 10
x_data, y_data = [], []
for i in range(len(sc_data) - featureDays):
  x = sc_data[i:i+featureDays]
  y = sc_data[i+featureDays]
  x_data.append(x)
  y_data.append(y)

x_data, y_data = np.array(x_data), np.array(y_data)

print(x_data.shape)
print(y_data.shape)
print(len(x_data)) #全部資料筆數
#+end_src
**** 分割訓練集與測試集
#+begin_src python -r -n :results output :exports both
ratio = 0.8
train_size = round(len(x_data) * ratio)
print(train_size)
x_train, y_train = x_data[:train_size], y_data[:train_size]
x_test, y_test = x_data[train_size:], y_data[train_size:]

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)
#+end_src
*** 建立、編譯、訓練模型
**** 建立模型
#+begin_src python -r -n :results output :exports both
import tensorflow as tf
#建構LSTM模型
model = tf.keras.Sequential()
# LSTM層
model.add(tf.keras.layers.LSTM(units=64, unroll = False, input_shape=(featureDays,1)))
# Dense層
model.add(tf.keras.layers.Dense(units=1))
#+end_src
#+begin_src python -r -n :results output :exports both
model.summary()
#+end_src
**** 編譯模型
#+begin_src python -r -n :results output :exports both
model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])
#+end_src
**** 訓練模型
#+begin_src python -r -n :results output :exports both
model.fit(x_train, y_train,
          validation_split=0.2,
          batch_size=200, epochs=20)
#+end_src
*** 性能測試
**** loss
#+begin_src python -r -n :results output :exports both
score = model.evaluate(x_test, y_test)
print('loss:', score[0])
#+end_src
**** predict
#+begin_src python -r -n :results output :exports both
predict = model.predict(x_test)
predict = scaler.inverse_transform(predict)
predict = np.reshape(predict, (predict.size,))
ans = scaler.inverse_transform(y_test)
ans = np.reshape(ans, (ans.size,))
print(predict[:3])
print(ans[:3])
#+end_src
**** plot
#+begin_src python -r -n :results output :exports both
plt.plot(predict)
plt.plot(ans)
plt.show()
#+end_src

* 待用 :noexport:
** 待用1
RNN 能夠處理「任意個數的輸入序列」，所以十分適合用在「語言塑模」或「語音辨識」。理論上，RNN 可以用來處理任何問題，因為它已被證明具有「圖靈完備性」(Turing-Complete)。以遞迴關係的函數表示 RNN 可將其視為 \(S_t=f(S_{t-1},X_t)\)，這裡的\(S_t\)表示第\(t\)步的狀態，它是由函數\(f\)對上一步(\(t-1\))的狀態(即\(S_{t-1}\))與這一步的輸入\(X_t\)所計算出來的結果，這裡的函數\(f\)可以是任何可微分的函數，如\(S_t=tang(S_{t-1}*W+X_t*U)\)。
正因為每個狀態都會與之前所有的計算有關，其所代表的重要含義為：隨著時間的推移，RNNs 可以說是有記憶力的，因為狀態 S 包含了之前所有步驟的資訊。

語言塑模的目標是計算「字的序列」的機率，這在「語音辨識」、OCR、「機器翻譯」、「拼字校正」上都非常重要。以「字」為基準的「語言模型」是由「字的序列」來定義機率分佈，給定一個長度為\(m\)的字序列，它會為整個字序列給定一個機率\(P(w_1,...,w_m)\)，其「聯合機率」(joint probability)可以由公式[[eqn:JointProbability]]中的連鎖規則(chain rule)計算出來：
#+NAME: eqn:JointProbability
\begin{equation}
P(w_1,...,w_m)=P(w_1)P(w_2|w_1)P(w3|w_2,w_1)...P(w_m|w_1,...,w_{m-1})
\end{equation}

這個聯合機率一般是基於一個「獨立性假設」(independence assumption)，即，第 i 個字只會相依於它之前的 n-1 個字，如果我們的模型是連續 n 個字的聯合機率，就稱為「n元」(n-gram)。例：
- 1-gram / unigram: "The", "quick", "brown" and "fox"
- 2-grams / bigram: "The quick", "quick brown" and "brown fox"
- 3-grams / trigram: "The quick brown" and "quick brown fox"
- 4-grams: "The quick brown fox"

現在，如果我們有一個巨大的語料庫(corpus of text)，我們就可以用一個特定的 n(通常為 2-4)搜尋所有「n元」在「語料庫」中出現的次數，進而在「給定前 n-1 個字的前提下」，估計出每個 n 元中最後一個字出現的機率。
** LSTM
#+CAPTION: Caption
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/lstm-cell.png]]

基本上一個 LSTM 細胞裡頭會有 3 個閘門（Gates）來控制細胞在不同時間點的記憶狀態：

- Forget Gate：決定細胞是否要遺忘目前的記憶狀態
- Input Gate：決定目前輸入有沒有重要到值得處理
- Output Gate：決定更新後的記憶狀態有多少要輸出

透過這些閘門控管機制，LSTM 可以將很久以前的記憶狀態儲存下來，在需要的時候再次拿出來使用。值得一提的是，這些閘門的參數也都是神經網路自己訓練出來的。

#+CAPTION: LSTM 細胞頂端那條 cell state 正代表著細胞記憶的轉換過程
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/lstm-cell-detailed.png]]

想像 LSTM 細胞裡頭的記憶狀態是一個包裹，上面那條直線就代表著一個輸送帶。

LSTM 可以把任意時間點的記憶狀態（包裹）放上該輸送帶，然後在未來的某個時間點將其原封不動地取下來使用。
#+CAPTION: Caption
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/accumulation-conveyor-101.jpg]]

** Bi-directional RNN:
另一個循環網絡的變種 - 雙向循環網絡(Bi-directional RNN)也是現階段自然語言處理和語音分析中的重要模型。開發雙向循環網絡的原因是語言/語音的構成取決於上下文，即「現在」依託於「過去」和「未來」。單向的循環網絡僅著重於從「過去」推出「現在」，而無法對「未來」的依賴性有效的建模。

** RNN: Recurrent Neural Network，Recursive neural networks。
雖然很多時候我們把這兩種網絡都叫做RNN，但事實上這兩種網路的結構事實上是不同的。而我們常常把兩個網絡放在一起的原因是：它們都可以處理有序列的問題，比如時間序列等。
  舉個最簡單的例子，我們預測股票走勢用RNN就比普通的DNN效果要好，原因是股票走勢和時間相關，今天的價格和昨天、上周、上個月都有關係。而RNN有「記憶」能力，可以「模擬」數據間的依賴關係(Dependency)。

** RNN
RNN 是一種有「記憶力」的神經網路，其最為人所知的形式如下：
#+CAPTION: Caption
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 500
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/rnn-static.png]]
如同上圖等號左側所示，RNN 跟一般深度學習中常見的前饋神經網路（Feedforward Neural Network, 後簡稱 FFNN）最不一樣的地方在於它有一個迴圈（Loop）。
要了解這個迴圈在 RNN 裏頭怎麼運作，現在讓我們想像有一個輸入序列 X（Input Sequence）其長相如下：
$$ X = [ x_0, x_1, x_2, \dots x_t ]$$
1. 不同於 FFNN，RNN 在第一個時間點 $t_0$ 並不會直接把整個序列 $X$ 讀入。反之，在第一個時間點 $t_0$，它只將該序列中的第一個元素 $x_0$ 讀入中間的細胞 A。細胞 A 則會針對 $x_0$ 做些處理以後，更新自己的「狀態」並輸出第一個結果 $h_0$ 。
2. 在下個時間點 $t_1$，RNN 如法炮製，讀入序列 $X$ 中的下一個元素 $x_1$，並利用剛剛處理完 $x_0$ 得到的細胞狀態，處理 $x_1$ 並更新自己的狀態（也被稱為記憶），接著輸出另個結果 $h_1$。
3. 剩下的 $x_t$ 都會被以同樣的方式處理。但不管輸入的序列 $X$ 有多長，RNN 的本體從頭到尾都是等號左邊的樣子：迴圈代表細胞 A 利用「上」一個時間點（比方說 $t_1$）儲存的狀態，來處理當下的輸入（比方說 $x_2$ ）。

但如果你將不同時間點（$t_0$, $t_1$ ...）的 RNN 以及它的輸入一起截圖，並把所有截圖從左到右一字排開的話，就會長得像等號右邊的形式。將 RNN 以右邊的形式表示的話，你可以很清楚地了解，當輸入序列越長，向右展開的 RNN 也就越長。（模型也就需要訓練更久時間，這也是為何我們在資料前處理時設定了序列的最長長度）

為了確保你 100 % 理解 RNN，讓我們假設剛剛的序列 X 實際上是一個內容如下的英文問句：
#+begin_src python -r -n :results output :exports both
X = [ What, time, is, it, ? ]
#+end_src
而且 RNN 已經處理完前兩個元素 What 和 time 了。

則接下來 RNN 會這樣處理剩下的句子：
#+CAPTION: Caption
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/rnn-animate.gif]]
就像你現在閱讀這段話一樣，你是由左到右逐字在大腦裡處理我現在寫的文字，同時不斷地更新你腦中的記憶狀態。

每當下個詞彙映入眼中，你腦中的處理都會跟以下兩者相關：
- 前面所有已讀的詞彙
- 目前腦中的記憶狀態

當然，實際人腦的閱讀機制更為複雜，但 RNN抓到這個處理精髓，利用內在迴圈以及細胞內的「記憶狀態」來處理序列資料。

* Footnotes

[fn:1] [[https://ithelp.ithome.com.tw/articles/10191820][Day 06：處理影像的利器 -- 卷積神經網路(Convolutional Neural Network)]]
