:PROPERTIES:
:ID:       20221023T101626.420918
:END:
#+title: 監督式學習

# -*- org-export-babel-evaluate: nil -*-
#+TAGS: AI, Machine Learning, SVM, RBM
#+OPTIONS: toc:2 ^:nil num:5
#+PROPERTY: header-args :eval never-export
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../css/muse.css" />
#+begin_export html
<a href="https://letranger.github.io/AI/20221023101626-監督式學習.html"><img align="right" alt="Hits" src="https://hits.sh/letranger.github.io/AI/20221023101626-監督式學習.html.svg"/></a>
#+end_export

* Read :noexport:
- [[https://medium.com/udacity/shannon-entropy-information-gain-and-picking-balls-from-buckets-5810d35d54b4https://medium.com/udacity/shannon-entropy-information-gain-and-picking-balls-from-buckets-5810d35d54b4][熵及資訊獲利]]
- [[http://debussy.im.nuu.edu.tw/sjchen/MachineLearning/final/CLS_DT.pdf][決策樹學習： 聯合大學資管系]]

* 簡介
監督式學習獲得的結果可以是數值、也可以是類別，以結果分類，我們可以將監督式學分大致分為兩類：[[id:6ae7fb7a-0b38-4448-b19f-073d262513f2][迴歸]](結果為數值)與[[id:1592687a-cca7-4473-83a0-682a36394a28][分類]](結果為類別)。
** 監督式學習的主要類型
*** 分類(Cliasification)

分類問題也稱為離散(discrete)預測問題，因為每個分類都是一個離散群組。In supervised learning, the training set you feed to the algorithm includes the desired solutions, called labels[fn:1].

#+CAPTION: 典型的監督式學習：垃圾郵件分類
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/2022-04-30_10-38-58.jpg]]

可再細分為:
- Binary classification
- Multiclass classification

典型的分類案例: MNIST, IRIS
*** 迴歸(Regression)
另一種監督式學習為[[id:6ae7fb7a-0b38-4448-b19f-073d262513f2][迴歸]]（regression），即，根據一組預測特徵（predictor，如里程數、車齡、品牌）來預測目標數值（如二手車車價）[fn:1]，這個目標數值也是label。

有些迴歸演算法也可以用來分類，例如Logistic，它可以輸出一個數值，以這個數值來表示對應到特定類別的機率，例如，某封email為垃圾郵件的機率為20%、某張圖片為狗的機率為70%。

迴歸問題可再細分為兩類：
- Linear regression:
  * 假設輸入變量(x)與單一輸出變量(y)間存在線性關係，並以此建立模型。
  * 優點: 簡單、容易解釋
  * 缺點: 輸入與輸出變量關係為線性時會導致低度擬合
  * 例: 身高與體重間的關係
- Logistic regression
  * 也是線性方法，但使用logist function轉換輸出的預測結果，其輸出結果為類別機率(class probabilities)
  * 優點: 簡單、容易解釋
  * 缺點: 輸入與輸出變量關係為線性時無法處理分類問題

典型迴歸案例: Boston Housing Data
** 範例
*** 信用卡詐欺
以信用卡公司來說，信用卡詐欺可以透過各種資料對照評分，預測出該事件的分數。例如地點：一個平時生活在台灣的用戶忽然在日本刷卡、或一個平時都是白天刷卡消費的用戶忽然在凌晨三點刷卡，這些事件都可以當成評分指標(特徵)，提供模型做為預測是否為信用卡詐欺的預測依據。
*** 影像辨識
** 特例
[[id:20221023T101626.420918][監督式學習]]主要包括[[id:1592687a-cca7-4473-83a0-682a36394a28][分類]]和[[id:6ae7fb7a-0b38-4448-b19f-073d262513f2][迴歸]]，但也包括以下奇特的例子：
- 序列生成(sequence generation)：給定一張圖，產生一個標題來描述該圖片，有時也可以使用一部份連續的資料進行預測。
- 語法樹預測(syntax tree prediction)：給定一個句子，以語意的結構為節點，預測並分解成語法樹。
- 物體偵測(object detection)：給定一張圖片，繪製邊界框來標示圖片內不同的物體。這也可以視為分類問題（給定許多候選邊界框，對每個邊界框的內容進行分類）或并用分類和迴歸技巧，透過向量迴歸預測邊界框。
- 圖像分割(image segmentation)：給定一張圖片，用像素遮罩(pixel-level mask)來區別不同物體。
- Linear Regression: 拿前幾天的空氣 PM 值預估未來的空氣 PM 值
- Classification
  1) Binary Classification: 垃圾郵件分類
  2) Multi-class Classification: 手語翻譯
  3) k-Nearest Neighbors (KNN)
  4) Naive Bayes Classifiers
  5) Decision Tree
  6) Neural Networks (Deep Learning)
  7) Ensembles of Decision Trees
  8) Linear Models: Logistic regression

* 監督式學習基本步驟
1. 準備學習對象的資料
2. 將資料分為輸入資料（特徵）與輸出資料（標籤、即該組特徵的答案）
3. 將特徵輸入類神經網路
4. 將類神經網路的預測結果與標籤進行比較、計算二者間的差異
5. 將4.的差異回饋給模型、依此更新模型中的參數
6. 回到3.

* 監督式學習演算法
** K-nearest neighbors (KNN)
KNN藉由找出與新資料點最相近的 /k/ 個已具有label的資料點，讓這些資料點投票決定新資料點的label。
- 優點: 能處理更複雜的非線性關係，但仍可被解釋
- 缺點: 隨著資料與features的數量增加，KNN的效果也會降低； /k/ 值的選擇也會影響KNN的效果，太小的 /k/ 值會導致過度擬合、太高的 /k/ 值則會低度擬合。
- 應用: 經常用於推薦系統
** Methods based on tree(Decision tree and Random Forest)
- Single decision tree: 遍歷所有訓練資枓來建立規則，但容易過度擬合
- Bagging: 將上述tree加入bootstrap aggregation(如bagging)，即，使用多次隨機實例採樣(multiple random samples of instances)，並為每次採樣建立一棵decision tree，並對每個資料實例進行預測，預測方式為透過平均每棵樹的預測結果，藉由這種方式可以解決decision tree容易過度擬合的問題。
- Random forest: 除了將資料實例進行採樣，也對每棵decision tree的分支條件中 *待預測label* 進行隨機採樣，而非使用所有的待預測label。透過這種方式，random forest可以建立出彼此相關性更低的decision，進而改善過度擬合與泛化誤差。
** Boosting
同樣是建立許多樹，但是它 *依多建立每棵decision tree* , 利用前一棵decision所習得的資訊來改善下一棵decision tree的預測結果。是所有tree-based solution中表現最好的方式，也是許多machine learning比賽的常勝軍。
- 優點：performance佳，能處理資料缺失與特徵分類問題
- 缺點：可解釋性低
** SVM(Support Vector Machines)
使用演算法和已知的label在空間中建構超平面來分類資料
** 神經網路

* 監督式學習實作
** [[id:1592687a-cca7-4473-83a0-682a36394a28][分類]]
** [[id:6ae7fb7a-0b38-4448-b19f-073d262513f2][迴歸]]

* Footnotes

[fn:1] Hands-On Machine Learning with Scikit-Learn: Aurelien Geron

[fn:2] [[https://towardsai.net/p/programming/decision-trees-explained-with-a-practical-example-fe47872d3b53][Decision Trees Explained With a Practical Example]]

[fn:3] DEFINITION NOT FOUND.
