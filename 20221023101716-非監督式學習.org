:PROPERTIES:
:ID:       20221023T101716.467694
:ROAM_ALIASES: Unsupervised Learning
:END:
#+title: 非監督式學習

# -*- org-export-babel-evaluate: nil -*-
#+TAGS: AI
#+OPTIONS: toc:3 ^:nil num:3
#+OPTIONS: H:4
#+PROPERTY: header-args :eval never-export
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../css/muse.css" />
#+EXCLUDE_TAGS: noexport
#+begin_export html
<a href="https://letranger.github.io/AI/20221023101716-非監督式學習.html"><img align="right" alt="Hits" src="https://hits.sh/letranger.github.io/AI/20221023101716-非監督式學習.html.svg"/></a>
#+end_export

* to read :noexport:
- [[https://ithelp.ithome.com.tw/articles/10307959][Day 28 - 自動編碼器的實作 ]]
- [[https://u9534056.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E4%BB%BB%E5%8B%99-%E7%9B%A3%E7%9D%A3%E5%AD%B8%E7%BF%92-%E5%8D%8A%E7%9B%A3%E7%9D%A3%E5%AD%B8%E7%BF%92-%E7%84%A1%E7%9B%A3%E7%9D%A3%E5%AD%B8%E7%BF%92-9b75972f91d6][機器學習任務：監督學習/半監督學習/無監督學習]]
- [[https://pyecontech.com/2020/03/24/svm/][[機器學習首部曲] 支援向量機 SVM]]
- [[https://www.dexweng.com/ml-intro/][究竟什麼是機器學習?所有種類及演算法一次告訴你！]]
- [[https://www.sciencehistory.org/stories/magazine/ronald-fisher-a-bad-cup-of-tea-and-the-birth-of-modern-statistics/][Distillations magazine- 紅茶與牛奶的故事]]
- [[https://www.youtube.com/watch?v=cVMX-k0rW68&list=RDcVMX-k0rW68&index=1][Soundtrack]]
- [[https://alankrantas.medium.com/kmeans-%E8%83%BD%E5%BE%9E%E8%B3%87%E6%96%99%E4%B8%AD%E6%89%BE%E5%87%BA-k-%E5%80%8B%E5%88%86%E9%A1%9E%E7%9A%84%E9%9D%9E%E7%9B%A3%E7%9D%A3%E5%BC%8F%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%BC%94%E7%AE%97%E6%B3%95-%E6%89%80%E4%BB%A5%E5%AE%83%E5%88%B0%E5%BA%95%E6%9C%89%E5%95%A5%E7%94%A8-%E4%BD%BF%E7%94%A8-scikit-learn-%E8%88%87-python-5dd8c0c8b167][KMeans：能從資料中找出 K 個分類的非監督式機器學習演算法 — — 所以它到底有啥用？（使用 scikit-learn 與 Python）]]

* 非監督式學習
#+CAPTION: AI, Machine Learning與Deep Learning
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AI,_Machine_Learning與Deep_Learning/2024-02-19_16-24-48_2024-02-19_16-23-09.png]]
** 目的
非監督式學習接收未被標記的數據，並通過演算法根據資料的基礎結構(如常見的模式、特色、或是其他因素)將數據分類，而非 *做出預測* 。例如：
- 將網站訪客進行分類: 性別、喜好、上網時段
- 將一堆照片依類型分類: cat、automobile、truck、frog、ship...
  #+CAPTION: 照片分類
  #+LABEL:fig:Labl
  #+name: fig:Name
  #+ATTR_LATEX: :width 300
  #+ATTR_ORG: :width 300
  #+ATTR_HTML: :width 500
  [[file:images/2022-04-30_10-57-36.jpg]]
- 異常檢測(Anamaly Detection): 例如，找出不尋常的信用卡交易以防止詐騙、找出製程中有缺陷的產品、將資料組中的離群值挑出來再傳給另一個演算法
  #+CAPTION: Novelty Detection
  #+LABEL:fig:Labl
  #+name: fig:Name
  #+ATTR_LATEX: :width 300
  #+ATTR_ORG: :width 300
  #+ATTR_HTML: :width 500
  [[file:images/2022-04-30_11-35-44.jpg]]

** 非監督式學習的常見演算法
為了讓相近的資料可以聚集在一起，通常還是會將資料的特徵值數值化，再透過計算資料間的「距離」進行分群，在此常以「歐幾里得距離」為計算方式。常見的分群演算法包括：
*** 分群(clustering)
聚類(集群)
- K-Means
- DBSCAN
- 階層式分群分析(Hierarchical Cluster Analysis, HCA)
*** 異常檢測與新穎檢測
- One-class SVM
- 孤立森林(Isolation Forest)
*** 降維
降維有兩大分支：線性投影與流形學習[fn:1]。
**** 線性投影
***** 主成分分析(Principal component analysis, PCA)
PCA有數種變形：mini-batch變形式PCA(incremental PCA)、非線性變形(kernel PCA)、稀疏變形(sparse PCA)
***** 奇異值分解(Singular value decomposition, SVD)
降低原來特徵所組成的矩陣的秩（rank)，使得原來的矩陣可以使用擁有較小的秩的矩陣所組成的線性組合來表示。
***** 隨機投影(Random projection)
由高維投影至低維空間，但同時保留點與點間的矩離，可以使用隨機高斯矩陣（random Gaussian matrix)或隨機稀疏矩陣(random sparse matrix)來實現。
- principal component analysis
- singular value decomposition
- random projection.
**** 流形學習(Manifold learning)
***** Isomap
透過估算點與粌近點的捷線(geodesic)或曲線距離(curved distance)，而非使用歐式距離(Euclidean distance)來學習資料流形的內蘊幾何。
***** t-distributed stochastic neighbor embedding(t-SNE)
將高維度空間的資料嵌入至二維或三維的空間
***** multidimensional scaling (MDS)
***** locally linear embedding (LLE)
***** dictionary learning
***** random trees embedding
***** independent component analysis

* 聚類(集群)
- 任務: grouping objects together based on similarity.
- 應用:
  - 在信用卡詐欺偵測中，聚類可以將詐欺交易分組在一起，將其與正常交易分開​​。
  - 如果我們的資料集中的觀測值只有幾個標籤，我們可以先使用聚類對觀測值進行分組（不使用標籤）。 然後，我們可以將少數標記觀測值的標籤轉移到同一組內的其餘觀測值。 這是遷移學習的一種形式，也是機器學習中一個快速發展的領域。

** K-Means
先來這裡([[https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/][K-Means Clustering Demo]])試一下什麼是K-Means Cluestering。

「群集」的概念簡單來說就是將相近的資料彼此分在同一群體。

K-means演算法：將n個點劃分到K個聚落中，如此一來每個點都屬於離其最近的聚落中心所對應之聚落，以之作為分群的標準。
#+begin_src python -r -n :results output :exports none
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)
plt.scatter(X[:, 0], X[:, 1], s=50)
plt.savefig("images/blobsScatter.png", dpi=300)
#plt.show() 
#+end_src
#+CAPTION: scikit-learn blobs
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/blobsScatter.png]]
*** K-Means原理
八張未標註動物名稱(標籤)的照片，每張照片有兩個特徵值
#+CAPTION: 資料庫樣本
#+LABEL:fig:Labl
#+name: fig:kmeans-1
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/聚類(集群)/2024-02-10_20-19-55_2024-02-10_20-19-45.png]]

八張照片的特徵分佈如下
#+begin_src python -r -n :results output :exports none
import numpy as np
import matplotlib.pyplot as plt

cats = np.array([[1, 9], [2, 6], [3, 5]])
dogs = np.array([[4, 8], [6, 3], [7, 6], [8, 8], [9, 4]])
cc = cats.mean(axis=0).transpose()
dc = dogs.mean(axis=0).transpose()
nn = np.array([[5],[6]])
cats = cats.transpose()
dogs = dogs.transpose()
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

size = 300
plt.xlim(0, 10)
plt.ylim(0, 10)
plt.xticks(np.arange(0, 11, 1))
plt.xlabel('特徵值1:頭部大小')
plt.ylabel('特徵值2:尾巴長度')
plt.yticks(np.arange(0, 11, 1))

plt.scatter(cats[0], cats[1],
            color='#29c4bd', marker='o', s=size, zorder=2)
plt.scatter(dogs[0], dogs[1],
            color='#29c4bd', marker='o', s=size, zorder=2)

for x,y  in zip(cats[0], cats[1]):
    label = "({0},{1})".format(x,y)
    plt.annotate(label, (x,y), textcoords="offset points", xytext=(0,10), ha='center')
for x,y  in zip(dogs[0], dogs[1]):
    label = "({0},{1})".format(x,y)
    plt.annotate(label, (x,y), textcoords="offset points", xytext=(0,10), ha='center')

plt.grid()
plt.savefig('images/kms-1.png', dpi=300)
#+end_src
#+RESULTS:
#+CAPTION: 待處理資料
#+LABEL:fig:Labl
#+name: fig:kmeans-2
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/kms-1.png]]

K-means 演算法執行步驟如下:
**** 決定K值
K 值指的是現有訓練資料(八張照片)要分成的群數，此處K值為2。
**** 選定K個中心點
任意選定 K 個(K=2)中心點，在實際的程式實作可以亂數隨機產生這K個資料點。如圖[[fig:kmeans-3]]所示，隨機指定的兩群資料點的中心點為(5，5)、(6，9)。
#+begin_src python -r -n :results output :exports none
import numpy as np
import matplotlib.pyplot as plt

cats = np.array([[1, 9], [2, 6], [3, 5]])
dogs = np.array([[4, 8], [6, 3], [7, 6], [8, 8], [9, 4]])
cc = cats.mean(axis=0).transpose()
dc = dogs.mean(axis=0).transpose()
nn = np.array([[5],[6]])
cats = cats.transpose()
dogs = dogs.transpose()

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

size = 300
plt.xlim(0, 10)
plt.ylim(0, 10)
plt.xticks(np.arange(0, 11, 1))
plt.xlabel('特徵值1:頭部大小')
plt.ylabel('特徵值2:尾巴長度')
plt.yticks(np.arange(0, 11, 1))

plt.scatter(cats[0], cats[1], color='#29c4bd', marker='o', s=size, zorder=2)
plt.scatter(dogs[0], dogs[1], color='#29c4bd', marker='o', s=size, zorder=2)
#挑中心點
plt.scatter(5, 5, color='green', marker='*', s=size, zorder=2)
plt.scatter(6, 9, color='orange', marker='*', s=size, zorder=2)

for x,y  in zip(cats[0], cats[1]):
    label = "({0},{1})".format(x,y)
    plt.annotate(label, (x,y), textcoords="offset points", xytext=(0,10), ha='center')
for x,y  in zip(dogs[0], dogs[1]):
    label = "({0},{1})".format(x,y)
    plt.annotate(label, (x,y), textcoords="offset points", xytext=(0,10), ha='center')
##狗的中心點
x, y = 5, 5
label = "中心點1({0},{1})".format(x, y)
plt.annotate(label, (x,y), textcoords="offset points", xytext=(10,-20), ha='center')
##貓的中心點
x, y = 6, 9
label = "中心點2({0},{1})".format(x, y)
plt.annotate(label, (x,y), textcoords="offset points", xytext=(10,-20), ha='center')
#新圖點
#x, y = nn[0][0], nn[1][0]
#print(x, y)
#label = "？({0},{1})".format(x, y)
#plt.annotate(label, (x,y), textcoords="offset points", xytext=(0,-20), ha='center')
#print(np.sqrt(np.square(nn[0][0]-cc[0])+np.square(nn[1][0]-cc[1])))
#print(np.sqrt(np.square(nn[0][0]-dc[0])+np.square(nn[1][0]-dc[1])))
plt.grid()
plt.savefig('images/kms-2.png', dpi=300)
#+end_src
#+RESULTS:
#+CAPTION: 標題
#+LABEL:fig:Labl
#+name: fig:kmeans-3
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
file:images/kms-2.png
**** 將資料點分群
接下來為所有資料點計算各自與中心點的「歐幾里德距離」，決定該資料點要被歸入哪一個資料群，計算過程及結果如圖[[fig:kmeans-4]]、[[fig:kmeans-5]]所示。
#+begin_src python -r -n :results output :exports none
import numpy as np
import matplotlib.pyplot as plt

cats = np.array([[1, 9], [2, 6], [3, 5], [4, 8]])
dogs = np.array([[6, 3], [7, 6], [8, 8], [9, 4]])
c1 = np.array([[5],[6]])
c2 = np.array([[6],[9]])

#nn = np.array([[5],[6]])
cats = cats.transpose()
dogs = dogs.transpose()
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

size = 300
plt.xlim(0, 10)
plt.ylim(0, 10)
plt.xticks(np.arange(0, 11, 1))
plt.xlabel('特徵值1')
plt.ylabel('特徵值2')
plt.yticks(np.arange(0, 11, 1))

plt.scatter(cats[0], cats[1],
            color='#29c4bd', marker='o', s=size, zorder=2)
plt.scatter(dogs[0], dogs[1],
            color='#29c4bd', marker='o', s=size, zorder=2)
#挑中心點
plt.scatter(c1[0], c1[1],
            color='green', marker='*', s=size, zorder=2)
plt.scatter(c2[0], c2[1],
            color='orange', marker='*', s=size, zorder=2)

#計算各點與中心點1的歐幾里德，畫線
dist1 = []
c1xs, c1ys = np.repeat(c1[0],4), np.repeat(c1[1],4)
for cx, cy, x, y in zip(c1xs, c1ys, cats[0], cats[1]):
    dist1.append(np.sqrt(np.square(cx-x)+np.square(cy-y)))
    plt.plot([cx, x], [cy, y], '--', color='#dd99ff')
dist2 = []
c1xs, c1ys = np.repeat(c1[0],4), np.repeat(c1[1],4)
for cx, cy, x, y in zip(c1xs, c1ys, dogs[0], dogs[1]):
    dist2.append(np.sqrt(np.square(cx-x)+np.square(cy-y)))
    plt.plot([cx, x], [cy, y], '--', color='#dd99ff')

#計算各點與中心點2的歐幾里德，畫線
#dist3 = []
#c1xs, c1ys = np.repeat(c2[0],4), np.repeat(c2[1],4)
#for cx, cy, x, y in zip(c1xs, c1ys, cats[0], cats[1]):
#    print(cx, cy, x, y)
#    dist3.append(np.sqrt(np.square(cx-x)+np.square(cy-y)))
#    plt.plot([cx, x], [cy, y], '--', color='#dd99ff')
#dist4 = []
#c1xs, c1ys = np.repeat(c2[0],4), np.repeat(c2[1],4)
#for cx, cy, x, y in zip(c1xs, c1ys, dogs[0], dogs[1]):
#    print(cx, cy, x, y)
#    dist4.append(np.sqrt(np.square(cx-x)+np.square(cy-y)))
#    plt.plot([cx, x], [cy, y], '--', color='#dd99ff')
#
for x,y  in zip(cats[0], cats[1]):
    label = "({0},{1})".format(x,y)
    plt.annotate(label, (x,y), textcoords="offset points", xytext=(0,10), ha='center')
for x,y  in zip(dogs[0], dogs[1]):
    label = "({0},{1})".format(x,y)
    plt.annotate(label, (x,y), textcoords="offset points", xytext=(0,10), ha='center')
##中心點1
x, y = c1[0][0], c1[1][0]
label = "中心點1({0},{1})".format(x, y)
plt.annotate(label, (x,y), textcoords="offset points", xytext=(10,-20), ha='center')
##中心點2
x, y = c2[0][0], c2[1][0]
label = "中心點2({0},{1})".format(x, y)
plt.annotate(label, (x,y), textcoords="offset points", xytext=(10,10), ha='center')
plt.grid()
plt.savefig('images/km-3.png', dpi=300)
#+end_src
#+RESULTS:
#+CAPTION: 標題
#+LABEL:fig:Labl
#+name: fig:kmeans-4
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/km-3.png]]
#+begin_src python -r -n :results output :exports none
import numpy as np
import matplotlib.pyplot as plt

cats = np.array([[1, 9], [2, 6], [3, 5], [4, 8]])
dogs = np.array([[6, 3], [7, 6], [8, 8], [9, 4]])
c1 = np.array([[5],[6]])
c2 = np.array([[6],[9]])

#nn = np.array([[5],[6]])
cats = cats.transpose()
dogs = dogs.transpose()
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

size = 300
plt.xlim(0, 10)
plt.ylim(0, 10)
plt.xticks(np.arange(0, 11, 1))
plt.xlabel('特徵值1')
plt.ylabel('特徵值2')
plt.yticks(np.arange(0, 11, 1))

plt.scatter(cats[0], cats[1],
            color='#29c4bd', marker='o', s=size, zorder=2)
plt.scatter(dogs[0], dogs[1],
            color='#29c4bd', marker='o', s=size, zorder=2)
#挑中心點
plt.scatter(c1[0], c1[1],
            color='green', marker='*', s=size, zorder=2)
plt.scatter(c2[0], c2[1],
            color='orange', marker='*', s=size, zorder=2)

#計算各點與中心點1的歐幾里德，畫線
#dist1 = []
#c1xs, c1ys = np.repeat(c1[0],4), np.repeat(c1[1],4)
#for cx, cy, x, y in zip(c1xs, c1ys, cats[0], cats[1]):
#    dist1.append(np.sqrt(np.square(cx-x)+np.square(cy-y)))
#    plt.plot([cx, x], [cy, y], '--', color='#dd99ff')
#dist2 = []
#c1xs, c1ys = np.repeat(c1[0],4), np.repeat(c1[1],4)
#for cx, cy, x, y in zip(c1xs, c1ys, dogs[0], dogs[1]):
#    dist2.append(np.sqrt(np.square(cx-x)+np.square(cy-y)))
#    plt.plot([cx, x], [cy, y], '--', color='#dd99ff')

#計算各點與中心點2的歐幾里德，畫線
dist3 = []
c1xs, c1ys = np.repeat(c2[0],4), np.repeat(c2[1],4)
for cx, cy, x, y in zip(c1xs, c1ys, cats[0], cats[1]):
    dist3.append(np.sqrt(np.square(cx-x)+np.square(cy-y)))
    plt.plot([cx, x], [cy, y], '--', color='#dd99ff')
dist4 = []
c1xs, c1ys = np.repeat(c2[0],4), np.repeat(c2[1],4)
for cx, cy, x, y in zip(c1xs, c1ys, dogs[0], dogs[1]):
    dist4.append(np.sqrt(np.square(cx-x)+np.square(cy-y)))
    plt.plot([cx, x], [cy, y], '--', color='#dd99ff')

for x,y  in zip(cats[0], cats[1]):
    label = "({0},{1})".format(x,y)
    plt.annotate(label, (x,y), textcoords="offset points", xytext=(0,10), ha='center')
for x,y  in zip(dogs[0], dogs[1]):
    label = "({0},{1})".format(x,y)
    plt.annotate(label, (x,y), textcoords="offset points", xytext=(0,10), ha='center')
##中心點1
x, y = c1[0][0], c1[1][0]
label = "中心點1({0},{1})".format(x, y)
plt.annotate(label, (x,y), textcoords="offset points", xytext=(10,-20), ha='center')
##中心點2
x, y = c2[0][0], c2[1][0]
label = "中心點2({0},{1})".format(x, y)
plt.annotate(label, (x,y), textcoords="offset points", xytext=(10,10), ha='center')
plt.grid()
plt.savefig('images/km-4.png', dpi=300)
#+end_src
#+RESULTS:
#+CAPTION: 標題
#+LABEL:fig:Labl
#+name: fig:kmeans-5
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/km-4.png]]

最後將計算結果記錄如下圖，進行第一輸分群：


#+CAPTION: 標題
#+LABEL:fig:Labl
#+name: fig:kmeans-6
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/聚類(集群)/2024-02-11_15-59-29_2024-02-10_21-14-09.png]]
**** 為 K 個群裡的資料點找出新中心點
依前一步驟的分類，此 8 張資料點已分為兩群，接下來我們再為這兩群資料點找出各自的新中心點，計算方式如下:
- 新[[color:green][★X]]值: 2+3+4+6+7+9 =5.17
- 新[[color:green][★Y]]值: 6+5+8+3+6+4 =5.33
- 新[[color:orange][★X]]值:1+8=4.50
- 新[[color:orange][★Y]]值:9+8=8.50
#+begin_src python -r -n :results output :exports none
import numpy as np
import matplotlib.pyplot as plt

green = np.array([[2, 6], [3, 5], [4, 8], [6, 3], [7, 6], [9, 4]])
orange = np.array([[1, 9], [8, 8]])
c1 = green.mean(axis=0).transpose()
c2 = orange.mean(axis=0).transpose()
#nn = np.array([[5],[6]])
# 新的中心點:
green = green.transpose()
orange = orange.transpose()
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
plt.grid()
size = 300
plt.xlim(0, 10)
plt.ylim(0, 10)
plt.xticks(np.arange(0, 11, 1))
plt.xlabel('特徵值1')
plt.ylabel('特徵值2')
plt.yticks(np.arange(0, 11, 1))

plt.scatter(green[0], green[1],
            color='green', marker='o', s=size, zorder=2)
plt.scatter(orange[0], orange[1],
            color='orange', marker='o', s=size, zorder=2)
#挑中心點
plt.scatter(c1[0], c1[1],
            color='green', marker='*', s=size, zorder=2)
plt.scatter(c2[0], c2[1],
            color='orange', marker='*', s=size, zorder=2)

#計算各點與中心點1的歐幾里德，畫線
dist1 = []
c1xs, c1ys = np.repeat(c1[0],6), np.repeat(c1[1],6)

#計算各點與中心點2(orange)的歐幾里德，畫線
dist3 = []
c1xs, c1ys = np.repeat(c2[0],6), np.repeat(c2[1],6)
for cx, cy, x, y in zip(c1xs, c1ys, green[0], green[1]):
    dist3.append(np.sqrt(np.square(cx-x)+np.square(cy-y)))
    #plt.plot([cx, x], [cy, y], '--', color='#e8c495')
dist4 = []
c1xs, c1ys = np.repeat(c2[0],2), np.repeat(c2[1],2)
for cx, cy, x, y in zip(c1xs, c1ys, orange[0], orange[1]):
    dist4.append(np.sqrt(np.square(cx-x)+np.square(cy-y)))
for x,y  in zip(green[0], green[1]):
    label = "({0},{1})".format(x,y)
    plt.annotate(label, (x,y), textcoords="offset points", xytext=(0,10), ha='center')
for x,y  in zip(orange[0], orange[1]):
    label = "({0},{1})".format(x,y)
    plt.annotate(label, (x,y), textcoords="offset points", xytext=(0,10), ha='center')
###中心點1
x, y = c1[0], c1[1]
label = "新中心點1\n({0:.2f},{1:.2f})".format(x, y)
plt.annotate(label, (x,y), textcoords="offset points", xytext=(20,10), ha='center')
###中心點2
x, y = c2[0], c2[1]
label = "新中心點2\n({0:.2f},{1:.2f})".format(x, y)
plt.annotate(label, (x,y), textcoords="offset points", xytext=(20,10), ha='center')
plt.savefig('images/km5.png', dpi=300)
#+end_src
#+CAPTION: 標題
#+LABEL:fig:Labl
#+name: fig:kmeans-7
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/km5.png]]

這個結果看起來不太合理對吧，至少(4,8)這點應該要歸入[[color:orange][★]]這組才對。沒關係，因為還沒完成。
**** 重覆步驟 (3)、(4) 進行下一輪的分群，直到分群結果不再變化
接下來就繼續計算各點到新中心點[[color:green][★]](5.17, 5.33)、[[color:orange][★]](4.50, 8.50)的距離、依新的距離重新對資料點進行分群(即步驟3)，再求出新的中心點(即步驟4)，如此重覆不斷進行，直到分群結果不再變動即告完成。
**** 如何訂K值
- 用K-means演算法需設定「K值」，但難免會面臨難以決定分群數量的狀況。同樣的資料如果要分成3群、4群、5群，就必須做三次不同的操作，而且分群的結果彼此之間不一定有其關聯性。
- 利用「階層式分群法」透過階層架構的方式，以對特徵距離的分析，將資料層層反覆地進行分裂或聚合，彈性決定群數。
*** K-Means實作:隨機數字 :sklearn:
#+begin_src python -r -n :results output :exports none
# 隨機生成100個(x, y)
import pandas as pd
import matplotlib.pyplot as plt

data = {
    'x': [25, 34, 22, 27, 33, 33, 31, 22, 35, 34, 67, 54, 57, 43, 50, 57, 59, 52, 65, 47, 49, 48, 35, 33, 44, 45, 38,
          43, 51, 46],
    'y': [79, 51, 53, 78, 59, 74, 73, 57, 69, 75, 51, 32, 40, 47, 53, 36, 35, 58, 59, 50, 25, 20, 14, 12, 20, 5, 29, 27,
          8, 7]
    }
samples = pd.DataFrame(data)
plt.scatter(samples['x'], samples['y'])
plt.savefig("images/kmeansScatterData.png", dpi=300)
#plt.show()
#+end_src

#+RESULTS:
#+CAPTION: 原始資料
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/kmeansScatterData.png]]

#+begin_src python -r -n :results output :exports both
# 隨機生成100個(x, y)
import pandas as pd
data = {
    'x': [25, 34, 22, 27, 33, 33, 31, 22, 35, 34, 67, 54, 57, 43, 50, 57, 59, 52, 65, 47, 49, 48, 35, 33, 44, 45, 38,
          43, 51, 46],
    'y': [79, 51, 53, 78, 59, 74, 73, 57, 69, 75, 51, 32, 40, 47, 53, 36, 35, 58, 59, 50, 25, 20, 14, 12, 20, 5, 29, 27,
          8, 7]
    }
samples = pd.DataFrame(data)

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3) #預計分為三群，迭代次數由模型自行定義
kmeans.fit(samples)
cluster = kmeans.predict(samples)

plt.scatter(samples['x'], samples['y'], c=cluster, cmap=plt.cm.Set1)
plt.savefig("images/kmeansScatter.png", dpi=300)
#plt.show()
#+end_src

#+RESULTS:

#+CAPTION: scikit-KMeans
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/kmeansScatter.png]]
*** K-Means應用: 壓縮影像
:PROPERTIES:
:CUSTOM_ID: NS-KM-Image
:END:
#+begin_src python -r -n :results output :exports both
import numpy as np
import matplotlib.pyplot as plt # 需安裝 pillow 才能讀 JPEG
from matplotlib import image
from sklearn.cluster import MiniBatchKMeans

# K 值 (要保留的顏色數量)
K = 4
# 讀取圖片
image = image.imread(r'./images/Photo42.jpg')
w, h, d = tuple(image.shape)
image_data = np.reshape(image, (w * h, d))/ 255
# 將顏色分類為 K 種
kmeans = MiniBatchKMeans(n_clusters=K, batch_size=10)
labels = kmeans.fit_predict(image_data)
centers = kmeans.cluster_centers_
# 根據分類將顏色寫入新的影像陣列
image_compressed = np.zeros(image.shape)
label_idx = 0
for i in range(w):
  for j in range(h):
    image_compressed[i][j] = centers[labels[label_idx]]
    label_idx += 1

plt.imsave(r'images/compressTest.jpg', image_compressed)
#+end_src

#+begin_src python -r -n :results output :exports none
#輸出說明
import numpy as np
import matplotlib.pyplot as plt # 需安裝 pillow 才能讀 JPEG
from matplotlib import image
from sklearn.cluster import MiniBatchKMeans

# K 值 (要保留的顏色數量)
K = 4

# 讀取圖片
image = image.imread(r'./images/Photo42.jpg')
w, h, d = tuple(image.shape)
print(w,h,d)
image_data = np.reshape(image, (w * h, d))/ 255
print(image_data.shape)
print(image_data[0])
print(image_data[1])
# 將顏色分類為 K 種
kmeans = MiniBatchKMeans(n_clusters=K, batch_size=10)
labels = kmeans.fit_predict(image_data)
print(labels[:10])
centers = kmeans.cluster_centers_
print(centers[:10])
# 根據分類將顏色寫入新的影像陣列
image_compressed = np.zeros(image.shape)
label_idx = 0
for i in range(w):
  for j in range(h):
    image_compressed[i][j] = centers[labels[label_idx]]
    label_idx += 1

plt.imsave(r'images/compressTest.jpg', image_compressed)
#+end_src

#+RESULTS:
: 480 640 3
: (307200, 3)
: [0.20784314 0.16078431 0.23921569]
: [0.21960784 0.17254902 0.25098039]
: [0 0 0 0 0 0 0 0 2 2]
: [[0.1535014  0.10980392 0.17348273]
:  [0.59200603 0.36930618 0.34788839]
:  [0.39191176 0.24676471 0.26073529]
:  [0.87828054 0.70392157 0.77662142]]
#+CAPTION: 以KMeans壓縮圖片色彩
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/compressTest.jpg]]
*** [小組作業]K-Means分群實作 :TNFSH:
以K-Means對鳶尾花資料(特徵值)進行分群

作業內容須包含:
- 程式碼
- 以不同特徵值(\(C^4_2\))配對進行cluster，畫出scatter
- 以不同特徵值(\(C^4_3\))配對進行cluster，畫出3D scatter
- 對於輸出之結果應輔以文字說明解釋。
- 以pdf繳交報告，報告首頁需列出組員列表(姓名、教學網ID)
*** [小組作業]以K-Means壓縮影像實作 :TNFSH:
參考前述[K-Means應用: 壓縮影像]，自行找一張圖(jpg)進行以下測試
- 以不同K值、batchSize進行影像壓縮，並探討在不同情況下的壓縮效果(包含影像大小及品質)
- 以不同類型(顏色數量:全彩、256色、灰階)的圖片進行測試
- 對於輸出之結果應輔以文字說明解釋。
- 以pdf繳交報告，報告首頁需列出組員列表(姓名、教學網ID)
*** sklearn Kmeans: MNist :noexport:
**** 讀入資料
#+begin_src python -r  :async :results output :exports both :session KM-Mist
'''Main'''
import numpy as np
import pandas as pd
import os, time, pickle, gzip
import datetime

'''Data Prep'''
from sklearn import preprocessing as pp

'''Data Viz'''
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
color = sns.color_palette()
# Load the datasets
current_path = os.getcwd()
file = os.path.sep.join(['', 'datasets', 'mnist.pkl.gz'])

f = gzip.open(current_path+file, 'rb')
train_set, validation_set, test_set = pickle.load(f, encoding='latin1')
f.close()

X_train, y_train = train_set[0], train_set[1]
X_validation, y_validation = validation_set[0], validation_set[1]
X_test, y_test = test_set[0], test_set[1]

# Create Pandas DataFrames from the datasets
train_index = range(0,len(X_train))
validation_index = range(len(X_train), len(X_train)+len(X_validation))
test_index = range(len(X_train)+len(X_validation), \
                   len(X_train)+len(X_validation)+len(X_test))

X_train = pd.DataFrame(data=X_train,index=train_index)
y_train = pd.Series(data=y_train,index=train_index)

X_validation = pd.DataFrame(data=X_validation,index=validation_index)
y_validation = pd.Series(data=y_validation,index=validation_index)

X_test = pd.DataFrame(data=X_test,index=test_index)
y_test = pd.Series(data=y_test,index=test_index)

#+end_src

#+RESULTS:
**** Dimensionality Reduction
#+begin_src python -r -n :async :results output :exports both :session KM-Mist
# Principal Component Analysis
from sklearn.decomposition import PCA

n_components = 784
whiten = False
random_state = 2018

pca = PCA(n_components=n_components, whiten=whiten, random_state=random_state)

X_train_PCA = pca.fit_transform(X_train)
X_train_PCA = pd.DataFrame(data=X_train_PCA, index=train_index)

 # Log data
cwd = os.getcwd()
log_dir = cwd+"/datasets/"
y_train[0:2000].to_csv(log_dir+'labels.tsv', sep = '\t', index=False, header=False)

# Write dimensions to CSV
X_train_PCA.iloc[0:2000,0:3].to_csv(log_dir+'pca_data.tsv', sep = '\t', index=False, header=False)
#+end_src

#+RESULTS:
**** K-Means Inertia
Inertia measures how well a dataset was clustered by K-Means. It is calculated by measuring the distance between each data point and its centroid, squaring this distance, and summing these squares across one cluster[fn:1].
#+begin_src python -r -n :async :results output :exports both :session KM-Mist
# K-means - Inertia as the number of clusters varies
from sklearn.cluster import KMeans

n_clusters = 10
n_init = 10
max_iter = 300
tol = 0.0001
random_state = 2018

kMeans_inertia = pd.DataFrame(data=[],index=range(2,21), \
                              columns=['inertia'])
for n_clusters in range(2,21):
    kmeans = KMeans(n_clusters=n_clusters, n_init=n_init, \
                max_iter=max_iter, tol=tol, random_state=random_state)

    cutoff = 99
    kmeans.fit(X_train_PCA.loc[:,0:cutoff])
    kMeans_inertia.loc[n_clusters] = kmeans.inertia_

# Plot inertia relative to k # of clusters
kMeans_inertia.plot()
plt.savefig('images/km-inertia.png', dpi=300)
#+end_src

#+RESULTS:
#+CAPTION: 不同cluster的效能
#+LABEL:fig:Labl
#+name: fig:inertia
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/km-inertia.png]]

如圖[[fig:inertia]]所示，隨著cluster數目的增加、inertia會隨之下降。也就是說，隨著cluster數目的增加，同一cluster內的樣本同質性也就越高。
**** k-Means Accuracy
#+begin_src python -r -n :async :results output :exports both :session KM-Mist
# K-means - Inertia as the number of clusters varies
# Define analyze cluster function
def analyzeCluster(clusterDF, labelsDF):
    countByCluster = pd.DataFrame(data=clusterDF['cluster'].value_counts())
    countByCluster.reset_index(inplace=True,drop=False)
    countByCluster.columns = ['cluster','clusterCount']

    preds = pd.concat([labelsDF,clusterDF], axis=1)
    preds.columns = ['trueLabel','cluster']

    countByLabel = pd.DataFrame(data=preds.groupby('trueLabel').count())

    countMostFreq = \
        pd.DataFrame(data=preds.groupby('cluster').agg( \
                        lambda x:x.value_counts().iloc[0]))
    countMostFreq.reset_index(inplace=True,drop=False)
    countMostFreq.columns = ['cluster','countMostFrequent']

    accuracyDF = countMostFreq.merge(countByCluster, \
                        left_on="cluster",right_on="cluster")
    overallAccuracy = accuracyDF.countMostFrequent.sum()/ \
                        accuracyDF.clusterCount.sum()

    accuracyByLabel = accuracyDF.countMostFrequent/ \
                        accuracyDF.clusterCount

    return countByCluster, countByLabel, countMostFreq, \
            accuracyDF, overallAccuracy, accuracyByLabel

# K-means - Accuracy as the number of clusters varies
n_clusters = 5
n_init = 10
max_iter = 300
tol = 0.0001
random_state = 2018

kMeans_inertia = \
    pd.DataFrame(data=[],index=range(2,21),columns=['inertia'])
overallAccuracy_kMeansDF = \
    pd.DataFrame(data=[],index=range(2,21),columns=['overallAccuracy'])

for n_clusters in range(2,21):
    kmeans = KMeans(n_clusters=n_clusters, n_init=n_init, \
                max_iter=max_iter, tol=tol, random_state=random_state)

    cutoff = 99
    kmeans.fit(X_train_PCA.loc[:,0:cutoff])
    kMeans_inertia.loc[n_clusters] = kmeans.inertia_
    X_train_kmeansClustered = kmeans.predict(X_train_PCA.loc[:,0:cutoff])
    X_train_kmeansClustered = pd.DataFrame(data=X_train_kmeansClustered, index=X_train.index, columns=['cluster'])

    countByCluster_kMeans, countByLabel_kMeans, countMostFreq_kMeans, accuracyDF_kMeans, overallAccuracy_kMeans, accuracyByLabel_kMeans = analyzeCluster(X_train_kmeansClustered, y_train)

    overallAccuracy_kMeansDF.loc[n_clusters] = overallAccuracy_kMeans

# Plot accuracy
overallAccuracy_kMeansDF.plot()
plt.savefig('images/km-accuracy.png', dpi=300)
#+end_src

#+RESULTS:

#+CAPTION: Caption
#+LABEL:fig:Labl
#+name: fig:km-accuracy
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/km-accuracy.png]]

如圖[[fig:km-accuracy]]，分類的精確程度隨著cluster數目的增加而提高，

** Hierarchical clustering
:PROPERTIES:
:CUSTOM_ID: NS-Hie-cluster
:END:
階層式分群法(Hierarchical Clustering)透過一種階層架構的方式，將資料層層反覆地進行分裂或聚合，以產生最後的樹狀結構，常見的方式有兩種：
- 聚合式階層分群法(Agglomerative Clustering): 是一種“bottom-up”的方法，也就是先準備好解決問題可能所需的基本元件或方案，再將這些基本元件組合起來，由小而大最後得到整體。因此在階層式分群法中，就是將每個資料點都視為一個個體，再一一聚合[fn:2]，如圖[[fig:buttomup]][fn:3]。
  #+CAPTION: Buttom-up
  #+LABEL:fig:Labl
  #+name: fig:buttomup
  #+ATTR_LATEX: :width 300
  #+ATTR_ORG: :width 300
  #+ATTR_HTML: :width 500
  [[file:images/聚類(集群)/2024-02-11_21-29-19_2024-02-11_21-28-47.png]]
- 分裂式階層分群法(Divisive Clustering): 是一種“top-down”的方法，先對問題有整體的概念，然後再逐步加上細節，最後讓整體的輪廓越來越清楚。而此法在階層式分群法中，先將整個資料集視為一體，再一一的分裂[fn:2]，如圖[[fig:topdown]][fn:3]。
  #+CAPTION: Top-down
  #+LABEL:fig:Labl
  #+name: fig:topdown
  #+ATTR_LATEX: :width 300
  #+ATTR_ORG: :width 300
  #+ATTR_HTML: :width 500
  [[file:images/聚類(集群)/2024-02-11_21-30-15_2024-02-11_21-30-06.png]]
*** 聚合式階層分群法(Agglomerative)
如果採用聚合的方式，階層式分群法可由樹狀結構的底部開始，將資料或群聚逐次合併。

聚合式階層分群步驟：
1. 將各個資料點先視為個別的「群」。
2. 比較各個群之間的距離，找出距離最短的兩個群。
3. 將其合併變成一個新群。
4. 不斷重複直到群的數量符合所要求的數目。
**** 聚合式階層分群: step by step
1. 假設現在有6筆資料，分別標記A、B、C、D、E及F，且每筆資料都是一個群。
   #+CAPTION: hierar-1
   #+LABEL:fig:Labl
   #+name: fig:Name
   #+ATTR_LATEX: :width 300
   #+ATTR_ORG: :width 300
   #+ATTR_HTML: :width 500
   [[file:images/聚類(集群)/2024-02-11_09-04-56_2024-02-11_09-04-42.png]]
2. 首先找距離最近的兩個群，在此例為A、B。將A與B結合為新的一群G1，就將這些點分成五群了，其中有四群還是單獨的點。
   #+CAPTION: 標題
   #+LABEL:fig:Labl
   #+name: fig:Name
   #+ATTR_LATEX: :width 300
   #+ATTR_ORG: :width 300
   #+ATTR_HTML: :width 500
   [[file:images/聚類(集群)/2024-02-11_09-06-09_2024-02-11_09-06-00.png]]
3. 接著，再繼續找距離最近的兩個群，依此範例應為D與E，結合為新的一群G2。
   #+CAPTION: 標題
   #+LABEL:fig:Labl
   #+name: fig:Name
   #+ATTR_LATEX: :width 300
   #+ATTR_ORG: :width 300
   #+ATTR_HTML: :width 500
   [[file:images/聚類(集群)/2024-02-11_09-06-59_2024-02-11_09-06-54.png]]
4. 將F與G2合而為新的群G3，這時，這些資料已經被分為三群了。
   #+CAPTION: 標題
   #+LABEL:fig:Labl
   #+name: fig:Name
   #+ATTR_LATEX: :width 300
   #+ATTR_ORG: :width 300
   #+ATTR_HTML: :width 500
   [[file:images/聚類(集群)/2024-02-11_09-18-01_2024-02-11_09-07-48.png]]
**** 如何定義兩個群聚之間的距離
***** 單一連結聚合
Single-linkage agglomerative algorithm, 群聚與群聚間的距離可以定義為不同群聚中最接近兩點間的距離。

在分屬不同的兩群中，選擇最接近的兩點之距離，即代表兩群間的距離。因此在群與群間進行聚合時，依據此值最小者做為選取下一步結合之對象。

#+CAPTION: 標題
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 100
#+ATTR_ORG: :width 100
#+ATTR_HTML: :width 300
[[file:images/聚類(集群)/2024-02-11_10-13-45_2024-02-11_09-23-14.png]]

公式: \( d(G1, G2)=\min\limits_{ A \in G1, B \in G2 }  d(A,B)\)

G1、G3與C之間如何聚合？
- G1與C之間的距離d(G1,C)＝d(B,C)
- G3與C之間的距離d(G3,C)＝d(F,C)
- G1與G3之間的距離d(G1,G3)＝d(B,D)

計算完各群間的距離後，可知d(G3,C)為最短距離，因此G3將與C聚合，成為新群G4。

#+CAPTION: 標題
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/聚類(集群)/2024-02-11_10-16-41_2024-02-11_10-16-28.png]]

倘若要再聚合，由於剩下G1與G4，可聚合成為G5。

#+CAPTION: 標題
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/聚類(集群)/2024-02-11_10-17-37_2024-02-11_10-17-28.png]]
***** 完整連結聚合
Complete-linkage agglomerative algorithm, 群聚間的距離定義為不同群聚中最遠兩點間的距離，這樣可以保證這兩個集合合併後, 任何一對的距離不會大於 d。

在分屬不同的兩群中，選擇最遠的兩點之距離，即代表兩群間的距離。因此在群與群間進行聚合時，依據此值最小者做為選取下一步結合之對象。

公式: \(d(G1,G2)=\max\limits_{A \in G1, B \in G2}d(A,B)\)

#+CAPTION: 標題
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 100
#+ATTR_ORG: :width 100
#+ATTR_HTML: :width 300
[[file:images/聚類(集群)/2024-02-11_10-18-31_2024-02-11_10-18-24.png]]

G1、G3與C之間如何聚合？
- G1與C之間的距離d(G1,C)＝d(A,C)
- G3與C之間的距離d(G3,C)＝d(E,C)
- G1與G3之間的距離d(G1,G3)＝d(A,E)

#+CAPTION: 標題
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/聚類(集群)/2024-02-11_10-20-11_2024-02-11_10-20-03.png]]


計算完各群間的距離後，可知d(G1,C)為最短距離，因此G1將與C聚合，成為新群G4。

倘若要再聚合，由於剩下G3與G4，可聚合成為G5。

#+CAPTION: 標題
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/聚類(集群)/2024-02-11_10-21-08_2024-02-11_10-21-02.png]]
***** 平均連結聚合
Average-linkage agglomerative algorithm, 群聚間的距離定義為不同群聚間各點與各點間距離總和的平均。沃德法（Ward's method）：群聚間的距離定義為在將兩群合併後，各點到合併後的群中心的距離平方和。

在分屬不同的兩群中，各點之距離的平均，即代表兩群間的距離。因此在群與群間進行聚合時，依據此值最小者做為選取下一步結合之對象。
G1、G3與C之間如何聚合？

#+CAPTION: 標題
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 100
#+ATTR_ORG: :width 100
#+ATTR_HTML: :width 300
[[file:images/聚類(集群)/2024-02-11_10-24-58_2024-02-11_10-24-49.png]]

公式: \(d(G1,G2)=\sum\limits_{A \in G1, B \in G2}\frac{d(A,B)}{|G1|\times|G2|}\)

- \( d(G1, C)=\frac{d(A,C)+d(B,C)}{2\times1}\)
- \( d(G3, C)=\frac{d(D,C)+d(E,C)+d(F,C)}{3\times1}\)
- \( d(G1, G3)=\frac{d(A,D)+d(A,E)+d(A,F)+d(B,D)+d(B,E)+d(B,F)}{2\times3}\)
**** 決定群數
可以依照使用者的群數需求或相似度要求，來決定要在哪一層時停止聚合資料。若以完整連結的群間距離計算方式為例，圖上的虛線代表不同的群數，端看使用者需求來決定。
#+CAPTION: 標題
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/聚類(集群)/2024-02-11_10-47-21_2024-02-11_10-47-15.png]]
**** 聚合式階層分群實作
***** scikit-learn: Agglomerative Clustering
****** 分兩群
#+begin_src python -r -n :async :results output :exports both :session aggclu
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt
import numpy as np

# randomly chosen dataset
X = np.array([[1, 2], [1, 4], [1, 0], [2, 1], [2, 3], [2, 4],
              [3, 1], [3, 3], [3, 4], [4, 2], [4, 4], [4, 0]])
clustering = AgglomerativeClustering(n_clusters = 2).fit(X)
print('分兩群:',clustering.labels_)
#+end_src

#+RESULTS:
: 分兩群: [0 1 0 0 1 1 0 1 1 0 1 0]

#+begin_src python -r -n :async :results output :exports none :session aggclu
plt.cla()
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
plt.grid()
size = 300
plt.xlim(-1, 6)
plt.ylim(-1, 6)

# 為每個群集指定不同的顏色
colors = ['orange' if label == 0 else 'green' for label in clustering.labels_]
plt.scatter(X[:,0], X[:,1], color=colors, marker='o', s=size, zorder=2)
plt.savefig('images/aggclu-1.png', dpi=300)
#+end_src

#+RESULTS:
: [0 1 0 0 1 1 0 1 1 0 1 0]

#+CAPTION: 分成兩組
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/aggclu-1.png]]
****** 分三群
#+begin_src python -r -n :async :results output :exports both :session aggclu
clustering = AgglomerativeClustering(n_clusters = 3).fit(X)
print('分三群:',clustering.labels_)
#+end_src

#+RESULTS:
: 分三群: [1 0 1 1 0 0 1 0 0 2 0 2]

#+begin_src python -r -n :async :results output :exports none :session aggclu
plt.cla()
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
plt.grid()
size = 300
plt.xlim(-1, 6)
plt.ylim(-1, 6)

# 為每個群集指定不同的顏色
colors = ['orange', 'green', 'red']  # 這裡定義了三種不同的顏色

# 根據 clustering.labels_ 中的值選擇對應的顏色
colors = [colors[label] for label in clustering.labels_]

plt.scatter(X[:,0], X[:,1], color=colors, marker='o', s=size, zorder=2)
plt.savefig('images/aggclu-2.png', dpi=300)
#+end_src

#+RESULTS:
#+CAPTION: 分成三群
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/aggclu-2.png]]
****** 分四群
#+begin_src python -r -n :async :results output :exports both :session aggclu
clustering = AgglomerativeClustering(n_clusters = 4).fit(X)
print('分四群:',clustering.labels_)
#+end_src

#+RESULTS:
: 分四群: [0 3 0 0 3 3 0 1 1 2 1 2]

#+begin_src python -r -n :async :results output :exports none :session aggclu
plt.cla()
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
plt.grid()
size = 300
plt.xlim(-1, 6)
plt.ylim(-1, 6)

# 為每個群集指定不同的顏色
colors = ['orange', 'green', 'red', 'blue']  # 這裡定義了三種不同的顏色

# 根據 clustering.labels_ 中的值選擇對應的顏色
colors = [colors[label] for label in clustering.labels_]

plt.scatter(X[:,0], X[:,1], color=colors, marker='o', s=size, zorder=2)
plt.savefig('images/aggclu-3.png', dpi=300)
#+end_src

#+RESULTS:
#+CAPTION: 分成四群
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/aggclu-3.png]]
***** SciPy: scipy.cluster.hierarchy[一次分完]
#+begin_src python -r -n :async :results output :exports both :session hierarchy
import numpy as np
import matplotlib.pyplot as plt
import scipy.cluster.hierarchy as sch

# randomly chosen dataset
X = np.array([[1, 2], [1, 4], [1, 0], [2, 1], [2, 3], [2, 4],
              [3, 1], [3, 3], [3, 4], [4, 2], [4, 4], [4, 0]])
y = np.array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'])

dis=sch.linkage(X,metric='euclidean', method='ward')
#metric: 距離的計算方式
#method: 群與群之間的計算方式，”single”, “complete”, “average”,
#                      “weighted”, “centroid”, “median”, “ward”

sch.dendrogram(dis, labels = y)

plt.title('Hierarchical Clustering')
plt.xticks(rotation=30)
plt.savefig("images/hierarCluster-1.png", dpi=300)
#plt.show()
#+end_src
#+RESULTS:
#+CAPTION: Hierarchical Clustering
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/hierarCluster-1.png]]
***** SciPy: scipy.cluster.hierarchy[逐步分群]
#+begin_src python -r -n :async :results output :exports both :session hierarchy
import numpy as np
import matplotlib.pyplot as plt
import scipy.cluster.hierarchy as sch

# randomly chosen dataset
X = np.array([[1, 2], [1, 4], [1, 0], [2, 1], [2, 3], [2, 4],
              [3, 1], [3, 3], [3, 4], [4, 2], [4, 4], [4, 0]])
y = np.array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'])

#metric: 距離的計算方式
#method: 群與群之間的計算方式，”single”, “complete”, “average”, “weighted”, “centroid”, “median”, “ward”

plt.cla()
# Setting the truncate_mode to 'lastp' to see incremental clustering
plt.figure(figsize=(10, 20))
for i in range(2, len(y) + 1):
    plt.subplot( 6, 2, i - 1)
    labels = y[:i]  # Adjusting labels for each step
    x_step = X[:i]
    dis=sch.linkage(x_step, metric='euclidean', method='ward')
    sch.dendrogram(dis, labels=labels, truncate_mode='lastp', p=i)
    plt.title(f'Step {i}')

plt.suptitle('Hierarchical Clustering Steps')
plt.tight_layout(rect=[0, 0.03, 1, 0.95])

plt.title('Hierarchical Clustering')
plt.xticks(rotation=30)
plt.savefig("images/hierarCluster-2.png", dpi=300)
#plt.show()
#+end_src
#+RESULTS:
#+CAPTION: Hierarchical Clustering
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/hierarCluster-2.png]]
***** 利用距離決定群數，或直接給定群數
建構好聚落樹狀圖後，我們可以依照距離的切割來進行分類，也可以直接給定想要分類的群數，讓系統自動切割到相對應的距離。
- 距離切割
  所給出的樹狀圖，y軸代表距離，我們可以用特徵之間的距離進行分群的切割。
#+begin_src python -r -n :async :results output :exports both :session hierarchy
max_dis=5
clusters=sch.fcluster(dis,max_dis,criterion='distance')
import matplotlib.pyplot as plt
plt.figure()
plt.scatter(X[:,0], X[:,1], c=clusters, cmap=plt.cm.Set1)
plt.savefig("images/clusterScatter.png", dpi=300)
#+end_src

#+RESULTS:

#+RESULTS:
#+CAPTION: Caption
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/clusterScatter.png]]
- 直接給定群數
  同時，我們也可以像sklearn一樣，直接給定我們所想要分出的群數。
#+begin_src python -r -n :async :results output :exports both :session hierarchy
k=4
clusters=sch.fcluster(dis,k,criterion='maxclust')

import matplotlib.pyplot as plt
plt.figure()
plt.scatter(X[:,0], X[:,1], c=clusters, cmap=plt.cm.Set1)
plt.savefig("images/clusterScatter-1.png", dpi=300)
#+end_src

#+RESULTS:
#+CAPTION: Caption
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/clusterScatter-1.png]]
**** 如何評估最佳分群數:K
- [[https://jimmy-huang.medium.com/kmeans%E5%88%86%E7%BE%A4%E6%BC%94%E7%AE%97%E6%B3%95-%E8%88%87-silhouette-%E8%BC%AA%E5%BB%93%E5%88%86%E6%9E%90-8be17e634589][Kmeans分群演算法 與 Silhouette 輪廓分析]]
- [[https://www.geeksforgeeks.org/implementing-agglomerative-clustering-using-sklearn/][Implementing Agglomerative Clustering using Sklearn]]
*** [課堂任務]聚合式階層分群 :TNFSH:
**** 資料
在此給定資料並以數值化座標平面表示，其中包含A、B、C、D、E、F、G及H共8個點。假設B與C點合併為G1；G與H點合併為G2，而G2加入F點後形成G3。
每個資料點有兩個特徵值(如圖[[fig:clusterTask1]])：
- x = np.array([1,2,3,2,5,5,6,7])
- y = np.array([4,2,2,6,5,0,1,2])
#+begin_src python -r -n :results output :exports none
import matplotlib.pyplot as plt
import matplotlib.transforms as transforms
from matplotlib.patches import Ellipse
import numpy as np

def get_ellipse_params(points):
    from sklearn.linear_model import LinearRegression
    ''' Calculate the parameters needed to graph an ellipse around a cluster of points in 2D.

    Calculate the height, width and angle of an ellipse to enclose the points in a cluster.
    Calculate the width by finding the maximum distance between the x-coordinates of points
    in the cluster, and the height by finding the maximum distance between the y-coordinates
    in the cluster. Multiple both by a scale factor to give padding around the points when
    constructing the ellipse. Calculate the angle by taking the inverse tangent of the
    gradient of the regression line. Note that tangent solutions repeat every 180 degrees,
    and so to ensure the correct solution has been found for plotting, add a correction
    factor of +/- 90 degrees if the magnitude of the angle exceeds 45 degrees.

    Args:
        points (ndarray): The points in a cluster to enclose with an ellipse, containing n
        ndarray elements representing each point, each with d elements
        representing the coordinates for the point.

    Returns:
        width (float):  The width of the ellipse.
        height (float): The height of the ellipse.
        angle (float):  The angle of the ellipse in degrees.
        '''
    mean = np.mean(points, axis=0)
    if points.ndim == 1:
        width, height, angle = 0.1, 0.1, 0
        return width, height, angle
    else:
        SCALE = 1.7
        width = max(np.amax(points[:,0]) - np.amin(points[:,0]), 0.1)
        height = max(np.amax(points[:,1]) - np.amin(points[:,1]), 0.1)

        # Calculate angle
        x_reg, y_reg = [[p[0]] for p in points], [[p[1]] for p in points]
        grad = LinearRegression().fit(x_reg, y_reg).coef_[0][0]
        angle = np.degrees(np.arctan(grad))

        # Account for multiple solutions of arctan
        if angle < -45: angle += 90
        elif angle > 45: angle -= 90

        return mean, width*SCALE, height*SCALE, angle

# 生成一些示例数据
x = np.array([1,2,3,2,5,5,6,7])
y = np.array([4,2,2,6,5,0,1,2])
labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']
# 绘制散点图
plt.scatter(x, y)
for i, label in enumerate(labels):
    plt.annotate(label, (x[i]+0.1, y[i]))

#G1
selected_points = np.array([[2, 2],[3,2]])
mean, width, height, angle = get_ellipse_params(selected_points)
ellipse = Ellipse(xy=mean, width=width, height=height, angle=angle, edgecolor='r', fc='None')
plt.gca().add_patch(ellipse)
plt.annotate('G1', (2.5, 2.3), color='r')

#G2
selected_points = np.array([[6, 1],[7,2]])
mean, width, height, angle = get_ellipse_params(selected_points)
ellipse = Ellipse(xy=mean, width=width, height=height, angle=angle, edgecolor='blue', fc='None')
plt.gca().add_patch(ellipse)
plt.annotate('G2', (5.4, 1.7), color='blue')

#G2
selected_points = np.array([[6, 1],[7,2],[5,0]])
mean, width, height, angle = get_ellipse_params(selected_points)
ellipse = Ellipse(xy=mean, width=width, height=height, angle=angle, edgecolor='green', fc='None')
plt.gca().add_patch(ellipse)
plt.annotate('G3', (4.5, 2.2), color='green')

#
# 调用函数绘制椭圆
#confidence_ellipse(sx, sy, plt.gca(), n_std=0.5, facecolor='none', edgecolor='red')

# 选择要框住的点
selected_points = np.array([[2, 2], [3, 2]])

# 绘制所选择的点
plt.scatter(selected_points[:, 0], selected_points[:, 1], color='red', marker='o')

# 设置图形标题和坐标轴标签
plt.title('Data')
plt.xlabel('X')
plt.ylabel('Y')
plt.grid()
plt.savefig("images/clusterTask1.png", dpi=300)
#+end_src
#+RESULTS:
#+CAPTION: 資料分佈圖
#+LABEL:fig:Labl
#+name: fig:clusterTask1
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/聚類(集群)/2024-02-14_15-43-38_2024-02-14_15-43-04.png]]
**** 任務1
請利用「單一連結」的群間距離計算方式完成聚合式階層式分群。
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/聚類(集群)/2024-02-14_15-45-04_2024-02-14_15-44-51.png]]
***** Step 1
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/聚類(集群)/2024-02-14_16-05-52_2024-02-14_16-05-35.png]]
|    | A | D | E | G1 | G3 |
|----+---+---+---+----+----|
| A  |   |   |   |    |    |
| D  |   |   |   |    |    |
| E  |   |   |   |    |    |
| G1 |   |   |   |    |    |
| G3 |   |   |   |    |    |
註: A可與D或與G1合併，在此選擇將A與G1合併為G4。
***** Step 2
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/聚類(集群)/2024-02-14_15-51-00_2024-02-14_15-50-44.png]]
|    | D | E | G3 | G4 |
|----+---+---+----+----|
| D  |   |   |    |    |
| E  |   |   |    |    |
| G3 |   |   |    |    |
| G4 |   |   |    |    |
***** Step 3
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/聚類(集群)/2024-02-14_15-53-20_2024-02-14_15-53-08.png]]
|    | E | G3 | G5 |
|----+---+----+----|
| E  |   |    |    |
| G3 |   |    |    |
| G5 |   |    |    |
***** Step 4
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/聚類(集群)/2024-02-14_15-54-50_2024-02-14_15-54-36.png]]
|    | E | G6 |
|----+---+----|
| E  |   |    |
| G6 |   |    |
**** 任務2
請利用「完整連結」的群間距離計算方式完成聚合式階層式分群。
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/聚類(集群)/2024-02-14_15-45-04_2024-02-14_15-44-51.png]]
***** Step 1
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/聚類(集群)/2024-02-14_16-09-10_2024-02-14_16-09-01.png]]
|    | A | D | E | G1 | G3 |
|----+---+---+---+----+----|
| A  |   |   |   |    |    |
| D  |   |   |   |    |    |
| E  |   |   |   |    |    |
| G1 |   |   |   |    |    |
| G3 |   |   |   |    |    |
***** Step 2
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/聚類(集群)/2024-02-14_16-14-30_2024-02-14_16-14-22.png]]
|    | E | G1 | G3 | G4 |
|----+---+----+----+----|
| E  |   |    |    |    |
| G1 |   |    |    |    |
| G3 |   |    |    |    |
| G4 |   |    |    |    |
註: G4可與E或與G1合併，在此選擇將G4與G1合併為G5。
***** Step 3
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/聚類(集群)/2024-02-14_16-15-28_2024-02-14_16-15-15.png]]
|    | E | G3 | G5 |
|----+---+----+----|
| E  |   |    |    |
| G3 |   |    |    |
| G5 |   |    |    |
***** Step 4
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/聚類(集群)/2024-02-14_16-16-33_2024-02-14_16-16-28.png]]
|    | G3 | G6 |
|----+---+----|
| G3 |   |    |
| G6 |   |    |
**** 任務3
請利用「平均連結」的群間距離計算方式完成聚合式階層式分群。
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/聚類(集群)/2024-02-14_15-45-04_2024-02-14_15-44-51.png]]
***** Step 1
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/聚類(集群)/2024-02-14_16-18-48_2024-02-14_16-18-39.png]]
|    |  A  |  D  |  E  | G1  | G3  |
|    | <c> | <c> | <c> | <c> | <c> |
|----+-----+-----+-----+-----+-----|
| A  |     |     |     |     |     |
| D  |     |     |     |     |     |
| E  |     |     |     |     |     |
| G1 |     |     |     |     |     |
| G3 |     |     |     |     |     |
***** Step 2
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/聚類(集群)/2024-02-14_16-22-19_2024-02-14_16-22-13.png]]
|    | E | G1 | G3 | G4 |
|----+---+---+----+----|
| E  |   |   |    |    |
| G1 |   |   |    |    |
| G3 |   |   |    |    |
| G4 |   |   |    |    |
***** Step 3
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/聚類(集群)/2024-02-14_16-24-31_2024-02-14_16-24-25.png]]
|    | E | G3 | G5 |
|----+---+----+----|
| E  |   |    |    |
| G3 |   |    |    |
| G5 |   |    |    |
***** Step 4
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/聚類(集群)/2024-02-14_16-25-47_2024-02-14_16-25-40.png]]
|    | G3 | G6 |
|----+----+----|
| G3 |    |    |
| G6 |    |    |
**** 任務4
請以「單一連結」完成之聚合式階層式分群結果，寫出各種不同分群數量時，各群所包含的資料內容。
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/聚類(集群)/2024-02-14_16-29-58_2024-02-14_16-29-33.png]]
**** solution :noexport:
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/聚類(集群)/2024-02-14_16-27-20_2024-02-14_16-27-08.png]]
*** TNFSH作業: 聚合式分群作業 :TNFSH:
電子商務網站黃色鬼屋近日收集了200位VIP客戶資料，想將這些客戶依其同質性進行分類。
**** 資料
#+CAPTION: 黃色鬼屋VIP資料
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 400
[[file:images/聚類(集群)/2024-02-14_15-28-32_2024-02-14_15-28-25.png]]
- 資料集URL: https://raw.githubusercontent.com/letranger/AI/gh-pages/Downloads/schopaholic.csv
- CID: 客戶編號
- Gd: 性別(Male/Female)
- Age: 年齡
- Income: 月收入(單位為萬元)
- ShopSco: 這是黃色鬼屋自訂的敗家分數，範圍由0~100
**** 任務
- 畫出200位VIP客戶的性別、年齡、月收入、敗家分數的分佈狀況，例如:
  #+ATTR_LATEX: :width 200
  #+ATTR_ORG: :width 200
  #+ATTR_HTML: :width 300
  [[file:images/hierarchTask2.png]]
- 利用聚合式分群的模型幫黃色鬼屋完成以下工作
  - 將階層圖畫出來，例如:
    #+ATTR_LATEX: :width 200
    #+ATTR_ORG: :width 200
    #+ATTR_HTML: :width 300
    file:images/hierarchTask4.png
  - 輸出分成5群的結果，例如:
    #+RESULTS:
    : 第1群客戶ID: 127 129 131 135 ...
    : 第2群客戶ID: 28 44 46 47 48 ...
    : 第3群客戶ID: 124 126 128 130 ...
    : 第4群客戶ID: 2 4 6 8 10 12 14 ...
    : 第5群客戶ID: 1 3 5 7 9 11 13 ...
**** solution :noexport:
#+begin_src python -r -n :results output :exports both
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import preprocessing, cluster
import plotly as py
import plotly.graph_objs as go

import warnings
warnings.filterwarnings('ignore')

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS'] # 步驟一（替換系統中的字型，這裡用的是Mac OSX系統）
plt.rcParams['axes.unicode_minus'] = False  # 步驟二（解決座標軸負數的負號顯示問題）

from sklearn import preprocessing
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering
df = pd.read_csv('https://raw.githubusercontent.com/letranger/AI/gh-pages/Downloads/schopaholic.csv')

plt.cla()
plt.figure(1 , figsize = (40 , 20))
n = 0
for x in ['Age' , 'Income' , 'ShopSco']:
    n += 1
    plt.subplot(1 , 3 , n)
    plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)
    sns.distplot(df[x] , bins = 15)
    plt.title('{}分佈'.format(x))
plt.savefig('images/hierarchTask2.png', dpi=300)

label_encoder = preprocessing.LabelEncoder()
df['Gd'] = label_encoder.fit_transform(df['Gd'])

plt.cla()
plt.clf()
plt.figure(1, figsize = (50 ,20))
dendrogram = sch.dendrogram(sch.linkage(df, method  = "ward"))

plt.tick_params(axis='x', labelsize=2)  # Reduce x-axis tick font size
plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=True, labelsize=1.9)
plt.title('Dendrogram')
plt.xlabel('客戶')
plt.ylabel('距離')
plt.savefig('images/hierarchTask4.png', dpi=300)

# 進一步輸出分群結果
hc = AgglomerativeClustering(n_clusters = 5, metric = 'euclidean', linkage ='average')
df['Cluster'] = hc.fit_predict(df[['Gd','Age','Income','ShopSco']])

for cluster_id in range(5):
    cluster_df = df[df['Cluster'] == cluster_id]
    cluster_cid = cluster_df['CID'].tolist()
    print(f'第{cluster_id+1}群客戶ID:', *cluster_cid)
#+end_src

#+RESULTS:
: 第1群客戶ID: 127 129 131 135 137 139 141 145 147 149 151 153 155 157 159 161 163 165 167 169 171 173 175 177 179 181 183 185 187 189 191 193 195 197 199
: 第2群客戶ID: 28 44 46 47 48 49 50 51 52 53 54 55 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 125 133 143
: 第3群客戶ID: 124 126 128 130 132 134 136 138 140 142 144 146 148 150 152 154 156 158 160 162 164 166 168 170 172 174 176 178 180 182 184 186 188 190 192 194 196 198 200
: 第4群客戶ID: 2 4 6 8 10 12 14 16 18 20 22 24 26 30 32 34 36 38 40 42
: 第5群客戶ID: 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 56
#+CAPTION: Caption
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/hierarchTask2.png]]

#+CAPTION: Caption
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/hierarchTask4.png]]
*** 分裂式階層分群法(Divisive Clustering)
如果採用分裂的方式，則由樹狀結構的頂端開始，將群聚逐次分裂。步驟：
1. 將所有資料先視為同一群，再依據群內的相異，分裂成兩群。
2. 接著，再從兩群中，找群內相異度最高的那群，再分裂一次，變成三群…，重複操作直到分出來的群數達到目標群數。
**** 分裂式階層分群實作
#+begin_src python -r -n :results output :exports none
# 隨機生成100個(x, y)
import numpy as np
X = np.array([[1, 2], [1, 4], [1, 0], [2, 1], [2, 3], [2, 4],
              [3, 1], [3, 3], [3, 4], [4, 2], [4, 4], [4, 0]])

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans


# Plot the dendrogram step by step
plt.figure(figsize=(18, 40))
plt.title('Agglomerative Clustering Dendrogram')
plt.xlabel('Sample index')
plt.ylabel('Distance')

for i in range(2, len(X) + 1):
    plt.subplot(6, 2, i - 1)
    kmeans = KMeans(n_clusters=i)
    kmeans.fit(X)
    cluster = kmeans.predict(X)
    plt.scatter(X[:,0], X[:,1], c=cluster, cmap=plt.cm.Set1, s=200)
    plt.grid()
    plt.title(f'Step {i}')
plt.tight_layout()
plt.savefig("images/topdown.png", dpi=300)
#+end_src
#+RESULTS:
#+CAPTION: Caption
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 600
[[file:images/topdown.png]]

** DBSCAN
DBSCAN will group together closely packed points, where close together is defined as a minimum number of points that must exist within a certain distance. If the point is within a certain distance of multiple clusters, it will be grouped with the cluster to which it is most densely located. Any instance that is not within this certain distance of another cluster is labeled an outlier.

In k-means and hierarchical clustering, all points had to be clustered, and outliers were poorly dealt with. In DBSCAN, we can explicitly label points as outliers and avoid having to cluster them. This is powerful. Compared to the other clustering algorithms, DBSCAN is much less prone to the distortion typically caused by outliers in the data. Also, like hierarchical clustering—and unlike k-means—we do not need to prespecify the number of clusters.

*** 實作
**** 讀入資料
#+begin_src python -r -n :async :results output :exports both :session KM-DBSCAN
'''Main'''
import numpy as np
import pandas as pd
import os, time, pickle, gzip
import datetime

'''Data Prep'''
from sklearn import preprocessing as pp

'''Data Viz'''
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
color = sns.color_palette()
# Load the datasets
current_path = os.getcwd()
file = os.path.sep.join(['', 'datasets', 'mnist.pkl.gz'])

f = gzip.open(current_path+file, 'rb')
train_set, validation_set, test_set = pickle.load(f, encoding='latin1')
f.close()

X_train, y_train = train_set[0], train_set[1]
X_validation, y_validation = validation_set[0], validation_set[1]
X_test, y_test = test_set[0], test_set[1]

# Create Pandas DataFrames from the datasets
train_index = range(0,len(X_train))
validation_index = range(len(X_train), len(X_train)+len(X_validation))
test_index = range(len(X_train)+len(X_validation), \
                   len(X_train)+len(X_validation)+len(X_test))

X_train = pd.DataFrame(data=X_train,index=train_index)
y_train = pd.Series(data=y_train,index=train_index)

X_validation = pd.DataFrame(data=X_validation,index=validation_index)
y_validation = pd.Series(data=y_validation,index=validation_index)

X_test = pd.DataFrame(data=X_test,index=test_index)
y_test = pd.Series(data=y_test,index=test_index)

#+end_src

#+RESULTS:
**** 降維
#+begin_src python -r -n :async :results output :exports both :session KM-DBSCAN
# Principal Component Analysis
from sklearn.decomposition import PCA

n_components = 784
whiten = False
random_state = 2018

pca = PCA(n_components=n_components, whiten=whiten, \
          random_state=random_state)

X_train_PCA = pca.fit_transform(X_train)
X_train_PCA = pd.DataFrame(data=X_train_PCA, index=train_index)

 # Log data
cwd = os.getcwd()
log_dir = cwd+"/datasets/"
y_train[0:2000].to_csv(log_dir+'labels.tsv', sep = '\t', index=False, header=False)

# Write dimensions to CSV
X_train_PCA.iloc[0:2000,0:3].to_csv(log_dir+'pca_data.tsv', sep = '\t', index=False, header=False)
#+end_src

#+RESULTS:
**** DBSCAN
#+begin_src python -r -n :async :results output :exports both :session KM-DBSCAN
# Perform DBSCAN
def analyzeCluster(clusterDF, labelsDF):
    countByCluster = pd.DataFrame(data=clusterDF['cluster'].value_counts())
    countByCluster.reset_index(inplace=True,drop=False)
    countByCluster.columns = ['cluster','clusterCount']

    preds = pd.concat([labelsDF,clusterDF], axis=1)
    preds.columns = ['trueLabel','cluster']

    countByLabel = pd.DataFrame(data=preds.groupby('trueLabel').count())

    countMostFreq = \
        pd.DataFrame(data=preds.groupby('cluster').agg( \
                        lambda x:x.value_counts().iloc[0]))
    countMostFreq.reset_index(inplace=True,drop=False)
    countMostFreq.columns = ['cluster','countMostFrequent']

    accuracyDF = countMostFreq.merge(countByCluster, \
                        left_on="cluster",right_on="cluster")
    overallAccuracy = accuracyDF.countMostFrequent.sum()/ \
                        accuracyDF.clusterCount.sum()

    accuracyByLabel = accuracyDF.countMostFrequent/ \
                        accuracyDF.clusterCount

    return countByCluster, countByLabel, countMostFreq, \
            accuracyDF, overallAccuracy, accuracyByLabel

from sklearn.cluster import DBSCAN

eps = 3
min_samples = 5
leaf_size = 30
n_jobs = 4

db = DBSCAN(eps=eps, min_samples=min_samples, leaf_size=leaf_size,
            n_jobs=n_jobs)

cutoff = 99
X_train_PCA_dbscanClustered = db.fit_predict(X_train_PCA.loc[:,0:cutoff])
X_train_PCA_dbscanClustered = \
    pd.DataFrame(data=X_train_PCA_dbscanClustered, index=X_train.index, \
                 columns=['cluster'])

countByCluster_dbscan, countByLabel_dbscan, countMostFreq_dbscan, \
    accuracyDF_dbscan, overallAccuracy_dbscan, accuracyByLabel_dbscan \
    = analyzeCluster(X_train_PCA_dbscanClustered, y_train)

overallAccuracy_dbscan
# Print overall accuracy
print("Overall accuracy from DBSCAN: ",overallAccuracy_dbscan)

# Show cluster results
print("Cluster results for DBSCAN")
countByCluster_dbscan

#+end_src

#+RESULTS:
: Overall accuracy from DBSCAN:  0.242
: Cluster results for DBSCAN

* 降維
:PROPERTIES:
:ID:       c729083a-c391-4283-9d50-0a91dd71da69
:END:

進行非監督式學習時，為了加速計算，最好能「在不損失過多資訊的前提下簡化資料」,降維(dimensionality reduction)就是其中一種手段。例如，汽車的里程數與車齡就有合併的依據。

- 本例以Colab為執行平台，透過資料的圖形化分佈觀察不同降維的效果。
- 於Colab執行時可以先將例中的savefig()註解掉

降維的主要目的在於壓縮資料，有以下幾種做法：
** 以主成份分析(PCA)對非監督式數據壓縮
「特徵選擇」需要原始的「特徵」；而「特徵提取」則是在於「轉換」數據，或是「投影」(project)數據到一個新的「特徵空間」，特徵提取不僅能改善儲存空間的使用或是提高學習演算法的計算效率，也可以有效地藉由降低「維數災難」來提高預測的正確性，特別是在處理非正規化模型時。
*** 主成分分析 1

「主成份分析」(principal component analysis, PCA)是一種非監督式線性變換技術」，經常應用於「特徵提取」與「降維」，其他應用包括「探索式數據分析」和「股票市場分析」中的雜訊消除、生物資訊學領域中的「基因數據分析」與「基因表現層分析」。

這邊先簡單說維度詛咒，預測/分類能力通常是隨著維度數(變數)增加而上生，但當模型樣本數沒有繼續增加的情況下，預測/分類能力增加到一定程度之後，預測/分類能力會隨著維度的繼續增加而減小[fn:4]。

主成份分析的基本假設是希望資料可以在特徵空間找到一個投影軸(向量)投影後可以得到這組資料的最大變異量。以圖[[fig:pca-1]]為例，PCA 的目的在於找到一個向量可以投影(圖中紅色的線)，讓投影後的資料變異量最大。

#+CAPTION: PCA-1 [fn:31]
#+LABEL:fig: pca-1
#+name: fig:pca-1
#+ATTR_LATEX: :width 500
#+ATTR_HTML: :width 600
[[file:images/pca-1.png]]

**** 投影(projection)

假設有一個點藍色的點對原點的向量為\(\vec{x_i}\)，有一個軸為 v，他的投影(正交為虛線和藍色線為 90 度)向量為紅色那條線，紅色線和黑色線的夾角為\(\theta\)，\(\vec{x_i}\)投影長度為藍色線，其長度公式為\(\left\|{x_i}\right\|cos\theta\)。

#+CAPTION: PCA-2 [fn:31]
#+LABEL:fig: pca-2
#+name: fig:pca-2
#+ATTR_HTML: :width 300
#+ATTR_LATEX: :width 200
[[file:images/pca-2.png]]

假設有一組資料六個點(\(x_1, x_2, x_3, x_4, x_5, x_6\))，有兩個投影向量\(\vec{v}\)和\(\vec{v'}\)(如圖[[fig:pca-3]])，投影下來後，資料在\(\vec{v'}\)上的變異量比\(v\)上的變異量小。

#+CAPTION: PCA-3 [fn:31]
#+LABEL:fig: pca-3
#+name: fig:pca-3
#+ATTR_HTML: :width 500
#+ATTR_LATEX: :width 500
[[file:images/pca-3.png]]

從圖[[fig:pca-4]]也可以看出這些資料在\(v\)向量資料投影后有較大的變異量(較之投影於\(\vec{v'}\))。

#+CAPTION: PCA-4 [fn:31]
#+LABEL:fig: pca-4
#+name: fig:pca-4
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 500
[[file:images/pca-4.png]]

**** 變異量的計算

典型的變異數公式如下：
$\sigma^2 = \frac{1}{N}\sum\limits_{i=1}^N (X -\mu)^2$

若要計算前述所有資料點(\(x_1, x_2, x_3, x_4, x_5, x_6\))在\(v\)上的投影\(v^Tx_1, v^Tx_2, v^Tx_3, v^Tx_4, v^Tx_5, v^Tx_6\) ，則其變異數公式為
$\sigma^2 = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i -\mu)^2$

又因 PCA 之前提假設是將資 shift 到 0(即，變異數的平均數為 0)以簡化運算，其公式會變為
$\sigma^2 = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i -\mu)^2 = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i - 0)^2 = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i)^2$

而機器學習處理的資料點通常為多變量，故上述式子會以矩陣方式呈現

$\Sigma = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i)(v^Tx_i)^T = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_iv^Tx_iv) = v^T(\frac{1}{N}\sum\limits_{i=1}^Nx_iX_i^T)v = v^TCv$

其中 C 為共變異數矩陣(covariance matrix)

$C=\frac{1}{n}\sum\limits_{i=1}^nx_ix_i^T,\cdots x_i = \begin{bmatrix}
x_1^{(1)}     \\
x_2^{(2)}     \\
\vdots  \\
x_i^{(d)}     \\
\end{bmatrix}$

主成份分析的目的則是在找出一個投影向量讓投影後的資料變異量最大化（最佳化問題）：

$v = \mathop{\arg\max}\limits_{x \in \mathcal{R}^d,\left\|v\right\|=1} {v^TCv}$

進一步轉成 Lagrange、透過偏微分求解，其實就是解 C 的特徵值(eigenvalue, \(\lambda\))和特徵向量(eigenvector, \(v\))。
*** 主成份分析 2

回到前述例子(身高和體重)，下左圖，經由 PCA 可以萃取出兩個特徵成分(投影軸，下圖右的兩條垂直的紅線，較長的紅線軸為變異量較大的主成份)。此範例算最大主成份的變異量為 13.26，第二大主成份的變異量為 1.23。

#+CAPTION: PCA-5 [fn:31]
#+LABEL:fig: pca-5
#+name: fig:pca-5
#+ATTR_LATEX: :width 500
#+ATTR_HTML: :width 500
[[file:images/pca-5.png]]

PCA 投影完的資料為下圖，從下圖可知，PC1 的變異足以表示此筆資料資訊。

#+CAPTION: PCA-6 [fn:31]
#+LABEL:fig: pca-6
#+name: fig:pca-6
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 500
[[file:images/pca-6.png]]

此做法可以有效的減少維度數，但整體變異量並沒有減少太多，此例從兩個變成只有一個，但變異量卻可以保留(13.26/(13.26+1.23)= 91.51%)，兩維度的資料做 PCA，對資料進行降維比較沒有感覺，但講解圖例比較容易。
*** 主成份分析的主要步驟

1. 標準化數據集
1. 建立共變數矩陣
1. 從共變數矩陣分解出特徵值與特徵向量
1. 以遞減方式對特徵值進行排序，以便對特徵向量排名

#+BEGIN_SRC python -r -n :results output :exports both
  import pandas as pd
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import StandardScaler
  import numpy as np
  import matplotlib.pyplot as plt
  from sklearn.decomposition import PCA

  # ## Extracting the principal components step-by-step

  df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/'
                        'machine-learning-databases/wine/wine.data',
                        header=None)

  df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                     'Alcalinity of ash', 'Magnesium', 'Total phenols',
                     'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                     'Color intensity', 'Hue',
                     'OD280/OD315 of diluted wines', 'Proline']

  print(df_wine.head())

  # Splitting the data into 70% training and 30% test subsets.

  X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values

  X_train, X_test, y_train, y_test = train_test_split(X, y,
                                     test_size=0.3,
                                     stratify=y, random_state=0)

  # 1. Standardizing the data.
  sc = StandardScaler()
  X_train_std = sc.fit_transform(X_train)
  X_test_std = sc.transform(X_test)

  # 2. Eigendecomposition of the covariance matrix.
  cov_mat = np.cov(X_train_std.T)
  eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

  print('\nEigenvalues \n%s' % eigen_vals)

  # ## Total and explained variance

  tot = sum(eigen_vals)
  var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]
  cum_var_exp = np.cumsum(var_exp)

  plt.bar(range(1, 14), var_exp, alpha=0.5, align='center',
          label='individual explained variance')
  plt.step(range(1, 14), cum_var_exp, where='mid',
           label='cumulative explained variance')
  plt.ylabel('Explained variance ratio')
  plt.xlabel('Principal component index')
  plt.legend(loc='best')
  plt.tight_layout()
  plt.savefig('05_02.png', dpi=300)
  #plt.show()

#+END_SRC

#+RESULTS:
#+begin_example
   Class label  Alcohol  ...  OD280/OD315 of diluted wines  Proline
0            1    14.23  ...                          3.92     1065
1            1    13.20  ...                          3.40     1050
2            1    13.16  ...                          3.17     1185
3            1    14.37  ...                          3.45     1480
4            1    13.24  ...                          2.93      735

[5 rows x 14 columns]

Eigenvalues
[4.84274532 2.41602459 1.54845825 0.96120438 0.84166161 0.6620634
 0.51828472 0.34650377 0.3131368  0.10754642 0.21357215 0.15362835
 0.1808613 ]
#+end_example

#+CAPTION: Principal component index
#+LABEL:fig: 05_02
#+name: fig:05_02
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 500
[[file:images/05_02.png]]

雖然上圖的「解釋變異數」圖有點類似隨機森林評估特徵值重要性的結果，但二者最大的不同處在於 PCA 為一種非監督式方法，也就是說，關於類別標籤資訊是被忽略的。
*** 特徵轉換

在分解「共變數矩陣」成為「特徵對」後，接下來要將資料集轉換為新的「主成份」，其步驟如下：
1. 選取\(k\)個最大特徵值所對應的 k 個特徵向量，其中\(k\)為新「特徵空間」的維數(\(k \le d\))。
1. 用最前面的\(k\)個特徵向量建立「投影矩陣」(project matrix)\(W\)。
1. 使用投影矩陣\(W\)，輸入值為\(d\)維數據集、輸出值為新的\(k\)維「特徵子空間」。

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  import pandas as pd
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import StandardScaler
  import numpy as np
  import matplotlib.pyplot as plt
  from sklearn.decomposition import PCA

  # ## Extracting the principal components step-by-step

  df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/'
                          'machine-learning-databases/wine/wine.data',
                          header=None)

  #  df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
  #                     'Alcalinity of ash', 'Magnesium', 'Total phenols',
  #                     'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
  #                     'Color intensity', 'Hue',
  #                     'OD280/OD315 of diluted wines', 'Proline']

      # Splitting the data into 70% training and 30% test subsets.
  X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values
  X_train, X_test, y_train, y_test = train_test_split(X, y,
                                      test_size=0.3,
                                      stratify=y, random_state=0)
  # 1. Standardizing the data.
  sc = StandardScaler()
  X_train_std = sc.fit_transform(X_train)
  X_test_std = sc.transform(X_test)
  # 2. Eigendecomposition of the covariance matrix.
  cov_mat = np.cov(X_train_std.T)
  eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)
  # ## Total and explained variance
  #tot = sum(eigen_vals)
  #var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]
  #cum_var_exp = np.cumsum(var_exp)
  # ## Feature transformation
  # Make a list of (eigenvalue, eigenvector) tuples
  eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])
                  for i in range(len(eigen_vals))]
  # Sort the (eigenvalue, eigenvector) tuples from high to low
  eigen_pairs.sort(key=lambda k: k[0], reverse=True)
  w = np.hstack((eigen_pairs[0][1][:, np.newaxis],
                  eigen_pairs[1][1][:, np.newaxis]))
  print('Matrix W:\n', w)
  print(X_train_std[0].dot(w)) (ref:x-train-dot)
  X_train_pca = X_train_std.dot(w) (ref:x-train-pca)
  # plot
  colors = ['r', 'b', 'g']
  markers = ['s', 'x', 'o']

  for l, c, m in zip(np.unique(y_train), colors, markers):
      plt.scatter(X_train_pca[y_train == l, 0],
                  X_train_pca[y_train == l, 1],
                  c=c, label=l, marker=m)

  plt.xlabel('PC 1')
  plt.ylabel('PC 2')
  plt.legend(loc='lower left')
  plt.tight_layout()
  plt.savefig('05_03.png', dpi=300)
  #plt.show()

#+END_SRC

#+RESULTS:
#+begin_example
Matrix W:
 [[-0.13724218  0.50303478]
 [ 0.24724326  0.16487119]
 [-0.02545159  0.24456476]
 [ 0.20694508 -0.11352904]
 [-0.15436582  0.28974518]
 [-0.39376952  0.05080104]
 [-0.41735106 -0.02287338]
 [ 0.30572896  0.09048885]
 [-0.30668347  0.00835233]
 [ 0.07554066  0.54977581]
 [-0.32613263 -0.20716433]
 [-0.36861022 -0.24902536]
 [-0.29669651  0.38022942]]
[2.38299011 0.45458499]
#+end_example

使用上述程式碼產生的 13*2 維的投影矩陣可以轉換一個樣本\(x\)(以\(1 \times 13\)維的列向量表示)到 PCA 子空間(\(x'\))(前兩個主成份)：\(x' = xW\)(程式碼第[[(x-train-dot)]]行)；同樣的，我們也可以將整個\(124 \times 13\)維的訓練數據集轉換到兩個主成份(\(124 \times 2\)維)(程式第[[(x-train-pca)]]行)，最後，將轉換過的\(124 \times 2\)維矩陣以二維散點圖表示：

#+CAPTION: 05_03
#+LABEL:fig:05_03
#+name: fig:05_03
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 500
[[file:images/05_03.png]]

由圖[[fig:05_03]]中可看出，與第二個主成份(y 軸)相比，數據沿著第一主成份(x 軸)的分散程度更嚴重，而由此圖也可判斷，該數據應可以一個「線性分類器」進行有效分類。
*** 以 Scikit-learn 進行主成份分析
#+BEGIN_SRC python -r -n :results output :exports both
  from matplotlib.colors import ListedColormap
  import pandas as pd
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import StandardScaler
  import numpy as np
  import matplotlib.pyplot as plt
  from sklearn.decomposition import PCA
  from sklearn.linear_model import LogisticRegression

  # ## Extracting the principal components step-by-step

  df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/'
                          'machine-learning-databases/wine/wine.data',
                          header=None)

  #  df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
  #                     'Alcalinity of ash', 'Magnesium', 'Total phenols',
  #                     'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
  #                     'Color intensity', 'Hue',
  #                     'OD280/OD315 of diluted wines', 'Proline']

      # Splitting the data into 70% training and 30% test subsets.
  X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values
  X_train, X_test, y_train, y_test = train_test_split(X, y,
                                      test_size=0.3,
                                      stratify=y, random_state=0)
  # 1. Standardizing the data.
  sc = StandardScaler()
  X_train_std = sc.fit_transform(X_train)
  X_test_std = sc.transform(X_test)

  def plot_decision_regions(X, y, classifier, resolution=0.02):
      # setup marker generator and color map
      markers = ('s', 'x', 'o', '^', 'v')
      colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
      cmap = ListedColormap(colors[:len(np.unique(y))])

      # plot the decision surface
      x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
      x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
      xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                             np.arange(x2_min, x2_max, resolution))
      Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
      Z = Z.reshape(xx1.shape)
      plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)
      plt.xlim(xx1.min(), xx1.max())
      plt.ylim(xx2.min(), xx2.max())

      # plot class samples
      for idx, cl in enumerate(np.unique(y)):
          plt.scatter(x=X[y == cl, 0],
                      y=X[y == cl, 1],
                      alpha=0.6,
                      c=cmap(idx),
                      edgecolor='black',
                      marker=markers[idx],
                      label=cl)

  # Training logistic regression classifier using the first 2 principal components.
  pca = PCA(n_components=2)
  X_train_pca = pca.fit_transform(X_train_std) (ref:pca-fit)
  X_test_pca = pca.transform(X_test_std)

  lr = LogisticRegression()
  lr = lr.fit(X_train_pca, y_train)

  plot_decision_regions(X_train_pca, y_train, classifier=lr)
  plt.xlabel('PC 1')
  plt.ylabel('PC 2')
  plt.legend(loc='lower left')
  plt.tight_layout()
  plt.savefig('05_04.png', dpi=300)
  #plt.show()
  plot_decision_regions(X_test_pca, y_test, classifier=lr)
  plt.xlabel('PC 1')
  plt.ylabel('PC 2')
  plt.legend(loc='lower left')
  plt.tight_layout()
  plt.savefig('05_05.png', dpi=300)
  #plt.show()
#+END_SRC



PCA 類別是 scikit-learn 中許多轉換類別之一，首先使用訓練數據集來 fit 模型並轉換數據集(程式第[[(pca-fit)]]行)，最後以 Logistic 迴歸對數據進行分類。圖[[fig:05_04]]為訓練集資料的分類結果，圖[[fig:05_05]]測為測試資料集分類結果，可以看出二者差異不大。

#+CAPTION: PCA 訓練數據
#+LABEL:fig:05_04
#+name: fig:05_04
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 500
[[file:images/05_04.png]]

#+CAPTION: PCA 測試數據
#+LABEL:fig:05_05
#+name: fig:05_05
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 500
[[file:images/05_05.png]]

** 利用線性判別分析(LDA)做監督式數據壓縮
LDA 的全稱是 Linear Discriminant Analysis（線性判別分析），是一種 supervised learning。因為是由 Fisher 在 1936 年提出的，所以也叫 Fisher's Linear Discriminant。「線性判別分析」(linear discriminant analysis, LDA)為一種用來做「特徵提取」的技術，藉由降維來處理「維數災難」，可提高非正規化模型的計算效率。PCA 在於找出一個在數據集中最大化變異數的正交成分軸； 而 LDA 則是要找出可以最佳化類別分離的特徵子空間。

從主觀的理解上，主成分分析到底是什麼？它其實是對數據在高維空間下的一個投影轉換，通過一定的投影規則將原來從一個角度看到的多個維度映射成較少的維度。到底什麼是映射，下面的圖就可以很好地解釋這個問題——正常角度看是兩個半橢圓形分佈的數據集，但經過旋轉（映射）之後是兩條線性分佈數據集。[fn:5]

#+ATTR_LATEX: :environment longtable :align |p{3.5cm}|p{3.5cm}|p{3.5cm}|p{3.5cm}|
|---------------------------+---------------------------+---------------------------+---------------------------|
| file:images/lda-rot-1.jpg | [[file:images/lda-rot-2.jpg]] | [[file:images/lda-rot-3.jpg]] | [[file:images/lda-rot-4.jpg]] |
|---------------------------+---------------------------+---------------------------+---------------------------|
|                         1 |                         2 |                         3 |                         4 |
|---------------------------+---------------------------+---------------------------+---------------------------|
| [[file:images/lda-rot-5.jpg]] | [[file:images/lda-rot-6.jpg]] | [[file:images/lda-rot-7.jpg]] | [[file:images/lda-rot-8.jpg]] |
|---------------------------+---------------------------+---------------------------+---------------------------|
|                         5 |                         6 |                         7 |                         8 |
|---------------------------+---------------------------+---------------------------+---------------------------|

LDA 與 PCA 都是常用的降維方法，二者的區別在於[fn:5]：
- 出發思想不同。PCA 主要是從特徵的協方差角度，去找到比較好的投影方式，即選擇樣本點投影具有最大方差的方向（ 在信號處理中認為信號具有較大的方差，噪聲有較小的方差，信噪比就是信號與噪聲的方差比，越大越好。）；而 LDA 則更多的是考慮了分類標籤信息，尋求投影后不同類別之間數據點距離更大化以及同一類別數據點距離最小化，即選擇分類性能最好的方向。
- 學習模式不同。PCA 屬於無監督式學習，因此大多場景下只作為數據處理過程的一部分，需要與其他算法結合使用，例如將 PCA 與聚類、判別分析、回歸分析等組合使用；LDA 是一種監督式學習方法，本身除了可以降維外，還可以進行預測應用，因此既可以組合其他模型一起使用，也可以獨立使用。
- 降維後可用維度數量不同。LDA 降維後最多可生成 C-1 維子空間（分類標籤數-1），因此 LDA 與原始維度 N 數量無關，只有數據標籤分類數量有關；而 PCA 最多有 n 維度可用，即最大可以選擇全部可用維度。

圖[[fig:pca-lda]]左側是 PCA 的降維思想，它所作的只是將整組數據整體映射到最方便表示這組數據的坐標軸上，映射時沒有利用任何數據內部的分類信息。因此，雖然 PCA 後的數據在表示上更加方便（降低了維數並能最大限度的保持原有信息），但在分類上也許會變得更加困難；圖[[fig:pca-lda]]右側是 LDA 的降維思想，可以看到 LDA 充分利用了數據的分類信息，將兩組數據映射到了另外一個坐標軸上，使得數據更易區分了（在低維上就可以區分，減少了運算量）。

#+CAPTION: PCA LDA 差異
#+LABEL:fig:pca-lda
#+name: fig:pca-lda
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/pca-lda.png]]

線性判別分析 LDA 算法由於其簡單有效性在多個領域都得到了廣泛地應用，是目前機器學習、數據挖掘領域經典且熱門的一個算法；但是算法本身仍然存在一些侷限性：
- 當樣本數量遠小於樣本的特徵維數，樣本與樣本之間的距離變大使得距離度量失效，使 LDA 算法中的類內、類間離散度矩陣奇異，不能得到最優的投影方向，在人臉識別領域中表現得尤為突出
- LDA 不適合對非高斯分佈的樣本進行降維
- LDA 在樣本分類信息依賴方差而不是均值時，效果不好
- LDA 可能過度擬合數據

** 降維實作 :noexport:
*** 讀入資料
#+begin_src python -r -n :async :results output :exports both :session DD
'''Main'''
import numpy as np
import pandas as pd
import os, time, pickle, gzip
import datetime

'''Data Prep'''
from sklearn import preprocessing as pp

'''Data Viz'''
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
color = sns.color_palette()
# Load the datasets
current_path = os.getcwd()
file = os.path.sep.join(['', 'Downloads', 'mnist.pkl.gz'])
# Directly download from internet / 僅供參考、未經測試
#r=requests.post(url)
#data=r.content
#df=pd.read_pickle(io.BytesIO(data))

f = gzip.open(current_path+file, 'rb')
train_set, validation_set, test_set = pickle.load(f, encoding='latin1')
f.close()

X_train, y_train = train_set[0], train_set[1]
X_validation, y_validation = validation_set[0], validation_set[1]
X_test, y_test = test_set[0], test_set[1]

# Create Pandas DataFrames from the datasets
train_index = range(0,len(X_train))
validation_index = range(len(X_train), len(X_train)+len(X_validation))
test_index = range(len(X_train)+len(X_validation), \
                   len(X_train)+len(X_validation)+len(X_test))

X_train = pd.DataFrame(data=X_train,index=train_index)
y_train = pd.Series(data=y_train,index=train_index)

X_validation = pd.DataFrame(data=X_validation,index=validation_index)
y_validation = pd.Series(data=y_validation,index=validation_index)

X_test = pd.DataFrame(data=X_test,index=test_index)
y_test = pd.Series(data=y_test,index=test_index)

def one_hot(series):
    label_binarizer = pp.LabelBinarizer()
    label_binarizer.fit(range(max(series)+1))
    return label_binarizer.transform(series)
# Define reversal of one-hot encoder function
def reverse_one_hot(originalSeries, newSeries):
    label_binarizer = pp.LabelBinarizer()
    label_binarizer.fit(range(max(originalSeries)+1))
    return label_binarizer.inverse_transform(newSeries)
# Create one-hot vectors for the labels
y_train_oneHot = one_hot(y_train)
y_validation_oneHot = one_hot(y_validation)
y_test_oneHot = one_hot(y_test)
#+end_src

#+RESULTS:
*** 主成分分析
**** PCA
PCA會找資料在低維度空間的表示方法，同時盡可能保留資料的變異性。
#+begin_src python -r -n :async :results output :exports both :session DD
# Principal Component Analysisva
from sklearn.decomposition import PCA

n_components = 784
whiten = False
random_state = 2018

pca = PCA(n_components=n_components, whiten=whiten, \
          random_state=random_state)

X_train_PCA = pca.fit_transform(X_train)
X_train_PCA = pd.DataFrame(data=X_train_PCA, index=train_index)
# Percentage of Variance Captured by 784 principal components
print("Variance Explained by all 784 principal components: ", \
      sum(pca.explained_variance_ratio_))
# Percentage of Variance Captured by X principal components
importanceOfPrincipalComponents = \
    pd.DataFrame(data=pca.explained_variance_ratio_)
importanceOfPrincipalComponents = importanceOfPrincipalComponents.T

print('Variance Captured by First 10 Principal Components: ',
      importanceOfPrincipalComponents.loc[:,0:9].sum(axis=1).values)
print('Variance Captured by First 20 Principal Components: ',
      importanceOfPrincipalComponents.loc[:,0:19].sum(axis=1).values)
print('Variance Captured by First 50 Principal Components: ',
      importanceOfPrincipalComponents.loc[:,0:49].sum(axis=1).values)
print('Variance Captured by First 100 Principal Components: ',
      importanceOfPrincipalComponents.loc[:,0:99].sum(axis=1).values)
print('Variance Captured by First 200 Principal Components: ',
      importanceOfPrincipalComponents.loc[:,0:199].sum(axis=1).values)
print('Variance Captured by First 300 Principal Components: ',
      importanceOfPrincipalComponents.loc[:,0:299].sum(axis=1).values)
#+end_src

#+RESULTS:
: Variance Explained by all 784 principal components:  0.999999953893564
: Variance Captured by First 10 Principal Components:  [0.4887603]
: Variance Captured by First 20 Principal Components:  [0.64397883]
: Variance Captured by First 50 Principal Components:  [0.8248605]
: Variance Captured by First 100 Principal Components:  [0.91465884]
: Variance Captured by First 200 Principal Components:  [0.9665012]
: Variance Captured by First 300 Principal Components:  [0.98624915]
由結果看，若將MNIST的原始784個特徵值縮減至300個，仍有近99%的解釋力，即，能捕捉到99%的變異量。PCA能讓我們縮減原始資料的維度，同時保持最多的顯著資訊。

***** 2個成分
如果只拿第1、第2個主成分特徵來進行預測，圖示結果如下：
#+begin_src python -r -n :async :results output :exports both :session DD
# Define scatterplot function
def scatterPlot(xDF, yDF, algoName):
    tempDF = pd.DataFrame(data=xDF.loc[:,0:1], index=xDF.index)
    tempDF = pd.concat((tempDF,yDF), axis=1, join="inner")
    tempDF.columns = ["First Vector", "Second Vector", "Label"]
    sns.lmplot(x="First Vector", y="Second Vector", hue="Label", data=tempDF, fit_reg=False)
    ax = plt.gca()
    ax.set_title("Separation of Observations using "+algoName)

# View scatterplot
scatterPlot(X_train_PCA, y_train, "PCA")
#plt.show()
plt.savefig('images/PCA-MNIST-1.png', dpi=300, bbox_inches='tight')
#+end_src

#+RESULTS:

#+CAPTION: PCA降維
#+LABEL:fig:PCA-MNIST-1
#+name: fig:PCA-MNIST-1
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/PCA-MNIST-1.png]]

由上圖可以看出PCA光找出最有價值的兩個特徵值就能對大致區分數0~9的不同類別，這在非監督式學習中是大分有用的。當資料集有數百萬個特徵、數十億筆資籵時，PCA可以大幅減少機器學習的訓練時間。

***** 3個成分 :noexport:
如果拿前3個主成分特徵來進行預測，圖示結果如下：
#+begin_src python -r -n :async :results output :exports both :session DD
# Define scatterplot function
import plotly.express as px

def scatter3dPlot(xDF, yDF, algoName):
    tempDF = pd.DataFrame(data=xDF.loc[:,0:2], index=xDF.index)
    tempDF = pd.concat((tempDF,yDF.astype(str)), axis=1, join="inner")
    tempDF.columns = ["First Vector", "Second Vector", "Third Vector", "Label"]
    fig = px.scatter_3d(tempDF, x='First Vector', y='Second Vector', z='Third Vector',
              color='Label',
                    opacity=0.6,
                    width=800,
                    height=800)
    fig.update_traces(marker_size = 4)
    plt.title("Separation of Observations using "+algoName)
    fig.show()

# View scatterplot
scatter3dPlot(X_train_PCA, y_train, "PCA")
#+end_src

#+RESULTS:

#+CAPTION: Caption
#+LABEL:fig:PCA-MNIST-1
#+name: fig:PCA-MNIST-1
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/PCA-MNIST-1a.png]]

由上圖可以看出PCA光找出最有價值的兩個特徵值就能對大致區分數0~9的不同類別，這在非監督式學習中是大分有用的。當資料集有數百萬個特徵、數十億筆資籵時，PCA可以大幅減少機器學習的訓練時間。
**** Incremental PCA
當資枓集大到無法載入記憶體時，可以小批次的遞增使用PCA，將資料集逐批送入記憶體，其結果與PCA相仿。
#+begin_src python -r -n :async :results output :exports both :session DD
# Incremental PCA
plt.cla()
from sklearn.decomposition import IncrementalPCA

n_components = 784
batch_size = None

incrementalPCA = IncrementalPCA(n_components=n_components, batch_size=batch_size)

X_train_incrementalPCA = incrementalPCA.fit_transform(X_train)
X_train_incrementalPCA = \
    pd.DataFrame(data=X_train_incrementalPCA, index=train_index)

X_validation_incrementalPCA = incrementalPCA.transform(X_validation)
X_validation_incrementalPCA = \
    pd.DataFrame(data=X_validation_incrementalPCA, index=validation_index)

scatterPlot(X_train_incrementalPCA, y_train, "Incremental PCA")
plt.savefig('images/PCA-MNIST-2.png', dpi=300, bbox_inches='tight')
#+end_src

#+RESULTS:

#+CAPTION: Incremental PCA
#+LABEL:fig:PCA-MNIST-2
#+name: fig:PCA-MNIST-2
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/PCA-MNIST-2.png]]
**** Sparse PCA
一般的PCA希望儘量縮小特徵空間，提高空間中資枓點的密度。但有些機器學習可能需要讓資料點的密度更稀疏，此時可使用Sparse PCA，其稀疏程度由aplha控制。
- 計算速度會較慢，故只取10000個樣本訓練
#+begin_src python -r -n :async :results output :exports both :session DD
# Sparse PCA
plt.cla()
from sklearn.decomposition import SparsePCA

n_components = 100
alpha = 0.0001
random_state = 2018
n_jobs = -1

sparsePCA = SparsePCA(n_components=n_components, \
                alpha=alpha, random_state=random_state, n_jobs=n_jobs)

sparsePCA.fit(X_train.loc[:10000,:])
X_train_sparsePCA = sparsePCA.transform(X_train)
X_train_sparsePCA = pd.DataFrame(data=X_train_sparsePCA, index=train_index)

X_validation_sparsePCA = sparsePCA.transform(X_validation)
X_validation_sparsePCA = \
    pd.DataFrame(data=X_validation_sparsePCA, index=validation_index)

scatterPlot(X_train_sparsePCA, y_train, "Sparse PCA")
plt.savefig('images/PCA-MNIST-3.png', dpi=300, bbox_inches='tight')

#+end_src

#+RESULTS:

#+CAPTION: Sparse PCA
#+LABEL:fig:PCA-MNIST-3
#+name: fig:PCA-MNIST-3
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/PCA-MNIST-3.png]]
**** Kernel PCA
非線性投影PCA，透過學習相似度函數(kernel function)，kernel PCA找出大多數資枓點聚集的隱含特徵空間，使用kernel PCA需要設定預期的成分數量、kernel的型態、kernel的係數(gamma)，常見的kernel PCA有radial basis function kernel、RBF kernel。
#+begin_src python -r -n :async :results output :exports both :session DD
# Kernel PCA
plt.cla()
from sklearn.decomposition import KernelPCA

n_components = 100
kernel = 'rbf'
gamma = None
random_state = 2018
n_jobs = 1

kernelPCA = KernelPCA(n_components=n_components, kernel=kernel, \
                      gamma=gamma, n_jobs=n_jobs, random_state=random_state)

kernelPCA.fit(X_train.loc[:10000,:])
X_train_kernelPCA = kernelPCA.transform(X_train)
X_train_kernelPCA = pd.DataFrame(data=X_train_kernelPCA,index=train_index)

X_validation_kernelPCA = kernelPCA.transform(X_validation)
X_validation_kernelPCA = \
    pd.DataFrame(data=X_validation_kernelPCA, index=validation_index)

scatterPlot(X_train_kernelPCA, y_train, "Kernel PCA")
plt.savefig('images/PCA-MNIST-4.png', dpi=300, bbox_inches='tight')
#+end_src

#+RESULTS:

#+CAPTION: Kernel PCA
#+LABEL:fig:PCA-MNIST-4
#+name: fig:PCA-MNIST-4
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/PCA-MNIST-4.png]]
*** 奇異值分解(Singular Value Decomposition，SVD)
SVD是在機器學習領域廣泛應用的演算法，它不光可以用於降維演算法中的特徵分解，還可以用於推薦系統，以及自然語言處理等領域，目的在減少原始特徵值矩陣的秩，目前幾乎所有封裝好的PCA算法內部採用的都是SVD算法進行特徵值、特徵向量以及K值的求解。

SVD是一種線性代數的技術，它將一個矩陣分解為三個矩陣的乘積，包括一個左奇異向量矩陣、一個對角奇異值矩陣和一個右奇異向量矩陣。
#+begin_src python -r -n :async :results output :exports both :session DD
# Singular Value Decomposition
plt.cla()
from sklearn.decomposition import TruncatedSVD

n_components = 200
algorithm = 'randomized'
n_iter = 5
random_state = 2018

svd = TruncatedSVD(n_components=n_components, algorithm=algorithm, \
                   n_iter=n_iter, random_state=random_state)

X_train_svd = svd.fit_transform(X_train)
X_train_svd = pd.DataFrame(data=X_train_svd, index=train_index)

X_validation_svd = svd.transform(X_validation)
X_validation_svd = pd.DataFrame(data=X_validation_svd, index=validation_index)

scatterPlot(X_train_svd, y_train, "Singular Value Decomposition")
plt.savefig('images/SVD-MNIST.png', dpi=300, bbox_inches='tight')
#+end_src

#+RESULTS:

#+CAPTION: Caption
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/SVD-MNIST.png]]
*** 字典學習
不依賴幾何指標或距離指標，當資料量很大時，嚴格分析每個樣本就會消耗大量時間，MiniBatch方過降低計算精度來換取執行時間，但仍能藉由龐大的資料量來取得合理的效能。
#+begin_src python -r -n :async :results output :exports both :session DD
# Mini-batch dictionary learning
plt.cla()
from sklearn.decomposition import MiniBatchDictionaryLearning

n_components = 50
alpha = 1
batch_size = 200
max_iter = 1000
random_state = 2018

miniBatchDictLearning = MiniBatchDictionaryLearning( \
                        n_components=n_components, alpha=alpha, \
                        batch_size=batch_size, max_iter = max_iter, \
                        random_state=random_state)

miniBatchDictLearning.fit(X_train.loc[:,:10000])
X_train_miniBatchDictLearning = miniBatchDictLearning.fit_transform(X_train)
X_train_miniBatchDictLearning = pd.DataFrame( \
    data=X_train_miniBatchDictLearning, index=train_index)

X_validation_miniBatchDictLearning = \
    miniBatchDictLearning.transform(X_validation)
X_validation_miniBatchDictLearning = \
    pd.DataFrame(data=X_validation_miniBatchDictLearning, \
    index=validation_index)

scatterPlot(X_train_miniBatchDictLearning, y_train, \
            "Mini-batch Dictionary Learning")
plt.savefig('images/DIC-MNIST.png', dpi=300, bbox_inches='tight')
#+end_src

#+RESULTS:
: /usr/local/lib/python3.11/site-packages/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
:   new_code = orthogonal_mp_gram(
: /usr/local/lib/python3.11/site-packages/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
:   new_code = orthogonal_mp_gram(

#+CAPTION: 字典學習
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/DIC-MNIST.png]]
*** 獨立成份分析
Independent component analysis
#+begin_src python -r -n :async :results output :exports both :session DD
# Independent Component Analysis
plt.cla()
from sklearn.decomposition import FastICA

n_components = 25
algorithm = 'parallel'
whiten='arbitrary-variance'
max_iter = 1000
random_state = 2018

fastICA = FastICA(n_components=n_components, algorithm=algorithm, \
                  whiten=whiten, max_iter=max_iter, random_state=random_state)

X_train_fastICA = fastICA.fit_transform(X_train)
X_train_fastICA = pd.DataFrame(data=X_train_fastICA, index=train_index)

X_validation_fastICA = fastICA.transform(X_validation)
X_validation_fastICA = pd.DataFrame(data=X_validation_fastICA, \
                                    index=validation_index)

scatterPlot(X_train_fastICA, y_train, "Independent Component Analysis")
plt.savefig('images/ICA-MNIST.png', dpi=300, bbox_inches='tight')
#+end_src

#+RESULTS:
: /usr/local/lib/python3.11/site-packages/sklearn/decomposition/_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
:   warnings.warn(

#+CAPTION: 獨立成份分析
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/ICA-MNIST.png]]
*** Isomap
非線性投影，基本的流形學習方法為isometric mapping，簡稱isomap。Isomap透過計算點與點間的成對距離（曲線距離或捷線距離，而非歐幾里德距離）來學習能代表原始特徵集的一個新低維度embedding。
#+begin_src python -r -n :async :results output :exports both :session DD
# Isomap
plt.cla()
from sklearn.manifold import Isomap

n_neighbors = 5
n_components = 10
n_jobs = 4

isomap = Isomap(n_neighbors=n_neighbors, \
                n_components=n_components, n_jobs=n_jobs)

isomap.fit(X_train.loc[0:5000,:])
X_train_isomap = isomap.transform(X_train)
X_train_isomap = pd.DataFrame(data=X_train_isomap, index=train_index)

X_validation_isomap = isomap.transform(X_validation)
X_validation_isomap = pd.DataFrame(data=X_validation_isomap, \
                                   index=validation_index)

scatterPlot(X_train_isomap, y_train, "Isomap")
plt.savefig('images/ISOMAP-MNIST.png', dpi=300, bbox_inches='tight')
#+end_src

#+RESULTS:
#+CAPTION: Isomap
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/ISOMAP-MNIST.png]]
*** 局部線性嵌入法(Locally Linear Embedding)
LLE透過以下方式來找出高維資枓中的非線性結構
- 分割資料成為較小的子集（包含數個點的鄰近區域）
- 將每個子集塑模成一個線性的embedding
#+begin_src python -r -n :async :results output :exports both :session DD
# Locally Linear Embedding (LLE)
from sklearn.manifold import LocallyLinearEmbedding
import matplotlib.pyplot as plt
plt.cla()
n_neighbors = 10
n_components = 2
method = 'modified'
n_jobs = 4
random_state = 2018

lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, \
        n_components=n_components, method=method, \
        random_state=random_state, n_jobs=n_jobs)

lle.fit(X_train.loc[0:5000,:])
X_train_lle = lle.transform(X_train)
X_train_lle = pd.DataFrame(data=X_train_lle, index=train_index)

X_validation_lle = lle.transform(X_validation)
X_validation_lle = pd.DataFrame(data=X_validation_lle, index=validation_index)

scatterPlot(X_train_lle, y_train, "Locally Linear Embedding")
plt.savefig('images/LLE-MNIST.png', dpi=300, bbox_inches='tight')
#+end_src

#+RESULTS:

#+CAPTION: 局部線性嵌入法
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/LLE-MNIST.png]]
*** t-Distributed Stochastic Neighbor Embedding
t-SNE建立兩個機率分佈來將高維資料點塑模至二維或三維空間，並使在此空間中彼此相似的點靠近、不相似的點疏遠。
#+begin_src python -r -n :async :results output :exports both :session DD
# t-SNE
plt.cla()
from sklearn.manifold import TSNE

n_components = 2
learning_rate = 300
perplexity = 30
early_exaggeration = 12
init = 'random'
random_state = 2018

tSNE = TSNE(n_components=n_components, learning_rate=learning_rate, \
            perplexity=perplexity, early_exaggeration=early_exaggeration, \
            init=init, random_state=random_state)

X_train_tSNE = tSNE.fit_transform(X_train_PCA.loc[:5000,:9])
X_train_tSNE = pd.DataFrame(data=X_train_tSNE, index=train_index[:5001])

scatterPlot(X_train_tSNE, y_train, "t-SNE")
plt.savefig('images/tSNE-MNIST.png', dpi=300, bbox_inches='tight')
#+end_src

#+RESULTS:

#+CAPTION: t-Distributed Stochastic Neighbor Embedding
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/tSNE-MNIST.png]]
*** 隨機投影
#+CAPTION: 將資料空間由三維壓縮至二維
#+LABEL:fig:Labl
#+name: fig:randomProjection
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/降維/2024-02-12_14-16-20_2024-02-12_14-16-10.png]]

也是線性投影，將高維度空間裡的點嵌到較低維度的空間中，但仍維持點與點間的距離，如圖[[fig:randomProjection]][fn:6]。有兩種版本：
- 高斯隨機投影
- 稀疏隨機投影
**** 高斯隨機投影(Gaussian Random Projection)
可以指定在縮減的特徵空間中想要擁有的元素數量(eps值), eps控制了嵌入的品質，其值越高、維度數量也越高。
- 實驗結果：改eps後看不出來圖的差異....
***** pes=0.4
#+begin_src python -r -n :async :results output :exports both :session DD
# Gaussian Random Projection
plt.cla()
from sklearn.random_projection import GaussianRandomProjection

n_components = 'auto'
eps = 0.4
random_state = 2018

GRP = GaussianRandomProjection(n_components=n_components, eps=eps, \
                               random_state=random_state)

X_train_GRP = GRP.fit_transform(X_train)
X_train_GRP = pd.DataFrame(data=X_train_GRP, index=train_index)

X_validation_GRP = GRP.transform(X_validation)
X_validation_GRP = pd.DataFrame(data=X_validation_GRP, index=validation_index)

scatterPlot(X_train_GRP, y_train, "Gaussian Random Projection")
plt.savefig('images/GRP-MNIST-1.png', dpi=300, bbox_inches='tight')
#+end_src

#+RESULTS:
#+CAPTION: 高斯隨機投影(pes=0.4)
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/GRP-MNIST-1.png]]
***** eps=0.8
#+begin_src python -r -n :async :results output :exports both :session DD
# Gaussian Random Projection
plt.cla()
from sklearn.random_projection import GaussianRandomProjection

n_components = 'auto'
eps = 0.8
random_state = 2018

GRP = GaussianRandomProjection(n_components=n_components, eps=eps, \
                               random_state=random_state)

X_train_GRP = GRP.fit_transform(X_train)
X_train_GRP = pd.DataFrame(data=X_train_GRP, index=train_index)

X_validation_GRP = GRP.transform(X_validation)
X_validation_GRP = pd.DataFrame(data=X_validation_GRP, index=validation_index)

scatterPlot(X_train_GRP, y_train, "Gaussian Random Projection")
plt.savefig('images/GRP-MNIST-2.png', dpi=300)
#+end_src

#+RESULTS:
#+CAPTION: 高斯隨機投影(pes=0.8)
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/GRP-MNIST-2.png]]
**** 稀疏矩陣投影
在轉換過程中保留了一定程度的資料點稀疏度，也較有效率
#+begin_src python -r -n :async :results output :exports both :session DD
# Sparse Random Projection
plt.cla()
from sklearn.random_projection import SparseRandomProjection

n_components = 'auto'
density = 'auto'
eps = 0.5
dense_output = False
random_state = 2018

SRP = SparseRandomProjection(n_components=n_components, \
        density=density, eps=eps, dense_output=dense_output, \
        random_state=random_state)

X_train_SRP = SRP.fit_transform(X_train)
X_train_SRP = pd.DataFrame(data=X_train_SRP, index=train_index)

X_validation_SRP = SRP.transform(X_validation)
X_validation_SRP = pd.DataFrame(data=X_validation_SRP, index=validation_index)

scatterPlot(X_train_SRP, y_train, "Sparse Random Projection")
plt.savefig('images/SRP-MNIST.png', dpi=300)
#+end_src

#+RESULTS:
:   fig = plt.figure(figsize=figsize)

#+CAPTION: 稀疏矩陣投影
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/SRP-MNIST.png]]
*** 多維標度(Multidimensional Scaling) :noexport:
MDS，學習原始資料集點與點間的相似度，將相似度塑模至低維度空間
#+begin_src python -r -n :async :results output :exports both :session DD
# Multidimensional Scaling
plt.cla()
from sklearn.manifold import MDS

n_components = 2
n_init = 12
max_iter = 1200
metric = True
n_jobs = 4
random_state = 2018

mds = MDS(n_components=n_components, n_init=n_init, max_iter=max_iter, \
          metric=metric, n_jobs=n_jobs, random_state=random_state)

X_train_mds = mds.fit_transform(X_train.loc[0:1000,:])
X_train_mds = pd.DataFrame(data=X_train_mds, index=train_index[0:1001])

scatterPlot(X_train_mds, y_train, "Multidimensional Scaling")
plt.savefig('images/MDS-MNIST.png', dpi=300, bbox_inches='tight')
#+end_src

#+RESULTS:
: /usr/local/lib/python3.11/site-packages/sklearn/manifold/_mds.py:298: FutureWarning:
:
: The default value of `normalized_stress` will change to `'auto'` in version 1.4. To suppress this warning, manually set the value of `normalized_stress`.

#+CAPTION: Multidimensional Scaling
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/MDS-MNIST.png]]

* 異常偵測 :noexport:
在現實的狀況下，詐欺的樣式會隨時間改變，如果只依賴訓練集的label來判斷，時間一久效能就會下降。故需要非監督式學習的詐欺偵測系統來協助。
** 準備資料
共有284807筆信用卡交易、其中有492筆詐欺交易(class=1)
#+begin_src python -r -n :async :results output :exports both :session AD
# Import libraries
'''Main'''
import numpy as np
import pandas as pd
import os, time
import pickle, gzip

'''Data Viz'''
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
import matplotlib as mpl

'''Data Prep and Model Evaluation'''
from sklearn import preprocessing as pp
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_curve, average_precision_score
from sklearn.metrics import roc_curve, auc, roc_auc_score

import numpy as np
import pandas as pd
data = pd.read_csv("https://media.githubusercontent.com/media/francis-kang/handson-unsupervised-learning/master/datasets/credit_card_data/credit_card.csv")
#data = pd.read_csv("datasets/credit_card.csv")
# Count total fraud
print("Number of fraudulent transactions:", data['Class'].sum())

# Suppress warnings
pd.set_option('mode.chained_assignment', None)

# Split to train and test and scale features
dataX = data.drop(['Class'],axis=1)
dataY = data.loc[:,'Class'].copy()

X_train, X_test, y_train, y_test = \
    train_test_split(dataX, dataY, test_size=0.33, \
                    random_state=2018, stratify=dataY)

featuresToScale = X_train.columns
sX = pp.StandardScaler(copy=True)
X_train.loc[:,featuresToScale] = sX.fit_transform(X_train.loc[:,featuresToScale])
X_test.loc[:,featuresToScale] = sX.transform(X_test.loc[:,featuresToScale])
#+end_src

#+RESULTS:
: Number of fraudulent transactions: 492

** 定義異常評分函數
降維演算法在縮減維度時，會試圖將重建誤差最小化；對於信用卡交易資料來說，那些難以被塑模的交易會產生最大的重建誤差。
#+begin_src python -r -n :async :results output :exports both :session AD
# Calculate reconstruction error
def anomalyScores(originalDF, reducedDF):
    loss = np.sum((np.array(originalDF)-np.array(reducedDF))**2, axis=1)
    loss = pd.Series(data=loss,index=originalDF.index)
    loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))
    return loss
#+end_src

#+RESULTS:

** 評估指標：畫圖
使用precision-recall曲線、average precision和auROC做為評估指標。
#+begin_src python -r -n :async :results output :exports both :session AD
# Plot results
def setPlot():
    import matplotlib.pyplot as plt
    from matplotlib import rcParams
    rcParams.update({'figure.autolayout': True})
    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
    plt.rcParams['axes.unicode_minus'] = False

def plotResults(trueLabels, anomalyScores, returnPreds = False, imgName=''):
    plt.cla()
    setPlot()
    preds = pd.concat([trueLabels, anomalyScores], axis=1)
    preds.columns = ['trueLabel', 'anomalyScore']
    precision, recall, thresholds = \
        precision_recall_curve(preds['trueLabel'],preds['anomalyScore'])
    average_precision = \
        average_precision_score(preds['trueLabel'],preds['anomalyScore'])

    plt.step(recall, precision, color='k', alpha=0.7, where='post')
    plt.fill_between(recall, precision, step='post', alpha=0.3, color='k')

    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.ylim([0.0, 1.05])
    plt.xlim([0.0, 1.0])

    plt.title('Precision-Recall curve: 平均精確率:{0:0.2f}'.format(average_precision))

    plt.savefig('images/'+imgName+'-1.png', dpi=300, bbox_inches='tight')

    fpr, tpr, thresholds = roc_curve(preds['trueLabel'], \
                                     preds['anomalyScore'])
    areaUnderROC = auc(fpr, tpr)
    plt.cla()
    setPlot()
    plt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')
    plt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic: 曲線以下面積:{0:0.2f}'.format(areaUnderROC))
    plt.legend(loc="lower right")
    plt.savefig('images/'+imgName+'-2.png', dpi=300, bbox_inches='tight')
    if returnPreds==True:
        return preds


# View scatterplot
def scatterPlot(xDF, yDF, algoName, imgName=''):
    plt.cla()
    setPlot()
    tempDF = pd.DataFrame(data=xDF.loc[:,0:1], index=xDF.index)
    tempDF = pd.concat((tempDF,yDF), axis=1, join="inner")
    tempDF.columns = ["First Vector", "Second Vector", "Label"]
    sns.lmplot(x="First Vector", y="Second Vector", hue="Label", \
               data=tempDF, fit_reg=False)
    ax = plt.gca()
    ax.set_title("演算法:"+algoName)
    plt.savefig('images/'+imgName+'.png', dpi=300, bbox_inches='tight')
#+end_src

#+RESULTS:

** PCA異常偵測
使用PCA模型來重建信用卡交易、計算重交的交易與原始交易的差異，那些PCA重建的較差的交易就是異常(可能為詐欺)。對於PCA來說，保留越多主成分、越有助於PCA學習到原始交易的資料結構，但若保留太多主成分，PCA可能太容易重建原始交易，反而讓所有的重建誤差都變小。
#+begin_src python -r -n :async :results output :exports both :session AD
# 30 principal components
from sklearn.decomposition import PCA

n_components = 30 #保留30o固主成分
whiten = False
random_state = 2018

pca = PCA(n_components=n_components, whiten=whiten, \
          random_state=random_state)

X_train_PCA = pca.fit_transform(X_train)
X_train_PCA = pd.DataFrame(data=X_train_PCA, index=X_train.index)

X_train_PCA_inverse = pca.inverse_transform(X_train_PCA)
X_train_PCA_inverse = pd.DataFrame(data=X_train_PCA_inverse, index=X_train.index)

scatterPlot(X_train_PCA, y_train, 'AD-PCA', 'AD-PCA')
anomalyScoresPCA = anomalyScores(X_train, X_train_PCA_inverse)
preds = plotResults(y_train, anomalyScoresPCA, True, 'AD-PCA')

#+end_src

#+RESULTS:
#+CAPTION: PCA異常偵測/30
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-PCA.png]]

#+CAPTION: PCA異常偵測/30
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-PCA-1.png]]

#+CAPTION: PCA異常偵測/30
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-PCA-2.png]]

平均精確率不到1%，太差，必須不斷實驗找出最佳的PCA成份(http://bit.ly/2Gd4v7e)
*** 最後找出27個
#+begin_src python -r -n :async :results output :exports both :session AD
# 27 principal components
from sklearn.decomposition import PCA

n_components = 27
whiten = False
random_state = 2018

pca = PCA(n_components=n_components, whiten=whiten, random_state=random_state)

X_train_PCA = pca.fit_transform(X_train)
X_train_PCA = pd.DataFrame(data=X_train_PCA, index=X_train.index)

X_train_PCA_inverse = pca.inverse_transform(X_train_PCA)
X_train_PCA_inverse = pd.DataFrame(data=X_train_PCA_inverse, index=X_train.index)

scatterPlot(X_train_PCA, y_train, 'AD-PCA', 'AD-PCA1')
# View plot
anomalyScoresPCA = anomalyScores(X_train, X_train_PCA_inverse)
preds = plotResults(y_train, anomalyScoresPCA, True, 'AD-PCA1')
#+end_src
#+RESULTS:
#+CAPTION: PCA異常偵測/27
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-PCA1.png]]

#+CAPTION: PCA異常偵測/27
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-PCA1-1.png]]
#+CAPTION: PCA異常偵測/27
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-PCA1-2.png]]
*** 分析結果
#+begin_src python -r -n :async :results output :exports both :session AD
# Analyze results
preds.sort_values(by="anomalyScore",ascending=False,inplace=True)
cutoff = 350
predsTop = preds[:cutoff]
print("Precision: ",np.round(predsTop. \
            anomalyScore[predsTop.trueLabel==1].count()/cutoff,2))
print("Recall: ",np.round(predsTop. \
            anomalyScore[predsTop.trueLabel==1].count()/y_train.sum(),2))
print("Fraud Caught out of 330 Cases:", predsTop.trueLabel.sum())
#+end_src

#+RESULTS:
: Precision:  0.75
: Recall:  0.79
: Fraud Caught out of 330 Cases: 261
*** <完整> :noexport:
#+begin_src python -r -n :async :results output :exports both
# Import libraries

'''Main'''
import numpy as np
import pandas as pd
import os, time
import pickle, gzip

'''Data Viz'''
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
import matplotlib as mpl

'''Data Prep and Model Evaluation'''
from sklearn import preprocessing as pp
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_curve, average_precision_score
from sklearn.metrics import roc_curve, auc, roc_auc_score

import numpy as np
import pandas as pd
data = pd.read_csv("./credit_card.csv")
# Count total fraud
print("Number of fraudulent transactions:", data['Class'].sum())

# Suppress warnings
pd.set_option('mode.chained_assignment', None)

# Split to train and test and scale features
dataX = data.drop(['Class'],axis=1)
dataY = data.loc[:,'Class'].copy()

X_train, X_test, y_train, y_test = \
    train_test_split(dataX, dataY, test_size=0.33, \
                    random_state=2018, stratify=dataY)

featuresToScale = X_train.columns
sX = pp.StandardScaler(copy=True)
X_train.loc[:,featuresToScale] = sX.fit_transform(X_train.loc[:,featuresToScale])
X_test.loc[:,featuresToScale] = sX.transform(X_test.loc[:,featuresToScale])


def anomalyScores(originalDF, reducedDF):
    loss = np.sum((np.array(originalDF)-np.array(reducedDF))**2, axis=1)
    loss = pd.Series(data=loss,index=originalDF.index)
    loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))
    return loss

# Plot results
def setPlot():
    import matplotlib.pyplot as plt
    from matplotlib import rcParams
    rcParams.update({'figure.autolayout': True})
    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
    plt.rcParams['axes.unicode_minus'] = False

def plotResults(trueLabels, anomalyScores, returnPreds = False, imgName=''):
    plt.cla()
    setPlot()
    preds = pd.concat([trueLabels, anomalyScores], axis=1)
    preds.columns = ['trueLabel', 'anomalyScore']
    precision, recall, thresholds = \
        precision_recall_curve(preds['trueLabel'],preds['anomalyScore'])
    average_precision = \
        average_precision_score(preds['trueLabel'],preds['anomalyScore'])

    plt.step(recall, precision, color='k', alpha=0.7, where='post')
    plt.fill_between(recall, precision, step='post', alpha=0.3, color='k')

    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.ylim([0.0, 1.05])
    plt.xlim([0.0, 1.0])

    plt.title('Precision-Recall curve: 平均精確率:{0:0.2f}'.format(average_precision))

    plt.savefig('images/'+imgName+'-1.png', dpi=300, bbox_inches='tight')

    fpr, tpr, thresholds = roc_curve(preds['trueLabel'], \
                                     preds['anomalyScore'])
    areaUnderROC = auc(fpr, tpr)
    plt.cla()
    setPlot()
    plt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')
    plt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic: 曲線以下面積:{0:0.2f}'.format(areaUnderROC))
    plt.legend(loc="lower right")
    plt.savefig('images/'+imgName+'-2.png', dpi=300, bbox_inches='tight')


    if returnPreds==True:
        return preds

# View scatterplot
def scatterPlot(xDF, yDF, algoName, imgName=''):
    plt.cla()
    setPlot()
    tempDF = pd.DataFrame(data=xDF.loc[:,0:1], index=xDF.index)
    tempDF = pd.concat((tempDF,yDF), axis=1, join="inner")
    tempDF.columns = ["First Vector", "Second Vector", "Label"]
    sns.lmplot(x="First Vector", y="Second Vector", hue="Label", \
               data=tempDF, fit_reg=False)
    ax = plt.gca()
    ax.set_title("演算法:"+algoName)
    plt.savefig('images/'+imgName+'.png', dpi=300, bbox_inches='tight')

from sklearn.decomposition import PCA

n_components = 30 #保留30個主成分
whiten = False
random_state = 2018

pca = PCA(n_components=n_components, whiten=whiten, \
          random_state=random_state)

X_train_PCA = pca.fit_transform(X_train)
X_train_PCA = pd.DataFrame(data=X_train_PCA, index=X_train.index)

X_train_PCA_inverse = pca.inverse_transform(X_train_PCA)
X_train_PCA_inverse = pd.DataFrame(data=X_train_PCA_inverse, index=X_train.index)

scatterPlot(X_train_PCA, y_train, 'AD-PCA', 'AD-PCA')
anomalyScoresPCA = anomalyScores(X_train, X_train_PCA_inverse)
preds = plotResults(y_train, anomalyScoresPCA, True, 'AD-PCA')


#+end_src

#+RESULTS:
: Number of fraudulent transactions: 492

** Sparse PCA異常偵測
#+begin_src python -r -n :async :results output :exports both :session AD
# Sparse PCA
from sklearn.decomposition import SparsePCA

n_components = 27
alpha = 0.0001
random_state = 2018
n_jobs = -1

sparsePCA = SparsePCA(n_components=n_components, \
                alpha=alpha, random_state=random_state, n_jobs=n_jobs)

sparsePCA.fit(X_train.loc[:,:])
X_train_sparsePCA = sparsePCA.transform(X_train)
X_train_sparsePCA = pd.DataFrame(data=X_train_sparsePCA, index=X_train.index)

scatterPlot(X_train_sparsePCA, y_train, "Sparse PCA", "AD-SPCA")
# View plot
X_train_sparsePCA_inverse = np.array(X_train_sparsePCA). \
    dot(sparsePCA.components_) + np.array(X_train.mean(axis=0))
X_train_sparsePCA_inverse = \
    pd.DataFrame(data=X_train_sparsePCA_inverse, index=X_train.index)

anomalyScoresSparsePCA = anomalyScores(X_train, X_train_sparsePCA_inverse)
preds = plotResults(y_train, anomalyScoresSparsePCA, True, 'AD-SPCA')
#+end_src

#+RESULTS:

#+CAPTION: Sparse PCA異常偵測/27
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-SPCA.png]]

#+CAPTION: Sparse PCA異常偵測/27
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-SPCA-1.png]]
#+CAPTION: Sparse PCA異常偵測/27
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-SPCA-2.png]]
*** 分析結果
#+begin_src python -r -n :async :results output :exports both :session AD
# Analyze results
preds.sort_values(by="anomalyScore",ascending=False,inplace=True)
cutoff = 350
predsTop = preds[:cutoff]
print("Precision: ",np.round(predsTop. \
            anomalyScore[predsTop.trueLabel==1].count()/cutoff,2))
print("Recall: ",np.round(predsTop. \
            anomalyScore[predsTop.trueLabel==1].count()/y_train.sum(),2))
print("Fraud Caught out of 330 Cases:", predsTop.trueLabel.sum())
#+end_src

#+RESULTS:
: Precision:  0.75
: Recall:  0.79
: Fraud Caught out of 330 Cases: 261

** Kernel PCA異常偵測
#+begin_src python -r -n :async :results output :exports both :session AD
# Kernel PCA
from sklearn.decomposition import KernelPCA

n_components = 27
kernel = 'rbf'
gamma = None
fit_inverse_transform = True
random_state = 2018
n_jobs = 1

kernelPCA = KernelPCA(n_components=n_components, kernel=kernel, \
                gamma=gamma, fit_inverse_transform= \
                fit_inverse_transform, n_jobs=n_jobs, \
                random_state=random_state)

kernelPCA.fit(X_train.iloc[:2000])
X_train_kernelPCA = kernelPCA.transform(X_train)
X_train_kernelPCA = pd.DataFrame(data=X_train_kernelPCA, \
                                 index=X_train.index)

X_train_kernelPCA_inverse = kernelPCA.inverse_transform(X_train_kernelPCA)
X_train_kernelPCA_inverse = pd.DataFrame(data=X_train_kernelPCA_inverse, \
                                         index=X_train.index)

scatterPlot(X_train_kernelPCA, y_train, "Kernel PCA", "AD-KPCA")
# View plot
# View plot
anomalyScoresKernelPCA = anomalyScores(X_train, X_train_kernelPCA_inverse)
preds = plotResults(y_train, anomalyScoresKernelPCA, True, 'AD-KPCA')
#+end_src

#+RESULTS:

#+CAPTION: Kernel PCA異常偵測/27
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-KPCA.png]]

#+CAPTION: Kernel PCA異常偵測/27
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-KPCA-1.png]]
#+CAPTION: Kernel PCA異常偵測/27
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-KPCA-2.png]]
*** 分析結果
#+begin_src python -r -n :async :results output :exports both :session AD
# Analyze results
preds.sort_values(by="anomalyScore",ascending=False,inplace=True)
cutoff = 350
predsTop = preds[:cutoff]
print("Precision: ",np.round(predsTop. \
            anomalyScore[predsTop.trueLabel==1].count()/cutoff,2))
print("Recall: ",np.round(predsTop. \
            anomalyScore[predsTop.trueLabel==1].count()/y_train.sum(),2))
print("Fraud Caught out of 330 Cases:", predsTop.trueLabel.sum())
# Write dimensions to CSV
X_train_kernelPCA.loc[sample_indices,:].to_csv('kernel_pca_data.tsv', sep = '\t', index=False, header=False)
#+end_src

#+RESULTS:
: Precision:  0.22
: Recall:  0.23
: Fraud Caught out of 330 Cases: 77
結果不如普通的PCA與sparse PCA

** 高斯隨機投影異常偵測 :noexport:
#+begin_src python -r -n :async :results output :exports both :session AD
# Gaussian Random Projection
from sklearn.random_projection import GaussianRandomProjection

n_components = 27
eps = 0.1
random_state = 2018

GRP = GaussianRandomProjection(n_components=n_components, eps=eps, random_state=random_state)

X_train_GRP = GRP.fit_transform(X_train)
X_train_GRP = pd.DataFrame(data=X_train_GRP, index=X_train.index)

scatterPlot(X_train_GRP, y_train, "Gaussian Random Projection", "AD-GRP")
# View plot
X_train_GRP_inverse = np.array(X_train_GRP).dot(GRP.components_)
X_train_GRP_inverse = pd.DataFrame(data=X_train_GRP_inverse, \
                                   index=X_train.index)

anomalyScoresGRP = anomalyScores(X_train, X_train_GRP_inverse)
preds = plotResults(y_train, anomalyScoresGRP, True, "AD-GRP")
#+end_src

#+RESULTS:
#+CAPTION: 高斯隨機投影異常偵測/27
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-GRP.png]]
#+CAPTION: 高斯隨機投影異常偵測/27
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-GRP-1.png]]
#+CAPTION: 高斯隨機投影異常偵測/27
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-GRP-2.png]]
*** 分析結果
#+begin_src python -r -n :async :results output :exports both :session AD
# Analyze results
preds.sort_values(by="anomalyScore",ascending=False,inplace=True)
cutoff = 350
predsTop = preds[:cutoff]
print("Precision: ",np.round(predsTop.anomalyScore[predsTop.trueLabel==1].count()/cutoff,2))
print("Recall: ",np.round(predsTop.anomalyScore[predsTop.trueLabel==1].count()/y_train.sum(),2))
print("Fraud Caught out of 330 Cases:", predsTop.trueLabel.sum())
# Write dimensions to CSV

X_train_GRP.loc[sample_indices,:].to_csv('gaussian_random_projection_data.tsv', sep = '\t', index=False, header=False)
#+end_src

#+RESULTS:
: dfe5ac4c-00a7-4c96-a664-16aa6bb229fc

** 稀疏隨機投影異常偵測
#+begin_src python -r -n :async :results output :exports both :session AD
# Sparse Random Projection
from sklearn.random_projection import SparseRandomProjection

n_components = 27
density = 'auto'
eps = .01
dense_output = True
random_state = 2018

SRP = SparseRandomProjection(n_components=n_components, \
        density=density, eps=eps, dense_output=dense_output, \
                                random_state=random_state)

X_train_SRP = SRP.fit_transform(X_train)
X_train_SRP = pd.DataFrame(data=X_train_SRP, index=X_train.index)

scatterPlot(X_train_SRP, y_train, "Sparse Random Projection", "AD-SRP")
# View plot
X_train_SRP_inverse = np.array(X_train_SRP).dot(SRP.components_.todense())
X_train_SRP_inverse = pd.DataFrame(data=X_train_SRP_inverse, index=X_train.index)

anomalyScoresSRP = anomalyScores(X_train, X_train_SRP_inverse)
preds = plotResults(y_train, anomalyScoresSRP, True, "AD-SRP")
#+end_src

#+RESULTS:

#+CAPTION: 稀疏隨機投影異常偵測
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-SRP.png]]
#+CAPTION: 稀疏隨機投影異常偵測
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-SRP-1.png]]
#+CAPTION: 稀疏隨機投影異常偵測
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-SRP-2.png]]
*** 分析結果
#+begin_src python -r -n :async :results output :exports both :session AD
# Analyze results
preds.sort_values(by="anomalyScore",ascending=False,inplace=True)
cutoff = 350
predsTop = preds[:cutoff]
print("Precision: ",np.round(predsTop. \
            anomalyScore[predsTop.trueLabel==1].count()/cutoff,2))
print("Recall: ",np.round(predsTop. \
            anomalyScore[predsTop.trueLabel==1].count()/y_train.sum(),2))
print("Fraud Caught out of 330 Cases:", predsTop.trueLabel.sum())
# Write dimensions to CSV
X_train_SRP.loc[sample_indices,:].to_csv('sparse_random_projection_data.tsv', sep = '\t', index=False, header=False)
#+end_src

#+RESULTS:
: Precision:  0.21
: Recall:  0.22
: Fraud Caught out of 330 Cases: 73

** 字典學習異常偵測
#+begin_src python -r -n :async :results output :exports both :session AD
# Mini-batch dictionary learning
from sklearn.decomposition import MiniBatchDictionaryLearning

n_components = 28
alpha = 1
batch_size = 200
max_iter = 200
random_state = 2018

miniBatchDictLearning = MiniBatchDictionaryLearning( \
    n_components=n_components, alpha=alpha, batch_size=batch_size, \
    max_iter=max_iter, random_state=random_state)

miniBatchDictLearning.fit(X_train)
X_train_miniBatchDictLearning = \
    miniBatchDictLearning.fit_transform(X_train)
X_train_miniBatchDictLearning = \
    pd.DataFrame(data=X_train_miniBatchDictLearning, index=X_train.index)

scatterPlot(X_train_miniBatchDictLearning, y_train, \
            "Mini-batch Dictionary Learning", "AD-MBDL")
# View plot
X_train_miniBatchDictLearning_inverse = \
    np.array(X_train_miniBatchDictLearning). \
    dot(miniBatchDictLearning.components_)

X_train_miniBatchDictLearning_inverse = \
    pd.DataFrame(data=X_train_miniBatchDictLearning_inverse, \
                 index=X_train.index)

anomalyScoresMiniBatchDictLearning = anomalyScores(X_train, \
    X_train_miniBatchDictLearning_inverse)
preds = plotResults(y_train, anomalyScoresMiniBatchDictLearning, True, "AD-MBDL")
#+end_src

#+RESULTS:

#+CAPTION: 字典學習異常偵測
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-MBDL.png]]
#+CAPTION: 字典學習異常偵測
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-MBDL-1.png]]
#+CAPTION: 字典學習異常偵測
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-MBDL-2.png]]
*** 分析結果
#+begin_src python -r -n :async :results output :exports both :session AD
# Analyze results
preds.sort_values(by="anomalyScore",ascending=False,inplace=True)
cutoff = 350
predsTop = preds[:cutoff]
print("Precision: ",np.round(predsTop. \
            anomalyScore[predsTop.trueLabel==1].count()/cutoff,2))
print("Recall: ",np.round(predsTop. \
            anomalyScore[predsTop.trueLabel==1].count()/y_train.sum(),2))
print("Fraud Caught out of 330 Cases:", predsTop.trueLabel.sum())
# Write dimensions to CSV
X_train_miniBatchDictLearning.loc[sample_indices,:].to_csv('dictionary_learning_data.tsv', sep = '\t', index=False, header=False)
#+end_src

#+RESULTS:
: Precision:  0.43
: Recall:  0.46
: Fraud Caught out of 330 Cases: 151

** ICA異常偵測
#+begin_src python -r -n :async :results output :exports both :session AD
# Independent Component Analysis
from sklearn.decomposition import FastICA

n_components = 27
algorithm = 'parallel'
whiten = 'arbitrary-variance'
max_iter = 200
random_state = 2018

fastICA = FastICA(n_components=n_components, \
    algorithm=algorithm, whiten=whiten, max_iter=max_iter, \
    random_state=random_state)

X_train_fastICA = fastICA.fit_transform(X_train)
X_train_fastICA = pd.DataFrame(data=X_train_fastICA, index=X_train.index)

X_train_fastICA_inverse = fastICA.inverse_transform(X_train_fastICA)
X_train_fastICA_inverse = pd.DataFrame(data=X_train_fastICA_inverse, \
                                       index=X_train.index)

scatterPlot(X_train_fastICA, y_train, "Independent Component Analysis", "AD-ICA")
# View plot
anomalyScoresFastICA = anomalyScores(X_train, X_train_fastICA_inverse)
preds = plotResults(y_train, anomalyScoresFastICA, True, "AD-ICA")
#+end_src

#+RESULTS:

#+CAPTION: ICA異常偵測
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-ICA.png]]
#+CAPTION: ICA異常偵測
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-ICA-1.png]]
#+CAPTION: ICA異常偵測
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/AD-ICA-2.png]]
*** 分析結果
#+begin_src python -r -n :async :results output :exports both :session AD
# Analyze results
preds.sort_values(by="anomalyScore",ascending=False,inplace=True)
cutoff = 350
predsTop = preds[:cutoff]
print("Precision: ",np.round(predsTop. \
            anomalyScore[predsTop.trueLabel==1].count()/cutoff,2))
print("Recall: ",np.round(predsTop. \
            anomalyScore[predsTop.trueLabel==1].count()/y_train.sum(),2))
print("Fraud Caught out of 330 Cases:", predsTop.trueLabel.sum())
# Write dimensions to CSV
# Write dimensions to CSV
X_train_fastICA.loc[sample_indices,:].to_csv('independent_component_analysis_data.tsv', sep = '\t', index=False, header=False)
#+end_src

#+RESULTS:
: Precision:  0.75
: Recall:  0.79
: Fraud Caught out of 330 Cases: 261

** 結論
普通PCA與ICA能捕捉到超過80%的已知詐欺，並有80%的精確率，較之監督式學習能捕捉到90%，已十分難得。

* 信用卡詐欺偵測 :noexport:
- 資料來源: Hands-On Unsupervised Learning Using Python: How to Build Applied Machine Learning Solutions from Ulabled Data
- Code: https://github.com/aapatel09/handson-unsupervised-learning/blob/master/02_end_to_end_machine_learning_project.ipynb
** 資料取得
*** for Google colab
#+begin_src python -r -n :async :results output :exports both
%mkdir "/content/drive/My Drive/403AI"
#+end_src
#+begin_src python -r -n :results output :exports both
# Define functions to connect to Google and change directories
def connectDrive():
    from google.colab import drive
    drive.mount('/content/drive', force_remount=True)

def changeDirectory(path):
    import os
    original_path = os.getcwd()
    os.chdir(path)
    new_path = os.getcwd()
    print("Original path: ",original_path)
    print("New path: ",new_path)

# Connect to Google Drive
connectDrive()

# Change path
changeDirectory("/content/drive/My Drive/403AI/")
#+end_src
*** Import Libraries
#+begin_src python -r -n :async :results output :exports both :session creditCard
'''Main'''
import numpy as np
import pandas as pd
import os

'''Data
 Viz'''
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
color = sns.color_palette()

#%matplotlib inline
'''Data Prep'''
from sklearn import preprocessing as pp
from scipy.stats import pearsonr
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
from sklearn.metrics import precision_recall_curve, average_precision_score
from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.metrics import confusion_matrix, classification_report

'''Algos'''
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
# import xgboost as xgb
import lightgbm as lgb
#+end_src

#+RESULTS:
** 資料準備
*** 取得資料
#+begin_src python -r -n :async :results output :exports both :session creditCard
# Acquire Data
data = pd.read_csv("https://media.githubusercontent.com/media/francis-kang/handson-unsupervised-learning/master/datasets/credit_card_data/credit_card.csv")
print(data)
#+end_src

#+RESULTS:
#+begin_example
            Time         V1         V2        V3        V4        V5        V6        V7  ...       V23       V24       V25       V26       V27       V28  Amount  Class
0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599  ... -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62      0
1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803  ...  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69      0
2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461  ...  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0
3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609  ... -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50      0
4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941  ... -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99      0
...          ...        ...        ...       ...       ...       ...       ...       ...  ...       ...       ...       ...       ...       ...       ...     ...    ...
284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473 -2.606837 -4.918215  ...  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77      0
284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229  1.058415  0.024330  ...  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79      0
284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515  3.031260 -0.296827  ... -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88      0
284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961  0.623708 -0.686180  ... -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00      0
284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546 -0.649617  1.577006  ...  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00      0

[284807 rows x 31 columns]
#+end_example

*** 資料探索
#+begin_src python -r -n :async :results output :exports both :session creditCard
print(data.shape)
print(data.head(3))
print(data.describe())
print(data.columns)
#+end_src

#+RESULTS:
#+begin_example
(284807, 31)
   Time        V1        V2        V3        V4  ...       V26       V27       V28  Amount  Class
0   0.0 -1.359807 -0.072781  2.536347  1.378155  ... -0.189115  0.133558 -0.021053  149.62      0
1   0.0  1.191857  0.266151  0.166480  0.448154  ...  0.125895 -0.008983  0.014724    2.69      0
2   1.0 -1.358354 -1.340163  1.773209  0.379780  ... -0.139097 -0.055353 -0.059752  378.66      0

[3 rows x 31 columns]
                Time            V1            V2  ...           V28         Amount          Class
count  284807.000000  2.848070e+05  2.848070e+05  ...  2.848070e+05  284807.000000  284807.000000
mean    94813.859575  1.168375e-15  3.416908e-16  ... -1.227390e-16      88.349619       0.001727
std     47488.145955  1.958696e+00  1.651309e+00  ...  3.300833e-01     250.120109       0.041527
min         0.000000 -5.640751e+01 -7.271573e+01  ... -1.543008e+01       0.000000       0.000000
25%     54201.500000 -9.203734e-01 -5.985499e-01  ... -5.295979e-02       5.600000       0.000000
50%     84692.000000  1.810880e-02  6.548556e-02  ...  1.124383e-02      22.000000       0.000000
75%    139320.500000  1.315642e+00  8.037239e-01  ...  7.827995e-02      77.165000       0.000000
max    172792.000000  2.454930e+00  2.205773e+01  ...  3.384781e+01   25691.160000       1.000000

[8 rows x 31 columns]
Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',
       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',
       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',
       'Class'],
      dtype='object')
#+end_example
**** 找出非數值資料
#+begin_src python -r -n :async :results output :exports both :session creditCard
import numpy as np
import pandas as pd
# data = pd.read_csv("./credit_card.csv") #直接從磁碟機中讀回
nanCounter = pd.isna(data).sum() #將np.isnan以pd.isna取代
print(nanCounter)
#+end_src

#+RESULTS:
#+begin_example
Time      0
V1        0
V2        0
V3        0
V4        0
V5        0
V6        0
V7        0
V8        0
V9        0
V10       0
V11       0
V12       0
V13       0
V14       0
V15       0
V16       0
V17       0
V18       0
V19       0
V20       0
V21       0
V22       0
V23       0
V24       0
V25       0
V26       0
V27       0
V28       0
Amount    0
Class     0
dtype: int64
#+end_example
**** 找出缺漏值
#+begin_src python -r -n :async :results output :exports both :session creditCard
distinctCounter = data.apply(lambda x: len(x.unique()))
print(distinctCounter)
#+end_src

#+RESULTS:
#+begin_example
Time      124592
V1        275663
V2        275663
V3        275663
V4        275663
V5        275663
V6        275663
V7        275663
V8        275663
V9        275663
V10       275663
V11       275663
V12       275663
V13       275663
V14       275663
V15       275663
V16       275663
V17       275663
V18       275663
V19       275663
V20       275663
V21       275663
V22       275663
V23       275663
V24       275663
V25       275663
V26       275663
V27       275663
V28       275663
Amount     32767
Class          2
dtype: int64
#+end_example
*** 建立feature matrix和labels array
#+begin_src python -r -n :async :results output :exports both :session creditCard
# Generate feature matrix and labels array
dataX = data.copy().drop(['Class'],axis=1)
dataY = data['Class'].copy()
print(dataX.head(3))
print(dataY.head(3))
#+end_src

#+RESULTS:
#+begin_example
   Time        V1        V2        V3        V4        V5        V6        V7        V8  ...       V21       V22       V23       V24       V25       V26       V27       V28  Amount
0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599  0.098698  ... -0.018307  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62
1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803  0.085102  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69
2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461  0.247676  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66

[3 rows x 30 columns]
0    0
1    0
2    0
Name: Class, dtype: int64
#+end_example
*** 特徵工程與特徵選擇
#+begin_src python -r -n :async :results output :exports both :session creditCard
correlationMatrix = pd.DataFrame(data=[],index=dataX.columns,columns=dataX.columns)
for i in dataX.columns:
    for j in dataX.columns:
        correlationMatrix.loc[i,j] = np.round(pearsonr(dataX.loc[:,i],dataX.loc[:,j])[0],2)

count_classes = pd.value_counts(data['Class'],sort=True).sort_index()
ax = sns.barplot(x=count_classes.index, y=[tuple(count_classes/len(data))[0],tuple(count_classes/len(data))[1]])
ax.set_title('Frequency Percentage by Class')
ax.set_xlabel('Class')
ax.set_ylabel('Frequency Percentage')
#plt.show()
plt.savefig('images/creditCardFreq.png', dpi=300)
#+end_src

#+CAPTION: Frequency Percentage by Class
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/creditCardFreq.png]]
** 模型準備
*** 切分訓練集與測試集
#+begin_src python -r -n :async :results output :exports both :session creditCard
X_train, X_test, y_train, y_test = train_test_split(dataX,
                                    dataY, test_size=0.33,
                                    random_state=2018, stratify=dataY)

print(len(X_train))
print(len(X_test))
#+end_src

#+RESULTS:
: 190820
: 93987
*** 選擇成本函數(loss function)
這是監督式分類問題，可以使用二元分類對數損失來計算實際label與模型預測二者間的交叉熵。
$$ log loss=-\frac{1}{N}\sum_{i=1}^N{\sum^M_{j=1}{y_{i,j}log(P_{i,j})} $$
其中
- N為資料數量
- M為label數量
- 若第 $i$ 項為 $j$ 類別， $y_{i,j}$ 值為1，反之為0
- $P_{i,j}$ 為預測類別項目 $i$ 為 $j$ 類別的機率
*** k-Fold交叉驗證
為了有效評估「模型演算法預測未曾見過的樣本」的表現成效，訓練集可再進一步拆成訓練集與驗證集，可以用 /k-fold/ 交叉驗證來實作。
#+begin_src python -r -n :async :results output :exports both :session creditCard
k_fold = StratifiedKFold(n_splits=5,shuffle=True,random_state=2018)
print(k_fold)
#+end_src

#+RESULTS:
: StratifiedKFold(n_splits=5, random_state=2018, shuffle=True)

*** Feature scaling
** 模型訓練-1
先從最簡單的分類法做起：Logistic Regression分類
*** 模型1: Logistic Regression
- 懲罰項設為L2對離群值較不敏感，且會給全部特徵值分配非零權重，較為穩定；L1則會對最重要的特徵值分配最高權重，其他特徵值則給予接近零的權重，較不穩定。
- C為正規化的強度，正規化主要用來解決過度擬合的問題。C為一正浮點數，其值越小，正規化強度越強，預設值為1。
- 模型class_weight設為balanced，目的在告訴演算法這個訓練集的label類別分佈不平衡，在訓練時演算法就會對正向label加大權重。
#+begin_src python -r -n :async :results output :exports both :creditCard
#設定超參數
from sklearn.linear_model import LogisticRegression
penalty = 'l2'
C = 1.0
class_weight = 'balanced'
random_state = 2018
solver = 'liblinear'
n_jobs = 1

logReg = LogisticRegression(penalty=penalty, C=C,
            class_weight=class_weight, random_state=random_state, solver=solver, n_jobs=n_jobs)
print(logReg)
#+end_src

#+RESULTS:
: LogisticRegression(class_weight='balanced', n_jobs=1, random_state=2018,
:                    solver='liblinear')

*** 訓練模型
#+begin_src python -r -n :async :results output :exports both :session creditCard
trainingScores = []
cvScores = []
predictionsBasedOnKFolds = pd.DataFrame(data=[], index=y_train.index,columns=[0,1])

penalty = 'l2'
C = 1.0
class_weight = 'balanced'
random_state = 2018
solver = 'liblinear'
n_jobs = 1
k_fold = StratifiedKFold(n_splits=5,shuffle=True,random_state=2018)

logReg = LogisticRegression(penalty=penalty, C=C,
            class_weight=class_weight, random_state=random_state, solver=solver, n_jobs=n_jobs)

model = logReg
print(model)
for train_index, cv_index in k_fold.split(np.zeros(len(X_train)),y_train.ravel()):
    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], X_train.iloc[cv_index,:]
    y_train_fold, y_cv_fold = y_train.iloc[train_index], y_train.iloc[cv_index]

    model.fit(X_train_fold, y_train_fold)
    loglossTraining = log_loss(y_train_fold, model.predict_proba(X_train_fold)[:,1])
    trainingScores.append(loglossTraining)

    predictionsBasedOnKFolds.loc[X_cv_fold.index,:] = model.predict_proba(X_cv_fold)
    loglossCV = log_loss(y_cv_fold, predictionsBasedOnKFolds.loc[X_cv_fold.index,1])
    cvScores.append(loglossCV)

    print('Training Log Loss: ', loglossTraining)
    print('CV Log Loss: ', loglossCV)

loglossLogisticRegression = log_loss(y_train, predictionsBasedOnKFolds.loc[:,1])
print('Logistic Regression Log Loss: ', loglossLogisticRegression)
#+end_src

#+RESULTS:
#+begin_example
LogisticRegression(class_weight='balanced', n_jobs=1, random_state=2018,
                   solver='liblinear')
Training Log Loss:  0.10954763675404679
CV Log Loss:  0.10872713310759863
Training Log Loss:  0.10468236808859577
CV Log Loss:  0.10407923786166381
Training Log Loss:  0.11571053800039245
CV Log Loss:  0.11812201922603925
Training Log Loss:  0.11574462696076625
CV Log Loss:  0.11833216026592738
Training Log Loss:  0.09716362850332248
CV Log Loss:  0.09707807427732842
Logistic Regression Log Loss:  0.10926772494771149
#+end_example

*** 完整版程式碼(trainCreditCard-1.py)
#+begin_src python -r -n :async :results output :exports both :session creditCard
#!/usr/bin/env python3
'''Main'''
import numpy as np
import pandas as pd
import os

'''Data Viz'''
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
color = sns.color_palette()

#%matplotlib inline

'''Data Prep'''
from sklearn import preprocessing as pp
from scipy.stats import pearsonr
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
from sklearn.metrics import precision_recall_curve, average_precision_score
from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.metrics import confusion_matrix, classification_report

'''Algos'''
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
# import xgboost as xgb
#import lightgbm as lgb


data = pd.read_csv("./credit_card.csv")
nanCounter = pd.isna(data).sum() #將np.isnan以pd.isna取代

dataX = data.copy().drop(['Class'],axis=1)
dataY = data['Class'].copy()

X_train, X_test, y_train, y_test = train_test_split(dataX,
                                    dataY, test_size=0.33,
                                    random_state=2018, stratify=dataY)

k_fold = StratifiedKFold(n_splits=5,shuffle=True,random_state=2018)

from sklearn.linear_model import LogisticRegression
penalty = 'l2'
C = 1.0
class_weight = 'balanced'
random_state = 2018
solver = 'liblinear'
n_jobs = 1

logReg = LogisticRegression(penalty=penalty, C=C,
            class_weight=class_weight, random_state=random_state, solver=solver, n_jobs=n_jobs)

trainingScores = []
cvScores = []
predictionsBasedOnKFolds = pd.DataFrame(data=[], index=y_train.index,columns=[0,1])

model = logReg
print(model)
for train_index, cv_index in k_fold.split(np.zeros(len(X_train)),y_train.ravel()):
    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], X_train.iloc[cv_index,:]
    y_train_fold, y_cv_fold = y_train.iloc[train_index], y_train.iloc[cv_index]

    model.fit(X_train_fold, y_train_fold)
    loglossTraining = log_loss(y_train_fold, model.predict_proba(X_train_fold)[:,1])
    trainingScores.append(loglossTraining)

    predictionsBasedOnKFolds.loc[X_cv_fold.index,:] = model.predict_proba(X_cv_fold)
    loglossCV = log_loss(y_cv_fold, predictionsBasedOnKFolds.loc[X_cv_fold.index,1])
    cvScores.append(loglossCV)

    print('Training Log Loss: ', loglossTraining)
    print('CV Log Loss: ', loglossCV)

loglossLogisticRegression = log_loss(y_train, predictionsBasedOnKFolds.loc[:,1])
print('Logistic Regression Log Loss: ', loglossLogisticRegression)

#+end_src

#+RESULTS:
#+begin_example
LogisticRegression(class_weight='balanced', n_jobs=1, random_state=2018,
                   solver='liblinear')
Training Log Loss:  0.10939361490760419
CV Log Loss:  0.10853402466643607
Training Log Loss:  0.10453309543196382
CV Log Loss:  0.10404365007856392
Training Log Loss:  0.11558188743919326
CV Log Loss:  0.11799026783957066
Training Log Loss:  0.11560666592384615
CV Log Loss:  0.11818686208380477
Training Log Loss:  0.09707169357423985
CV Log Loss:  0.0969591251780277
Logistic Regression Log Loss:  0.10914278596928062
#+end_example

正常而言，Training Log Loss應該會略低於CV Log Loss，二者的值相近，表示未發生過擬合狀況（Training Log Loss很低但CV Log Loss很高）。
** 評估指標
- 召回率(recall): 找出了幾筆在訓練集中的詐欺交易？
- 精確率(precision): 被模型標示為詐欺的交易中，有幾筆為真的詐欺交易
*** 混淆矩陣(Confusion Matrix)
此例的label分類高度不平衡，使用混淆矩陣意義不大。若預測所有的交易均非詐欺，則結果會有284315筆真陰性、492筆偽陰性、0筆真陽性、0筆偽陽性，精確度為0。
*** 精確率-召回率曲線
對於類別不平衡的資料集，比較好的效能評估方案為精準率與召回率。
*** precision=真陽性個數/(真陽性個數+偽陽性個數)
即，所有的陽性預測中，有多少是對的預測？
*** recall=真陽性個數/(真陽性個數+偽陰性個數)
即，模型能捕捉到多少個詐欺交易？
*** 取捨
- 高recall低precision: 雖然預測中會有很多真的詐欺，但也會出現太多誤判
- 低precision高recall: 因為標記許多詐欺案例，因此能捕捉到許多詐欺交易，但也有許多被詐欺交易的case並不是真的詐欺
- 權衡：precision-recall curve，可以在每個門檻值下計算出最佳的average precision
*** 接收者操作特徵(Receiver Operating Characteristic)
ROC將真陽性率當Y軸、將偽陽性率當X軸，真陽性率也可以被當成敏感度，而偽陽性率也可以被當作l-specificity。
#+begin_src python -r -n :async :results output :exports both :session creditCard
preds = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,1]], axis=1)
preds.columns = ['trueLabel','prediction']
predictionsBasedOnKFoldsLogisticRegression = preds.copy()

precision, recall, thresholds = precision_recall_curve(preds['trueLabel'],
                                                       preds['prediction'])
average_precision = average_precision_score(preds['trueLabel'],
                                            preds['prediction'])
plt.figure()
plt.step(recall, precision, color='k', alpha=0.7, where='post')
plt.fill_between(recall, precision, step='post', alpha=0.3, color='k')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.ylim([0.0, 1.05])
plt.xlim([0.0, 1.0])
plt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(average_precision))
plt.savefig('images/prec-recall.png', dpi=300)
fpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])
areaUnderROC = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')
plt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic: \
          Area under the curve = {0:0.2f}'.format(areaUnderROC))
plt.legend(loc="lower right")
plt.savefig('images/auROC.png', dpi=300)

#+end_src

#+RESULTS:

由圖[[fig:prec-recall]]可以看出此模型能達到近80%的recall(捕捉到了80%的詐欺交易)，以及近乎70%的precision(所有被標記為詐欺的case中，有70%為真的詐欺，但仍有30$交易被不正確的標記為詐欺)
#+RESULTS:
#+CAPTION: 模型Recall
#+LABEL:fig:prec-recall
#+name: fig:prec-recall
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/prec-recall.png]]

圖[[fig:auROC]]的auROC曲線允許我們在盡可能保持偽陽率低的情況下，決定有多少的詐欺案例能被捕捉到。

#+CAPTION: 模型auROC
#+LABEL:fig:auROC
#+name: fig:auROC
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/auROC.png]]
** 模型訓練#2
*** 模型#2: 隨機森林
**** 設定超參數
- n_estimators = 10: 建立十顆樹並將這十顆樹的預測結果平均
- 這個case有30個特徵值，每顆樹會取總特徵值數的平方根值作為特徵數量，此例每顆樹會取5個特徵值
#+begin_src python -r -n :async :results output :exports both :session creditCard
n_estimators = 10
max_features = 'auto'
max_depth = None
min_samples_split = 2
min_samples_leaf = 1
min_weight_fraction_leaf = 0.0
max_leaf_nodes = None
bootstrap = True
oob_score = False
n_jobs = -1
random_state = 2018
class_weight = 'balanced'

RFC = RandomForestClassifier(n_estimators=n_estimators,
        max_features=max_features, max_depth=max_depth,
        min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,
        min_weight_fraction_leaf=min_weight_fraction_leaf,
        max_leaf_nodes=max_leaf_nodes, bootstrap=bootstrap,
        oob_score=oob_score, n_jobs=n_jobs, random_state=random_state,
        class_weight=class_weight)
#+end_src

#+RESULTS:
**** 訓練模型
執行k-fold交叉驗證五次，每次用4/5訓練集做為訓練、1/5作為預測
#+begin_src python -r -n :async :results output :exports both :session creditCard
trainingScores = []
cvScores = []
predictionsBasedOnKFolds = pd.DataFrame(data=[],
                                        index=y_train.index,columns=[0,1])

model = RFC

for train_index, cv_index in k_fold.split(np.zeros(len(X_train)),
                                          y_train.ravel()):
    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \
        X_train.iloc[cv_index,:]
    y_train_fold, y_cv_fold = y_train.iloc[train_index], \
        y_train.iloc[cv_index]

    model.fit(X_train_fold, y_train_fold)
    loglossTraining = log_loss(y_train_fold, \
                                model.predict_proba(X_train_fold)[:,1])
    trainingScores.append(loglossTraining)

    predictionsBasedOnKFolds.loc[X_cv_fold.index,:] = \
        model.predict_proba(X_cv_fold)
    loglossCV = log_loss(y_cv_fold, \
        predictionsBasedOnKFolds.loc[X_cv_fold.index,1])
    cvScores.append(loglossCV)

    print('Training Log Loss: ', loglossTraining)
    print('CV Log Loss: ', loglossCV)

loglossRandomForestsClassifier = log_loss(y_train,
                                          predictionsBasedOnKFolds.loc[:,1])
print('Random Forests Log Loss: ', loglossRandomForestsClassifier)
#+end_src

#+RESULTS:
: be675c0e-a262-4fe5-9b43-d9beaead1590
- 可以看出Training Log Loss明顯低於CV Log Loss，表示可能有過擬合的現象
- 但這兩個Loss指標仍明顯優於Logistic Regression模型(大概為後者的1/10)
**** 評估結果
#+begin_src python -r -n :async :results output :exports both :session creditCard
plt.cla()
preds = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,1]], axis=1)
preds.columns = ['trueLabel','prediction']
predictionsBasedOnKFoldsRandomForests = preds.copy()

precision, recall, thresholds = precision_recall_curve(preds['trueLabel'],
                                                       preds['prediction'])
average_precision = average_precision_score(preds['trueLabel'],
                                            preds['prediction'])

plt.step(recall, precision, color='k', alpha=0.7, where='post')
plt.fill_between(recall, precision, step='post', alpha=0.3, color='k')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.ylim([0.0, 1.05])
plt.xlim([0.0, 1.0])

plt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(
          average_precision))
plt.savefig('images/prec-recall-2.png', dpi=300)

fpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])
areaUnderROC = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')
plt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic: \
          Area under the curve = {0:0.2f}'.format(
          areaUnderROC))
plt.legend(loc="lower right")
plt.savefig('images/auROC-2.png', dpi=300)
#+end_src

#+RESULTS:
#+CAPTION: Caption
#+LABEL:fig:prec-recal-2
#+name: fig:prec-recal-2
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/prec-recall-2.png]]
- 但隨機森林的auROC為0.92，不如Logistic Regression模型的0.97
#+CAPTION: Caption
#+LABEL:fig:auROC-2
#+name: fig:auROC-2
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/auROC-2.png]]
*** 模型#3: Gradient Boosting Machine (XGBoost) :noexport:
Gradient boosting有兩種版本: XBGoost和微軟的LightGBM(效能較快)
**** 設定超參數
#+begin_src python -r -n :async :results output :exports both :session creditCard
params_xGB = {
    'nthread':16, #number of cores
    'learning rate': 0.3, #range 0 to 1, default 0.3
    'gamma': 0, #range 0 to infinity, default 0
        # increase to reduce complexity (increase bias, reduce variance)
    'max_depth': 6, #range 1 to infinity, default 6
    'min_child_weight': 1, #range 0 to infinity, default 1
    'max_delta_step': 0, #range 0 to infinity, default 0
    'subsample': 1.0, #range 0 to 1, default 1
        # subsample ratio of the training examples
    'colsample_bytree': 1.0, #range 0 to 1, default 1
        # subsample ratio of features
    'objective':'binary:logistic',
    'num_class':1,
    'eval_metric':'logloss',
    'seed':2018,
    'silent':1
}
#+end_src

#+RESULTS:
**** 訓練模型
#+begin_src python -r -n :async :results output :exports both :session creditCard
trainingScores = []
cvScores = []
predictionsBasedOnKFolds = pd.DataFrame(data=[], index=y_train.index,columns=['prediction'])

for train_index, cv_index in k_fold.split(np.zeros(len(X_train)), y_train.ravel()):
    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], X_train.iloc[cv_index,:]
    y_train_fold, y_cv_fold = y_train.iloc[train_index], y_train.iloc[cv_index]

    dtrain = xgb.DMatrix(data=X_train_fold, label=y_train_fold)
    dCV = xgb.DMatrix(data=X_cv_fold)

    bst = xgb.cv(params_xGB, dtrain, num_boost_round=2000,
                 nfold=5, early_stopping_rounds=200, verbose_eval=50)

    best_rounds = np.argmin(bst['test-logloss-mean'])
    bst = xgb.train(params_xGB, dtrain, best_rounds)

    loglossTraining = log_loss(y_train_fold, bst.predict(dtrain))
    trainingScores.append(loglossTraining)

    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = bst.predict(dCV)
    loglossCV = log_loss(y_cv_fold, predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])
    cvScores.append(loglossCV)

    print('Training Log Loss: ', loglossTraining)
    print('CV Log Loss: ', loglossCV)

loglossXGBoostGradientBoosting = log_loss(y_train, predictionsBasedOnKFolds.loc[:,'prediction'])
print('XGBoost Gradient Boosting Log Loss: ', loglossXGBoostGradientBoosting)
#+end_src

#+RESULTS:
: 751b021c-ca08-429e-aaa2-b66dd0fc6f5d

**** 完整程式版(trainXGBoost.py)
#+begin_src python -r -n :async :results output :exports both :session XGBoost
'''Main'''
import numpy as np
import pandas as pd
import os

'''Data Viz'''
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
color = sns.color_palette()

#%matplotlib inline

'''Data Prep'''
from sklearn import preprocessing as pp
from scipy.stats import pearsonr
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
from sklearn.metrics import precision_recall_curve, average_precision_score
from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.metrics import confusion_matrix, classification_report

'''Algos'''
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
import lightgbm as lgb


data = pd.read_csv("./credit_card.csv")
nanCounter = pd.isna(data).sum() #將np.isnan以pd.isna取代

dataX = data.copy().drop(['Class'],axis=1)
dataY = data['Class'].copy()

X_train, X_test, y_train, y_test = train_test_split(dataX,
                                    dataY, test_size=0.33,
                                    random_state=2018, stratify=dataY)

k_fold = StratifiedKFold(n_splits=5,shuffle=True,random_state=2018)

# 超參數
params_xGB = {
    'nthread':16, #number of cores
    'learning_rate':0.3, #range 0 to 1, default 0.3
    'gamma':0, #range 0 to infinity, default 0
        # increase to reduce complexity (increase bias, reduce variance)
    'max_depth': 6, #range 1 to infinity, default 6
    'max_depth': inf,
    'min_child_weight': 1, #range 0 to infinity, default 1
    'max_delta_step': 0, #range 0 to infinity, default 0
    'subsample': 1.0, #range 0 to 1, default 1
        # subsample ratio of the training examples
    'colsample_bytree': 1.0, #range 0 to 1, default 1
        # subsample ratio of features
    'objective':'binary:logistic',
    'num_class':1,
    'eval_metric':'logloss',
    'seed':2018,
    'silent':1
}

# 訓練模型
trainingScores = []
cvScores = []
predictionsBasedOnKFolds = pd.DataFrame(data=[],
                                    index=y_train.index,columns=['prediction'])

for train_index, cv_index in k_fold.split(np.zeros(len(X_train)), y_train.ravel()):
    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], X_train.iloc[cv_index,:]
    y_train_fold, y_cv_fold = y_train.iloc[train_index], y_train.iloc[cv_index]

    dtrain = xgb.DMatrix(data=X_train_fold, label=y_train_fold)
    dCV = xgb.DMatrix(data=X_cv_fold)

    bst = xgb.cv(params_xGB, dtrain, num_boost_round=2000,
                 nfold=5, early_stopping_rounds=200, verbose_eval=50)

    best_rounds = np.argmin(bst['test-logloss-mean'])
    bst = xgb.train(params_xGB, dtrain, best_rounds)

    loglossTraining = log_loss(y_train_fold, bst.predict(dtrain))
    trainingScores.append(loglossTraining)

    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = bst.predict(dCV)
    loglossCV = log_loss(y_cv_fold, predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])
    cvScores.append(loglossCV)

    print('Training Log Loss: ', loglossTraining)
    print('CV Log Loss: ', loglossCV)

loglossXGBoostGradientBoosting = log_loss(y_train, predictionsBasedOnKFolds.loc[:,'prediction'])
print('XGBoost Gradient Boosting Log Loss: ', loglossXGBoostGradientBoosting)


#+end_src

#+RESULTS:
: 322ee662-9334-4789-aa20-bbceb778d9a8
會跑很久....中間會出現Warning
Training Log Loss與CV Log Loss都較Random Forest有巨大改善
**** 評估結果
#+begin_src python -r -n :async :results output :exports both :session XGBoost
plt.cla()
preds = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,'prediction']], axis=1)
preds.columns = ['trueLabel','prediction']
predictionsBasedOnKFoldsXGBoostGradientBoosting = preds.copy()

precision, recall, thresholds = \
    precision_recall_curve(preds['trueLabel'],preds['prediction'])
average_precision = \
    average_precision_score(preds['trueLabel'],preds['prediction'])

plt.step(recall, precision, color='k', alpha=0.7, where='post')
plt.fill_between(recall, precision, step='post', alpha=0.3, color='k')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.ylim([0.0, 1.05])
plt.xlim([0.0, 1.0])

plt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(
          average_precision))
plt.savefig('images/prec-recall-3.png', dpi=300)

fpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])
areaUnderROC = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')
plt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic: \
        Area under the curve = {0:0.2f}'.format(areaUnderROC))
plt.legend(loc="lower right")
plt.savefig('images/auROC-3.png', dpi=300)

#+end_src

#+RESULTS:
#+CAPTION: Caption
#+LABEL:fig:prec-recal-3
#+name: fig:prec-recal-3
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/prec-recall-3.png]]
- 但隨機森林的auROC為0.92，不如Logistic Regression模型的0.97
#+CAPTION: Caption
#+LABEL:fig:auROC-3
#+name: fig:auROC-3
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/auROC-3.png]]

*** 模型#4: Gradient Boosting Machine (LightGBM)
**** 讀資料
#+begin_src python -r -n :async :results output :exports both :session LightGBM
'''Main'''
import numpy as np
import pandas as pd
import os

'''Data Viz'''
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
color = sns.color_palette()

#%matplotlib inline

'''Data Prep'''
from sklearn import preprocessing as pp
from scipy.stats import pearsonr
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
from sklearn.metrics import precision_recall_curve, average_precision_score
from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.metrics import confusion_matrix, classification_report

'''Algos'''
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
import lightgbm as lgb


#data = pd.read_csv("./credit_card.csv")
data = pd.read_csv("https://media.githubusercontent.com/media/francis-kang/handson-unsupervised-learning/master/datasets/credit_card_data/credit_card.csv")
nanCounter = pd.isna(data).sum() #將np.isnan以pd.isna取代

dataX = data.copy().drop(['Class'],axis=1)
dataY = data['Class'].copy()

X_train, X_test, y_train, y_test = train_test_split(dataX,
                                    dataY, test_size=0.33,
                                    random_state=2018, stratify=dataY)

k_fold = StratifiedKFold(n_splits=5,shuffle=True,random_state=2018)

#+end_src

#+RESULTS:

**** 設定超參數
#+begin_src python -r -n :async :results output :exports both :session LightGBM
params_lightGB = {
    'task': 'train',
    'num_class':1,
    'boosting': 'gbdt',
    'objective': 'binary',
    'metric': 'binary_logloss',
    'metric_freq':50,
    'is_training_metric':False,
    'max_depth':4,
    'num_leaves': 31,
    'learning_rate': 0.01,
    'feature_fraction': 1.0,
    'bagging_fraction': 1.0,
    'bagging_freq': 0,
    'bagging_seed': 2018,
    'verbose': -1,
    'num_threads':16
}
#+end_src

#+RESULTS:

**** 訓練模型
#+begin_src python -r -n :async :results output :exports both  :session LightGBM

trainingScores = []
cvScores = []
predictionsBasedOnKFolds = pd.DataFrame(data=[],
                                index=y_train.index,columns=['prediction'])

for train_index, cv_index in k_fold.split(np.zeros(len(X_train)),
                                          y_train.ravel()):
    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \
        X_train.iloc[cv_index,:]
    y_train_fold, y_cv_fold = y_train.iloc[train_index], \
        y_train.iloc[cv_index]

    lgb_train = lgb.Dataset(X_train_fold, y_train_fold)
    lgb_eval = lgb.Dataset(X_cv_fold, y_cv_fold, reference=lgb_train)
    gbm = lgb.train(params_lightGB, lgb_train, num_boost_round=2000,
                   valid_sets=lgb_eval)

    loglossTraining = log_loss(y_train_fold, \
                gbm.predict(X_train_fold, num_iteration=gbm.best_iteration))
    trainingScores.append(loglossTraining)

    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = \
        gbm.predict(X_cv_fold, num_iteration=gbm.best_iteration)
    loglossCV = log_loss(y_cv_fold, \
        predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])
    cvScores.append(loglossCV)

    print('Training Log Loss: ', loglossTraining)
    print('CV Log Loss: ', loglossCV)

loglossLightGBMGradientBoosting = \
    log_loss(y_train, predictionsBasedOnKFolds.loc[:,'prediction'])
print('LightGBM Gradient Boosting Log Loss: ', loglossLightGBMGradientBoosting)
#+end_src

#+RESULTS:
#+begin_example
Training Log Loss:  5.722067577767659e-05
CV Log Loss:  0.0028905982750420615
Training Log Loss:  5.326493165009444e-05
CV Log Loss:  0.00341349889324051
Training Log Loss:  6.167817302035956e-05
CV Log Loss:  0.0026930532519383577
Training Log Loss:  6.96104776294407e-05
CV Log Loss:  0.0029882236623969124
Training Log Loss:  5.316601718227403e-05
CV Log Loss:  0.004136416692029885
LightGBM Gradient Boosting Log Loss:  0.0032243581549295445
#+end_example

**** 評估結果
#+begin_src python -r -n :async :results output :exports both  :session LightGBM
plt.cla()
preds = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,'prediction']], axis=1)
preds.columns = ['trueLabel','prediction']
predictionsBasedOnKFoldsLightGBMGradientBoosting = preds.copy()

precision, recall, thresholds = \
    precision_recall_curve(preds['trueLabel'],preds['prediction'])
average_precision = \
    average_precision_score(preds['trueLabel'],preds['prediction'])

plt.step(recall, precision, color='k', alpha=0.7, where='post')
plt.fill_between(recall, precision, step='post', alpha=0.3, color='k')
plt.savefig('images/prec-recall-4.png', dpi=300)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.ylim([0.0, 1.05])
plt.xlim([0.0, 1.0])

plt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(
          average_precision))

fpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])
areaUnderROC = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')
plt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic: \
Area under the curve = {0:0.2f}'.format(areaUnderROC))
plt.legend(loc="lower right")
plt.savefig('images/auROC-4.png', dpi=300)
#+end_src

#+RESULTS:
結果跑不出來，用session跑，除錯有問題，看不到過程
#+CAPTION: Caption
#+LABEL:fig:prec-recal-3
#+name: fig:prec-recal-3
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/prec-recall-4.png]]
- 但隨機森林的auROC為0.92，不如Logistic Regression模型的0.97
#+CAPTION: Caption
#+LABEL:fig:auROC-3
#+name: fig:auROC-3
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/auROC-4.png]]
** Stack
可以將不同家族的模型Stack來改善單一模型的效能，從每個單一模型中取得k-fold交叉驗證的預測結果（第一層預測），將這些預測結果加到原始訓練資料集中，再採用k-fold交叉驗證，利用原始的特徵和第一層預測資料集進行訓練。
*** 合併
#+begin_src python -r -n :async :results output :exports both
# Without XGBoost
predictionsBasedOnKFoldsFourModels = pd.DataFrame(data=[],index=y_train.index)
predictionsBasedOnKFoldsFourModels = predictionsBasedOnKFoldsFourModels.join(
    predictionsBasedOnKFoldsLogisticRegression['prediction'].astype(float), \
    how='left').join(predictionsBasedOnKFoldsRandomForests['prediction'] \
    .astype(float),how='left',rsuffix="2").join( \
    predictionsBasedOnKFoldsLightGBMGradientBoosting['prediction'].astype(float), \
    how='left',rsuffix="4")
predictionsBasedOnKFoldsFourModels.columns = \
    ['predsLR','predsRF','predsLightGBM']

predictionsBasedOnKFoldsFourModels = pd.DataFrame(data=[],index=y_train.index)
predictionsBasedOnKFoldsFourModels = predictionsBasedOnKFoldsFourModels.join(
    predictionsBasedOnKFoldsLogisticRegression['prediction'].astype(float), \
    how='left').join(predictionsBasedOnKFoldsRandomForests['prediction'] \
    .astype(float),how='left',rsuffix="2").join( \
    predictionsBasedOnKFoldsXGBoostGradientBoosting['prediction'].astype(float), \
    how='left',rsuffix="3").join( \
    predictionsBasedOnKFoldsLightGBMGradientBoosting['prediction'].astype(float), \
    how='left',rsuffix="4")
predictionsBasedOnKFoldsFourModels.columns = \
    ['predsLR','predsRF','predsXGB','predsLightGBM']
X_trainWithPredictions = \
    X_train.merge(predictionsBasedOnKFoldsFourModels,
                  left_index=True,right_index=True)
params_lightGB = {
    'task': 'train',
    'num_class':1,
    'boosting': 'gbdt',
    'objective': 'binary',
    'metric': 'binary_logloss',
    'metric_freq':50,
    'is_training_metric':False,
    'max_depth':4,
    'num_leaves': 31,
    'learning_rate': 0.01,
    'feature_fraction': 1.0,
    'bagging_fraction': 1.0,
    'bagging_freq': 0,
    'bagging_seed': 2018,
    'verbose': -1,
    'num_threads':16
}
#+end_src

#+RESULTS:

* 非監督式學習分群的原理 :noexport:
** Linear projection
- PCA: Principal component analysis，用來識別整個feature set中哪些特徵最為重要，且最能解釋資料實例間的可變性。
  + standard PCA
  + incremental PCA
  + sparse PCA
  + kernel PCA
- SVD: Singular value decomposition，降低原來features matrix的rank，使原來的matrix可以分解成數個較小rank的matrix。
- Random projection
- PCA
#+CAPTION: PCA
#+name: fig:DimensionalityReduction-1
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 500
#+RESULTS:
[[file:images/PCA.jpg]]
** Nonlinear dimensionality reduction
- Isomap
- t-SNE: t-distributed stochastic neighbor embedding
- Dictionary learning
- ICA: Independent component analysis
- LDA: Latent Dirichlet allocation
- MDS: Multidimensional scaling
- LLE: Locally linear embedding
** Dimensionality reduction
- 找出原始資料裡最主要的方面來代替原始資料，使得在損失少部分原始資訊的基礎上，最大程度的降低原始資料的維度。
- 維度縮減讓非監督式學習能更正確地辨識patterns，並更有效率地解決大規模資料所導致的昂貴運算問題
- 降維原理
#+CAPTION: 基本的降維#1
#+name: fig:DimensionalityReduction-1
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 500
#+RESULTS:
[[file:images/DimensionalityReduction-1.jpg]]
- 降維的選擇
#+CAPTION: 基本的降維#2
#+name: fig:DimensionalityReduction-1
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 500
#+RESULTS:
[[file:images/DimensionalityReduction-2.jpg]]

* Footnotes

[fn:1] [[https://www.oreilly.com/library/view/hands-on-unsupervised-learning/9781492035633/][Hands-On Unsupervised Learning Using Python]]

[fn:2] [[https://medium.com/ai-academy-taiwan/clustering-method-4-ed927a5b4377][Clustering method 4]]

[fn:3] [[https://builtin.com/machine-learning/agglomerative-clustering][Hierarchical Clustering: Agglomerative + Divisive Clustering]]

[fn:4][[https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8-%E7%B5%B1%E8%A8%88%E5%AD%B8%E7%BF%92-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-principle-component-analysis-pca-58229cd26e71][機器/統計學習:主成分分析(Principal Component Analysis, PCA)]]

[fn:5][[https://blog.csdn.net/dongyanwen6036/article/details/78311071][LDA與PCA都是常用的降維方法，二者的區別]]

[fn:6] [[https://medium.com/data-science-in-your-pocket/random-projection-for-dimension-reduction-27d2ec7d40cd][Random Projection for Dimension Reduction]]

[fn:7] Hands-On Machine Learning with Scikit-Learn: Aurelien Geron
