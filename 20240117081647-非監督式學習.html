<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-11-27 Wed 14:54 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>非監督式學習</title>
<meta name="author" content="Yung-Chin Yen" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="../css/muse.css" />
<script src="../css/copy_code.js"></script>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">非監督式學習</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org6406fb5">1. 非監督式學習</a>
<ul>
<li><a href="#org3ab972b">1.1. 目的</a></li>
<li><a href="#org2bc565f">1.2. 非監督式學習的常見演算法</a>
<ul>
<li><a href="#orgfda280e">1.2.1. 分群(clustering)</a></li>
<li><a href="#org5109243">1.2.2. 異常檢測與新穎檢測</a></li>
<li><a href="#orgbffc56e">1.2.3. 降維</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgd12aea8">2. 聚類(群集)</a>
<ul>
<li><a href="#org7942f86">2.1. K-Means</a>
<ul>
<li><a href="#org9c22374">2.1.1. K-means演算法</a></li>
<li><a href="#org09ecdd8">2.1.2. K-Means原理</a></li>
<li><a href="#org467b7c4">2.1.3. K-Means實作:隨機數字&#xa0;&#xa0;&#xa0;<span class="tag"><span class="sklearn">sklearn</span></span></a></li>
<li><a href="#orgd30bfd5">2.1.4. 評估效能</a></li>
<li><a href="#NS-KM-Image">2.1.5. K-Means應用: 壓縮影像</a></li>
<li><a href="#org526caa6">2.1.6. [小組作業]K-Means分群實作&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></a></li>
<li><a href="#org346e519">2.1.7. [小組作業]以K-Means壓縮影像實作&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></a></li>
</ul>
</li>
<li><a href="#NS-Hie-cluster">2.2. Hierarchical clustering</a>
<ul>
<li><a href="#org3bb9aad">2.2.1. 聚合式階層分群法(Agglomerative)</a></li>
<li><a href="#org9525e90">2.2.2. [課堂任務]聚合式階層分群&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></a></li>
<li><a href="#org3aaa8f7">2.2.3. TNFSH作業: 聚合式分群作業&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></a></li>
<li><a href="#org66fb2c2">2.2.4. 分裂式階層分群法(Divisive Clustering)</a></li>
</ul>
</li>
<li><a href="#orga821c4e">2.3. DBSCAN</a>
<ul>
<li><a href="#org07d4043">2.3.1. 實作</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org314a712">3. 降維</a>
<ul>
<li><a href="#orge2a1696">3.1. 以主成份分析(PCA)對非監督式數據壓縮</a>
<ul>
<li><a href="#org6657b96">3.1.1. 主成分分析 1</a></li>
<li><a href="#org45ef7d9">3.1.2. 主成份分析 2</a></li>
<li><a href="#org5ae0dda">3.1.3. 主成份分析的主要步驟</a></li>
<li><a href="#org573dc8a">3.1.4. 特徵轉換</a></li>
<li><a href="#orgebaeb00">3.1.5. 以 Scikit-learn 進行主成份分析</a></li>
</ul>
</li>
<li><a href="#orgcc920ec">3.2. 利用線性判別分析(LDA)做監督式數據壓縮</a></li>
</ul>
</li>
</ul>
</div>
</div>
#+begin_export ascii
<p>
 href=&ldquo;<a href="https://letranger.github.io/AI/20240117081647-%E9%9D%9E%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92.html">https://letranger.github.io/AI/20240117081647-%E9%9D%9E%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92.html</a>&rdquo;&gt;&lt;img align=&ldquo;right&rdquo; alt=&ldquo;Hits&rdquo; src="<img src="https://hits.sh/letranger.github.io/AI/20240117081647-%E9%9D%9E%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92.html.svg%22/" alt="" />&gt;&lt;/a&gt;
#+end_export
</p>
<div id="outline-container-org6406fb5" class="outline-2">
<h2 id="org6406fb5"><span class="section-number-2">1.</span> 非監督式學習</h2>
<div class="outline-text-2" id="text-1">

<div id="orge50871c" class="figure">
<p><img src="images/AI,_Machine_Learning與Deep_Learning/2024-02-19_16-24-48_2024-02-19_16-23-09.png" alt="2024-02-19_16-24-48_2024-02-19_16-23-09.png" width="500" />
</p>
<p><span class="figure-number">Figure 1: </span>AI, Machine Learning與Deep Learning</p>
</div>
</div>
<div id="outline-container-org3ab972b" class="outline-3">
<h3 id="org3ab972b"><span class="section-number-3">1.1.</span> 目的</h3>
<div class="outline-text-3" id="text-1-1">
<p>
非監督式學習接收未被標記的數據，並通過演算法根據資料的基礎結構(如常見的模式、特色、或是其他因素)將數據分類，而非 <b>做出預測</b> 。例如：
</p>
<ul class="org-ul">
<li>將網站訪客進行分類: 性別、喜好、上網時段</li>
<li><p>
將一堆照片依類型分類: cat、automobile、truck、frog、ship&#x2026;
</p>

<div id="orgae990ce" class="figure">
<p><img src="images/2022-04-30_10-57-36.jpg" alt="2022-04-30_10-57-36.jpg" width="500" />
</p>
<p><span class="figure-number">Figure 2: </span>照片分類</p>
</div></li>
<li><p>
異常檢測(Anamaly Detection): 例如，找出不尋常的信用卡交易以防止詐騙、找出製程中有缺陷的產品、將資料組中的離群值挑出來再傳給另一個演算法
</p>

<div id="orga5ca473" class="figure">
<p><img src="images/2022-04-30_11-35-44.jpg" alt="2022-04-30_11-35-44.jpg" width="500" />
</p>
<p><span class="figure-number">Figure 3: </span>Novelty Detection</p>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org2bc565f" class="outline-3">
<h3 id="org2bc565f"><span class="section-number-3">1.2.</span> 非監督式學習的常見演算法</h3>
<div class="outline-text-3" id="text-1-2">
<p>
為了讓相近的資料可以聚集在一起，通常還是會將資料的特徵值數值化，再透過計算資料間的「距離」進行分群，在此常以「歐幾里得距離」為計算方式。常見的分群演算法包括：
</p>
</div>
<div id="outline-container-orgfda280e" class="outline-4">
<h4 id="orgfda280e"><span class="section-number-4">1.2.1.</span> 分群(clustering)</h4>
<div class="outline-text-4" id="text-1-2-1">
<p>
聚類(集群)
</p>
<ul class="org-ul">
<li>K-Means</li>
<li>DBSCAN</li>
<li>階層式分群分析(Hierarchical Cluster Analysis, HCA)</li>
</ul>
</div>
</div>
<div id="outline-container-org5109243" class="outline-4">
<h4 id="org5109243"><span class="section-number-4">1.2.2.</span> 異常檢測與新穎檢測</h4>
<div class="outline-text-4" id="text-1-2-2">
<ul class="org-ul">
<li>One-class SVM</li>
<li>孤立森林(Isolation Forest)</li>
</ul>
</div>
</div>
<div id="outline-container-orgbffc56e" class="outline-4">
<h4 id="orgbffc56e"><span class="section-number-4">1.2.3.</span> 降維</h4>
<div class="outline-text-4" id="text-1-2-3">
<p>
降維有兩大分支：線性投影與流形學習<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>。
</p>
</div>
<div id="outline-container-orgc3fd388" class="outline-5">
<h5 id="orgc3fd388">線性投影</h5>
<div class="outline-text-5" id="text-orgc3fd388">
</div>
<ul class="org-ul">
<li><a id="org6da55aa"></a>主成分分析(Principal component analysis, PCA)<br />
<div class="outline-text-6" id="text-org6da55aa">
<p>
PCA有數種變形：mini-batch變形式PCA(incremental PCA)、非線性變形(kernel PCA)、稀疏變形(sparse PCA)
</p>
</div>
</li>
<li><a id="org579eeb1"></a>奇異值分解(Singular value decomposition, SVD)<br />
<div class="outline-text-6" id="text-org579eeb1">
<p>
降低原來特徵所組成的矩陣的秩（rank)，使得原來的矩陣可以使用擁有較小的秩的矩陣所組成的線性組合來表示。
</p>
</div>
</li>
<li><a id="org076edc4"></a>隨機投影(Random projection)<br />
<div class="outline-text-6" id="text-org076edc4">
<p>
由高維投影至低維空間，但同時保留點與點間的矩離，可以使用隨機高斯矩陣（random Gaussian matrix)或隨機稀疏矩陣(random sparse matrix)來實現。
</p>
<ul class="org-ul">
<li>principal component analysis</li>
<li>singular value decomposition</li>
<li>random projection.</li>
</ul>
</div>
</li>
</ul>
</div>
<div id="outline-container-org8058cee" class="outline-5">
<h5 id="org8058cee">流形學習(Manifold learning)</h5>
<div class="outline-text-5" id="text-org8058cee">
</div>
<ul class="org-ul">
<li><a id="orgfab2cfe"></a>Isomap<br />
<div class="outline-text-6" id="text-orgfab2cfe">
<p>
透過估算點與粌近點的捷線(geodesic)或曲線距離(curved distance)，而非使用歐式距離(Euclidean distance)來學習資料流形的內蘊幾何。
</p>
</div>
</li>
<li><a id="org0172a7b"></a>t-distributed stochastic neighbor embedding(t-SNE)<br />
<div class="outline-text-6" id="text-org0172a7b">
<p>
將高維度空間的資料嵌入至二維或三維的空間
</p>
</div>
</li>
<li><a id="org636b841"></a>multidimensional scaling (MDS)<br /></li>
<li><a id="org246654c"></a>locally linear embedding (LLE)<br /></li>
<li><a id="org15c5325"></a>dictionary learning<br /></li>
<li><a id="org65abd2c"></a>random trees embedding<br /></li>
<li><a id="org751a83f"></a>independent component analysis<br /></li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-orgd12aea8" class="outline-2">
<h2 id="orgd12aea8"><span class="section-number-2">2.</span> 聚類(群集)</h2>
<div class="outline-text-2" id="text-2">
<p>
「群集」 的概念簡單來說，就是將相似的資料分在同一群體內，從而找出數據中的隱藏結構。K-Means 是一種常用的非監督式學習演算法，用來進行資料分群。
</p>

<ul class="org-ul">
<li>任務: grouping objects together based on similarity.</li>
<li>應用:
<ul class="org-ul">
<li>在信用卡詐欺偵測中，聚類可以將詐欺交易分組在一起，將其與正常交易分開​​。</li>
<li>學生同質性分組</li>
<li>如果我們的資料集中的觀測值只有幾個標籤，我們可以先使用聚類對觀測值進行分組（不使用標籤）。 然後，我們可以將少數標記觀測值的標籤轉移到同一組內的其餘觀測值。 這是遷移學習的一種形式，也是機器學習中一個快速發展的領域。</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org7942f86" class="outline-3">
<h3 id="org7942f86"><span class="section-number-3">2.1.</span> K-Means</h3>
<div class="outline-text-3" id="text-2-1">
<p>
先來這裡(<a href="https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/">K-Means Clustering Demo</a>)試一下什麼是K-Means Cluestering。
</p>
</div>
<div id="outline-container-org9c22374" class="outline-4">
<h4 id="org9c22374"><span class="section-number-4">2.1.1.</span> K-means演算法</h4>
<div class="outline-text-4" id="text-2-1-1">
<p>
將 n 個點劃分為 K 個群集，每個點會根據與各群集中心的距離被歸入最近的群集，這樣每個群集內的資料點都與其中心點最接近。K-Means 的核心是根據歐幾里德距離來衡量資料點之間的相似性，並不斷調整群集中心以優化分群結果。將n個點劃分到K個聚落中，如此一來每個點都屬於離其最近的聚落中心所對應之聚落，以之作為分群的標準。
</p>


<div id="org925e921" class="figure">
<p><img src="images/blobsScatter.png" alt="blobsScatter.png" width="500" />
</p>
<p><span class="figure-number">Figure 4: </span>scikit-learn blobs</p>
</div>
</div>
</div>
<div id="outline-container-org09ecdd8" class="outline-4">
<h4 id="org09ecdd8"><span class="section-number-4">2.1.2.</span> K-Means原理</h4>
<div class="outline-text-4" id="text-2-1-2">
<p>
假設我們有八張沒有標註名稱的動物照片，這些照片中的每張都有兩個特徵值，代表這些動物的特徵分佈，如圖<a href="#orge135e22">5</a>、<a href="#org5dfbaea">6</a>所示。
</p>

<p>
K-means 演算法的步驟：
</p>
<ol class="org-ol">
<li>決定 K 值: 首先 ，我們需要決定要將資料分成幾個群集，這裡假設K 值為2，表示要將這 8 張照片分為 2 個群集。</li>
<li>選定 K 個中心點: 隨機 選擇 K 個中心點。在這個例子中，我們隨機選定兩個初始中心點，分別位於 (5, 5) 和 (6, 9) 的位置，如圖<a href="#org943c7e8">7</a>所示。</li>
<li>將資料點分群: 接著 ，我們計算每個資料點到各中心點的歐幾里德距離，根據距離將每個資料點分到與它最近的中心點所代表的群集。如圖<a href="#org392388d">8</a>和圖<a href="#org4a0f280">9</a>所示，資料點根據距離自動被歸類進群集。</li>
<li>調整群集中心點: 將所 有資料點分群後，重新計算每個群集的中心點。</li>
<li>這裡的步驟3, 4會不斷重複，直到群集中心點不再改變或改變幅度極小，表示分群過程完成。</li>
<li>記錄分群結果: 將最終的分群結果記錄下來，展示每個資料點所歸屬的群集。演算法完成後，資料點會被分成兩個群集，且每個群集的中心點也會進一步優化。</li>
</ol>

<p>
這個步驟可以反覆進行，直到模型收斂，即中心點不再變化為止。
八張未標註動物名稱(標籤)的照片，每張照片有兩個特徵值
</p>

<div id="orge135e22" class="figure">
<p><img src="images/聚類(集群)/2024-02-10_20-19-55_2024-02-10_20-19-45.png" alt="2024-02-10_20-19-55_2024-02-10_20-19-45.png" width="500" />
</p>
<p><span class="figure-number">Figure 5: </span>資料庫樣本</p>
</div>

<p>
八張照片的特徵分佈如下
</p>

<div id="org5dfbaea" class="figure">
<p><img src="images/kms-1.png" alt="kms-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 6: </span>待處理資料</p>
</div>

<p>
詳細的K-means 演算法執行步驟如下:
</p>
</div>
<div id="outline-container-orgaea8e15" class="outline-5">
<h5 id="orgaea8e15">1. 決定K值</h5>
<div class="outline-text-5" id="text-orgaea8e15">
<p>
K 值指的是現有訓練資料(八張照片)要分成的群數，此處K值為2。
</p>
</div>
</div>
<div id="outline-container-org0fe602d" class="outline-5">
<h5 id="org0fe602d">2. 選定K個中心點</h5>
<div class="outline-text-5" id="text-org0fe602d">
<p>
任意選定 K 個(K=2)中心點，在實際的程式實作可以亂數隨機產生這K個資料點。如圖<a href="#org943c7e8">7</a>所示，隨機指定的兩群資料點的中心點為(5，5)、(6，9)。
</p>

<div id="org943c7e8" class="figure">
<p><img src="images/kms-2.png" alt="kms-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 7: </span>標題</p>
</div>
</div>
</div>
<div id="outline-container-orgf739143" class="outline-5">
<h5 id="orgf739143">3. 將資料點分群</h5>
<div class="outline-text-5" id="text-orgf739143">
<p>
接下來為所有資料點計算各自與中心點的「歐幾里德距離」，決定該資料點要被歸入哪一個資料群，計算過程及結果如圖<a href="#org392388d">8</a>、<a href="#org4a0f280">9</a>所示。
</p>

<div id="org392388d" class="figure">
<p><img src="images/km-3.png" alt="km-3.png" width="500" />
</p>
<p><span class="figure-number">Figure 8: </span>標題</p>
</div>

<div id="org4a0f280" class="figure">
<p><img src="images/km-4.png" alt="km-4.png" width="500" />
</p>
<p><span class="figure-number">Figure 9: </span>標題</p>
</div>

<p>
最後將計算結果記錄如下圖，進行第一輸分群：
</p>



<div id="org10dfc2b" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_15-59-29_2024-02-10_21-14-09.png" alt="2024-02-11_15-59-29_2024-02-10_21-14-09.png" width="500" />
</p>
<p><span class="figure-number">Figure 10: </span>標題</p>
</div>
</div>
</div>
<div id="outline-container-orgb475886" class="outline-5">
<h5 id="orgb475886">4. 為 K 個群裡的資料點找出新中心點</h5>
<div class="outline-text-5" id="text-orgb475886">
<p>
依前一步驟的分類，此 8 張資料點已分為兩群，接下來我們再為這兩群資料點找出各自的新中心點，計算方式如下:
</p>
<ul class="org-ul">
<li>新<span style="color:green;">★X</span>值: 2+3+4+6+7+9 =5.17</li>
<li>新<span style="color:green;">★Y</span>值: 6+5+8+3+6+4 =5.33</li>
<li>新<span style="color:orange;">★X</span>值:1+8=4.50</li>
<li>新<span style="color:orange;">★Y</span>值:9+8=8.50</li>
</ul>

<div id="org51bfdfa" class="figure">
<p><img src="images/km5.png" alt="km5.png" width="500" />
</p>
<p><span class="figure-number">Figure 11: </span>標題</p>
</div>

<p>
這個結果看起來不太合理對吧，至少(4,8)這點應該要歸入<span style="color:orange;">★</span>這組才對。沒關係，因為還沒完成。
</p>
</div>
</div>
<div id="outline-container-org5932b21" class="outline-5">
<h5 id="org5932b21">5. 重覆步驟 (3)、(4) 進行下一輪的分群，直到分群結果不再變化</h5>
<div class="outline-text-5" id="text-org5932b21">
<p>
接下來就繼續計算各點到新中心點<span style="color:green;">★</span>(5.17, 5.33)、<span style="color:orange;">★</span>(4.50, 8.50)的距離、依新的距離重新對資料點進行分群(即步驟3)，再求出新的中心點(即步驟4)，如此重覆不斷進行，直到分群結果不再變動即告完成。
</p>
</div>
</div>
<div id="outline-container-orgb015ce9" class="outline-5">
<h5 id="orgb015ce9">如何訂K值</h5>
<div class="outline-text-5" id="text-orgb015ce9">
<ul class="org-ul">
<li>用K-means演算法需設定「K值」，但難免會面臨難以決定分群數量的狀況。同樣的資料如果要分成3群、4群、5群，就必須做三次不同的操作，而且分群的結果彼此之間不一定有其關聯性。</li>
<li>利用「階層式分群法」透過階層架構的方式，以對特徵距離的分析，將資料層層反覆地進行分裂或聚合，彈性決定群數。</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org467b7c4" class="outline-4">
<h4 id="org467b7c4"><span class="section-number-4">2.1.3.</span> K-Means實作:隨機數字&#xa0;&#xa0;&#xa0;<span class="tag"><span class="sklearn">sklearn</span></span></h4>
<div class="outline-text-4" id="text-2-1-3">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38568;&#27231;&#29983;&#25104;100&#20491;(x, y)</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 3: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">data</span> = {
<span class="linenr"> 6: </span>    <span style="color: #98be65;">'x'</span>: [<span style="color: #da8548; font-weight: bold;">25</span>, <span style="color: #da8548; font-weight: bold;">34</span>, <span style="color: #da8548; font-weight: bold;">22</span>, <span style="color: #da8548; font-weight: bold;">27</span>, <span style="color: #da8548; font-weight: bold;">33</span>, <span style="color: #da8548; font-weight: bold;">33</span>, <span style="color: #da8548; font-weight: bold;">31</span>, <span style="color: #da8548; font-weight: bold;">22</span>, <span style="color: #da8548; font-weight: bold;">35</span>, <span style="color: #da8548; font-weight: bold;">34</span>, <span style="color: #da8548; font-weight: bold;">67</span>, <span style="color: #da8548; font-weight: bold;">54</span>, <span style="color: #da8548; font-weight: bold;">57</span>, <span style="color: #da8548; font-weight: bold;">43</span>, <span style="color: #da8548; font-weight: bold;">50</span>, <span style="color: #da8548; font-weight: bold;">57</span>, <span style="color: #da8548; font-weight: bold;">59</span>, <span style="color: #da8548; font-weight: bold;">52</span>, <span style="color: #da8548; font-weight: bold;">65</span>, <span style="color: #da8548; font-weight: bold;">47</span>, <span style="color: #da8548; font-weight: bold;">49</span>, <span style="color: #da8548; font-weight: bold;">48</span>, <span style="color: #da8548; font-weight: bold;">35</span>, <span style="color: #da8548; font-weight: bold;">33</span>, <span style="color: #da8548; font-weight: bold;">44</span>, <span style="color: #da8548; font-weight: bold;">45</span>, <span style="color: #da8548; font-weight: bold;">38</span>,
<span class="linenr"> 7: </span>          <span style="color: #da8548; font-weight: bold;">43</span>, <span style="color: #da8548; font-weight: bold;">51</span>, <span style="color: #da8548; font-weight: bold;">46</span>],
<span class="linenr"> 8: </span>    <span style="color: #98be65;">'y'</span>: [<span style="color: #da8548; font-weight: bold;">79</span>, <span style="color: #da8548; font-weight: bold;">51</span>, <span style="color: #da8548; font-weight: bold;">53</span>, <span style="color: #da8548; font-weight: bold;">78</span>, <span style="color: #da8548; font-weight: bold;">59</span>, <span style="color: #da8548; font-weight: bold;">74</span>, <span style="color: #da8548; font-weight: bold;">73</span>, <span style="color: #da8548; font-weight: bold;">57</span>, <span style="color: #da8548; font-weight: bold;">69</span>, <span style="color: #da8548; font-weight: bold;">75</span>, <span style="color: #da8548; font-weight: bold;">51</span>, <span style="color: #da8548; font-weight: bold;">32</span>, <span style="color: #da8548; font-weight: bold;">40</span>, <span style="color: #da8548; font-weight: bold;">47</span>, <span style="color: #da8548; font-weight: bold;">53</span>, <span style="color: #da8548; font-weight: bold;">36</span>, <span style="color: #da8548; font-weight: bold;">35</span>, <span style="color: #da8548; font-weight: bold;">58</span>, <span style="color: #da8548; font-weight: bold;">59</span>, <span style="color: #da8548; font-weight: bold;">50</span>, <span style="color: #da8548; font-weight: bold;">25</span>, <span style="color: #da8548; font-weight: bold;">20</span>, <span style="color: #da8548; font-weight: bold;">14</span>, <span style="color: #da8548; font-weight: bold;">12</span>, <span style="color: #da8548; font-weight: bold;">20</span>, <span style="color: #da8548; font-weight: bold;">5</span>, <span style="color: #da8548; font-weight: bold;">29</span>, <span style="color: #da8548; font-weight: bold;">27</span>,
<span class="linenr"> 9: </span>          <span style="color: #da8548; font-weight: bold;">8</span>, <span style="color: #da8548; font-weight: bold;">7</span>]
<span class="linenr">10: </span>    }
<span class="linenr">11: </span><span style="color: #dcaeea;">samples</span> = pd.DataFrame(data)
<span class="linenr">12: </span>plt.scatter(samples[<span style="color: #98be65;">'x'</span>], samples[<span style="color: #98be65;">'y'</span>])
<span class="linenr">13: </span>plt.savefig(<span style="color: #98be65;">"images/kmeansScatterData.png"</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">14: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
</pre>
</div>


<div id="org003672c" class="figure">
<p><img src="images/kmeansScatterData.png" alt="kmeansScatterData.png" width="500" />
</p>
<p><span class="figure-number">Figure 12: </span>原始資料</p>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38568;&#27231;&#29983;&#25104;100&#20491;(x, y)</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 3: </span><span style="color: #dcaeea;">data</span> = {
<span class="linenr"> 4: </span>    <span style="color: #98be65;">'x'</span>: [<span style="color: #da8548; font-weight: bold;">25</span>, <span style="color: #da8548; font-weight: bold;">34</span>, <span style="color: #da8548; font-weight: bold;">22</span>, <span style="color: #da8548; font-weight: bold;">27</span>, <span style="color: #da8548; font-weight: bold;">33</span>, <span style="color: #da8548; font-weight: bold;">33</span>, <span style="color: #da8548; font-weight: bold;">31</span>, <span style="color: #da8548; font-weight: bold;">22</span>, <span style="color: #da8548; font-weight: bold;">35</span>, <span style="color: #da8548; font-weight: bold;">34</span>, <span style="color: #da8548; font-weight: bold;">67</span>, <span style="color: #da8548; font-weight: bold;">54</span>, <span style="color: #da8548; font-weight: bold;">57</span>, <span style="color: #da8548; font-weight: bold;">43</span>, <span style="color: #da8548; font-weight: bold;">50</span>, <span style="color: #da8548; font-weight: bold;">57</span>, <span style="color: #da8548; font-weight: bold;">59</span>, <span style="color: #da8548; font-weight: bold;">52</span>, <span style="color: #da8548; font-weight: bold;">65</span>, <span style="color: #da8548; font-weight: bold;">47</span>, <span style="color: #da8548; font-weight: bold;">49</span>, <span style="color: #da8548; font-weight: bold;">48</span>, <span style="color: #da8548; font-weight: bold;">35</span>, <span style="color: #da8548; font-weight: bold;">33</span>, <span style="color: #da8548; font-weight: bold;">44</span>, <span style="color: #da8548; font-weight: bold;">45</span>, <span style="color: #da8548; font-weight: bold;">38</span>,
<span class="linenr"> 5: </span>          <span style="color: #da8548; font-weight: bold;">43</span>, <span style="color: #da8548; font-weight: bold;">51</span>, <span style="color: #da8548; font-weight: bold;">46</span>],
<span class="linenr"> 6: </span>    <span style="color: #98be65;">'y'</span>: [<span style="color: #da8548; font-weight: bold;">79</span>, <span style="color: #da8548; font-weight: bold;">51</span>, <span style="color: #da8548; font-weight: bold;">53</span>, <span style="color: #da8548; font-weight: bold;">78</span>, <span style="color: #da8548; font-weight: bold;">59</span>, <span style="color: #da8548; font-weight: bold;">74</span>, <span style="color: #da8548; font-weight: bold;">73</span>, <span style="color: #da8548; font-weight: bold;">57</span>, <span style="color: #da8548; font-weight: bold;">69</span>, <span style="color: #da8548; font-weight: bold;">75</span>, <span style="color: #da8548; font-weight: bold;">51</span>, <span style="color: #da8548; font-weight: bold;">32</span>, <span style="color: #da8548; font-weight: bold;">40</span>, <span style="color: #da8548; font-weight: bold;">47</span>, <span style="color: #da8548; font-weight: bold;">53</span>, <span style="color: #da8548; font-weight: bold;">36</span>, <span style="color: #da8548; font-weight: bold;">35</span>, <span style="color: #da8548; font-weight: bold;">58</span>, <span style="color: #da8548; font-weight: bold;">59</span>, <span style="color: #da8548; font-weight: bold;">50</span>, <span style="color: #da8548; font-weight: bold;">25</span>, <span style="color: #da8548; font-weight: bold;">20</span>, <span style="color: #da8548; font-weight: bold;">14</span>, <span style="color: #da8548; font-weight: bold;">12</span>, <span style="color: #da8548; font-weight: bold;">20</span>, <span style="color: #da8548; font-weight: bold;">5</span>, <span style="color: #da8548; font-weight: bold;">29</span>, <span style="color: #da8548; font-weight: bold;">27</span>,
<span class="linenr"> 7: </span>          <span style="color: #da8548; font-weight: bold;">8</span>, <span style="color: #da8548; font-weight: bold;">7</span>]
<span class="linenr"> 8: </span>    }
<span class="linenr"> 9: </span><span style="color: #dcaeea;">samples</span> = pd.DataFrame(data)
<span class="linenr">10: </span>
<span class="linenr">11: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">12: </span><span style="color: #51afef;">from</span> sklearn.cluster <span style="color: #51afef;">import</span> KMeans
<span class="linenr">13: </span>
<span class="linenr">14: </span><span style="color: #dcaeea;">kmeans</span> = KMeans(n_clusters=<span style="color: #da8548; font-weight: bold;">3</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38928;&#35336;&#20998;&#28858;&#19977;&#32676;&#65292;&#36845;&#20195;&#27425;&#25976;&#30001;&#27169;&#22411;&#33258;&#34892;&#23450;&#32681;</span>
<span class="linenr">15: </span>kmeans.fit(samples)
<span class="linenr">16: </span><span style="color: #dcaeea;">cluster</span> = kmeans.predict(samples)
<span class="linenr">17: </span>
<span class="linenr">18: </span>plt.scatter(samples[<span style="color: #98be65;">'x'</span>], samples[<span style="color: #98be65;">'y'</span>], c=cluster, cmap=plt.cm.Set1)
<span class="linenr">19: </span>plt.savefig(<span style="color: #98be65;">"images/kmeansScatter.png"</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">20: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
</pre>
</div>


<div id="org293ce78" class="figure">
<p><img src="images/kmeansScatter.png" alt="kmeansScatter.png" width="500" />
</p>
<p><span class="figure-number">Figure 13: </span>scikit-KMeans</p>
</div>
</div>
</div>
<div id="outline-container-orgd30bfd5" class="outline-4">
<h4 id="orgd30bfd5"><span class="section-number-4">2.1.4.</span> 評估效能</h4>
<div class="outline-text-4" id="text-2-1-4">
<p>
在 K-Means 這種分群（非監督式學習）演算法中，沒有像監督式學習那樣的 score 或 R² 直接用來衡量模型的準確性，因為分群並沒有「正確答案」來比較。但可以使用其他的效能指標來評估分群的質量，以下是常用的幾個評估指標：
</p>
</div>
<div id="outline-container-org0c9d455" class="outline-5">
<h5 id="org0c9d455">1. Inertia（慣性）</h5>
<div class="outline-text-5" id="text-org0c9d455">
<ul class="org-ul">
<li>定義：慣性是 K-Means 內建的一個效能指標，表示資料點與其最近的群集中心點之間的平方和。它衡量了分群內的緊密度。</li>
<li>公式：每個資料點到其所屬群集中心的距離平方和的總和。</li>
<li>解讀：慣性值越小，表示分群內的資料點越接近彼此，模型表現越好。</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Inertia:'</span>, kmeans.inertia_)
</pre>
</div>

<pre class="example">
Inertia: 6647.977232845085
</pre>


<div id="orgdcbdfe8" class="figure">
<p><img src="images/k-inertia.png" alt="k-inertia.png" width="500" />
</p>
<p><span class="figure-number">Figure 14: </span>K 值與慣性值的關係</p>
</div>
</div>
</div>
<div id="outline-container-orgd52ab96" class="outline-5">
<h5 id="orgd52ab96">2. Silhouette Score（輪廓係數）</h5>
<div class="outline-text-5" id="text-orgd52ab96">
<ul class="org-ul">
<li>定義：Silhouette Score 衡量每個資料點與其所屬群集中的其他點的距離，以及與最近群集的距離。它同時考慮了群集內的緊密度和群集間的分離度。</li>
<li>範圍：Silhouette Score 的範圍是 -1 到 1，分數越接近 1，表示分群效果越好；接近 0 表示資料點處於兩個群集之間；負數表示資料點可能被分錯了群。</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> silhouette_score
<span class="linenr">2: </span>
<span class="linenr">3: </span><span style="color: #dcaeea;">score</span> = silhouette_score(samples, cluster)
<span class="linenr">4: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Silhouette Score:'</span>, score)
</pre>
</div>

<pre class="example">
Silhouette Score: 0.5107383117230143
</pre>
</div>
</div>
<div id="outline-container-org4592a4e" class="outline-5">
<h5 id="org4592a4e">3. Davies-Bouldin Index</h5>
<div class="outline-text-5" id="text-org4592a4e">
<ul class="org-ul">
<li>定義：這個指標衡量每個群集的緊密度與其他群集之間的分離度。值越小表示分群效果越好。</li>
<li>範圍：0 到無窮大，分數越小，表示群集之間的距離越大，分群效果越好。</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> davies_bouldin_score
<span class="linenr">2: </span>
<span class="linenr">3: </span><span style="color: #dcaeea;">db_score</span> = davies_bouldin_score(samples, cluster)
<span class="linenr">4: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Davies-Bouldin Index:'</span>, db_score)
</pre>
</div>

<pre class="example">
Davies-Bouldin Index: 0.6690027647535013
</pre>
</div>
</div>
</div>
<div id="outline-container-NS-KM-Image" class="outline-4">
<h4 id="NS-KM-Image"><span class="section-number-4">2.1.5.</span> K-Means應用: 壓縮影像</h4>
<div class="outline-text-4" id="text-NS-KM-Image">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#36664;&#20986;&#35498;&#26126;</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 4: </span><span style="color: #51afef;">from</span> matplotlib <span style="color: #51afef;">import</span> image
<span class="linenr"> 5: </span><span style="color: #51afef;">from</span> sklearn.cluster <span style="color: #51afef;">import</span> MiniBatchKMeans
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">K &#20540; (&#35201;&#20445;&#30041;&#30340;&#38991;&#33394;&#25976;&#37327;)</span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">K</span> = <span style="color: #da8548; font-weight: bold;">4</span>
<span class="linenr"> 9: </span>
<span class="linenr">10: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#21462;&#22294;&#29255;</span>
<span class="linenr">11: </span><span style="color: #dcaeea;">image</span> = image.imread(r<span style="color: #98be65;">'./images/Photo42.jpg'</span>)
<span class="linenr">12: </span><span style="color: #dcaeea;">w</span>, <span style="color: #dcaeea;">h</span>, <span style="color: #dcaeea;">d</span> = <span style="color: #c678dd;">tuple</span>(image.shape)
<span class="linenr">13: </span><span style="color: #c678dd;">print</span>(w,h,d)
<span class="linenr">14: </span><span style="color: #dcaeea;">image_data</span> = np.reshape(image, (w * h, d))/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">15: </span><span style="color: #c678dd;">print</span>(image_data.shape)
<span class="linenr">16: </span><span style="color: #c678dd;">print</span>(image_data[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">17: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#38991;&#33394;&#20998;&#39006;&#28858; K &#31278;</span>
<span class="linenr">18: </span><span style="color: #dcaeea;">kmeans</span> = MiniBatchKMeans(n_clusters=K, batch_size=<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">19: </span><span style="color: #dcaeea;">labels</span> = kmeans.fit_predict(image_data)
<span class="linenr">20: </span><span style="color: #c678dd;">print</span>(labels[:<span style="color: #da8548; font-weight: bold;">10</span>])
<span class="linenr">21: </span><span style="color: #dcaeea;">centers</span> = kmeans.cluster_centers_
<span class="linenr">22: </span><span style="color: #c678dd;">print</span>(centers.shape)
<span class="linenr">23: </span><span style="color: #c678dd;">print</span>(centers) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">K&#20491;&#20013;&#24515;&#40670;&#30340;RGB</span>
<span class="linenr">24: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26681;&#25818;&#20998;&#39006;&#23559;&#38991;&#33394;&#23531;&#20837;&#26032;&#30340;&#24433;&#20687;&#38499;&#21015;</span>
<span class="linenr">25: </span><span style="color: #dcaeea;">image_compressed</span> = np.zeros(image.shape)
<span class="linenr">26: </span><span style="color: #dcaeea;">label_idx</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">27: </span><span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(w):
<span class="linenr">28: </span>  <span style="color: #51afef;">for</span> j <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(h):
<span class="linenr">29: </span>    image_compressed[i][j] = centers[labels[label_idx]]
<span class="linenr">30: </span>    <span style="color: #dcaeea;">label_idx</span> += <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">31: </span>
<span class="linenr">32: </span>plt.imsave(r<span style="color: #98be65;">'images/compressTest.jpg'</span>, image_compressed)
</pre>
</div>

<pre class="example">
480 640 3
(307200, 3)
[0.20784314 0.16078431 0.23921569]
[0 3 3 3 3 0 3 3 3 3]
(4, 3)
[[0.13623196 0.08966844 0.14825708]
 [0.59140873 0.34351877 0.33948652]
 [0.86889281 0.7085929  0.73685242]
 [0.36261776 0.22364291 0.2474113 ]]
</pre>


<div id="org847a023" class="figure">
<p><img src="images/compressTest.jpg" alt="compressTest.jpg" width="500" />
</p>
<p><span class="figure-number">Figure 15: </span>以KMeans壓縮圖片色彩</p>
</div>
</div>
</div>
<div id="outline-container-org526caa6" class="outline-4">
<h4 id="org526caa6"><span class="section-number-4">2.1.6.</span> [小組作業]K-Means分群實作&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></h4>
<div class="outline-text-4" id="text-2-1-6">
<p>
以K-Means對鳶尾花資料(特徵值)進行分群
</p>

<p>
作業內容須包含:
</p>
<ul class="org-ul">
<li>程式碼</li>
<li>以不同特徵值(\(C^4_2\))配對進行cluster，畫出scatter</li>
<li>以不同特徵值(\(C^4_3\))配對進行cluster，畫出3D scatter</li>
<li>對於輸出之結果應輔以文字說明解釋。</li>
<li>以pdf繳交報告，報告首頁需列出組員列表(姓名、教學網ID)</li>
</ul>
</div>
</div>
<div id="outline-container-org346e519" class="outline-4">
<h4 id="org346e519"><span class="section-number-4">2.1.7.</span> [小組作業]以K-Means壓縮影像實作&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></h4>
<div class="outline-text-4" id="text-2-1-7">
<p>
參考前述[K-Means應用: 壓縮影像]，自行找一張圖(jpg)進行以下測試
</p>
<ul class="org-ul">
<li>以不同K值、batchSize進行影像壓縮，並探討在不同情況下的壓縮效果(包含影像大小及品質)</li>
<li>以不同類型(顏色數量:全彩、256色、灰階)的圖片進行測試</li>
<li>對於輸出之結果應輔以文字說明解釋。</li>
<li>以pdf繳交報告，報告首頁需列出組員列表(姓名、教學網ID)</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-NS-Hie-cluster" class="outline-3">
<h3 id="NS-Hie-cluster"><span class="section-number-3">2.2.</span> Hierarchical clustering</h3>
<div class="outline-text-3" id="text-NS-Hie-cluster">
<p>
階層式分群法(Hierarchical Clustering)透過一種階層架構的方式，將資料層層反覆地進行分裂或聚合，以產生最後的樹狀結構，常見的方式有兩種：
</p>
<ul class="org-ul">
<li><p>
聚合式階層分群法(Agglomerative Clustering): 是一種“bottom-up”的方法，也就是先準備好解決問題可能所需的基本元件或方案，再將這些基本元件組合起來，由小而大最後得到整體。因此在階層式分群法中，就是將每個資料點都視為一個個體，再一一聚合<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>，如圖<a href="#org96f5617">16</a><sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup>。
</p>

<div id="org96f5617" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_21-29-19_2024-02-11_21-28-47.png" alt="2024-02-11_21-29-19_2024-02-11_21-28-47.png" width="500" />
</p>
<p><span class="figure-number">Figure 16: </span>Buttom-up</p>
</div></li>
<li><p>
分裂式階層分群法(Divisive Clustering): 是一種“top-down”的方法，先對問題有整體的概念，然後再逐步加上細節，最後讓整體的輪廓越來越清楚。而此法在階層式分群法中，先將整個資料集視為一體，再一一的分裂<sup><a id="fnr.2.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>，如圖<a href="#org62619d5">17</a><sup><a id="fnr.3.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup>。
</p>

<div id="org62619d5" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_21-30-15_2024-02-11_21-30-06.png" alt="2024-02-11_21-30-15_2024-02-11_21-30-06.png" width="500" />
</p>
<p><span class="figure-number">Figure 17: </span>Top-down</p>
</div></li>
</ul>
</div>
<div id="outline-container-org3bb9aad" class="outline-4">
<h4 id="org3bb9aad"><span class="section-number-4">2.2.1.</span> 聚合式階層分群法(Agglomerative)</h4>
<div class="outline-text-4" id="text-2-2-1">
<p>
如果採用聚合的方式，階層式分群法可由樹狀結構的底部開始，將資料或群聚逐次合併。
</p>

<p>
聚合式階層分群步驟：
</p>
<ol class="org-ol">
<li>將各個資料點先視為個別的「群」。</li>
<li>比較各個群之間的距離，找出距離最短的兩個群。</li>
<li>將其合併變成一個新群。</li>
<li>不斷重複直到群的數量符合所要求的數目。</li>
</ol>
</div>
<div id="outline-container-orgfc5bb92" class="outline-5">
<h5 id="orgfc5bb92">聚合式階層分群: step by step</h5>
<div class="outline-text-5" id="text-orgfc5bb92">
<ol class="org-ol">
<li><p>
假設現在有6筆資料，分別標記A、B、C、D、E及F，且每筆資料都是一個群。
</p>

<div id="org47309fc" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_09-04-56_2024-02-11_09-04-42.png" alt="2024-02-11_09-04-56_2024-02-11_09-04-42.png" width="500" />
</p>
<p><span class="figure-number">Figure 18: </span>hierar-1</p>
</div></li>
<li><p>
首先找距離最近的兩個群，在此例為A、B。將A與B結合為新的一群G1，就將這些點分成五群了，其中有四群還是單獨的點。
</p>

<div id="orgc8b970d" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_09-06-09_2024-02-11_09-06-00.png" alt="2024-02-11_09-06-09_2024-02-11_09-06-00.png" width="500" />
</p>
<p><span class="figure-number">Figure 19: </span>標題</p>
</div></li>
<li><p>
接著，再繼續找距離最近的兩個群，依此範例應為D與E，結合為新的一群G2。
</p>

<div id="org9a2d72b" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_09-06-59_2024-02-11_09-06-54.png" alt="2024-02-11_09-06-59_2024-02-11_09-06-54.png" width="500" />
</p>
<p><span class="figure-number">Figure 20: </span>標題</p>
</div></li>
<li><p>
將F與G2合而為新的群G3，這時，這些資料已經被分為三群了。
</p>

<div id="orge138df1" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_09-18-01_2024-02-11_09-07-48.png" alt="2024-02-11_09-18-01_2024-02-11_09-07-48.png" width="500" />
</p>
<p><span class="figure-number">Figure 21: </span>標題</p>
</div></li>
</ol>
</div>
</div>
<div id="outline-container-org3109192" class="outline-5">
<h5 id="org3109192">如何定義兩個群聚之間的距離</h5>
<div class="outline-text-5" id="text-org3109192">
</div>
<ul class="org-ul">
<li><a id="org5174e1c"></a>單一連結聚合<br />
<div class="outline-text-6" id="text-org5174e1c">
<p>
Single-linkage agglomerative algorithm, 群聚與群聚間的距離可以定義為不同群聚中最接近兩點間的距離。
</p>

<p>
在分屬不同的兩群中，選擇最接近的兩點之距離，即代表兩群間的距離。因此在群與群間進行聚合時，依據此值最小者做為選取下一步結合之對象。
</p>


<div id="org9ddcb4a" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_10-13-45_2024-02-11_09-23-14.png" alt="2024-02-11_10-13-45_2024-02-11_09-23-14.png" width="300" />
</p>
<p><span class="figure-number">Figure 22: </span>標題</p>
</div>

<p>
公式: \( d(G1, G2)=\min\limits_{ A \in G1, B \in G2 }  d(A,B)\)
</p>

<p>
G1、G3與C之間如何聚合？
</p>
<ul class="org-ul">
<li>G1與C之間的距離d(G1,C)＝d(B,C)</li>
<li>G3與C之間的距離d(G3,C)＝d(F,C)</li>
<li>G1與G3之間的距離d(G1,G3)＝d(B,D)</li>
</ul>

<p>
計算完各群間的距離後，可知d(G3,C)為最短距離，因此G3將與C聚合，成為新群G4。
</p>


<div id="org3351bdf" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_10-16-41_2024-02-11_10-16-28.png" alt="2024-02-11_10-16-41_2024-02-11_10-16-28.png" width="500" />
</p>
<p><span class="figure-number">Figure 23: </span>標題</p>
</div>

<p>
倘若要再聚合，由於剩下G1與G4，可聚合成為G5。
</p>


<div id="orgb504229" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_10-17-37_2024-02-11_10-17-28.png" alt="2024-02-11_10-17-37_2024-02-11_10-17-28.png" width="500" />
</p>
<p><span class="figure-number">Figure 24: </span>標題</p>
</div>
</div>
</li>
<li><a id="org08e98eb"></a>完整連結聚合<br />
<div class="outline-text-6" id="text-org08e98eb">
<p>
Complete-linkage agglomerative algorithm, 群聚間的距離定義為不同群聚中最遠兩點間的距離，這樣可以保證這兩個集合合併後, 任何一對的距離不會大於 d。
</p>

<p>
在分屬不同的兩群中，選擇最遠的兩點之距離，即代表兩群間的距離。因此在群與群間進行聚合時，依據此值最小者做為選取下一步結合之對象。
</p>

<p>
公式: \(d(G1,G2)=\max\limits_{A \in G1, B \in G2}d(A,B)\)
</p>


<div id="orgac4a8e3" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_10-18-31_2024-02-11_10-18-24.png" alt="2024-02-11_10-18-31_2024-02-11_10-18-24.png" width="300" />
</p>
<p><span class="figure-number">Figure 25: </span>標題</p>
</div>

<p>
G1、G3與C之間如何聚合？
</p>
<ul class="org-ul">
<li>G1與C之間的距離d(G1,C)＝d(A,C)</li>
<li>G3與C之間的距離d(G3,C)＝d(E,C)</li>
<li>G1與G3之間的距離d(G1,G3)＝d(A,E)</li>
</ul>


<div id="org3dc3706" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_10-20-11_2024-02-11_10-20-03.png" alt="2024-02-11_10-20-11_2024-02-11_10-20-03.png" width="500" />
</p>
<p><span class="figure-number">Figure 26: </span>標題</p>
</div>


<p>
計算完各群間的距離後，可知d(G1,C)為最短距離，因此G1將與C聚合，成為新群G4。
</p>

<p>
倘若要再聚合，由於剩下G3與G4，可聚合成為G5。
</p>


<div id="orgacfbb3c" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_10-21-08_2024-02-11_10-21-02.png" alt="2024-02-11_10-21-08_2024-02-11_10-21-02.png" width="500" />
</p>
<p><span class="figure-number">Figure 27: </span>標題</p>
</div>
</div>
</li>
<li><a id="org8f46448"></a>平均連結聚合<br />
<div class="outline-text-6" id="text-org8f46448">
<p>
Average-linkage agglomerative algorithm, 群聚間的距離定義為不同群聚間各點與各點間距離總和的平均。沃德法（Ward&rsquo;s method）：群聚間的距離定義為在將兩群合併後，各點到合併後的群中心的距離平方和。
</p>

<p>
在分屬不同的兩群中，各點之距離的平均，即代表兩群間的距離。因此在群與群間進行聚合時，依據此值最小者做為選取下一步結合之對象。
G1、G3與C之間如何聚合？
</p>


<div id="org4a0199d" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_10-24-58_2024-02-11_10-24-49.png" alt="2024-02-11_10-24-58_2024-02-11_10-24-49.png" width="300" />
</p>
<p><span class="figure-number">Figure 28: </span>標題</p>
</div>

<p>
公式: \(d(G1,G2)=\sum\limits_{A \in G1, B \in G2}\frac{d(A,B)}{|G1|\times|G2|}\)
</p>

<ul class="org-ul">
<li>\( d(G1, C)=\frac{d(A,C)+d(B,C)}{2\times1}\)</li>
<li>\( d(G3, C)=\frac{d(D,C)+d(E,C)+d(F,C)}{3\times1}\)</li>
<li>\( d(G1, G3)=\frac{d(A,D)+d(A,E)+d(A,F)+d(B,D)+d(B,E)+d(B,F)}{2\times3}\)</li>
</ul>
</div>
</li>
</ul>
</div>
<div id="outline-container-org85b81ef" class="outline-5">
<h5 id="org85b81ef">決定群數</h5>
<div class="outline-text-5" id="text-org85b81ef">
<p>
可以依照使用者的群數需求或相似度要求，來決定要在哪一層時停止聚合資料。若以完整連結的群間距離計算方式為例，圖上的虛線代表不同的群數，端看使用者需求來決定。
</p>

<div id="orgc6f3296" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_10-47-21_2024-02-11_10-47-15.png" alt="2024-02-11_10-47-21_2024-02-11_10-47-15.png" width="500" />
</p>
<p><span class="figure-number">Figure 29: </span>標題</p>
</div>
</div>
</div>
<div id="outline-container-org504e727" class="outline-5">
<h5 id="org504e727">聚合式階層分群實作</h5>
<div class="outline-text-5" id="text-org504e727">
</div>
<ul class="org-ul">
<li><a id="org49b324b"></a>scikit-learn: Agglomerative Clustering<br />
<ul class="org-ul">
<li><a id="org785e738"></a>分兩群<br />
<div class="outline-text-7" id="text-org785e738">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> sklearn.cluster <span style="color: #51afef;">import</span> AgglomerativeClustering
<span class="linenr">2: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">3: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">4: </span>
<span class="linenr">5: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">randomly chosen dataset</span>
<span class="linenr">6: </span><span style="color: #dcaeea;">X</span> = np.array([[<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">2</span>], [<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">4</span>], [<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">0</span>], [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">1</span>], [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>], [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">4</span>],
<span class="linenr">7: </span>              [<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">1</span>], [<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>], [<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">4</span>], [<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">2</span>], [<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">4</span>], [<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">0</span>]])
<span class="linenr">8: </span><span style="color: #dcaeea;">clustering</span> = AgglomerativeClustering(n_clusters = <span style="color: #da8548; font-weight: bold;">2</span>).fit(X)
<span class="linenr">9: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#20998;&#20841;&#32676;:'</span>,clustering.labels_)
</pre>
</div>

<pre class="example">
分兩群: [0 1 0 0 1 1 0 1 1 0 1 0]
</pre>



<div id="org7775fd0" class="figure">
<p><img src="images/aggclu-1.png" alt="aggclu-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 30: </span>分成兩組</p>
</div>
</div>
</li>
<li><a id="orga852214"></a>[課堂練習]列出分群的組員<br />
<div class="outline-text-7" id="text-orga852214">
<p>
上例將這100個點分成兩群，請撰寫程式列出這兩群各包含哪些資料
</p>
</div>
<ul class="org-ul">
<li><a id="orgde306b2"></a>solution&#xa0;&#xa0;&#xa0;<span class="tag"><span class="noexpot">noexpot</span></span><br />
<div class="outline-text-8" id="text-orgde306b2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21015;&#20986;&#27599;&#19968;&#32676;&#26377;&#21738;&#20123;&#36039;&#26009;</span>
<span class="linenr"> 2: </span><span style="color: #dcaeea;">labels</span> = clustering.labels_
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">cluster_0</span> = samples[labels == <span style="color: #da8548; font-weight: bold;">0</span>]
<span class="linenr"> 5: </span><span style="color: #dcaeea;">cluster_1</span> = samples[labels == <span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'</span><span style="color: #a9a1e1;">\n</span><span style="color: #98be65;">&#32676;&#32068; 0 &#30340;&#36039;&#26009;:'</span>)
<span class="linenr"> 8: </span><span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">len</span>(cluster_0))
<span class="linenr"> 9: </span>
<span class="linenr">10: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'</span><span style="color: #a9a1e1;">\n</span><span style="color: #98be65;">&#32676;&#32068; 1 &#30340;&#36039;&#26009;:'</span>)
<span class="linenr">11: </span><span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">len</span>(cluster_1))
<span class="linenr">12: </span>
</pre>
</div>
</div>
</li>
</ul>
</li>
<li><a id="orgf5115a5"></a>分三群<br />
<div class="outline-text-7" id="text-orgf5115a5">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">clustering</span> = AgglomerativeClustering(n_clusters = <span style="color: #da8548; font-weight: bold;">3</span>).fit(X)
<span class="linenr">2: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#20998;&#19977;&#32676;:'</span>,clustering.labels_)
</pre>
</div>

<pre class="example">
分三群: [1 0 1 1 0 0 1 0 0 2 0 2]
</pre>



<div id="org3d5fe75" class="figure">
<p><img src="images/aggclu-2.png" alt="aggclu-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 31: </span>分成三群</p>
</div>
</div>
</li>
<li><a id="org5f47d88"></a>分四群<br />
<div class="outline-text-7" id="text-org5f47d88">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">clustering</span> = AgglomerativeClustering(n_clusters = <span style="color: #da8548; font-weight: bold;">4</span>).fit(X)
<span class="linenr">2: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#20998;&#22235;&#32676;:'</span>,clustering.labels_)
</pre>
</div>

<pre class="example">
分四群: [0 3 0 0 3 3 0 1 1 2 1 2]
</pre>



<div id="orgaf8835b" class="figure">
<p><img src="images/aggclu-3.png" alt="aggclu-3.png" width="500" />
</p>
<p><span class="figure-number">Figure 32: </span>分成四群</p>
</div>
</div>
</li>
</ul>
</li>
<li><a id="org3f4b65e"></a>SciPy: scipy.cluster.hierarchy[一次分完]<br />
<div class="outline-text-6" id="text-org3f4b65e">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 3: </span><span style="color: #51afef;">import</span> scipy.cluster.hierarchy <span style="color: #51afef;">as</span> sch
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">randomly chosen dataset</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">X</span> = np.array([[<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">2</span>], [<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">4</span>], [<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">0</span>], [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">1</span>], [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>], [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">4</span>],
<span class="linenr"> 7: </span>              [<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">1</span>], [<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>], [<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">4</span>], [<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">2</span>], [<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">4</span>], [<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">0</span>]])
<span class="linenr"> 8: </span><span style="color: #dcaeea;">y</span> = np.array([<span style="color: #98be65;">'A'</span>, <span style="color: #98be65;">'B'</span>, <span style="color: #98be65;">'C'</span>, <span style="color: #98be65;">'D'</span>, <span style="color: #98be65;">'E'</span>, <span style="color: #98be65;">'F'</span>, <span style="color: #98be65;">'G'</span>, <span style="color: #98be65;">'H'</span>, <span style="color: #98be65;">'I'</span>, <span style="color: #98be65;">'J'</span>, <span style="color: #98be65;">'K'</span>, <span style="color: #98be65;">'L'</span>])
<span class="linenr"> 9: </span>
<span class="linenr">10: </span><span style="color: #dcaeea;">dis</span>=sch.linkage(X,metric=<span style="color: #98be65;">'euclidean'</span>, method=<span style="color: #98be65;">'ward'</span>)
<span class="linenr">11: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">metric: &#36317;&#38626;&#30340;&#35336;&#31639;&#26041;&#24335;</span>
<span class="linenr">12: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">method: &#32676;&#33287;&#32676;&#20043;&#38291;&#30340;&#35336;&#31639;&#26041;&#24335;&#65292;&#8221;single&#8221;, &#8220;complete&#8221;, &#8220;average&#8221;,</span>
<span class="linenr">13: </span><span style="color: #5B6268;">#                      </span><span style="color: #5B6268;">&#8220;weighted&#8221;, &#8220;centroid&#8221;, &#8220;median&#8221;, &#8220;ward&#8221;</span>
<span class="linenr">14: </span>
<span class="linenr">15: </span>sch.dendrogram(dis, labels = y)
<span class="linenr">16: </span>
<span class="linenr">17: </span>plt.title(<span style="color: #98be65;">'Hierarchical Clustering'</span>)
<span class="linenr">18: </span>plt.xticks(rotation=<span style="color: #da8548; font-weight: bold;">30</span>)
<span class="linenr">19: </span>plt.savefig(<span style="color: #98be65;">"images/hierarCluster-1.png"</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">20: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
</pre>
</div>

<div id="org9b5afec" class="figure">
<p><img src="images/hierarCluster-1.png" alt="hierarCluster-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 33: </span>Hierarchical Clustering</p>
</div>
</div>
</li>
<li><a id="org6f8f238"></a>SciPy: scipy.cluster.hierarchy[逐步分群]<br />
<div class="outline-text-6" id="text-org6f8f238">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 3: </span><span style="color: #51afef;">import</span> scipy.cluster.hierarchy <span style="color: #51afef;">as</span> sch
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">randomly chosen dataset</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">X</span> = np.array([[<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">2</span>], [<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">4</span>], [<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">0</span>], [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">1</span>], [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>], [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">4</span>],
<span class="linenr"> 7: </span>              [<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">1</span>], [<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>], [<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">4</span>], [<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">2</span>], [<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">4</span>], [<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">0</span>]])
<span class="linenr"> 8: </span><span style="color: #dcaeea;">y</span> = np.array([<span style="color: #98be65;">'A'</span>, <span style="color: #98be65;">'B'</span>, <span style="color: #98be65;">'C'</span>, <span style="color: #98be65;">'D'</span>, <span style="color: #98be65;">'E'</span>, <span style="color: #98be65;">'F'</span>, <span style="color: #98be65;">'G'</span>, <span style="color: #98be65;">'H'</span>, <span style="color: #98be65;">'I'</span>, <span style="color: #98be65;">'J'</span>, <span style="color: #98be65;">'K'</span>, <span style="color: #98be65;">'L'</span>])
<span class="linenr"> 9: </span>
<span class="linenr">10: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">metric: &#36317;&#38626;&#30340;&#35336;&#31639;&#26041;&#24335;</span>
<span class="linenr">11: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">method: &#32676;&#33287;&#32676;&#20043;&#38291;&#30340;&#35336;&#31639;&#26041;&#24335;&#65292;&#8221;single&#8221;, &#8220;complete&#8221;, &#8220;average&#8221;, &#8220;weighted&#8221;, &#8220;centroid&#8221;, &#8220;median&#8221;, &#8220;ward&#8221;</span>
<span class="linenr">12: </span>
<span class="linenr">13: </span>plt.cla()
<span class="linenr">14: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Setting the truncate_mode to 'lastp' to see incremental clustering</span>
<span class="linenr">15: </span>plt.figure(figsize=(<span style="color: #da8548; font-weight: bold;">10</span>, <span style="color: #da8548; font-weight: bold;">20</span>))
<span class="linenr">16: </span><span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #c678dd;">len</span>(y) + <span style="color: #da8548; font-weight: bold;">1</span>):
<span class="linenr">17: </span>    plt.subplot( <span style="color: #da8548; font-weight: bold;">6</span>, <span style="color: #da8548; font-weight: bold;">2</span>, i - <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">18: </span>    <span style="color: #dcaeea;">labels</span> = y[:i]  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Adjusting labels for each step</span>
<span class="linenr">19: </span>    <span style="color: #dcaeea;">x_step</span> = X[:i]
<span class="linenr">20: </span>    <span style="color: #dcaeea;">dis</span>=sch.linkage(x_step, metric=<span style="color: #98be65;">'euclidean'</span>, method=<span style="color: #98be65;">'ward'</span>)
<span class="linenr">21: </span>    sch.dendrogram(dis, labels=labels, truncate_mode=<span style="color: #98be65;">'lastp'</span>, p=i)
<span class="linenr">22: </span>    plt.title(f<span style="color: #98be65;">'Step </span>{i}<span style="color: #98be65;">'</span>)
<span class="linenr">23: </span>
<span class="linenr">24: </span>plt.suptitle(<span style="color: #98be65;">'Hierarchical Clustering Steps'</span>)
<span class="linenr">25: </span>plt.tight_layout(rect=[<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0.03</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">0.95</span>])
<span class="linenr">26: </span>
<span class="linenr">27: </span>plt.title(<span style="color: #98be65;">'Hierarchical Clustering'</span>)
<span class="linenr">28: </span>plt.xticks(rotation=<span style="color: #da8548; font-weight: bold;">30</span>)
<span class="linenr">29: </span>plt.savefig(<span style="color: #98be65;">"images/hierarCluster-2.png"</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">30: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
</pre>
</div>

<div id="org9dd6ad4" class="figure">
<p><img src="images/hierarCluster-2.png" alt="hierarCluster-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 34: </span>Hierarchical Clustering</p>
</div>
</div>
</li>
<li><a id="orgecd2bef"></a>利用距離決定群數，或直接給定群數<br />
<div class="outline-text-6" id="text-orgecd2bef">
<p>
建構好聚落樹狀圖後，我們可以依照距離的切割來進行分類，也可以直接給定想要分類的群數，讓系統自動切割到相對應的距離。
</p>
<ul class="org-ul">
<li>距離切割
所給出的樹狀圖，y軸代表距離，我們可以用特徵之間的距離進行分群的切割。</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">max_dis</span>=<span style="color: #da8548; font-weight: bold;">5</span>
<span class="linenr">2: </span><span style="color: #dcaeea;">clusters</span>=sch.fcluster(dis,max_dis,criterion=<span style="color: #98be65;">'distance'</span>)
<span class="linenr">3: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">4: </span>plt.figure()
<span class="linenr">5: </span>plt.scatter(X[:,<span style="color: #da8548; font-weight: bold;">0</span>], X[:,<span style="color: #da8548; font-weight: bold;">1</span>], c=clusters, cmap=plt.cm.Set1)
<span class="linenr">6: </span>plt.savefig(<span style="color: #98be65;">"images/clusterScatter.png"</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>


<div id="org1a5799a" class="figure">
<p><img src="images/clusterScatter.png" alt="clusterScatter.png" width="500" />
</p>
<p><span class="figure-number">Figure 35: </span>Caption</p>
</div>
<ul class="org-ul">
<li>直接給定群數
同時，我們也可以像sklearn一樣，直接給定我們所想要分出的群數。</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">k</span>=<span style="color: #da8548; font-weight: bold;">4</span>
<span class="linenr">2: </span><span style="color: #dcaeea;">clusters</span>=sch.fcluster(dis,k,criterion=<span style="color: #98be65;">'maxclust'</span>)
<span class="linenr">3: </span>
<span class="linenr">4: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">5: </span>plt.figure()
<span class="linenr">6: </span>plt.scatter(X[:,<span style="color: #da8548; font-weight: bold;">0</span>], X[:,<span style="color: #da8548; font-weight: bold;">1</span>], c=clusters, cmap=plt.cm.Set1)
<span class="linenr">7: </span>plt.savefig(<span style="color: #98be65;">"images/clusterScatter-1.png"</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>


<div id="orgf898a4d" class="figure">
<p><img src="images/clusterScatter-1.png" alt="clusterScatter-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 36: </span>Caption</p>
</div>
</div>
</li>
</ul>
</div>
<div id="outline-container-org9481238" class="outline-5">
<h5 id="org9481238">如何評估最佳分群數:K</h5>
<div class="outline-text-5" id="text-org9481238">
<ul class="org-ul">
<li><a href="https://jimmy-huang.medium.com/kmeans%E5%88%86%E7%BE%A4%E6%BC%94%E7%AE%97%E6%B3%95-%E8%88%87-silhouette-%E8%BC%AA%E5%BB%93%E5%88%86%E6%9E%90-8be17e634589">Kmeans分群演算法 與 Silhouette 輪廓分析</a></li>
<li><a href="https://www.geeksforgeeks.org/implementing-agglomerative-clustering-using-sklearn/">Implementing Agglomerative Clustering using Sklearn</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org9525e90" class="outline-4">
<h4 id="org9525e90"><span class="section-number-4">2.2.2.</span> [課堂任務]聚合式階層分群&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></h4>
<div class="outline-text-4" id="text-2-2-2">
</div>
<div id="outline-container-org6e08c92" class="outline-5">
<h5 id="org6e08c92">資料</h5>
<div class="outline-text-5" id="text-org6e08c92">
<p>
在此給定資料並以數值化座標平面表示，其中包含A、B、C、D、E、F、G及H共8個點。假設B與C點合併為G1；G與H點合併為G2，而G2加入F點後形成G3。
每個資料點有兩個特徵值(如圖<a href="#org16ebb61">37</a>)：
</p>
<ul class="org-ul">
<li>x = np.array([1,2,3,2,5,5,6,7])</li>
<li>y = np.array([4,2,2,6,5,0,1,2])</li>
</ul>

<div id="org16ebb61" class="figure">
<p><img src="images/聚類(集群)/2024-02-14_15-43-38_2024-02-14_15-43-04.png" alt="2024-02-14_15-43-38_2024-02-14_15-43-04.png" width="300" />
</p>
<p><span class="figure-number">Figure 37: </span>資料分佈圖</p>
</div>
</div>
</div>
<div id="outline-container-org44cb8c2" class="outline-5">
<h5 id="org44cb8c2">任務1</h5>
<div class="outline-text-5" id="text-org44cb8c2">
<p>
請利用「單一連結」的群間距離計算方式完成聚合式階層式分群。
</p>

<div id="org32f06fe" class="figure">
<p><img src="images/聚類(集群)/2024-02-14_15-45-04_2024-02-14_15-44-51.png" alt="2024-02-14_15-45-04_2024-02-14_15-44-51.png" width="300" />
</p>
</div>
</div>
<ul class="org-ul">
<li><a id="orgab9c981"></a>Step 1<br />
<div class="outline-text-6" id="text-orgab9c981">

<div id="org7b0d48a" class="figure">
<p><img src="images/聚類(集群)/2024-02-14_16-05-52_2024-02-14_16-05-35.png" alt="2024-02-14_16-05-52_2024-02-14_16-05-35.png" width="300" />
</p>
</div>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">A</th>
<th scope="col" class="org-left">D</th>
<th scope="col" class="org-left">E</th>
<th scope="col" class="org-left">G1</th>
<th scope="col" class="org-left">G3</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">A</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">D</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">E</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">G1</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">G3</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
<p>
註: A可與D或與G1合併，在此選擇將A與G1合併為G4。
</p>
</div>
</li>
<li><a id="org47badb5"></a>Step 2<br />
<div class="outline-text-6" id="text-org47badb5">

<div id="org140b6e3" class="figure">
<p><img src="images/聚類(集群)/2024-02-14_15-51-00_2024-02-14_15-50-44.png" alt="2024-02-14_15-51-00_2024-02-14_15-50-44.png" width="300" />
</p>
</div>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">D</th>
<th scope="col" class="org-left">E</th>
<th scope="col" class="org-left">G3</th>
<th scope="col" class="org-left">G4</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">D</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">E</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">G3</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">G4</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
</div>
</li>
<li><a id="orgc0dc733"></a>Step 3<br />
<div class="outline-text-6" id="text-orgc0dc733">

<div id="orgfd25bfc" class="figure">
<p><img src="images/聚類(集群)/2024-02-14_15-53-20_2024-02-14_15-53-08.png" alt="2024-02-14_15-53-20_2024-02-14_15-53-08.png" width="300" />
</p>
</div>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">E</th>
<th scope="col" class="org-left">G3</th>
<th scope="col" class="org-left">G5</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">E</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">G3</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">G5</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
</div>
</li>
<li><a id="orge0499f1"></a>Step 4<br />
<div class="outline-text-6" id="text-orge0499f1">

<div id="org9cb036d" class="figure">
<p><img src="images/聚類(集群)/2024-02-14_15-54-50_2024-02-14_15-54-36.png" alt="2024-02-14_15-54-50_2024-02-14_15-54-36.png" width="300" />
</p>
</div>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">E</th>
<th scope="col" class="org-left">G6</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">E</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">G6</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
</div>
</li>
</ul>
</div>
<div id="outline-container-org4506a04" class="outline-5">
<h5 id="org4506a04">任務2</h5>
<div class="outline-text-5" id="text-org4506a04">
<p>
請利用「完整連結」的群間距離計算方式完成聚合式階層式分群。
</p>

<div id="orgbd11852" class="figure">
<p><img src="images/聚類(集群)/2024-02-14_15-45-04_2024-02-14_15-44-51.png" alt="2024-02-14_15-45-04_2024-02-14_15-44-51.png" width="300" />
</p>
</div>
</div>
<ul class="org-ul">
<li><a id="org335d71f"></a>Step 1<br />
<div class="outline-text-6" id="text-org335d71f">

<div id="org5e2eb05" class="figure">
<p><img src="images/聚類(集群)/2024-02-14_16-09-10_2024-02-14_16-09-01.png" alt="2024-02-14_16-09-10_2024-02-14_16-09-01.png" width="300" />
</p>
</div>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">A</th>
<th scope="col" class="org-left">D</th>
<th scope="col" class="org-left">E</th>
<th scope="col" class="org-left">G1</th>
<th scope="col" class="org-left">G3</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">A</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">D</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">E</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">G1</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">G3</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
</div>
</li>
<li><a id="org0f553ad"></a>Step 2<br />
<div class="outline-text-6" id="text-org0f553ad">

<div id="org4de4116" class="figure">
<p><img src="images/聚類(集群)/2024-02-14_16-14-30_2024-02-14_16-14-22.png" alt="2024-02-14_16-14-30_2024-02-14_16-14-22.png" width="300" />
</p>
</div>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">E</th>
<th scope="col" class="org-left">G1</th>
<th scope="col" class="org-left">G3</th>
<th scope="col" class="org-left">G4</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">E</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">G1</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">G3</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">G4</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
<p>
註: G4可與E或與G1合併，在此選擇將G4與G1合併為G5。
</p>
</div>
</li>
<li><a id="orge21763f"></a>Step 3<br />
<div class="outline-text-6" id="text-orge21763f">

<div id="org87254ec" class="figure">
<p><img src="images/聚類(集群)/2024-02-14_16-15-28_2024-02-14_16-15-15.png" alt="2024-02-14_16-15-28_2024-02-14_16-15-15.png" width="300" />
</p>
</div>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">E</th>
<th scope="col" class="org-left">G3</th>
<th scope="col" class="org-left">G5</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">E</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">G3</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">G5</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
</div>
</li>
<li><a id="org598e528"></a>Step 4<br />
<div class="outline-text-6" id="text-org598e528">

<div id="orgcd3d34b" class="figure">
<p><img src="images/聚類(集群)/2024-02-14_16-16-33_2024-02-14_16-16-28.png" alt="2024-02-14_16-16-33_2024-02-14_16-16-28.png" width="300" />
</p>
</div>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">G3</th>
<th scope="col" class="org-left">G6</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">G3</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">G6</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
</div>
</li>
</ul>
</div>
<div id="outline-container-orga57ef70" class="outline-5">
<h5 id="orga57ef70">任務3</h5>
<div class="outline-text-5" id="text-orga57ef70">
<p>
請利用「平均連結」的群間距離計算方式完成聚合式階層式分群。
</p>

<div id="org579cb76" class="figure">
<p><img src="images/聚類(集群)/2024-02-14_15-45-04_2024-02-14_15-44-51.png" alt="2024-02-14_15-45-04_2024-02-14_15-44-51.png" width="300" />
</p>
</div>
</div>
<ul class="org-ul">
<li><a id="org4ae413d"></a>Step 1<br />
<div class="outline-text-6" id="text-org4ae413d">

<div id="org847c33c" class="figure">
<p><img src="images/聚類(集群)/2024-02-14_16-18-48_2024-02-14_16-18-39.png" alt="2024-02-14_16-18-48_2024-02-14_16-18-39.png" width="300" />
</p>
</div>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />

<col  class="org-center" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-center">A</th>
<th scope="col" class="org-center">D</th>
<th scope="col" class="org-center">E</th>
<th scope="col" class="org-center">G1</th>
<th scope="col" class="org-center">G3</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">A</td>
<td class="org-center">&#xa0;</td>
<td class="org-center">&#xa0;</td>
<td class="org-center">&#xa0;</td>
<td class="org-center">&#xa0;</td>
<td class="org-center">&#xa0;</td>
</tr>

<tr>
<td class="org-left">D</td>
<td class="org-center">&#xa0;</td>
<td class="org-center">&#xa0;</td>
<td class="org-center">&#xa0;</td>
<td class="org-center">&#xa0;</td>
<td class="org-center">&#xa0;</td>
</tr>

<tr>
<td class="org-left">E</td>
<td class="org-center">&#xa0;</td>
<td class="org-center">&#xa0;</td>
<td class="org-center">&#xa0;</td>
<td class="org-center">&#xa0;</td>
<td class="org-center">&#xa0;</td>
</tr>

<tr>
<td class="org-left">G1</td>
<td class="org-center">&#xa0;</td>
<td class="org-center">&#xa0;</td>
<td class="org-center">&#xa0;</td>
<td class="org-center">&#xa0;</td>
<td class="org-center">&#xa0;</td>
</tr>

<tr>
<td class="org-left">G3</td>
<td class="org-center">&#xa0;</td>
<td class="org-center">&#xa0;</td>
<td class="org-center">&#xa0;</td>
<td class="org-center">&#xa0;</td>
<td class="org-center">&#xa0;</td>
</tr>
</tbody>
</table>
</div>
</li>
<li><a id="org004e596"></a>Step 2<br />
<div class="outline-text-6" id="text-org004e596">

<div id="orgbf5e181" class="figure">
<p><img src="images/聚類(集群)/2024-02-14_16-22-19_2024-02-14_16-22-13.png" alt="2024-02-14_16-22-19_2024-02-14_16-22-13.png" width="300" />
</p>
</div>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">E</th>
<th scope="col" class="org-left">G1</th>
<th scope="col" class="org-left">G3</th>
<th scope="col" class="org-left">G4</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">E</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">G1</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">G3</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">G4</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
</div>
</li>
<li><a id="orgcfa6124"></a>Step 3<br />
<div class="outline-text-6" id="text-orgcfa6124">

<div id="org5b9d9d6" class="figure">
<p><img src="images/聚類(集群)/2024-02-14_16-24-31_2024-02-14_16-24-25.png" alt="2024-02-14_16-24-31_2024-02-14_16-24-25.png" width="300" />
</p>
</div>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">E</th>
<th scope="col" class="org-left">G3</th>
<th scope="col" class="org-left">G5</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">E</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">G3</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">G5</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
</div>
</li>
<li><a id="org7accb39"></a>Step 4<br />
<div class="outline-text-6" id="text-org7accb39">

<div id="orgb15c72e" class="figure">
<p><img src="images/聚類(集群)/2024-02-14_16-25-47_2024-02-14_16-25-40.png" alt="2024-02-14_16-25-47_2024-02-14_16-25-40.png" width="300" />
</p>
</div>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">G3</th>
<th scope="col" class="org-left">G6</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">G3</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">G6</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
</div>
</li>
</ul>
</div>
<div id="outline-container-orgb7b5ce4" class="outline-5">
<h5 id="orgb7b5ce4">任務4</h5>
<div class="outline-text-5" id="text-orgb7b5ce4">
<p>
請以「單一連結」完成之聚合式階層式分群結果，寫出各種不同分群數量時，各群所包含的資料內容。
</p>

<div id="org865dcfd" class="figure">
<p><img src="images/聚類(集群)/2024-02-14_16-29-58_2024-02-14_16-29-33.png" alt="2024-02-14_16-29-58_2024-02-14_16-29-33.png" width="300" />
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org3aaa8f7" class="outline-4">
<h4 id="org3aaa8f7"><span class="section-number-4">2.2.3.</span> TNFSH作業: 聚合式分群作業&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></h4>
<div class="outline-text-4" id="text-2-2-3">
<p>
電子商務網站黃色鬼屋近日收集了200位VIP客戶資料，想將這些客戶依其同質性進行分類。
</p>
</div>
<div id="outline-container-org0675952" class="outline-5">
<h5 id="org0675952">資料</h5>
<div class="outline-text-5" id="text-org0675952">

<div id="org5991fb4" class="figure">
<p><img src="images/聚類(集群)/2024-02-14_15-28-32_2024-02-14_15-28-25.png" alt="2024-02-14_15-28-32_2024-02-14_15-28-25.png" width="400" />
</p>
<p><span class="figure-number">Figure 38: </span>黃色鬼屋VIP資料</p>
</div>
<ul class="org-ul">
<li>資料集URL: <a href="https://raw.githubusercontent.com/letranger/AI/gh-pages/Downloads/schopaholic.csv">https://raw.githubusercontent.com/letranger/AI/gh-pages/Downloads/schopaholic.csv</a></li>
<li>CID: 客戶編號</li>
<li>Gd: 性別(Male/Female)</li>
<li>Age: 年齡</li>
<li>Income: 月收入(單位為萬元)</li>
<li>ShopSco: 這是黃色鬼屋自訂的敗家分數，範圍由0~100</li>
</ul>
</div>
</div>
<div id="outline-container-org0400e3a" class="outline-5">
<h5 id="org0400e3a">任務</h5>
<div class="outline-text-5" id="text-org0400e3a">
<ul class="org-ul">
<li><p>
畫出200位VIP客戶的性別、年齡、月收入、敗家分數的分佈狀況，例如:
</p>

<div id="org1162062" class="figure">
<p><img src="images/hierarchTask2.png" alt="hierarchTask2.png" width="300" />
</p>
</div></li>
<li>利用階層式分群的方式幫黃色鬼屋完成以下工作
<ul class="org-ul">
<li><p>
將階層圖畫出來，例如:
</p>

<div id="orgad33a28" class="figure">
<p><img src="images/hierarchTask4.png" alt="hierarchTask4.png" width="300" />
</p>
</div></li>
<li><p>
輸出分成5群的結果，例如:
</p>
<pre class="example">
第1群客戶ID: 127 129 131 135 ...
第2群客戶ID: 28 44 46 47 48 ...
第3群客戶ID: 124 126 128 130 ...
第4群客戶ID: 2 4 6 8 10 12 14 ...
第5群客戶ID: 1 3 5 7 9 11 13 ...
</pre></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org66fb2c2" class="outline-4">
<h4 id="org66fb2c2"><span class="section-number-4">2.2.4.</span> 分裂式階層分群法(Divisive Clustering)</h4>
<div class="outline-text-4" id="text-2-2-4">
<p>
如果採用分裂的方式，則由樹狀結構的頂端開始，將群聚逐次分裂。步驟：
</p>
<ol class="org-ol">
<li>將所有資料先視為同一群，再依據群內的相異，分裂成兩群。</li>
<li>接著，再從兩群中，找群內相異度最高的那群，再分裂一次，變成三群…，重複操作直到分出來的群數達到目標群數。</li>
</ol>
</div>
<div id="outline-container-org6f82f2c" class="outline-5">
<h5 id="org6f82f2c">分裂式階層分群實作</h5>
<div class="outline-text-5" id="text-org6f82f2c">

<div id="org391d3ce" class="figure">
<p><img src="images/topdown.png" alt="topdown.png" width="600" />
</p>
<p><span class="figure-number">Figure 39: </span>Caption</p>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-orga821c4e" class="outline-3">
<h3 id="orga821c4e"><span class="section-number-3">2.3.</span> DBSCAN</h3>
<div class="outline-text-3" id="text-2-3">
<p>
DBSCAN will group together closely packed points, where close together is defined as a minimum number of points that must exist within a certain distance. If the point is within a certain distance of multiple clusters, it will be grouped with the cluster to which it is most densely located. Any instance that is not within this certain distance of another cluster is labeled an outlier.
</p>

<p>
In k-means and hierarchical clustering, all points had to be clustered, and outliers were poorly dealt with. In DBSCAN, we can explicitly label points as outliers and avoid having to cluster them. This is powerful. Compared to the other clustering algorithms, DBSCAN is much less prone to the distortion typically caused by outliers in the data. Also, like hierarchical clustering—and unlike k-means—we do not need to prespecify the number of clusters.
</p>
</div>
<div id="outline-container-org07d4043" class="outline-4">
<h4 id="org07d4043"><span class="section-number-4">2.3.1.</span> 實作</h4>
<div class="outline-text-4" id="text-2-3-1">
</div>
<div id="outline-container-org62441a4" class="outline-5">
<h5 id="org62441a4">讀入資料</h5>
<div class="outline-text-5" id="text-org62441a4">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #83898d;">'''Main'''</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 4: </span><span style="color: #51afef;">import</span> os, time, pickle, gzip
<span class="linenr"> 5: </span><span style="color: #51afef;">import</span> datetime
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span><span style="color: #98be65;">'''Data Prep'''</span>
<span class="linenr"> 8: </span><span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> preprocessing <span style="color: #51afef;">as</span> pp
<span class="linenr"> 9: </span>
<span class="linenr">10: </span><span style="color: #98be65;">'''Data Viz'''</span>
<span class="linenr">11: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">12: </span><span style="color: #51afef;">import</span> matplotlib <span style="color: #51afef;">as</span> mpl
<span class="linenr">13: </span><span style="color: #51afef;">import</span> seaborn <span style="color: #51afef;">as</span> sns
<span class="linenr">14: </span><span style="color: #dcaeea;">color</span> = sns.color_palette()
<span class="linenr">15: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Load the datasets</span>
<span class="linenr">16: </span><span style="color: #dcaeea;">current_path</span> = os.getcwd()
<span class="linenr">17: </span><span style="color: #c678dd;">file</span> = os.path.sep.join([<span style="color: #98be65;">''</span>, <span style="color: #98be65;">'datasets'</span>, <span style="color: #98be65;">'mnist.pkl.gz'</span>])
<span class="linenr">18: </span>
<span class="linenr">19: </span><span style="color: #dcaeea;">f</span> = gzip.<span style="color: #c678dd;">open</span>(current_path+<span style="color: #c678dd;">file</span>, <span style="color: #98be65;">'rb'</span>)
<span class="linenr">20: </span><span style="color: #dcaeea;">train_set</span>, <span style="color: #dcaeea;">validation_set</span>, <span style="color: #dcaeea;">test_set</span> = pickle.load(f, encoding=<span style="color: #98be65;">'latin1'</span>)
<span class="linenr">21: </span>f.close()
<span class="linenr">22: </span>
<span class="linenr">23: </span><span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">y_train</span> = train_set[<span style="color: #da8548; font-weight: bold;">0</span>], train_set[<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr">24: </span><span style="color: #dcaeea;">X_validation</span>, <span style="color: #dcaeea;">y_validation</span> = validation_set[<span style="color: #da8548; font-weight: bold;">0</span>], validation_set[<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr">25: </span><span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_test</span> = test_set[<span style="color: #da8548; font-weight: bold;">0</span>], test_set[<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr">26: </span>
<span class="linenr">27: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Create Pandas DataFrames from the datasets</span>
<span class="linenr">28: </span><span style="color: #dcaeea;">train_index</span> = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #c678dd;">len</span>(X_train))
<span class="linenr">29: </span><span style="color: #dcaeea;">validation_index</span> = <span style="color: #c678dd;">range</span>(<span style="color: #c678dd;">len</span>(X_train), <span style="color: #c678dd;">len</span>(X_train)+<span style="color: #c678dd;">len</span>(X_validation))
<span class="linenr">30: </span><span style="color: #dcaeea;">test_index</span> = <span style="color: #c678dd;">range</span>(<span style="color: #c678dd;">len</span>(X_train)+<span style="color: #c678dd;">len</span>(X_validation), \
<span class="linenr">31: </span>                   <span style="color: #c678dd;">len</span>(X_train)+<span style="color: #c678dd;">len</span>(X_validation)+<span style="color: #c678dd;">len</span>(X_test))
<span class="linenr">32: </span>
<span class="linenr">33: </span><span style="color: #dcaeea;">X_train</span> = pd.DataFrame(data=X_train,index=train_index)
<span class="linenr">34: </span><span style="color: #dcaeea;">y_train</span> = pd.Series(data=y_train,index=train_index)
<span class="linenr">35: </span>
<span class="linenr">36: </span><span style="color: #dcaeea;">X_validation</span> = pd.DataFrame(data=X_validation,index=validation_index)
<span class="linenr">37: </span><span style="color: #dcaeea;">y_validation</span> = pd.Series(data=y_validation,index=validation_index)
<span class="linenr">38: </span>
<span class="linenr">39: </span><span style="color: #dcaeea;">X_test</span> = pd.DataFrame(data=X_test,index=test_index)
<span class="linenr">40: </span><span style="color: #dcaeea;">y_test</span> = pd.Series(data=y_test,index=test_index)
<span class="linenr">41: </span>
</pre>
</div>
</div>
</div>
<div id="outline-container-org5b5860e" class="outline-5">
<h5 id="org5b5860e">降維</h5>
<div class="outline-text-5" id="text-org5b5860e">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Principal Component Analysis</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> PCA
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">n_components</span> = <span style="color: #da8548; font-weight: bold;">784</span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">whiten</span> = <span style="color: #a9a1e1;">False</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">random_state</span> = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">pca</span> = PCA(n_components=n_components, whiten=whiten, \
<span class="linenr"> 9: </span>          random_state=random_state)
<span class="linenr">10: </span>
<span class="linenr">11: </span><span style="color: #dcaeea;">X_train_PCA</span> = pca.fit_transform(X_train)
<span class="linenr">12: </span><span style="color: #dcaeea;">X_train_PCA</span> = pd.DataFrame(data=X_train_PCA, index=train_index)
<span class="linenr">13: </span>
<span class="linenr">14: </span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Log data</span>
<span class="linenr">15: </span><span style="color: #dcaeea;">cwd</span> = os.getcwd()
<span class="linenr">16: </span><span style="color: #dcaeea;">log_dir</span> = cwd+<span style="color: #98be65;">"/datasets/"</span>
<span class="linenr">17: </span>y_train[<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #da8548; font-weight: bold;">2000</span>].to_csv(log_dir+<span style="color: #98be65;">'labels.tsv'</span>, sep = <span style="color: #98be65;">'</span><span style="color: #a9a1e1;">\t</span><span style="color: #98be65;">'</span>, index=<span style="color: #a9a1e1;">False</span>, header=<span style="color: #a9a1e1;">False</span>)
<span class="linenr">18: </span>
<span class="linenr">19: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Write dimensions to CSV</span>
<span class="linenr">20: </span>X_train_PCA.iloc[<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #da8548; font-weight: bold;">2000</span>,<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #da8548; font-weight: bold;">3</span>].to_csv(log_dir+<span style="color: #98be65;">'pca_data.tsv'</span>, sep = <span style="color: #98be65;">'</span><span style="color: #a9a1e1;">\t</span><span style="color: #98be65;">'</span>, index=<span style="color: #a9a1e1;">False</span>, header=<span style="color: #a9a1e1;">False</span>)
</pre>
</div>
</div>
</div>
<div id="outline-container-org01a5cc5" class="outline-5">
<h5 id="org01a5cc5">DBSCAN</h5>
<div class="outline-text-5" id="text-org01a5cc5">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Perform DBSCAN</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">analyzeCluster</span>(clusterDF, labelsDF):
<span class="linenr"> 3: </span>    <span style="color: #dcaeea;">countByCluster</span> = pd.DataFrame(data=clusterDF[<span style="color: #98be65;">'cluster'</span>].value_counts())
<span class="linenr"> 4: </span>    countByCluster.reset_index(inplace=<span style="color: #a9a1e1;">True</span>,drop=<span style="color: #a9a1e1;">False</span>)
<span class="linenr"> 5: </span>    countByCluster.<span style="color: #dcaeea;">columns</span> = [<span style="color: #98be65;">'cluster'</span>,<span style="color: #98be65;">'clusterCount'</span>]
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>    <span style="color: #dcaeea;">preds</span> = pd.concat([labelsDF,clusterDF], axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 8: </span>    preds.<span style="color: #dcaeea;">columns</span> = [<span style="color: #98be65;">'trueLabel'</span>,<span style="color: #98be65;">'cluster'</span>]
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>    <span style="color: #dcaeea;">countByLabel</span> = pd.DataFrame(data=preds.groupby(<span style="color: #98be65;">'trueLabel'</span>).count())
<span class="linenr">11: </span>
<span class="linenr">12: </span>    <span style="color: #dcaeea;">countMostFreq</span> = \
<span class="linenr">13: </span>        pd.DataFrame(data=preds.groupby(<span style="color: #98be65;">'cluster'</span>).agg( \
<span class="linenr">14: </span>                        <span style="color: #51afef;">lambda</span> x:x.value_counts().iloc[<span style="color: #da8548; font-weight: bold;">0</span>]))
<span class="linenr">15: </span>    countMostFreq.reset_index(inplace=<span style="color: #a9a1e1;">True</span>,drop=<span style="color: #a9a1e1;">False</span>)
<span class="linenr">16: </span>    countMostFreq.<span style="color: #dcaeea;">columns</span> = [<span style="color: #98be65;">'cluster'</span>,<span style="color: #98be65;">'countMostFrequent'</span>]
<span class="linenr">17: </span>
<span class="linenr">18: </span>    <span style="color: #dcaeea;">accuracyDF</span> = countMostFreq.merge(countByCluster, \
<span class="linenr">19: </span>                        left_on=<span style="color: #98be65;">"cluster"</span>,right_on=<span style="color: #98be65;">"cluster"</span>)
<span class="linenr">20: </span>    <span style="color: #dcaeea;">overallAccuracy</span> = accuracyDF.countMostFrequent.<span style="color: #c678dd;">sum</span>()/ \
<span class="linenr">21: </span>                        accuracyDF.clusterCount.<span style="color: #c678dd;">sum</span>()
<span class="linenr">22: </span>
<span class="linenr">23: </span>    <span style="color: #dcaeea;">accuracyByLabel</span> = accuracyDF.countMostFrequent/ \
<span class="linenr">24: </span>                        accuracyDF.clusterCount
<span class="linenr">25: </span>
<span class="linenr">26: </span>    <span style="color: #51afef;">return</span> countByCluster, countByLabel, countMostFreq, \
<span class="linenr">27: </span>            accuracyDF, overallAccuracy, accuracyByLabel
<span class="linenr">28: </span>
<span class="linenr">29: </span><span style="color: #51afef;">from</span> sklearn.cluster <span style="color: #51afef;">import</span> DBSCAN
<span class="linenr">30: </span>
<span class="linenr">31: </span><span style="color: #dcaeea;">eps</span> = <span style="color: #da8548; font-weight: bold;">3</span>
<span class="linenr">32: </span><span style="color: #dcaeea;">min_samples</span> = <span style="color: #da8548; font-weight: bold;">5</span>
<span class="linenr">33: </span><span style="color: #dcaeea;">leaf_size</span> = <span style="color: #da8548; font-weight: bold;">30</span>
<span class="linenr">34: </span><span style="color: #dcaeea;">n_jobs</span> = <span style="color: #da8548; font-weight: bold;">4</span>
<span class="linenr">35: </span>
<span class="linenr">36: </span><span style="color: #dcaeea;">db</span> = DBSCAN(eps=eps, min_samples=min_samples, leaf_size=leaf_size,
<span class="linenr">37: </span>            n_jobs=n_jobs)
<span class="linenr">38: </span>
<span class="linenr">39: </span><span style="color: #dcaeea;">cutoff</span> = <span style="color: #da8548; font-weight: bold;">99</span>
<span class="linenr">40: </span><span style="color: #dcaeea;">X_train_PCA_dbscanClustered</span> = db.fit_predict(X_train_PCA.loc[:,<span style="color: #da8548; font-weight: bold;">0</span>:cutoff])
<span class="linenr">41: </span><span style="color: #dcaeea;">X_train_PCA_dbscanClustered</span> = \
<span class="linenr">42: </span>    pd.DataFrame(data=X_train_PCA_dbscanClustered, index=X_train.index, \
<span class="linenr">43: </span>                 columns=[<span style="color: #98be65;">'cluster'</span>])
<span class="linenr">44: </span>
<span class="linenr">45: </span>countByCluster_dbscan, countByLabel_dbscan, countMostFreq_dbscan, \
<span class="linenr">46: </span>    accuracyDF_dbscan, overallAccuracy_dbscan, accuracyByLabel_dbscan \
<span class="linenr">47: </span>    = analyzeCluster(X_train_PCA_dbscanClustered, y_train)
<span class="linenr">48: </span>
<span class="linenr">49: </span>overallAccuracy_dbscan
<span class="linenr">50: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Print overall accuracy</span>
<span class="linenr">51: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Overall accuracy from DBSCAN: "</span>,overallAccuracy_dbscan)
<span class="linenr">52: </span>
<span class="linenr">53: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Show cluster results</span>
<span class="linenr">54: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Cluster results for DBSCAN"</span>)
<span class="linenr">55: </span>countByCluster_dbscan
<span class="linenr">56: </span>
</pre>
</div>

<pre class="example">
Overall accuracy from DBSCAN:  0.242
Cluster results for DBSCAN
</pre>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-org314a712" class="outline-2">
<h2 id="org314a712"><span class="section-number-2">3.</span> 降維</h2>
<div class="outline-text-2" id="text-3">
<p>
進行非監督式學習時，為了加速計算，最好能「在不損失過多資訊的前提下簡化資料」,降維(dimensionality reduction)就是其中一種手段。例如，汽車的里程數與車齡就有合併的依據。
</p>

<ul class="org-ul">
<li>本例以Colab為執行平台，透過資料的圖形化分佈觀察不同降維的效果。</li>
<li>於Colab執行時可以先將例中的savefig()註解掉</li>
</ul>

<p>
降維的主要目的在於壓縮資料，有以下幾種做法：
</p>
</div>
<div id="outline-container-orge2a1696" class="outline-3">
<h3 id="orge2a1696"><span class="section-number-3">3.1.</span> 以主成份分析(PCA)對非監督式數據壓縮</h3>
<div class="outline-text-3" id="text-3-1">
<p>
「特徵選擇」需要原始的「特徵」；而「特徵提取」則是在於「轉換」數據，或是「投影」(project)數據到一個新的「特徵空間」，特徵提取不僅能改善儲存空間的使用或是提高學習演算法的計算效率，也可以有效地藉由降低「維數災難」來提高預測的正確性，特別是在處理非正規化模型時。
</p>
</div>
<div id="outline-container-org6657b96" class="outline-4">
<h4 id="org6657b96"><span class="section-number-4">3.1.1.</span> 主成分分析 1</h4>
<div class="outline-text-4" id="text-3-1-1">
<p>
「主成份分析」(principal component analysis, PCA)是一種非監督式線性變換技術」，經常應用於「特徵提取」與「降維」，其他應用包括「探索式數據分析」和「股票市場分析」中的雜訊消除、生物資訊學領域中的「基因數據分析」與「基因表現層分析」。
</p>

<p>
這邊先簡單說維度詛咒，預測/分類能力通常是隨著維度數(變數)增加而上生，但當模型樣本數沒有繼續增加的情況下，預測/分類能力增加到一定程度之後，預測/分類能力會隨著維度的繼續增加而減小<sup><a id="fnr.4" class="footref" href="#fn.4" role="doc-backlink">4</a></sup>。
</p>

<p>
主成份分析的基本假設是希望資料可以在特徵空間找到一個投影軸(向量)投影後可以得到這組資料的最大變異量。以圖<a href="#org41d04b9">40</a>為例，PCA 的目的在於找到一個向量可以投影(圖中紅色的線)，讓投影後的資料變異量最大。
</p>


<div id="org41d04b9" class="figure">
<p><img src="images/pca-1.png" alt="pca-1.png" width="600" />
</p>
<p><span class="figure-number">Figure 40: </span>PCA-1 [<a href="fn:31">fn:31</a>]</p>
</div>
</div>
<div id="outline-container-org601a875" class="outline-5">
<h5 id="org601a875">投影(projection)</h5>
<div class="outline-text-5" id="text-org601a875">
<p>
假設有一個點藍色的點對原點的向量為\(\vec{x_i}\)，有一個軸為 v，他的投影(正交為虛線和藍色線為 90 度)向量為紅色那條線，紅色線和黑色線的夾角為\(\theta\)，\(\vec{x_i}\)投影長度為藍色線，其長度公式為\(\left\|{x_i}\right\|cos\theta\)。
</p>


<div id="orgde29a74" class="figure">
<p><img src="images/pca-2.png" alt="pca-2.png" width="300" />
</p>
<p><span class="figure-number">Figure 41: </span>PCA-2 [<a href="fn:31">fn:31</a>]</p>
</div>

<p>
假設有一組資料六個點(\(x_1, x_2, x_3, x_4, x_5, x_6\))，有兩個投影向量\(\vec{v}\)和\(\vec{v'}\)(如圖<a href="#org011528f">42</a>)，投影下來後，資料在\(\vec{v'}\)上的變異量比\(v\)上的變異量小。
</p>


<div id="org011528f" class="figure">
<p><img src="images/pca-3.png" alt="pca-3.png" width="500" />
</p>
<p><span class="figure-number">Figure 42: </span>PCA-3 [<a href="fn:31">fn:31</a>]</p>
</div>

<p>
從圖<a href="#orgcdd9f04">43</a>也可以看出這些資料在\(v\)向量資料投影后有較大的變異量(較之投影於\(\vec{v'}\))。
</p>


<div id="orgcdd9f04" class="figure">
<p><img src="images/pca-4.png" alt="pca-4.png" width="500" />
</p>
<p><span class="figure-number">Figure 43: </span>PCA-4 [<a href="fn:31">fn:31</a>]</p>
</div>
</div>
</div>
<div id="outline-container-org4e9d1fa" class="outline-5">
<h5 id="org4e9d1fa">變異量的計算</h5>
<div class="outline-text-5" id="text-org4e9d1fa">
<p>
典型的變異數公式如下：
\(\sigma^2 = \frac{1}{N}\sum\limits_{i=1}^N (X -\mu)^2\)
</p>

<p>
若要計算前述所有資料點(\(x_1, x_2, x_3, x_4, x_5, x_6\))在\(v\)上的投影\(v^Tx_1, v^Tx_2, v^Tx_3, v^Tx_4, v^Tx_5, v^Tx_6\) ，則其變異數公式為
\(\sigma^2 = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i -\mu)^2\)
</p>

<p>
又因 PCA 之前提假設是將資 shift 到 0(即，變異數的平均數為 0)以簡化運算，其公式會變為
\(\sigma^2 = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i -\mu)^2 = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i - 0)^2 = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i)^2\)
</p>

<p>
而機器學習處理的資料點通常為多變量，故上述式子會以矩陣方式呈現
</p>

<p>
\(\Sigma = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i)(v^Tx_i)^T = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_iv^Tx_iv) = v^T(\frac{1}{N}\sum\limits_{i=1}^Nx_iX_i^T)v = v^TCv\)
</p>

<p>
其中 C 為共變異數矩陣(covariance matrix)
</p>

<p>
\(C=\frac{1}{n}\sum\limits_{i=1}^nx_ix_i^T,\cdots x_i = \begin{bmatrix}
x_1^{(1)}     \\
x_2^{(2)}     \\
\vdots  \\
x_i^{(d)}     \\
\end{bmatrix}\)
</p>

<p>
主成份分析的目的則是在找出一個投影向量讓投影後的資料變異量最大化（最佳化問題）：
</p>

<p>
\(v = \mathop{\arg\max}\limits_{x \in \mathcal{R}^d,\left\|v\right\|=1} {v^TCv}\)
</p>

<p>
進一步轉成 Lagrange、透過偏微分求解，其實就是解 C 的特徵值(eigenvalue, \(\lambda\))和特徵向量(eigenvector, \(v\))。
</p>
</div>
</div>
</div>
<div id="outline-container-org45ef7d9" class="outline-4">
<h4 id="org45ef7d9"><span class="section-number-4">3.1.2.</span> 主成份分析 2</h4>
<div class="outline-text-4" id="text-3-1-2">
<p>
回到前述例子(身高和體重)，下左圖，經由 PCA 可以萃取出兩個特徵成分(投影軸，下圖右的兩條垂直的紅線，較長的紅線軸為變異量較大的主成份)。此範例算最大主成份的變異量為 13.26，第二大主成份的變異量為 1.23。
</p>


<div id="org3e4e23d" class="figure">
<p><img src="images/pca-5.png" alt="pca-5.png" width="500" />
</p>
<p><span class="figure-number">Figure 44: </span>PCA-5 [<a href="fn:31">fn:31</a>]</p>
</div>

<p>
PCA 投影完的資料為下圖，從下圖可知，PC1 的變異足以表示此筆資料資訊。
</p>


<div id="org50f4ae2" class="figure">
<p><img src="images/pca-6.png" alt="pca-6.png" width="500" />
</p>
<p><span class="figure-number">Figure 45: </span>PCA-6 [<a href="fn:31">fn:31</a>]</p>
</div>

<p>
此做法可以有效的減少維度數，但整體變異量並沒有減少太多，此例從兩個變成只有一個，但變異量卻可以保留(13.26/(13.26+1.23)= 91.51%)，兩維度的資料做 PCA，對資料進行降維比較沒有感覺，但講解圖例比較容易。
</p>
</div>
</div>
<div id="outline-container-org5ae0dda" class="outline-4">
<h4 id="org5ae0dda"><span class="section-number-4">3.1.3.</span> 主成份分析的主要步驟</h4>
<div class="outline-text-4" id="text-3-1-3">
<ol class="org-ol">
<li>標準化數據集</li>
<li>建立共變數矩陣</li>
<li>從共變數矩陣分解出特徵值與特徵向量</li>
<li>以遞減方式對特徵值進行排序，以便對特徵向量排名</li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 5: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 6: </span>  <span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> PCA
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">## Extracting the principal components step-by-step</span>
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">df_wine</span> = pd.read_csv(<span style="color: #98be65;">'https://archive.ics.uci.edu/ml/'</span>
<span class="linenr">11: </span>                        <span style="color: #98be65;">'machine-learning-databases/wine/wine.data'</span>,
<span class="linenr">12: </span>                        header=<span style="color: #a9a1e1;">None</span>)
<span class="linenr">13: </span>
<span class="linenr">14: </span>  df_wine.<span style="color: #dcaeea;">columns</span> = [<span style="color: #98be65;">'Class label'</span>, <span style="color: #98be65;">'Alcohol'</span>, <span style="color: #98be65;">'Malic acid'</span>, <span style="color: #98be65;">'Ash'</span>,
<span class="linenr">15: </span>                     <span style="color: #98be65;">'Alcalinity of ash'</span>, <span style="color: #98be65;">'Magnesium'</span>, <span style="color: #98be65;">'Total phenols'</span>,
<span class="linenr">16: </span>                     <span style="color: #98be65;">'Flavanoids'</span>, <span style="color: #98be65;">'Nonflavanoid phenols'</span>, <span style="color: #98be65;">'Proanthocyanins'</span>,
<span class="linenr">17: </span>                     <span style="color: #98be65;">'Color intensity'</span>, <span style="color: #98be65;">'Hue'</span>,
<span class="linenr">18: </span>                     <span style="color: #98be65;">'OD280/OD315 of diluted wines'</span>, <span style="color: #98be65;">'Proline'</span>]
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #c678dd;">print</span>(df_wine.head())
<span class="linenr">21: </span>
<span class="linenr">22: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Splitting the data into 70% training and 30% test subsets.</span>
<span class="linenr">23: </span>
<span class="linenr">24: </span>  <span style="color: #dcaeea;">X</span>, <span style="color: #dcaeea;">y</span> = df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">1</span>:].values, df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">0</span>].values
<span class="linenr">25: </span>
<span class="linenr">26: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(X, y,
<span class="linenr">27: </span>                                     test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>,
<span class="linenr">28: </span>                                     stratify=y, random_state=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">29: </span>
<span class="linenr">30: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">1. Standardizing the data.</span>
<span class="linenr">31: </span>  <span style="color: #dcaeea;">sc</span> = StandardScaler()
<span class="linenr">32: </span>  <span style="color: #dcaeea;">X_train_std</span> = sc.fit_transform(X_train)
<span class="linenr">33: </span>  <span style="color: #dcaeea;">X_test_std</span> = sc.transform(X_test)
<span class="linenr">34: </span>
<span class="linenr">35: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">2. Eigendecomposition of the covariance matrix.</span>
<span class="linenr">36: </span>  <span style="color: #dcaeea;">cov_mat</span> = np.cov(X_train_std.T)
<span class="linenr">37: </span>  <span style="color: #dcaeea;">eigen_vals</span>, <span style="color: #dcaeea;">eigen_vecs</span> = np.linalg.eig(cov_mat)
<span class="linenr">38: </span>
<span class="linenr">39: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'</span><span style="color: #a9a1e1;">\n</span><span style="color: #98be65;">Eigenvalues </span><span style="color: #a9a1e1;">\n</span><span style="color: #98be65;">%s'</span> % eigen_vals)
<span class="linenr">40: </span>
<span class="linenr">41: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">## Total and explained variance</span>
<span class="linenr">42: </span>
<span class="linenr">43: </span>  <span style="color: #dcaeea;">tot</span> = <span style="color: #c678dd;">sum</span>(eigen_vals)
<span class="linenr">44: </span>  <span style="color: #dcaeea;">var_exp</span> = [(i / tot) <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">sorted</span>(eigen_vals, reverse=<span style="color: #a9a1e1;">True</span>)]
<span class="linenr">45: </span>  <span style="color: #dcaeea;">cum_var_exp</span> = np.cumsum(var_exp)
<span class="linenr">46: </span>
<span class="linenr">47: </span>  plt.bar(<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">14</span>), var_exp, alpha=<span style="color: #da8548; font-weight: bold;">0.5</span>, align=<span style="color: #98be65;">'center'</span>,
<span class="linenr">48: </span>          label=<span style="color: #98be65;">'individual explained variance'</span>)
<span class="linenr">49: </span>  plt.step(<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">14</span>), cum_var_exp, where=<span style="color: #98be65;">'mid'</span>,
<span class="linenr">50: </span>           label=<span style="color: #98be65;">'cumulative explained variance'</span>)
<span class="linenr">51: </span>  plt.ylabel(<span style="color: #98be65;">'Explained variance ratio'</span>)
<span class="linenr">52: </span>  plt.xlabel(<span style="color: #98be65;">'Principal component index'</span>)
<span class="linenr">53: </span>  plt.legend(loc=<span style="color: #98be65;">'best'</span>)
<span class="linenr">54: </span>  plt.tight_layout()
<span class="linenr">55: </span>  plt.savefig(<span style="color: #98be65;">'05_02.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">56: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">57: </span>
</pre>
</div>

<pre class="example" id="org0c2bdca">
   Class label  Alcohol  ...  OD280/OD315 of diluted wines  Proline
0            1    14.23  ...                          3.92     1065
1            1    13.20  ...                          3.40     1050
2            1    13.16  ...                          3.17     1185
3            1    14.37  ...                          3.45     1480
4            1    13.24  ...                          2.93      735

[5 rows x 14 columns]

Eigenvalues
[4.84274532 2.41602459 1.54845825 0.96120438 0.84166161 0.6620634
 0.51828472 0.34650377 0.3131368  0.10754642 0.21357215 0.15362835
 0.1808613 ]
</pre>


<div id="orgca2e735" class="figure">
<p><img src="images/05_02.png" alt="05_02.png" width="500" />
</p>
<p><span class="figure-number">Figure 46: </span>Principal component index</p>
</div>

<p>
雖然上圖的「解釋變異數」圖有點類似隨機森林評估特徵值重要性的結果，但二者最大的不同處在於 PCA 為一種非監督式方法，也就是說，關於類別標籤資訊是被忽略的。
</p>
</div>
</div>
<div id="outline-container-org573dc8a" class="outline-4">
<h4 id="org573dc8a"><span class="section-number-4">3.1.4.</span> 特徵轉換</h4>
<div class="outline-text-4" id="text-3-1-4">
<p>
在分解「共變數矩陣」成為「特徵對」後，接下來要將資料集轉換為新的「主成份」，其步驟如下：
</p>
<ol class="org-ol">
<li>選取\(k\)個最大特徵值所對應的 k 個特徵向量，其中\(k\)為新「特徵空間」的維數(\(k \le d\))。</li>
<li>用最前面的\(k\)個特徵向量建立「投影矩陣」(project matrix)\(W\)。</li>
<li>使用投影矩陣\(W\)，輸入值為\(d\)維數據集、輸出值為新的\(k\)維「特徵子空間」。</li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 5: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 6: </span>  <span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> PCA
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">## Extracting the principal components step-by-step</span>
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">df_wine</span> = pd.read_csv(<span style="color: #98be65;">'https://archive.ics.uci.edu/ml/'</span>
<span class="linenr">11: </span>                          <span style="color: #98be65;">'machine-learning-databases/wine/wine.data'</span>,
<span class="linenr">12: </span>                          header=<span style="color: #a9a1e1;">None</span>)
<span class="linenr">13: </span>
<span class="linenr">14: </span>  <span style="color: #5B6268;">#  </span><span style="color: #5B6268;">df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',</span>
<span class="linenr">15: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'Alcalinity of ash', 'Magnesium', 'Total phenols',</span>
<span class="linenr">16: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',</span>
<span class="linenr">17: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'Color intensity', 'Hue',</span>
<span class="linenr">18: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'OD280/OD315 of diluted wines', 'Proline']</span>
<span class="linenr">19: </span>
<span class="linenr">20: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Splitting the data into 70% training and 30% test subsets.</span>
<span class="linenr">21: </span>  <span style="color: #dcaeea;">X</span>, <span style="color: #dcaeea;">y</span> = df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">1</span>:].values, df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">0</span>].values
<span class="linenr">22: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(X, y,
<span class="linenr">23: </span>                                      test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>,
<span class="linenr">24: </span>                                      stratify=y, random_state=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">25: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">1. Standardizing the data.</span>
<span class="linenr">26: </span>  <span style="color: #dcaeea;">sc</span> = StandardScaler()
<span class="linenr">27: </span>  <span style="color: #dcaeea;">X_train_std</span> = sc.fit_transform(X_train)
<span class="linenr">28: </span>  <span style="color: #dcaeea;">X_test_std</span> = sc.transform(X_test)
<span class="linenr">29: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">2. Eigendecomposition of the covariance matrix.</span>
<span class="linenr">30: </span>  <span style="color: #dcaeea;">cov_mat</span> = np.cov(X_train_std.T)
<span class="linenr">31: </span>  <span style="color: #dcaeea;">eigen_vals</span>, <span style="color: #dcaeea;">eigen_vecs</span> = np.linalg.eig(cov_mat)
<span class="linenr">32: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">## Total and explained variance</span>
<span class="linenr">33: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">tot = sum(eigen_vals)</span>
<span class="linenr">34: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]</span>
<span class="linenr">35: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">cum_var_exp = np.cumsum(var_exp)</span>
<span class="linenr">36: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">## Feature transformation</span>
<span class="linenr">37: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Make a list of (eigenvalue, eigenvector) tuples</span>
<span class="linenr">38: </span>  <span style="color: #dcaeea;">eigen_pairs</span> = [(np.<span style="color: #c678dd;">abs</span>(eigen_vals[i]), eigen_vecs[:, i])
<span class="linenr">39: </span>                  <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #c678dd;">len</span>(eigen_vals))]
<span class="linenr">40: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Sort the (eigenvalue, eigenvector) tuples from high to low</span>
<span class="linenr">41: </span>  eigen_pairs.sort(key=<span style="color: #51afef;">lambda</span> k: k[<span style="color: #da8548; font-weight: bold;">0</span>], reverse=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">42: </span>  <span style="color: #dcaeea;">w</span> = np.hstack((eigen_pairs[<span style="color: #da8548; font-weight: bold;">0</span>][<span style="color: #da8548; font-weight: bold;">1</span>][:, np.newaxis],
<span class="linenr">43: </span>                  eigen_pairs[<span style="color: #da8548; font-weight: bold;">1</span>][<span style="color: #da8548; font-weight: bold;">1</span>][:, np.newaxis]))
<span class="linenr">44: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Matrix W:</span><span style="color: #a9a1e1;">\n</span><span style="color: #98be65;">'</span>, w)
<span id="coderef-x-train-dot" class="coderef-off"><span class="linenr">45: </span>  <span style="color: #c678dd;">print</span>(X_train_std[<span style="color: #da8548; font-weight: bold;">0</span>].dot(w))</span>
<span id="coderef-x-train-pca" class="coderef-off"><span class="linenr">46: </span>  <span style="color: #dcaeea;">X_train_pca</span> = X_train_std.dot(w)</span>
<span class="linenr">47: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot</span>
<span class="linenr">48: </span>  <span style="color: #dcaeea;">colors</span> = [<span style="color: #98be65;">'r'</span>, <span style="color: #98be65;">'b'</span>, <span style="color: #98be65;">'g'</span>]
<span class="linenr">49: </span>  <span style="color: #dcaeea;">markers</span> = [<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>]
<span class="linenr">50: </span>
<span class="linenr">51: </span>  <span style="color: #51afef;">for</span> l, c, m <span style="color: #51afef;">in</span> <span style="color: #c678dd;">zip</span>(np.unique(y_train), colors, markers):
<span class="linenr">52: </span>      plt.scatter(X_train_pca[y_train == l, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr">53: </span>                  X_train_pca[y_train == l, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">54: </span>                  c=c, label=l, marker=m)
<span class="linenr">55: </span>
<span class="linenr">56: </span>  plt.xlabel(<span style="color: #98be65;">'PC 1'</span>)
<span class="linenr">57: </span>  plt.ylabel(<span style="color: #98be65;">'PC 2'</span>)
<span class="linenr">58: </span>  plt.legend(loc=<span style="color: #98be65;">'lower left'</span>)
<span class="linenr">59: </span>  plt.tight_layout()
<span class="linenr">60: </span>  plt.savefig(<span style="color: #98be65;">'05_03.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">61: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">62: </span>
</pre>
</div>

<pre class="example" id="org0047543">
Matrix W:
 [[-0.13724218  0.50303478]
 [ 0.24724326  0.16487119]
 [-0.02545159  0.24456476]
 [ 0.20694508 -0.11352904]
 [-0.15436582  0.28974518]
 [-0.39376952  0.05080104]
 [-0.41735106 -0.02287338]
 [ 0.30572896  0.09048885]
 [-0.30668347  0.00835233]
 [ 0.07554066  0.54977581]
 [-0.32613263 -0.20716433]
 [-0.36861022 -0.24902536]
 [-0.29669651  0.38022942]]
[2.38299011 0.45458499]
</pre>

<p>
使用上述程式碼產生的 13*2 維的投影矩陣可以轉換一個樣本\(x\)(以\(1 \times 13\)維的列向量表示)到 PCA 子空間(\(x'\))(前兩個主成份)：\(x' = xW\)(程式碼第<a href="#coderef-x-train-dot" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-x-train-dot');" onmouseout="CodeHighlightOff(this, 'coderef-x-train-dot');">45</a>行)；同樣的，我們也可以將整個\(124 \times 13\)維的訓練數據集轉換到兩個主成份(\(124 \times 2\)維)(程式第<a href="#coderef-x-train-pca" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-x-train-pca');" onmouseout="CodeHighlightOff(this, 'coderef-x-train-pca');">46</a>行)，最後，將轉換過的\(124 \times 2\)維矩陣以二維散點圖表示：
</p>


<div id="org5439f88" class="figure">
<p><img src="images/05_03.png" alt="05_03.png" width="500" />
</p>
<p><span class="figure-number">Figure 47: </span>05_03</p>
</div>

<p>
由圖<a href="#org5439f88">47</a>中可看出，與第二個主成份(y 軸)相比，數據沿著第一主成份(x 軸)的分散程度更嚴重，而由此圖也可判斷，該數據應可以一個「線性分類器」進行有效分類。
</p>
</div>
</div>
<div id="outline-container-orgebaeb00" class="outline-4">
<h4 id="orgebaeb00"><span class="section-number-4">3.1.5.</span> 以 Scikit-learn 進行主成份分析</h4>
<div class="outline-text-4" id="text-3-1-5">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> matplotlib.colors <span style="color: #51afef;">import</span> ListedColormap
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr"> 4: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr"> 5: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 6: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 7: </span>  <span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> PCA
<span class="linenr"> 8: </span>  <span style="color: #51afef;">from</span> sklearn.linear_model <span style="color: #51afef;">import</span> LogisticRegression
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">## Extracting the principal components step-by-step</span>
<span class="linenr">11: </span>
<span class="linenr">12: </span>  <span style="color: #dcaeea;">df_wine</span> = pd.read_csv(<span style="color: #98be65;">'https://archive.ics.uci.edu/ml/'</span>
<span class="linenr">13: </span>                          <span style="color: #98be65;">'machine-learning-databases/wine/wine.data'</span>,
<span class="linenr">14: </span>                          header=<span style="color: #a9a1e1;">None</span>)
<span class="linenr">15: </span>
<span class="linenr">16: </span>  <span style="color: #5B6268;">#  </span><span style="color: #5B6268;">df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',</span>
<span class="linenr">17: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'Alcalinity of ash', 'Magnesium', 'Total phenols',</span>
<span class="linenr">18: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',</span>
<span class="linenr">19: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'Color intensity', 'Hue',</span>
<span class="linenr">20: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'OD280/OD315 of diluted wines', 'Proline']</span>
<span class="linenr">21: </span>
<span class="linenr">22: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Splitting the data into 70% training and 30% test subsets.</span>
<span class="linenr">23: </span>  <span style="color: #dcaeea;">X</span>, <span style="color: #dcaeea;">y</span> = df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">1</span>:].values, df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">0</span>].values
<span class="linenr">24: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(X, y,
<span class="linenr">25: </span>                                      test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>,
<span class="linenr">26: </span>                                      stratify=y, random_state=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">27: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">1. Standardizing the data.</span>
<span class="linenr">28: </span>  <span style="color: #dcaeea;">sc</span> = StandardScaler()
<span class="linenr">29: </span>  <span style="color: #dcaeea;">X_train_std</span> = sc.fit_transform(X_train)
<span class="linenr">30: </span>  <span style="color: #dcaeea;">X_test_std</span> = sc.transform(X_test)
<span class="linenr">31: </span>
<span class="linenr">32: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_decision_regions</span>(X, y, classifier, resolution=<span style="color: #da8548; font-weight: bold;">0.02</span>):
<span class="linenr">33: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">setup marker generator and color map</span>
<span class="linenr">34: </span>      <span style="color: #dcaeea;">markers</span> = (<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>, <span style="color: #98be65;">'^'</span>, <span style="color: #98be65;">'v'</span>)
<span class="linenr">35: </span>      <span style="color: #dcaeea;">colors</span> = (<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'lightgreen'</span>, <span style="color: #98be65;">'gray'</span>, <span style="color: #98be65;">'cyan'</span>)
<span class="linenr">36: </span>      <span style="color: #dcaeea;">cmap</span> = ListedColormap(colors[:<span style="color: #c678dd;">len</span>(np.unique(y))])
<span class="linenr">37: </span>
<span class="linenr">38: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot the decision surface</span>
<span class="linenr">39: </span>      <span style="color: #dcaeea;">x1_min</span>, <span style="color: #dcaeea;">x1_max</span> = X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">40: </span>      <span style="color: #dcaeea;">x2_min</span>, <span style="color: #dcaeea;">x2_max</span> = X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">41: </span>      <span style="color: #dcaeea;">xx1</span>, <span style="color: #dcaeea;">xx2</span> = np.meshgrid(np.arange(x1_min, x1_max, resolution),
<span class="linenr">42: </span>                             np.arange(x2_min, x2_max, resolution))
<span class="linenr">43: </span>      <span style="color: #dcaeea;">Z</span> = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
<span class="linenr">44: </span>      <span style="color: #dcaeea;">Z</span> = Z.reshape(xx1.shape)
<span class="linenr">45: </span>      plt.contourf(xx1, xx2, Z, alpha=<span style="color: #da8548; font-weight: bold;">0.4</span>, cmap=cmap)
<span class="linenr">46: </span>      plt.xlim(xx1.<span style="color: #c678dd;">min</span>(), xx1.<span style="color: #c678dd;">max</span>())
<span class="linenr">47: </span>      plt.ylim(xx2.<span style="color: #c678dd;">min</span>(), xx2.<span style="color: #c678dd;">max</span>())
<span class="linenr">48: </span>
<span class="linenr">49: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot class samples</span>
<span class="linenr">50: </span>      <span style="color: #51afef;">for</span> idx, cl <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(np.unique(y)):
<span class="linenr">51: </span>          plt.scatter(x=X[y == cl, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr">52: </span>                      y=X[y == cl, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">53: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">0.6</span>,
<span class="linenr">54: </span>                      c=cmap(idx),
<span class="linenr">55: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>,
<span class="linenr">56: </span>                      marker=markers[idx],
<span class="linenr">57: </span>                      label=cl)
<span class="linenr">58: </span>
<span class="linenr">59: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Training logistic regression classifier using the first 2 principal components.</span>
<span class="linenr">60: </span>  <span style="color: #dcaeea;">pca</span> = PCA(n_components=<span style="color: #da8548; font-weight: bold;">2</span>)
<span id="coderef-pca-fit" class="coderef-off"><span class="linenr">61: </span>  <span style="color: #dcaeea;">X_train_pca</span> = pca.fit_transform(X_train_std)</span>
<span class="linenr">62: </span>  <span style="color: #dcaeea;">X_test_pca</span> = pca.transform(X_test_std)
<span class="linenr">63: </span>
<span class="linenr">64: </span>  <span style="color: #dcaeea;">lr</span> = LogisticRegression()
<span class="linenr">65: </span>  <span style="color: #dcaeea;">lr</span> = lr.fit(X_train_pca, y_train)
<span class="linenr">66: </span>
<span class="linenr">67: </span>  plot_decision_regions(X_train_pca, y_train, classifier=lr)
<span class="linenr">68: </span>  plt.xlabel(<span style="color: #98be65;">'PC 1'</span>)
<span class="linenr">69: </span>  plt.ylabel(<span style="color: #98be65;">'PC 2'</span>)
<span class="linenr">70: </span>  plt.legend(loc=<span style="color: #98be65;">'lower left'</span>)
<span class="linenr">71: </span>  plt.tight_layout()
<span class="linenr">72: </span>  plt.savefig(<span style="color: #98be65;">'05_04.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">73: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">74: </span>  plot_decision_regions(X_test_pca, y_test, classifier=lr)
<span class="linenr">75: </span>  plt.xlabel(<span style="color: #98be65;">'PC 1'</span>)
<span class="linenr">76: </span>  plt.ylabel(<span style="color: #98be65;">'PC 2'</span>)
<span class="linenr">77: </span>  plt.legend(loc=<span style="color: #98be65;">'lower left'</span>)
<span class="linenr">78: </span>  plt.tight_layout()
<span class="linenr">79: </span>  plt.savefig(<span style="color: #98be65;">'05_05.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">80: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
</pre>
</div>



<p>
PCA 類別是 scikit-learn 中許多轉換類別之一，首先使用訓練數據集來 fit 模型並轉換數據集(程式第<a href="#coderef-pca-fit" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-pca-fit');" onmouseout="CodeHighlightOff(this, 'coderef-pca-fit');">61</a>行)，最後以 Logistic 迴歸對數據進行分類。圖<a href="#org1c93913">48</a>為訓練集資料的分類結果，圖<a href="#org1c5bc2f">49</a>測為測試資料集分類結果，可以看出二者差異不大。
</p>


<div id="org1c93913" class="figure">
<p><img src="images/05_04.png" alt="05_04.png" width="500" />
</p>
<p><span class="figure-number">Figure 48: </span>PCA 訓練數據</p>
</div>


<div id="org1c5bc2f" class="figure">
<p><img src="images/05_05.png" alt="05_05.png" width="500" />
</p>
<p><span class="figure-number">Figure 49: </span>PCA 測試數據</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgcc920ec" class="outline-3">
<h3 id="orgcc920ec"><span class="section-number-3">3.2.</span> 利用線性判別分析(LDA)做監督式數據壓縮</h3>
<div class="outline-text-3" id="text-3-2">
<p>
LDA 的全稱是 Linear Discriminant Analysis（線性判別分析），是一種 supervised learning。因為是由 Fisher 在 1936 年提出的，所以也叫 Fisher&rsquo;s Linear Discriminant。「線性判別分析」(linear discriminant analysis, LDA)為一種用來做「特徵提取」的技術，藉由降維來處理「維數災難」，可提高非正規化模型的計算效率。PCA 在於找出一個在數據集中最大化變異數的正交成分軸； 而 LDA 則是要找出可以最佳化類別分離的特徵子空間。
</p>

<p>
從主觀的理解上，主成分分析到底是什麼？它其實是對數據在高維空間下的一個投影轉換，通過一定的投影規則將原來從一個角度看到的多個維度映射成較少的維度。到底什麼是映射，下面的圖就可以很好地解釋這個問題——正常角度看是兩個半橢圓形分佈的數據集，但經過旋轉（映射）之後是兩條線性分佈數據集。<sup><a id="fnr.5" class="footref" href="#fn.5" role="doc-backlink">5</a></sup>
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right"><img src="images/lda-rot-1.jpg" alt="lda-rot-1.jpg" /></th>
<th scope="col" class="org-right"><img src="images/lda-rot-2.jpg" alt="lda-rot-2.jpg" /></th>
<th scope="col" class="org-right"><img src="images/lda-rot-3.jpg" alt="lda-rot-3.jpg" /></th>
<th scope="col" class="org-right"><img src="images/lda-rot-4.jpg" alt="lda-rot-4.jpg" /></th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">3</td>
<td class="org-right">4</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-right"><img src="images/lda-rot-5.jpg" alt="lda-rot-5.jpg" /></td>
<td class="org-right"><img src="images/lda-rot-6.jpg" alt="lda-rot-6.jpg" /></td>
<td class="org-right"><img src="images/lda-rot-7.jpg" alt="lda-rot-7.jpg" /></td>
<td class="org-right"><img src="images/lda-rot-8.jpg" alt="lda-rot-8.jpg" /></td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-right">5</td>
<td class="org-right">6</td>
<td class="org-right">7</td>
<td class="org-right">8</td>
</tr>
</tbody>
</table>

<p>
LDA 與 PCA 都是常用的降維方法，二者的區別在於<sup><a id="fnr.5.5" class="footref" href="#fn.5" role="doc-backlink">5</a></sup>：
</p>
<ul class="org-ul">
<li>出發思想不同。PCA 主要是從特徵的協方差角度，去找到比較好的投影方式，即選擇樣本點投影具有最大方差的方向（ 在信號處理中認為信號具有較大的方差，噪聲有較小的方差，信噪比就是信號與噪聲的方差比，越大越好。）；而 LDA 則更多的是考慮了分類標籤信息，尋求投影后不同類別之間數據點距離更大化以及同一類別數據點距離最小化，即選擇分類性能最好的方向。</li>
<li>學習模式不同。PCA 屬於無監督式學習，因此大多場景下只作為數據處理過程的一部分，需要與其他算法結合使用，例如將 PCA 與聚類、判別分析、回歸分析等組合使用；LDA 是一種監督式學習方法，本身除了可以降維外，還可以進行預測應用，因此既可以組合其他模型一起使用，也可以獨立使用。</li>
<li>降維後可用維度數量不同。LDA 降維後最多可生成 C-1 維子空間（分類標籤數-1），因此 LDA 與原始維度 N 數量無關，只有數據標籤分類數量有關；而 PCA 最多有 n 維度可用，即最大可以選擇全部可用維度。</li>
</ul>

<p>
圖<a href="#orga29fbb4">50</a>左側是 PCA 的降維思想，它所作的只是將整組數據整體映射到最方便表示這組數據的坐標軸上，映射時沒有利用任何數據內部的分類信息。因此，雖然 PCA 後的數據在表示上更加方便（降低了維數並能最大限度的保持原有信息），但在分類上也許會變得更加困難；圖<a href="#orga29fbb4">50</a>右側是 LDA 的降維思想，可以看到 LDA 充分利用了數據的分類信息，將兩組數據映射到了另外一個坐標軸上，使得數據更易區分了（在低維上就可以區分，減少了運算量）。
</p>


<div id="orga29fbb4" class="figure">
<p><img src="images/pca-lda.png" alt="pca-lda.png" width="500" />
</p>
<p><span class="figure-number">Figure 50: </span>PCA LDA 差異</p>
</div>

<p>
線性判別分析 LDA 算法由於其簡單有效性在多個領域都得到了廣泛地應用，是目前機器學習、數據挖掘領域經典且熱門的一個算法；但是算法本身仍然存在一些侷限性：
</p>
<ul class="org-ul">
<li>當樣本數量遠小於樣本的特徵維數，樣本與樣本之間的距離變大使得距離度量失效，使 LDA 算法中的類內、類間離散度矩陣奇異，不能得到最優的投影方向，在人臉識別領域中表現得尤為突出</li>
<li>LDA 不適合對非高斯分佈的樣本進行降維</li>
<li>LDA 在樣本分類信息依賴方差而不是均值時，效果不好</li>
<li>LDA 可能過度擬合數據</li>
</ul>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://www.oreilly.com/library/view/hands-on-unsupervised-learning/9781492035633/">Hands-On Unsupervised Learning Using Python</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://medium.com/ai-academy-taiwan/clustering-method-4-ed927a5b4377">Clustering method 4</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://builtin.com/machine-learning/agglomerative-clustering">Hierarchical Clustering: Agglomerative + Divisive Clustering</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4" role="doc-backlink">4</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8-%E7%B5%B1%E8%A8%88%E5%AD%B8%E7%BF%92-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-principle-component-analysis-pca-58229cd26e71">機器/統計學習:主成分分析(Principal Component Analysis, PCA)</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.5" class="footnum" href="#fnr.5" role="doc-backlink">5</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://blog.csdn.net/dongyanwen6036/article/details/78311071">LDA與PCA都是常用的降維方法，二者的區別</a>
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Yung-Chin Yen</p>
<p class="date">Created: 2024-11-27 Wed 14:54</p>
</div>
</body>
</html>
