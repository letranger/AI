:PROPERTIES:
:ID:       20221023T101414.457264
:ROAM_ALIASES: CNN
:END:
#+title: 卷積神經網路
#+INCLUDE: ../pdf.org
#+TAGS: AI, Machine Learning
#+OPTIONS: toc:2 ^:nil num:5
#+OPTIONS: H:4
#+PROPERTY: header-args :eval never-export
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../css/muse.css" />
#+EXCLUDE_TAGS: noexport
# -*- org-export-babel-evaluate: nil -*-

#+begin_export html
<a href="https://letranger.github.io/AI/20221023101414-卷積神經網路.html"><img align="right" alt="Hits" src="https://hits.sh/letranger.github.io/AI/20221023101414-卷積神經網路.html.svg"/></a>
#+end_export

* CNN 卷積神經網路
卷積神經網路(CNN)由Yann LeCun(法國電腦科學家，2018年圖靈獎得主)所提出，此人在機器學習、計算機視覺、計算機神經科學等領域都有很多貢獻。

#+CAPTION: Yann LeCun
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 100
#+ATTR_ORG: :width 100
#+ATTR_HTML: :width 300
[[file:images/AI的發展沿革/2024-01-29_14-26-12_2024-01-29_14-21-23.png]]

CNN的想法源自於對人類大腦認知方式的模仿，當我們辨識一個圖像，會先注意到顏色鮮明的點、線、面，之後將它們構成一個個不同的形狀(眼睛、鼻子、嘴巴 ...)，這種抽象化的過程就是 CNN 演算法建立模型的方式。卷積層(Convolution Layer) 就是由點的比對轉成局部的比對，透過一塊塊的特徵研判，逐步堆疊綜合比對結果，就可以得到比較好的辨識結果，過程如下圖[fn:1]。

#+CAPTION: CNN 概念
#+name: fig:CNN-1
#+ATTR_LATEX: :width 500
#+ATTR_ORG: :width 500
#+ATTR_HTML: :width 500
#+RESULTS:
[[file:images/CNN-1.png]]

那麼，CNN與之前我們介紹的神經網路(DNN)有什麼差異呢？

** CNN v.s. MLP
CNN 與[[id:d6daa102-05bb-475d-b619-db8b61e86030][神經網路]]所提及之DNN(或是MLP, Multilayer Perceptron, 多層感知器) 的主要差異可由下圖看出，即，CNN 在輪入層中多了卷積層與池化層，而DNN中的每一層都是Dense Layer，也就是Fully Connected Layer。
#+BEGIN_SRC ditaa :file images/cnnmlp.png
         DNN                   CNN

 +-----------------+    +-----------------+
 |   Input Layer   |    |    Input Layer  |
 +-----------------+    +-----------------+
          |                     |
          |                     v
          |             +-----------------+
          |             |Convolution Layer|
          |             +-----------------+
          |                     |
          |                     v
          |             +-----------------+
          |             |   Pooling Layer |
          |             +-----------------+
          |                     |
          v                     v
 +-----------------+    +-----------------+
 |   Dense Layer   |    |      ...        |
 +-----------------+    +-----------------+
          |                     |
          v                     v
 +-----------------+    +-----------------+
 |   Dense Layer   |    |   Dense Layer   |
 +-----------------+    +-----------------+
          |                     |
          v                     v
 +-----------------+    +-----------------+
 |   Output Layer  |    |   Output Layer  |
 +-----------------+    +-----------------+
#+END_SRC

#+RESULTS:
#+CAPTION: CNN 與 DNN 之差異
#+name: fig:cnnmlp
#+ATTR_LATEX: :width 200
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 200
[[file:images/cnnmlp.png]]

更進一步來看MPL與CNN的架構差異，我們同樣以圖形識別為例，在讀入一張圖片後，MLP的做法如下：

#+CAPTION: DNN原理
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 400
[[file:images/CNN_卷積神經網路/2024-04-17_09-06-44_2024-04-17_09-03-45.png]]

就是將2維圖片矩陣「攤平」為1維矩陣(也就會損失了圖片的空間特徵)，然後每個矩陣的值搭配一個權重往下一層傳送進行運算，可以想像，如果第二層有100個神經元，就代表會有100*(36+1)個參數，其中+1為bias。而CNN的做法則如下圖:

#+CAPTION: CNN原理
#+LABEL:fig:CNN-arch
#+name: fig:CNN-arch
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/CNN_卷積神經網路/2024-04-17_09-18-53_2024-04-17_09-11-28.png]]

而CNN則是透過卷積核(kernel, 即圖[[fig:CNN-arch]]中的filter)對整張圖做卷積，部份的保留了圖片的空間特徵，同時也減少了參數數量。

由此看來，較之CNN，DNN有兩個缺點[fn:2][fn:3]：
1. 需要大量記憶體:
   在處理 256x256 大小的彩色圖片時，會需要用到 256 * 256 * 3 =196,608 個 Input Neuron，如果中間的隱藏層有 1000 個 Neuron，每個神經元需要一個浮點數的權重值 (8 bytes)，那麼總共需要 196,608 * 1001 * 8 = 1.47 GB 的記憶體才夠。更何況這還只是個簡單的模型。
2. 多層感知器只針對圖片中每個單一像素去作判斷，完全捨棄重要的影像特徵。人類在判斷所看到的物體時，會從不同部位的特徵先作個別判斷，例如當你看到一架飛機，會先從機翼、機鼻、機艙體等這些特徵，再跟記憶中的印象來判斷是否為一架飛機，甚至再進一步判斷為客機還是戰鬥機。但是多層感知器沒有利用這些特徵，所以在影像的判讀上準確率就沒有接下來要討論的 CNN 來得好。

#+CAPTION: DNN神經網路架構
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/CNN_卷積神經網路/2024-02-26_12-34-58_1*4_BDTvgB6WoYVXyxO8lDGA.png]]

#+CAPTION: CNN架構
#+LABEL:fig:CNN-arch-1
#+name: fig:CNN-arch-1
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
#+RESULTS:
[[file:images/CNN-arch-1.jpg]]

與 DNN 相比，CNN多了卷積層與池化層，卷積層用來強調圖案(資料)特徵，池化層則可以縮減取樣，減少 overfitting 問題。二者的比較如下圖[fn:4]所示：

#+CAPTION: DNN v.s. CNN
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/CNN_卷積神經網路/2024-04-17_09-54-39_2024-04-17_09-54-28.png]]

** 卷積運算
想具體的了解卷積核的功能，可以先到[[https://setosa.io/ev/image-kernels/][這裡]]來體驗一下。

在[[id:d6daa102-05bb-475d-b619-db8b61e86030][神經網路]]中，我們曾經提出如下的卷積運算例子：
#+begin_quote
經由如下的矩陣計算，我們可以初步擬定一條分類規則：若是運算所得值大於0，則此圖像為＼；若是運算所得值小於0，則判定為／。
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/How_machine_recognize_image/2024-02-17_09-56-41_2024-02-17_09-56-25.png]]

在上述範例中：
- 矩陣K對矩陣A、B所進行的運算即為卷積(convolution)，矩陣K在[[id:20221023T101414.457264][卷積神經網路]]中稱之為卷積核(convolution kernel)，其作用即在於萃取出資料特徵。藉此，我們達成了利用數學運算來擷取圖像特徵，也可以理解到為何深度學習能夠過濾資料雜訊而完成圖像識別。
- 計算得到結果後，我們私自擬定一條分類規則：若是運算所得值大於0，則此圖像為＼；若是運算所得值小於0，則判定為／。在神經網路中，這就是[[id:d3bcc30a-3d94-4a3c-8e66-baaac7325c75][激勵函數]]([[id:d3bcc30a-3d94-4a3c-8e66-baaac7325c75][Activation Function]])。
#+end_quote

以全連接層來處理神經網路的問題在於：全連接層會忽略資料的「形狀」。例如，假設輸入資料為影像，則通常會包含水平、垂直、色版方向的三維形狀，然而當這些資料輸入全連接層時，三維資料就必須變為平面（一維資料），如前述的 MNist 資料集，輸入為(1, 28, 28)，即，一種顏色、28*28 像素，輸入全連接層後會變成一行的 784 個資料。

三維形狀的影像包含了許多重要的空間資料，例如，類似的空間有著相似的象素值、RGB 各色版間有緊密連接的關連性，距離較遠的像素彼此沒有關連...等特質，這些特質會在全連接層中被忽略掉。卷積層（Convolution layer）則能維持這些形狀，我們把 CNN 的卷積層輸出入資料稱作特徵圖（input / output feature map），而在卷積層中執行的處理則稱為卷積運算，若以影像處理來比喻卷積運算，則相當於「濾鏡效果」。卷積層的意義即是將原本一個影像經由卷積運算產生多個影像，每個影像均代表不同特徵。卷積運算方式如下：

典型的卷積運算如圖[[fig:CNN-struct1]]，此處特徵圖為(4,4)，濾鏡為(3,3)，輸出為(2,2)，這裡所謂的濾鏡就是CNN中的卷積核(convolution kernel)，其初始值由隨機方式產生。此外，在全連接網路層中，除了權重參數(weight)之外，還有偏權值(bias)，其結果如圖[[fig:CNN-struct2]]。
#+BEGIN_SRC dot :file images/CNN-struc1.png :cmdline -Kdot -Tpng
digraph structs {
  rankdir=LR;
	S[label = "⊗" shape=circle size = "1" color=white]
	struct1 [shape = record label="{1|2|3|0}|{0|1|2|<f1>3}|{3|0|1|2}|{2|3|0|1}"];
  struct2 [shape = record label="{2|0|1}|{<f21>0|1|<f22>2}|{1|0|2}"];
  struct3 [shape = record label="{15|16}|{<f3>6|15}"];
  struct1:f1 -> S;
  S -> struct2:f21;
  struct2:f22 -> struct3:f3;
}
#+END_SRC
#+CAPTION: 卷積運算
#+LABEL:fig:CNN-struct1
#+name: fig:CNN-struct1
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
#+RESULTS:
[[file:images/CNN-struc1.png]]

#+BEGIN_SRC dot :file images/CNN-struc2.png :cmdline -Kdot -Tpng
digraph structs {
  nodesep=0.2;
  ranksep = 0.3;
  rankdir=LR;
	S1[label = "⊗" shape=circle size = "1" color=white]
	S2[label = "+" shape=record size = "1" color=white]
	S3[label = "3" shape=record size = "1" color=white nodesep = 0.2]
	struct1 [shape = record label="{1|2|3|0}|{0|1|2|<f1>3}|{3|0|1|2}|{2|3|0|1}"];
  struct2 [shape = record label="{2|0|1}|{<f21>0|1|<f22>2}|{1|0|2}"];
  struct3 [shape = record label="{15|16}|{ <f31>6|<f32>15}"];
  struct5 [shape = record label="{18|19}|{<f5> 9|18}"];
  struct1:f1 -> S1;
  S1 -> struct2:f21;
  struct2:f22 -> struct3:f31;
  struct3:f32 -> S2 -> S3 -> struct5:f5;
}
#+END_SRC
#+CAPTION: 卷積運算 2
#+LABEL:fig:CNN-struct2
#+name: fig:CNN-struct2
#+ATTR_LATEX: :width 400px
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
#+RESULTS:
[[file:images/CNN-struc2.png]]

由圖[[fig:CNN-struct1]]、[[fig:CNN-struct2]]的卷積運算可以看出，輸入特徵值在經過運算後，其大小會縮小，可以預見的結果是，在經過多層神經網路反複進行卷積運算後，輸出特徵值大小很快就會縮小為 1，而無法再進行卷積運算。為了避免這種情況發生，在進行卷積運算前，可以針對輸入資料周圍補上一圈固定的資料（例如 0），這個動作稱為填補(padding)，如果我們對在卷積運算前對於圖[[fig:CNN-struct1]]中的輸入特徵值進行寬度 1 的填補，則其結果如圖[[fig:CNN-struct3]]所示。

#+BEGIN_SRC dot :file images/CNN-struc3.png :cmdline -Kdot -Tpng
digraph structs {
  rankdir=LR;
	S[label = "⊗" shape=circle size = "1" color=white]
	struct1 [shape = record label="{0|0|0|0|0|0}|{0|1|2|3|0|0}|{0|0|1|2|3|<f3>0}|{0|3|0|1|2|0}|{0|2|3|0|1|0}|{0|0|0|0|0|0}"];
  struct2 [shape = record label="{2|0|1}|{<f21>0|1|<f22>2}|{1|0|2}"];
  struct3 [shape = record label="{7|12|10|4}|{<f3>4|15|16|10}|{10|6|15|6}|{8|10|4|3}"];
  struct1:f1 -> S;
  S -> struct2:f21;
  struct2:f22 -> struct3:f3;
  }
#+END_SRC
#+CAPTION: 卷積運算 3
#+LABEL:fig:CNN-struct3
#+name: fig:CNN-struct3
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
#+RESULTS:
[[file:images/CNN-struc3.png]]

進行上述卷積運算時，每次自左而右、自上而下，自輸入特徵值取出一個與核大小相同的子矩陣與核進行運算，而每個取出的子矩陣的間隔稱為步幅(stride)，上述範例中的stride均為 1，若 stride 設為 2，則一個(7,7)的輸入特徵值與一個(3,3)的核進行運算後，其輸出特徵值大小只剩(3,3)。

上述範例中的特徵值均為二維矩陣，亦即，僅能表示水平與垂直方向的單色點陣圖，若輸入中包含色彩資料(RGB)，則輸入特徵圖將升及為三維矩陣（三層二維矩陣，每層表示一種 RGB 值），核的結構也是三維矩陣，但輸出特徵值則為二維矩陣。 以 MNist 資料集為例，將數字 7 的 28*28 影像以隨機產生的 5*5 濾鏡(或稱 filter weight、kernel)對其進行卷積，其結果如圖[[fig:CNN-FW-1]]所示，這種效果有助於提取輸入影像的不同特徵，例如邊緣、線條...等。
#+CAPTION: 卷積運算 4
#+LABEL:fig:CNN-FW-1
#+name: fig:CNN-FW-1
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
#+RESULTS:
[[file:images/CNN-FW-1.jpg]]

實際建構模型時，不會只進行單一 kenrel 的卷積，圖[[fig:CNN-FW-2]]即是隨機產生 16 個 kernel 對輸入影像提取不同特徵。
#+CAPTION: 卷積運算 5
#+LABEL:fig:CNN-FW-2
#+name: fig:CNN-FW-2
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
#+RESULTS:
[[file:images/CNN-FW-2.jpg]]

全連接層(又稱密集層, dense layer)和卷積層之間的根本區別在於：全連接層會由輸入的特徵空間中學習全域的 pattern，而卷積層則是學習局部的 pattern。以影像辨識為例，卷積層會把輸入分解成小小的 2D 窗格，然後從中找出 pattern。這個關鍵特性為 CNN 提供了兩個有趣的特質：

- 學習到的 pattern 具平移不變性(translation invariant)：在影像的右下角學習到的某個 pattern，CNN 可以在任何位置識別這樣的 pattern(如左上角)，相對的，當全連接層連接神經網路看到 pattern 出現在新位置時，就必須重新學習，這使的 CNN 可以更有效率地處理影像資料。
- 學習到 pattern 的空間層次結構(spatial hierarchies of patterns)：第一層卷積層會學習到諸如邊邊角角的小局部圖案，第二層卷積層則會基於第一層學到的特徵(小圖案)來學習較大的圖案，這使得 CNN 能夠有效地學習越來越複雜和抽象的視覺概念。

CNN 所運算的 3D 張量稱為特徵映射圖(feature maps，簡稱特徵圖)，特徵圖有 2 個空間軸(height, width)以及 1 個色深度軸(depth / channel)，對於 RGB 影像來說，depth 值為 3。卷積運算會從輸入特徵中萃取出各種小區塊 pattern，當它對整張影像都做完萃取之後，就會產生輸出特徵映射圖(output feature map)。輸出特徵映射圖仍是 3D 張量，具有寬度和高度，但此時其深度軸已不再代表 RGB 顏色值，此時它代表過濾器(filter)，每一種 filter 會對輸入資料進行特定面向的編碼、萃取出結果，例如，filter 可以萃取到「在輸入資料中出現一張臉」這種高階抽象的概念，將不是臉的都過濾掉。

** 池化層(Pooling Layer)
卷積層之間通常會加一個池化層(Pooling Layer)，它是一個壓縮圖片並保留重要資訊的方法，取樣的方法一樣是採滑動視窗，但是通常取最大值(Max-Pooling)，而非加權總和，若滑動視窗大小設為 2，『滑動步長』(Stride) 也為 2，則資料量就降為原本的四分之一，但因為取最大值，它還是保留局部範圍比對的最大可能性。也就是說，池化後的資訊更專注於圖片中是否存在相符的特徵，而非圖片中『哪裡』存在這些特徵，幫助 CNN 判斷圖片中是否包含某項特徵，而不必關心特徵所在的位置，這樣圖像偏移，一樣可以辨識出來。其架構如圖[[fig:CNN-Pooling-1]]所示。[fn:5]
#+CAPTION: CNN 之池化層
#+LABEL:fig:CNN-Pooling-1
#+name: fig:CNN-Pooling-1
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+RESULTS:
[[file:images/CNN-Pooling.png]]

池化層以縮減取樣(downsampling)的方式縮小影像可以帶來以下優點：
1. 減少需要處理的資料點：減少後續運算所需時間。
1. 讓影像位置差異變小：影像要辨識的目標（如 MNist 資料集的數字）可能因為在影像中的位置不同而影響辦識，減小影像可以讓數字的位置差異變小。
1. 參數的數量程計算量下降：同時也能控制 overfitting。

圖[[fig:CNN-PL-1]]即為使用 Max-Pool 對 16 個卷積影像進行縮減取樣(downsampling)的效果，將 16 個 28*28 個影像縮小為 16 個 14*14 的影像，但仍保持其特徵。MaxPooling 主要是由輸入特徵圖中做採樣並輸出樣本的最大值，它在概念上類似於卷積層操作，但並不是用卷積核(convolution kernel)張量積的方式來轉換局部區塊，而是經由手動編碼的 max 張量操作進行轉換。與卷積層操作的很大區別是 MaxPooling 通常用 2\times2 窗格和步長(strides)2 來完成，以便將特徵圖每一軸的採樣減少到原來的 1/2，而卷積層操作通常使用 3\times3 窗格且不指定步長(即使用預設步長 1)。

#+CAPTION: 池化效果
#+LABEL:fig:CNN-PL-1
#+name: fig:CNN-PL-1
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
#+RESULTS:
[[file:images/CNN-PL-1.jpg]]

為什麼要採用這種方式來縮小採樣特徵圖而不用原來的特徵圖尺寸一路執行下去？假設一個有不加入 MaxPooling 而是以全卷積層的模型設計如下：
#+BEGIN_SRC python -r -n :results output :exports both :eval no
model_no_max_pool = models.Sequential()
model_no_max_pool.add(layers.Conv2D(32, (3, 3), activation='relu'),
                      input_shape=(28, 28, 1))
model_no_max_pool.add(layers.Conv2D(64, (3, 3), activation='relu'),)
model_no_max_pool.add(layers.Conv2D(64, (3, 3), activation='relu'),)
#+END_SRC

上述設計的問題可以從以下兩個面向來看：[fn:6]
- 不利於學習特徵的空間層次結構。當 strides=1，第二層的 3\times3 窗格要走過 5\times5 的區域才能包含第一層的 7\times7 區城，進而生成下一層的完整 3\times3 區域，依此類推，第三層的 3\times3 窗格也要走過 5\times5 的區域才能包含第二層的 5\times5 區域。也就是說，第三層的 3\times3 窗格實際上僅包含來自原始輸入的 7\times7 區城的資訊。相對於初始輸入，由卷積神經網路學習的高階 pattern 仍然進展很小，不足以學習分類數字。我們希望的是：最後一個卷積層的輸出特徵已經能提供有關輸入資料的總體訊息。
- 最終特徵圖的每個樣本總係數為 22\times22\times64=3-976，這是相當巨大的 model，如果要將其展平以在頂部連接大小為 512 的 Dense 層，則該層將會有 1580 萬個參數，這對小 model 而言實在是太大，而且會導致 overfitting。

雖然 MaxPooling 並不是縮小探樣特徵圖的唯一方法（也可以透過卷積層的 strides 來調整，或是使用平均池化而非 MaxPooling）,不過就經驗來看，MaxPooling 往往比這些方案好。主要原因是：特徵通常是源自於空間某些有特色的 pattern，因此要取其最大值才更具訊息性，若採用平均值，則特色就被掩蓋了。

* 神經網路解題步驟
使用神經網路解決問題可大致分為兩個步驟：「學習」與「推論」。
- 學習指使用訓練資料進行權重參數的學習
- 推論指使用學習過的參數進行資料分類CNN
而實際的動手實作可再細分為以下幾個步驟
1. 收集資料 (Gathering data)
2. 準備數據 (Preparing that data)
3. 選擇模型 (Choosing a model)
4. 訓練機器 (Training)
5. 評估分析 (Evaluation)
6. 調整參數 (Hyperparameter tuning)
7. 預測推論 (Prediction)
#+latex:\newpage
實際動手玩一下神經網路架構: [[https://playground.tensorflow.org][Tensorflow Playground]]
** 收集資料
*** 資料類型
**** 人工收集
- 預測股市股價: 開盤、收盤、成交量、技術指標、財務指標、籌碼指標等等
- 以物品識別:大量物品照片並給予名稱(label)
- 以注音符號手寫辨識: 大量手寫照片及其對應答案(label)
**** 現成資料集
***** MNIST
資料集由 0~9 的數字影像構成(如圖[[fig:MNIST-set]])，共計 60000 張訓練影像、10000 張測試影像。
#+CAPTION: MNIST 資料集內容範例
#+LABEL:fig:Labl
#+name: fig:MNIST-set
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/MNIST.jpg]]
***** Boston housing
Boston Housing數據集包含有關波士頓不同房屋的數據(數據，如年份、面積)，本資料集中有506個樣本和13個特徵變量，目標是使用給定的特徵預測房屋價格的價值。
***** Iris
鳶尾花資料集是非常著名的生物資訊資料集之一，由英國統計學家 Ronald Fisher 在1936年時，對加斯帕半島上的鳶尾屬花朵所提取的花瓣花萼的長寬數據資料，依照山鳶尾，變色鳶尾，維吉尼亞鳶尾三類進行標示，共150筆資料[fn:7]。每筆資料有五個欄位：花萼長度(Sepal Length)、花萼寬度(Sepal Width)、花瓣長度(Petal Length) 、花瓣寬度(Petal Width)、類別(Class)，其中類有Setosa，Versicolor和Virginica三個品種。
#+CAPTION: Iris資料集
#+LABEL:fig:Iris
#+name: fig:Iris
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/cqy409dEexm96zavyuw.png]]
***** Cifar-10
由深度學習大師 Geoffrey Hinton 教授與其在加拿大多倫多大學的學生 Alex Krixhevsky 與 Vinoid Nair 所整理之影像資料集, 包含 6 萬筆 32*32 低解析度之彩色圖片, 其中 5 萬筆為訓練集; 1 萬筆為測試集, 是機器學習中常用的圖片辨識資料集
#+CAPTION: Cifar-10
#+LABEL:fig:Cifar10
#+name: fig:Cifar10
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/cifar10-ten-categories.jpg]]
*** DEMO
以keras為例，我們可以利用mnist.load_data()自網路下載手寫辨識資料集，資料集內容包含 *資料(x)* 和 *標籤(y)* 兩部份，load_data()這個function會把整份資料集分割成兩個子集合：訓練集(training set)和測試集(testing set)。
#+begin_src python -r -n :async :results output :exports both :session mnist-cnn
from keras.datasets import mnist
(X_train, y_Train), (x_test, y_Test) = mnist.load_data()
#+end_src

#+RESULTS:

** 準備數據
當我們在比較分析兩組數據資料時，可能會遭遇因單位的不同(例如：身高與體重)，或數字大小的代表性不同(例如：粉專1萬人與滿足感0.8)，造成各自變化的程度不一，進而影響統計分析的結果[fn:8]，即：那些變化量最大的因素（特徵）會主導分析結果。

正式將資料送至模型訓練前，資料應進行相應的缺漏值處理以及資料正規化，詳細處理方式可參考[[id:82e219c3-6ca0-43b0-bb11-e3a8454f089d][資料預處理]]。
*** DEMO範例
#+begin_src python -r -n :results output :exports both :noeval
x_train = x_train.reshape(60000, 784)
x_test = x_test.reshape(10000, 784)
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255

y_TrainOneHot = np_utils.to_categorical(y_Train)
y_TestOneHot = np_utils.to_categorical(y_Test)
#+end_src

** 選擇模型
整理完資料集後，接下來就是要選擇訓練用的模型，像是決策樹、LSTM、RNN等等都是機器學習中常使用的訓練模型，其中目前較常拿來訓練股市的是「LSTM」，中文叫做長短期記憶，是屬於深度學習中的一個模型。另一種CNN模型則適合處理圖形資料。
*** 語法
[[https://keras.io/api/models/model_training_apis/][Keras API reference / Models API / Model training APIs ]]
*** DEMO
**** CNN模型示例
#+begin_src python -r -n :results output :exports both :noeval
model = Sequential()
model.add(Conv2D(32, (3, 3), padding="same", activation="relu"))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, (3, 3), padding="same", activation="relu"))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128, (3, 3), padding="same", activation="relu"))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Activation("softmax"))
model.add(Dense(units=128,
                input_dim=784,
                kernel_initializer='normal',
                activation='relu'))
model.add(Dense(64, activation='relu')
model.add(Dense(units=10,
                kernel_initializer='normal',
                activation='softmax'))
#+end_src
#+CAPTION: MNIST-NeuralNet
#+LABEL:fig:MNIST-NeuralNet
#+ATTR_LATEX: :width 400px
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/MNIST-CNN.png]]

**** LSTM模型示例
#+begin_src python -r -n :results output :exports both :noeval
model = Sequential()
model.add(LSTM(128,
               input_shape=(x_train.shape[1:]),
               activation='relu',
               return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(128, activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(10, activation='softmax'))
#+end_src

** 訓練機器
選擇好訓練模型後，再來要將訓練集資料丟進去模型中做訓練，每層要放多少神經元、要跑幾層等等都會影響模型訓練出來的結果，這部分只能靠經驗跟不斷嘗試去學習，或是上網多爬文看別人怎麼撰寫訓練模型。

在真正訓練前應該再設定好模型的loss function, optimizer。
*** 語法
[[https://keras.io/api/models/model_training_apis/][Keras API reference / Models API / Model training APIs ]]
*** DEMO
**** CNN
#+begin_src python -r -n :results output :exports both :noeval
# optimizer, loss function
model.compile(loss='categorical_crossentropy',
              optimizer='adam', metrics=['accuracy'])

model.fit(x=x_Train,
          y=y_TrainOneHot,
          validation_split=0.2,
          epochs=5, batch_size=30, verbose=2)
#+end_src

**** LSTM
#+begin_src python -r -n :results output :exports both :noeval
# ptimizer, loss function
model.compile(optimizer=Adam(lr=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train,
          e pochs=3,
          validation_data=(x_test, y_test))
#+end_src

** 評估分析
當模型訓練完成後，接下來就是判斷該模型是否有過度擬合(overfitting)，這裡就是帶入測試集的資料進行評估，也可以嘗試利用交叉驗證的方式進行模型的擬合性判斷，以及利用RESM、MSE等統計計算來判斷模型的準確度
#+begin_src python -r -n :results output :exports both :noeval
scores = model.evaluate(x_Train, y_TestOneHot)
#+end_src

** 調整參數
到這大致上模型已經完成了50%，最後的一步就是進行參數的微調，我們也稱為「超參數 (Hyperparamters)」，讓整個模型更加的精準，但也不能過度的調整，因為會造成overfitting的結果，這個取捨就只能依照無窮盡的反覆迭帶去尋找了，這部分也是相對較耗時間的地方
*** model參數
- 調整model架構: [[https://medium.com/%E9%9B%9E%E9%9B%9E%E8%88%87%E5%85%94%E5%85%94%E7%9A%84%E5%B7%A5%E7%A8%8B%E4%B8%96%E7%95%8C/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-ml-note-cnn%E6%BC%94%E5%8C%96%E5%8F%B2-alexnet-vgg-inception-resnet-keras-coding-668f74879306][[機器學習 ML NOTE] CNN演化史(AlexNet、VGG、Inception、ResNet)+Keras Coding]]
- loss function: https://keras.io/api/losses/
- optimizers: https://keras.io/api/optimizers/
*** Hyperparameters
- batch size：一次迭代放入進行訓練或測試的影像數量。
- epoch：一種單位，所有影像皆被計算過1次後即為1 epoch
- [[http://elmer-storage.blogspot.com/2018/06/cnn-hyperparamters.html][CNN筆記 - 超參數 (Hyperparamters) ]]

** 預測推論
到此，模型已經正式完成。那，怎麼知道這個模型夠不夠優秀？能不能正常運作？最簡單的評估方式是以一筆新的資料來測試。
#+begin_src python -r -n :results output :exports both
prediction = model.predict_classes(x_Test4D_normalize)
print(prediction[:10])
#+end_src
以MNIST手寫數字辨識為例，我們可以自已寫一個數字餵給模型，看看模型是否能正確預測。

對於模型來說，這類全新的數據則是一個未知數，如果我們在訓練與測試階段都用同一批資料，這就好像上課與考試的內容都一樣，這會導致學生其實沒有學到真正的知識，而考試得到的分數也不會太準確。這種訓練資料與測試資料過於雷同而導致模型對新型別的資料欠缺處理能力的問題即為 *過度擬合(overfitting)* ；反之，如果我們的模型對於各種新型態資料都能辨識，則這個模型就具有 *泛化（Generalization）* 的預測能力。
#+latex:\newpage

* 實作1
- [[id:31d6a744-f7f7-47e4-ae33-3f9fa91c33bb][CNN實作]]

* 實作2: MNIST
:PROPERTIES:
:CUSTOM_ID: Hi-MNIST
:END:
** MNIST資料集
- MNIST 是機器學習領域中相當著名的資料集，號稱機器學習領域的「Hello world.」，其重要性不言可喻。
- MNIST 資料集由 0~9 的數字影像構成(如圖[[fig:MNIST-set]])，共計 60000 張訓練影像、10000 張測試影像。
- 一般的 MMIST 資料集的用法為：使用訓練影像進行學習，再利用學習後的模型預測能否正確分類測試影像。

準備資料是訓練模型的第一步，基礎資料可以是網上公開的資料集，也可以是自己的資料集。視覺、語音、語言等各種型別的資料在網上都能找到相應的資料集。

** 準備 MNIST 資料
MNIST 數據集來自美國國家標準與技術研究所, National Institute of Standards and Technology (NIST). 訓練集 (training set) 由來自 250 個不同人手寫的數字構成, 其中 50% 是高中學生, 50% 來自人口普查局 (the Census Bureau) 的工作人員. 測試集(test set) 也是同樣比例的手寫數字數據。MNIST 數據集可在 http://yann.lecun.com/exdb/mnist/ 獲取, 它包含了四個部分:
1. Training set images: train-images-idx3-ubyte.gz (9.9 MB, 解壓後 47 MB, 包含 60,000 個樣本)
1. Training set labels: train-labels-idx1-ubyte.gz (29 KB, 解壓後 60 KB, 包含 60,000 個標籤)
1. Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 解壓後 7.8 MB, 包含 10,000 個樣本)
1. Test set labels: t10k-labels-idx1-ubyte.gz (5KB, 解壓後 10 KB, 包含 10,000 個標籤)

雖然自己手動下載是個不錯的主意，可以讓你練習如何從網路抓檔案、解壓、設定這些檔案的下載路徑、然後再讀進來變成numpy的array或是pandas的dataframe，再把這些資料餵給tensorflow模組去建模型.....不過這對初學者來說著實是一件很麻煩的事，所以多數的套件都很貼心的幫你準備好了直接從網路抓下資料集的function，像tensorflow的load_data。
*** load data
MNIST 資料集是一個適合拿來當作 TensotFlow 的練習素材，在 Tensorflow 的現有套件中，也已經有內建好的 MNIST 資料集，我們只要在安裝好 TensorFlow 的 Python 環境中執行以下程式碼，即可將 MNIST 資料成功讀取進來。
#+BEGIN_SRC python  -n -r :results output :exports both :session mnist :async
import tensorflow as tf
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()    (ref:get-keras-mnist)
print(x_train.shape)
#+END_SRC

#+RESULTS:
: (60000, 28, 28)

在訓練模型之前，需要將樣本資料劃分為訓練集、測試集，有些情況下還會劃分為訓練集、測試集、驗證集。由上述程式第[[(get-keras-mnist)]]行可知，下載後的 MNIST 資料分成訓練資料(training data)與測試資料(testing data)，其中 x 為圖片、y為所對應數字。
#+BEGIN_SRC python -r -n :results output :exports both :session mnist :async
# 判斷資料形狀
print(x_train.shape)
print(x_test.shape)
# 第一個label的內容
print(y_train[0])
# 顯示影像內容
import matplotlib.pylab as plt
img = x_train[0]
plt.imshow(img)
plt.savefig("MNIST-Image.png")
#+END_SRC
#+RESULTS[301ed277c778e588011f39c44ec8462a701a3a8f]:
: (60000, 28, 28)
: (10000, 28, 28)
: 5

由上述程式輸出結果可以看到載入的 x 為大小為 28*28 的圖片共 60000 張，每一筆 MNIST 資料的照片(x)由 784 個 pixels 組成（28*28），照片內容如圖[[fig:MNIST-Image]]，訓練集的標籤(y)則為其對應的數字(0～9)，此例為 5。
#+CAPTION: MNIST 影像示例
#+name: fig:MNIST-Image
#+ATTR_LATEX: :width 200px
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 300
[[file:images/MNIST-Image.png]]

x 的影像資料為灰階影像，每個像素的數值介於 0~255 之間，矩陣裡每一項的資料則是代表每個 pixel 顏色深淺的數值，如下圖[[fig:MNIST-Matrix]]所示：
#+CAPTION: MNIST 資料矩陣
#+name: fig:MNIST-Matrix
#+ATTR_LATEX: :width 500
#+ATTR_ORG: :width 500
#+ATTR_HTML: :width 500
[[file:images/MNIST-Matrix.png]]
*** 資料預處理
**** 影像數值資料的[[https://aifreeblog.herokuapp.com/posts/54/data_science_203/][正規化]]
此例中每張圖被存成28X28個0~255的數值，分別代表影像中每個點(pixel)的灰階深度，在把影像丟進模型前，最好先將數值資料做正規化或標準化，讓數值都變為介於0到1間的數值。

以此狀況為例，把所有的數值除上255是個不錯的解決方案。
#+BEGIN_SRC python -r -n :results output :exports both :session mnist :async
print("====第0張圖的第七列原始內容===")
print(x_train[0][7])

X_train = x_train / 255
X_test = x_test / 255

print("====第0張圖的第七列正規化後的內容===")
print(X_train[0][7])
#+end_src

#+RESULTS:
: ====第0張圖的第七列原始內容===
: [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251
:   93  82  82  56  39   0   0   0   0   0]
: ====第0張圖的第七列正規化後的內容===
: [0.         0.         0.         0.         0.         0.
:  0.         0.19215686 0.93333333 0.99215686 0.99215686 0.99215686
:  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.98431373
:  0.36470588 0.32156863 0.32156863 0.21960784 0.15294118 0.
:  0.         0.         0.         0.        ]

**** Label的格式轉換
載入的 y 為所對應的數字 0~9，在這我們要運用 keras 中的 np_under_utils.to_under_categorical 將 y 轉成 one-hot 的形式，將他轉為一個 10 維的 vector，例如：我們所拿到的資料為 y=3，經過 np_utils.to_categorical，會轉換為 y=[0,0,0,1,0,0,0,0,0,0]。這部份的轉換程式碼如下：
#+BEGIN_SRC python -r -n :results output :exports both :session mnist :async
# 將y轉換成one-hot encoding
from tensorflow.keras.utils import to_categorical

print(y_train[0])
Y_train = to_categorical(y_train, 10)
Y_test = to_categorical(y_test, 10)
# 回傳處理完的資料
print(y_Train[0])
#+END_SRC

#+RESULTS:
: 5
: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]

結果可以看出第0張圖的label為5，表示這張圖不管寫的好或不好，像或不像，其真正的答案為5。

** 建立CNN模型
- 在這個模型中，第一層(同時也身兼輸入層)有32個卷積核，也就是有32個神經元
- 第一層的每個神經元有10個參數\(3*3+1\)，因為卷積核大小為\(3*3\)，還有一個bias
#+BEGIN_SRC python -r -n :results output :exports both :session mnist :async
from keras import layers
from keras import models
model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))
model.summary()  # 查看模型摘要
#+END_SRC

#+RESULTS:
#+begin_example
Model: "sequential_18"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 conv2d_32 (Conv2D)          (None, 26, 26, 32)        320

 max_pooling2d_25 (MaxPooli  (None, 13, 13, 32)        0
 ng2D)

 conv2d_33 (Conv2D)          (None, 11, 11, 64)        18496

 max_pooling2d_26 (MaxPooli  (None, 5, 5, 64)          0
 ng2D)

 flatten_7 (Flatten)         (None, 1600)              0

 dense_26 (Dense)            (None, 512)               819712

 dense_27 (Dense)            (None, 10)                5130

=================================================================
Total params: 843658 (3.22 MB)
Trainable params: 843658 (3.22 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
#+end_example

在編譯時，以adim優化器，由於使用 softmax 單元結束神經網路，所以配合使用 categorical_crossentropy 為損失基準。

#+NAME: 配置 model 以進行訓練
#+BEGIN_SRC python -r -n :results output :exports both :session mnist :async
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
#+END_SRC

#+RESULTS: 配置 model 以進行訓練

** 訓練CNN模型
#+BEGIN_SRC python -r -n :results output :exports both :session mnist :async
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 设置 Tensorflow 日志级别为 ERROR

history = model.fit(X_train, Y_train,  # 训练数据和标签
                    batch_size=128,     # 批次大小
                    epochs=10,          # 训练轮数
                    validation_split=0.2)  # 验证集比例（可选）
#+end_src

#+RESULTS:
#+begin_example
Epoch 1/10
375/375 [==============================] - 7s 18ms/step - loss: 0.0074 - accuracy: 0.9973 - val_loss: 0.0429 - val_accuracy: 0.9900
Epoch 2/10
375/375 [==============================] - 7s 19ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.0458 - val_accuracy: 0.9904
Epoch 3/10
375/375 [==============================] - 7s 19ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.0543 - val_accuracy: 0.9887
Epoch 4/10
375/375 [==============================] - 7s 19ms/step - loss: 0.0055 - accuracy: 0.9978 - val_loss: 0.0456 - val_accuracy: 0.9905
Epoch 5/10
375/375 [==============================] - 7s 19ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.0428 - val_accuracy: 0.9911
Epoch 6/10
375/375 [==============================] - 7s 19ms/step - loss: 9.6022e-04 - accuracy: 0.9997 - val_loss: 0.0503 - val_accuracy: 0.9912
Epoch 7/10
375/375 [==============================] - 7s 19ms/step - loss: 1.9383e-04 - accuracy: 1.0000 - val_loss: 0.0471 - val_accuracy: 0.9917
Epoch 8/10
375/375 [==============================] - 8s 21ms/step - loss: 4.2269e-05 - accuracy: 1.0000 - val_loss: 0.0484 - val_accuracy: 0.9922
Epoch 9/10
375/375 [==============================] - 8s 20ms/step - loss: 3.1525e-05 - accuracy: 1.0000 - val_loss: 0.0490 - val_accuracy: 0.9924
Epoch 10/10
375/375 [==============================] - 7s 20ms/step - loss: 1.5961e-05 - accuracy: 1.0000 - val_loss: 0.0504 - val_accuracy: 0.9923
#+end_example

** 評估CNN模型
#+BEGIN_SRC python -r -n :results output :exports both :session mnist :async
# 在测试数据上评估模型
loss, accuracy = model.evaluate(X_test, Y_test)
print('Test loss:', loss)
print('Test accuracy:', accuracy)

import matplotlib.pyplot as plt

# 获取训练历史中的损失值和准确率值
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']

# 绘制损失值折线图
plt.cla()
plt.plot(train_loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.savefig('images/cnn-mnist-Loss.png', dpi=300)

# 绘制准确率折线图
plt.cla()
plt.plot(train_accuracy, label='Training Accuracy')
plt.plot(val_accuracy, label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.savefig('images/cnn-mnist-Accuracy.png', dpi=300)
#+end_src

#+RESULTS:
: 313/313 [==============================] - 1s 4ms/step - loss: 0.0358 - accuracy: 0.9929
: Test loss: 0.03584390878677368
: Test accuracy: 0.992900013923645

#+CAPTION: Training and Validation Loss
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/cnn-mnist-Loss.png]]

#+CAPTION: Training and Validation Accuracy
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/cnn-mnist-Accuracy.png]]

** 預測結果
#+BEGIN_SRC python -r -n :results output :exports both :session mnist :async
# 使用模型进行预测
predictions = model.predict(x_test)

# 輸出前10個樣本的预测结果
for i in range(10):
    print("Predicted:", predictions[i].argmax(), "Actual:", Y_test[i].argmax())
#+end_src

#+RESULTS:
#+begin_example
313/313 [==============================] - 1s 4ms/step
Predicted: 7 Actual: 7
Predicted: 2 Actual: 2
Predicted: 1 Actual: 1
Predicted: 0 Actual: 0
Predicted: 4 Actual: 4
Predicted: 1 Actual: 1
Predicted: 4 Actual: 4
Predicted: 9 Actual: 9
Predicted: 6 Actual: 5
Predicted: 9 Actual: 9
#+end_example

** Crosstab
#+BEGIN_SRC python -r -n :results output :exports both :session mnist :async
import pandas as pd

# 假设 predictions 和 y_test 是模型预测结果和实际标签
# 假设 predictions 和 y_test 都是一维数组，每個元素是樣本对应的类别标签

# 将预测结果和实际标签转换为 Pandas Series
predicted_labels = pd.Series(predictions.argmax(axis=1), name='Predicted')
actual_labels = pd.Series(Y_test.argmax(axis=1), name='Actual')

# 使用 pd.crosstab 计算交叉表
crosstab_result = pd.crosstab(actual_labels, predicted_labels)

# 輸出交叉表
print(crosstab_result)
#+end_src

#+RESULTS:
#+begin_example
Predicted    0     1     2     3    4    5    6     7    8    9
Actual
0          977     0     0     0    1    0    0     1    1    0
1            0  1128     1     1    0    1    1     0    3    0
2            1     1  1026     0    0    0    0     4    0    0
3            0     0     1  1004    0    3    0     0    2    0
4            0     0     0     0  979    0    0     0    1    2
5            2     0     1     9    0  873    3     1    3    0
6            2     2     0     0    3    1  948     0    2    0
7            0     0     3     0    0    0    0  1021    1    3
8            2     0     1     0    0    0    0     2  968    1
9            0     0     0     1    8    2    0     0    3  995
#+end_example

* 實作3: Regression
CNN雖然更多場合是用於處理影像相關的處理工作，然而他也可以用來幫我們解決一些數學問題，例如畫迴歸線。開啟colab、新增一個筆記本，逐一將下列程式碼複製、貼到colab執行，觀察結果。

** 產生數據
#+begin_src python -r -n :results output :exports both :async
import matplotlib.pyplot as plt
import numpy as np

x = np.random.uniform(0.0, 3, (10))
y = 78 + 7.8*x + np.random.normal(0.0, 3, len(x))
print('x: ', x)
print('y: ', y)

plt.scatter(x, y)
#+end_src
#+RESULTS:
: x:  [1.04495966 2.63828713 1.05394772 ... 2.7829483  2.5038085  0.7773407 ]
: y:  [ 85.07647921  96.21107688  90.60605344 ...  97.41007778 102.38380208 80.86243195]

結果如下圖，一共有10個點，如何畫出一條迴歸線代表這10個點的趨勢？這個題目本身是什麼意思？
#+CAPTION: Caption
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/cnn-regression-1.png]]

想像一下，上面是10個學生的資料，X軸是學生對於數學的喜愛程度，Y軸是該生的數學期末考成績，看起來二者間是存在某種關係的。那，如果已知某生對數學的喜愛程為2.0，我們可以預測他的數學期末考成績嗎？---可以的，如果我們有一條像底下的迴歸線公式：
$$ MathScore_i = a * MathLove_i + b $$
如此一來，我們就能輸入喜愛程度，得到對數學成績的預測結果。

當樣本數不太多，而你的計算能力也不弱時，也許你可以手動將上述公式中的a與b求出，得到一條 $y=ax+b$ 的迴歸線，但萬一樣本數有2000個點呢(如圖[[fig:images2000]])?
#+begin_src python -r -n :session REG :results output :exports none
import matplotlib.pyplot as plt
import numpy as np

x = np.random.uniform(0.0, 3, (2000))
y = 78 + 7.8*x + np.random.normal(0.0, 3, len(x))
plt.cla()
plt.scatter(x, y, s=10)
plt.savefig("images/cnn-regression-2.png", dpi=300)
#+end_src

#+CAPTION: 2000個資料點
#+LABEL:fig:Labl
#+name: fig:images2000
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/cnn-regression-2.png]]

接下來我們想建一個CNN模型來幫我們畫一條迴歸線。

** scikit learn的線性迴歸模組

#+begin_src python -r -n :session REG :results output :exports both
from sklearn import linear_model
regr=linear_model.LinearRegression()

x = x.reshape(-1, 1)
# 改變x的格式形狀，
# 原本x為 [x1, x2, x3, ...]
# 要改格式符合scikit learn迴歸模組的需求，變成: [ [x1], [x2], [x3], ...]
y = y.reshape(-1, 1)
regr.fit(x, y)

#短短兩行code，模型已經建構完成了! 接下來，我們來看看訓練集的成果。
plt.clf()
plt.scatter(x, y, s=10)
plt.plot(x, regr.predict(x), color='red', linewidth=4)
print("====實際(前三筆)====")
print(y[:3])
print("====預測(前三筆)====")
print(regr.predict(x[:3]))
#+end_src

#+RESULTS:
: ====實際(前三筆)====
: [[85.07647921]
:  [96.21107688]
:  [90.60605344]]
: ====預測(前三筆)====
: [[86.09718013]
:  [98.58801576]
:  [86.16764168]]

#+CAPTION: Caption
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/cnn-regression-3.png]]

** 建立model來畫迴歸線
#+begin_src python -r -n :results output :exports both :session REG :async
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout

# A simple regression model
model = Sequential()
model.add(Dense(4, input_shape=(1,)))
model.add(Dropout(0.5))
model.add(Dense(8, input_shape=(1,)))
model.add(Dropout(0.5))
model.add(Dense(1, input_shape=(1,)))
model.compile(loss='mse', optimizer='rmsprop', metrics=['mse'])
print(model)
#+end_src
#+RESULTS:
: <Sequential name=sequential_1, built=True>

** 訓練model
#+begin_src python -r -n :results output :exports both :session REG :async
# 將實際資料分為訓練集與測試集
x_Train = x[:1500]
x_Test = x[1500:]
y_Train = y[:1500]
y_Test = y[1500:]
# The fit() method - trains the model
train_history = model.fit(x=x_Train, y=y_Train,
                          validation_split=0.2,
                          epochs=100, batch_size=200,
                          verbose=1)
print(train_history)
#+end_src

#+RESULTS:
#+begin_example
Epoch 1/100
6/6 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 8444.8213 - mse: 8444.8223 - val_loss: 8150.6479 - val_mse: 8155.1191
略...
Epoch 100/100
6/6 ━━━━━━━━━━━━━━━━━━━━ 0m 37m 0m  1m0s 0m 2ms/step - loss: 3507.2502 - mse: 3507.2505 - val_loss: 2969.5845 - val_mse: 2959.9399
<keras.src.callbacks.history.History object at 0x16c613e60>
#+end_example

** 查看訓練過程
#+begin_src python -r -n :results output :exports both :session REG :async
print(train_history.history.keys())
import matplotlib.pyplot as plt
plt.cla()
plt.title('Train History - Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.plot(train_history.history['loss'])
plt.plot(train_history.history['val_loss'])
#plt.show()
plt.savefig("images/cnn-regression-4.png", dpi=300)
#+end_src

#+RESULTS:
: dict_keys(['loss', 'mse', 'val_loss', 'val_mse'])

#+CAPTION: Caption
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 400
#+ATTR_HTML: :width 500
[[file:images/cnn-regression-4.png]]

** 評估model
#+begin_src python -r -n :results output :exports both :session REG :async
# The evaluate() method - gets the loss statistics
score = model.evaluate(x_Test, y_Test, batch_size=200)
print(score)
#+end_src

#+RESULTS:
: 3/3 [==============================] - 0s 990us/step - loss: 727.6573
: 727.6573486328125

** 預測結果
#+begin_src python -r -n :results output :exports both :async :session REG
# The predict() method - predict the outputs for the given inputs
print(y_Test[:5])
print(model.predict(x_Test[:5]))
#+end_src

#+RESULTS:
#+begin_example
[[92.71580667]
 [80.35926946]
 [94.14913803]
 [80.94617382]
 [94.81639264]]
1/1 [==============================] - 0s 87ms/step
[[74.310616]
 [49.009674]
 [75.51746 ]
 [54.195095]
 [68.86794 ]]
#+end_example

** 調整model/參數
- model架構
- loss function
- optimizer
- hyper parameters
*** case #1
#+begin_src python -r -n :results output :exports both :async :session REG
# A simple regression model
model = Sequential()
model.add(Dense(4, input_shape=(1,)))
model.add(Dense(8, input_shape=(1,)))
model.add(Dense(4, input_shape=(1,)))
model.add(Dense(1, input_shape=(1,)))
model.compile(loss='mean_squared_error', optimizer='rmsprop')
#mean_squared_logarithmic_error
#mean_absolute_percentage_error
train_history = model.fit(x=x_Train, y=y_Train,
                          validation_split=0.2,
                          epochs=100, batch_size=200,
                          verbose=0)
score = model.evaluate(x_Test, y_Test, batch_size=200)
print(score)
print(y_Test[:5])
print(model.predict(x_Test[:5]))
#+end_src

#+RESULTS:
#+begin_example
3/3 [==============================] - 0s 769us/step - loss: 479.3994
479.3994140625
[[92.71580667]
 [80.35926946]
 [94.14913803]
 [80.94617382]
 [94.81639264]]
1/1 [==============================] - 0s 34ms/step
[[101.1277 ]
 [ 54.4739 ]
 [103.35307]
 [ 64.03558]
 [ 91.09165]]
#+end_example
*** case #2
#+begin_src python -r -n :results output :exports both :async :session REG
model = Sequential()
model.add(Dense(4, input_shape=(1,)))
model.add(Dense(8, input_shape=(1,)))
model.add(Dense(16, input_shape=(1,)))
model.add(Dense(32, input_shape=(1,)))
model.add(Dense(16, input_shape=(1,)))
model.add(Dense(4, input_shape=(1,)))
model.add(Dense(1, input_shape=(1,)))
model.compile(loss='mse', optimizer='rmsprop')
train_history = model.fit(x=x_Train, y=y_Train,
                          validation_split=0.2,
                          epochs=100, batch_size=200,
                          verbose=0)
score = model.evaluate(x_Test, y_Test, batch_size=200)
print(score)
print(y_Test[:5])
print(model.predict(x_Test[:5]))
#+end_src

#+RESULTS:
#+begin_example
3/3 [==============================] - 0s 864us/step - loss: 14.2688
14.268836975097656
[[92.71580667]
 [80.35926946]
 [94.14913803]
 [80.94617382]
 [94.81639264]]
1/1 [==============================] - 0s 47ms/step
[[97.211815]
 [84.07756 ]
 [97.83831 ]
 [86.76943 ]
 [94.386406]]
#+end_example

* 實作4[回家踹]: Cifar-10
** 資料集下載/預處理
回家執行：下載需要 *一點* 時間....
#+BEGIN_SRC python -r -n :async :results output :exports both :session QQQ
### 1.  Import Library
from keras.datasets import cifar10
import numpy as np
np.random.seed(10)

### 2. 資料準備
(x_img_train,y_label_train),(x_img_test,y_label_test) = cifar10.load_data()
x_img_train_normalize = x_img_train.astype('float32') / 255.0
x_img_test_normalize = x_img_test.astype('float32') / 255.0
'''正規化'''
from tensorflow.keras.utils import to_categorical

y_label_train_OneHot = to_categorical(y_label_train)
y_label_test_OneHot = to_categorical(y_label_test)
#+end_src
** 建模
#+BEGIN_SRC python -r -n :async :results output :exports both :session QQQ
### 3. 建立模型
from keras import layers
from keras import models
'''卷積層1與池化層1'''
# 隨機產生32個3*3的濾鏡，輸入的影像為32*32*3(RGB)
model = models.Sequential()

model.add(layers.Conv2D(filters=32,kernel_size=(3,3), input_shape=(32, 32, 3),
                 activation='relu', padding='same'))
model.add(layers.Dropout(rate=0.25)) # 每次訓練迭代時會隨機放棄25%的神經元
model.add(layers.Conv2D(filters=32, kernel_size=(3, 3),
                 activation='relu', padding='same'))
# 進行第一次縮減取樣，將影像縮為16*16
model.add(layers.MaxPooling2D(pool_size=(2, 2)))
'''卷積層2與池化層2'''
# 將前一層傳進的32個16*16影像轉為64個16*16影像
model.add(layers.Conv2D(filters=64, kernel_size=(3, 3), ### 產生64個影像
                 activation='relu', padding='same'))
model.add(layers.Dropout(0.25))
model.add(layers.Conv2D(filters=64, kernel_size=(3, 3),
                 activation='relu', padding='same'))
# 再將64個16*16影像縮減取樣為8*8
model.add(layers.MaxPooling2D(pool_size=(2, 2)))
'''卷積層3與池化層3'''
# 將64個8*8個影像轉為128個
model.add(layers.Conv2D(filters=128, kernel_size=(3, 3),
                 activation='relu', padding='same'))
model.add(layers.Dropout(0.3))
model.add(layers.Conv2D(filters=128, kernel_size=(3, 3),
                 activation='relu', padding='same'))
# 再將128個8*8影像縮減取樣為4*4
model.add(layers.MaxPooling2D(pool_size=(2, 2)))
'''建立神經網路(平坦層、隱藏層、輸出層)'''
# 將128*4*4的3維矩陣轉為1維(2048個float數字，對應到2048個神經元)
model.add(layers.Flatten())
model.add(layers.Dropout(0.3))
model.add(layers.Dense(2500, activation='relu')) ### 隠藏層1有2500個神經元
model.add(layers.Dropout(0.3))
model.add(layers.Dense(1500, activation='relu')) ### 隠藏層2有1500神經元
model.add(layers.Dropout(0.3))
model.add(layers.Dense(10, activation='softmax')) ### 10個label
print(model.summary())
#+end_src

#+RESULTS:
#+begin_example
Model: "sequential_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 conv2d_19 (Conv2D)          (None, 32, 32, 32)        896

 dropout_18 (Dropout)        (None, 32, 32, 32)        0

 conv2d_20 (Conv2D)          (None, 32, 32, 32)        9248

 max_pooling2d_9 (MaxPoolin  (None, 16, 16, 32)        0
 g2D)

 conv2d_21 (Conv2D)          (None, 16, 16, 64)        18496

 dropout_19 (Dropout)        (None, 16, 16, 64)        0

 conv2d_22 (Conv2D)          (None, 16, 16, 64)        36928

 max_pooling2d_10 (MaxPooli  (None, 8, 8, 64)          0
 ng2D)

 conv2d_23 (Conv2D)          (None, 8, 8, 128)         73856

 dropout_20 (Dropout)        (None, 8, 8, 128)         0

 conv2d_24 (Conv2D)          (None, 8, 8, 128)         147584

 max_pooling2d_11 (MaxPooli  (None, 4, 4, 128)         0
 ng2D)

 flatten_3 (Flatten)         (None, 2048)              0

 dropout_21 (Dropout)        (None, 2048)              0

 dense_9 (Dense)             (None, 2500)              5122500

 dropout_22 (Dropout)        (None, 2500)              0

 dense_10 (Dense)            (None, 1500)              3751500

 dropout_23 (Dropout)        (None, 1500)              0

 dense_11 (Dense)            (None, 10)                15010

=================================================================
Total params: 9176018 (35.00 MB)
Trainable params: 9176018 (35.00 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
#+end_example
** 訓練、評估
#+BEGIN_SRC python -r -n :async :results output :exports both :session QQQ
### 4. 載入之前訓練的模型
try:
    model.load_weights("SaveModel/cifarCnnModelnew1.h5")
    print("載入模型成功!繼續訓練模型")
except:
    print("載入模型失敗!開始訓練一個新模型")

### 5. 訓練模型
# 訓練前先以compile設定模型, 設定內容包含
# 1. 損失函數, 2. 最佳化方法, 3. 評估模型的方法
model.compile(loss='categorical_crossentropy',
              optimizer='adam', metrics=['accuracy'])
# 開始訓練, 執行50次訓練週期、每一批次500筆資料
# 40000筆資料，每一批次500筆資料, 分為80批次進行訓練
# 每個epoch訓練完後會存一筆accuracy和loss記錄到train_history
train_history=model.fit(x_img_train_normalize, y_label_train_OneHot,
                        validation_split=0.2,
                        epochs=5, batch_size=500, verbose=1)
import matplotlib.pyplot as plt
def show_train_history(train_acc,test_acc):
    plt.plot(train_history.history[train_acc])
    plt.plot(train_history.history[test_acc])
    plt.title('Train History')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['train', 'test'], loc='upper left')
    #plt.show()
show_train_history('accuracy','val_accuracy')

### 6. 評估模型準確率
scores = model.evaluate(x_img_test_normalize,
                        y_label_test_OneHot, verbose=0)
print(scores[1])

### 7. 進行預測
prediction=model.predict(x_img_test_normalize)
print(prediction[:10])
#+end_src

#+RESULTS:
#+begin_example
載入模型失敗!開始訓練一個新模型
Epoch 1/5
80/80 [==============================] - 71s 879ms/step - loss: 1.9843 - accuracy: 0.2581 - val_loss: 2.0850 - val_accuracy: 0.2409
Epoch 2/5
80/80 [==============================] - 75s 934ms/step - loss: 1.5619 - accuracy: 0.4269 - val_loss: 1.6944 - val_accuracy: 0.3912
Epoch 3/5
80/80 [==============================] - 75s 937ms/step - loss: 1.3769 - accuracy: 0.4967 - val_loss: 1.5183 - val_accuracy: 0.4387
Epoch 4/5
80/80 [==============================] - 75s 938ms/step - loss: 1.2417 - accuracy: 0.5524 - val_loss: 1.2065 - val_accuracy: 0.5647
Epoch 5/5
80/80 [==============================] - 75s 944ms/step - loss: 1.1236 - accuracy: 0.5959 - val_loss: 1.1469 - val_accuracy: 0.5937
0.5929999947547913
313/313 [==============================] - 7s 21ms/step
[[6.46312302e-03 8.62431712e-03 2.74472516e-02 5.00399292e-01
  2.81555019e-02 9.06480253e-02 3.07343274e-01 4.68559610e-03
  2.12734081e-02 4.96020494e-03]
 [7.61443824e-02 3.05925645e-02 8.70503602e-04 7.35422189e-04
  8.07344739e-04 7.03643236e-05 3.87530803e-04 3.73235853e-05
  8.88279617e-01 2.07493710e-03]
 [2.19170123e-01 3.15457061e-02 2.23177653e-02 1.75716318e-02
  3.48090231e-02 5.49549609e-03 1.05224168e-02 3.76811461e-03
  6.39865100e-01 1.49346180e-02]
 [4.80774581e-01 5.90456650e-03 2.44085774e-01 1.69374775e-02
  1.14243045e-01 1.07119661e-02 1.00224596e-02 1.61196198e-02
  9.24141407e-02 8.78633745e-03]
 [5.03382122e-04 9.75311195e-05 6.01756126e-02 4.26706523e-02
  3.55109274e-01 6.51758304e-03 5.32512546e-01 2.19794596e-03
  1.07646527e-04 1.07893706e-04]
 [2.17650877e-03 6.94291375e-04 2.25894004e-02 6.15960397e-02
  2.69289650e-02 1.98316220e-02 8.60564947e-01 3.63973179e-03
  7.52524647e-04 1.22592552e-03]
 [7.00628571e-03 1.10443816e-01 2.33118124e-02 1.84527755e-01
  9.68367886e-03 1.67584091e-01 3.15246195e-01 1.13006774e-02
  1.70870777e-03 1.69187024e-01]
 [5.39396284e-03 2.00870680e-04 2.21061304e-01 5.73799871e-02
  2.93410629e-01 1.22425267e-02 4.05219078e-01 3.07954405e-03
  1.57088123e-03 4.41132550e-04]
 [7.44145736e-03 2.97625345e-04 2.10883513e-01 4.50782835e-01
  5.35527207e-02 2.20824048e-01 3.02264839e-02 2.32125558e-02
  1.64294522e-03 1.13576988e-03]
 [3.26189846e-02 7.81687021e-01 6.97707012e-03 4.02900390e-03
  3.50690144e-03 2.40204763e-03 1.28890313e-02 7.98477267e-04
  3.92462090e-02 1.15845300e-01]]
#+end_example
** 查看結果
#+BEGIN_SRC python -r -n :async :results output :exports both :session QQQ

### 8. 查看預測結果
label_dict={0:"airplane",1:"automobile",2:"bird",3:"cat",4:"deer",
            5:"dog",6:"frog",7:"horse",8:"ship",9:"truck"}

import matplotlib.pyplot as plt
def plot_images_labels_prediction(images,labels,prediction,idx,num=10):
    plt.cla()
    fig = plt.gcf()
    fig.set_size_inches(12, 14)
    if num>25: num=25
    for i in range(0, num):
        ax=plt.subplot(5,5, 1+i)
        ax.imshow(images[idx],cmap='binary')
        title = str(i) + ',' + label_dict[int(labels[idx][0])]
        # 將labels[idx]轉換為整數
        #title=str(i)+','+label_dict[labels[i][0]]
        if len(prediction)>0:
            title+='=>'+label_dict[np.argmax(prediction[i])]
        ax.set_title(title,fontsize=10)
        ax.set_xticks([]);ax.set_yticks([])
        idx+=1
    #plt.show()
    plt.savefig('images/cifar10-predict.png', dpi=300)
plot_images_labels_prediction(x_img_test,y_label_test,
                              prediction,0,10)
#+end_src

#+RESULTS:
#+CAPTION: Caption
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/cifar10-predict.png]]

#+BEGIN_SRC python -r -n :async :results output :exports both :session QQQ

### 9. 查看預測機率
Predicted_Probability=model.predict(x_img_test_normalize)
def show_Predicted_Probability(y, prediction,
                               x_img, Predicted_Probability, i):
    print('label:',label_dict[y[i][0]],
          'predict:',label_dict[np.argmax(prediction[i])])
    plt.figure(figsize=(2,2))
    plt.imshow(np.reshape(x_img_test[i],(32, 32, 3)))
    #plt.show()
    plt.savefig(f'images/cifar10-predict-image-{i}.png', dpi=300)
    print(f'images/cifar10-predict-image-{i}.png')
    for j in range(10):
        #print(label_dict[j])
        #print(Predicted_Probability[i][j])
        print(label_dict[j]+
              ' Probability:%1.9f'%(Predicted_Probability[i][j]))

show_Predicted_Probability(y_label_test, prediction,
                           x_img_test, Predicted_Probability, 0)
show_Predicted_Probability(y_label_test, prediction,
                           x_img_test, Predicted_Probability, 3)
#+end_src

#+RESULTS:
#+begin_example
313/313 [==============================] - 6s 20ms/step
label: cat predict: cat
images/cifar10-predict-image-0.png
airplane Probability:0.006463123
automobile Probability:0.008624317
bird Probability:0.027447252
cat Probability:0.500399292
deer Probability:0.028155502
dog Probability:0.090648025
frog Probability:0.307343274
horse Probability:0.004685596
ship Probability:0.021273408
truck Probability:0.004960205
label: airplane predict: airplane
images/cifar10-predict-image-3.png
airplane Probability:0.480774581
automobile Probability:0.005904566
bird Probability:0.244085774
cat Probability:0.016937478
deer Probability:0.114243045
dog Probability:0.010711966
frog Probability:0.010022460
horse Probability:0.016119620
ship Probability:0.092414141
truck Probability:0.008786337
#+end_example

#+CAPTION: Image 0
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 50
#+ATTR_ORG: :width 50
#+ATTR_HTML: :width 50
[[file:images/cifar10-predict-image-0.png]]
#+CAPTION: Image 3
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 50
#+ATTR_ORG: :width 50
#+ATTR_HTML: :width 50
[[file:images/cifar10-predict-image-3.png]]
** 儲存模型
#+BEGIN_SRC python -r -n :async :results output :exports both :session QQQ
### 10. confusion matrix
import pandas as pd
print(label_dict)
pd.crosstab(y_label_test.reshape(-1),np.argmax(prediction, axis=1),
            rownames=['label'],colnames=['predict'])
### 11. Save model to JSON
model_json = model.to_json()
with open("Downloads/cifarCnnModelnew.json", "w") as json_file:
    json_file.write(model_json)

### 12. Save Weight to h5
model.save_weights("Downloads/cifarCnnModelnew.h5")
print("Saved model to disk")
#+END_SRC

#+RESULTS:
: {0: 'airplane', 1: 'automobile', 2: 'bird', 3: 'cat', 4: 'deer', 5: 'dog', 6: 'frog', 7: 'horse', 8: 'ship', 9: 'truck'}
: Saved model to disk

#+latex:\newpage`

* [課堂練習][作業] :TNFSH:
** [課堂練習]Regression :TNFSH:
現在來看看另一組較複雜的數據資料，請你試著自己建個模型來預測這些詭異的資料...
請參考[[實作3: Regression]]，完成以下工作畫出Loss的折線圖來評估模型的優劣。
*** colab :noexport:
https://colab.research.google.com/drive/1tBJbV2_kpq6vPcA27MOxCQ5RAkVrulLM#scrollTo=hE4ch7l5qG77
*** 資料分佈
#+begin_src python -r -n :results output :exports both
import numpy as np
import matplotlib.pyplot as plt

# Seed the random number generator for reproducibility
np.random.seed(0)

x_data = np.linspace(-10, 10, num=2000)
y_data = 2.9 * np.cos(0.6 * x_data) + np.random.normal(size=len(x_data))

plt.scatter(x_data, y_data, s=6)
plt.savefig('images/random_curve.png', dpi=300)
#+end_src

#+RESULTS:

#+CAPTION: CNN迴歸練習
#+LABEL:fig:CNN_practice
#+name: fig:CNN_practice
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/random_curve.png]]
#+latex:\newpage
*** solution :noexport:
#+begin_src python -r -n :async :results output :exports both :session QQQ
import numpy as np
import matplotlib.pyplot as plt

# Seed the random number generator for reproducibility
np.random.seed(0)

x_data = np.linspace(-10, 10, num=2000)
y_data = 2.9 * np.cos(0.6 * x_data) + np.random.normal(size=len(x_data))

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout

# A simple regression model
model = Sequential()
model.add(Dense(16, input_shape=(1,)))
model.add(Dropout(0.5))
model.add(Dense(64, input_shape=(1,)))
model.add(Dropout(0.5))
model.add(Dense(32, input_shape=(1,)))
model.add(Dropout(0.5))
model.add(Dense(1, input_shape=(1,)))
model.compile(loss='mean_absolute_percentage_error', optimizer='rmsprop')

# The fit() method - trains the model
train_history = model.fit(x=x_data, y=y_data,
                epochs=100, batch_size=100,
                verbose=0)

#print(train_history.history)
#print(train_history.history.keys())

import matplotlib.pyplot as plt
plt.title('Train History')
plt.ylabel('loss')
plt.xlabel('Epoch')
plt.plot(train_history.history['loss'])
plt.savefig('images/cnn-regression-pract.png', dpi=300)
score = model.evaluate(x_data, y_data, batch_size=200)
print('得分:', score)
model.predict(np.expand_dims(x_data[:3],1))
print(x_data[:3])
print(y_data[:3])
#+end_src

#+RESULTS:
: 10/10 [==============================] - 0s 819us/step - loss: 100.2167
: 得分: 100.2166519165039
: 1/1 [==============================] - 0s 22ms/step
: [-10.          -9.989995    -9.97998999]
: [4.54854618 3.17973664 3.75330284]
#+CAPTION: Caption
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/cnn-regression-pract.png]]

** [作業]病例預測 :TNFSH:
:PROPERTIES:
:CUSTOM_ID: HomeWork-2
:END:
*** 背景
某醫學研究中心針對旗下醫院800名疑似患有「無定向喪心病狂間歇性全身機能失調症」的患者做了一份病徵研究，針對以下這些可能病徵進行程度檢驗
1. 抑鬱
2. 癲癇
3. 精神分裂
4. 輕挑驕傲
5. 沒大沒小
6. 有犯罪傾向
7. 月經前緊張(男患者嚴重的話也有)
8. 有自殺傾向
這800份資料可以[[https://letranger.github.io/downloads/qq.csv][點選這裡]]下載，每筆資料有九個欄位，前八欄分別對應到上述八項病徵，最後一欄為0/1，代表病患是否患有該病。

請你建立一個預測MODEL，以利該中心未來做檢測使用。將來只要遇到類似病情的個案，該醫院就能先針對這些特徵值進行檢測，並將檢測結果輸入此MODEL來預測個案是否為此病患者，並即時予以適當治療。
*** 作業要求
- 嗯，基本上就是自由心證，你能交多少就交多少，你想只交一張圖也行，你要從頭交待你在做什麼、每一個步驟有啥意義、一共測了幾種CASE、最後成果如何、你的心得....也行，看你的誠意啦-_-(這向來是最坑人的一句話)
- 我是這樣覺得啦...MODEL積木隨便叠一叠，精確度至少也不應該低於 *0.8* 吧...QQ
*** 參考答案 :noexport:
- [[https://colab.research.google.com/drive/1RjvgCt_QUPB7CQVTQ7DHL6qzfdfR1EYx][05.Keras糖尿病預測.ipynb]]
- https://www.kaggle.com/saurabh00007/diabetescsv?select=diabetes.csv

** [作業]照片分類 :TNFSH:
*** 基本要求
- 仿照MNist
- 自行蒐集/生成要分類的圖片，例如：注音符號辨識
- 需對圖片進行尺寸調整，並進行資料擴增

*** 繳交項目
**** 影像辨識實驗(PDF):
- 組員名單、每個人所負責的項目(完全沒做事的組員就在項目後寫"吉祥物")
- 實驗目的
- 實驗內容
- 資料預處理的過程
- 模型設計
- 訓練過程
- 訓練結果

*** Resources
**** 如何讀取自己的資料:  [[https://towardsdatascience.com/loading-custom-image-dataset-for-deep-learning-models-part-1-d64fa7aaeca6][Loading Custom Image Dataset for Deep Learning Models: Part 1]]
**** Typical steps for loading custom dataset for Deep Learning Models
1. Open the image file. The format of the file can be JPEG, PNG, BMP, etc.
1. Resize the image to match the input size for the Input layer of the Deep Learning model.
1. Convert the image pixels to float datatype.
1. Normalize the image to have pixel values scaled down between 0 and 1 from 0 to 255.
1. Image data for Deep Learning models should be either a numpy array or a tensor object.
**** Data augmentation
- [[https://www.gushiciku.cn/pl/pL8p/zh-tw][Kaggle知識點：資料擴增方法]]
- [[https://tw.leaderg.com/article/index?sn=11132][影像資料擴增 (Image Data Augmentation) 的原理與實作]]
- [[https://iter01.com/465481.html][深度學習領域的資料增強]]

* 實作4: 真實世界圖片辨識
使用少量資料訓練影像分類在實務的電腦視覺應用上十分常見，此處所謂少量樣本從幾百到幾萬張都算在內。此處以 4000 張為例(2000 cats v.s. 2000 dogs)，過程中使用 2000 張來訓練、1000 張用來驗證、1000 張用來測試。接下來導入以下技術來克服 overfitting:
- 資料擴增法(data augmentation):這是常用於減輕電腦視覺 overfitting 的強大技術，可以改善神經網路的成效，提升到 82%的準確率。
- 預先訓練神經網路的特徵萃取法(feature extraction with a pretrained network):應用於少量資料集的基本技術，可使神經網路成效達到 90%~96%的準確度。
- 微調預先訓練神經網路法(fine-tuning a pretrained network):也是常用於深度學習少量資料集的技術，將使神經網路準確率提升到 97%。

** 深度學習與少量資料的相關性
深度學習的基本特色是在它能自行在訓練資料中找到有趣的特徵，而不需要人為介入，但這只有在具備大量訓練樣本時才成立，特別是對於像圖片這類高維度(high-dimensional)的輸入樣本。所以也有人說深度學習一定要有大量資料才能進行。

然而樣本數與神經網路的大小與深度息息相關。只用幾十個樣本不可能訓練出可以解決複雜問題的卷積神經網路；相反的，如果只是要用來解決簡單任務，而且已經做好了 well-regularized 的小 model，那麼幾百個樣本或許就足夠了。因為卷積神經網路可以學習局部 pattern 且具平移不變性，所以在感知問題上具有高度的資料效率性。

此外，本質上，深度學習 model 是可高度再利用的。例如，使用大規模資料集訓練的影像 model 或語音轉文字的 model，只要進行小小的更改，便可以重新用於其他不同問題上。以電腦視覺的應用而言，許多預先訓練好的 model(通常是使用 Image-Net 資料集進行訓練)都是可公開下載的，以這些預先訓練好的 model 為基礎，再加以少量資料的訓練，就能產出更強大的 model。

** 實作
*** 下載資料
:PROPERTIES:
:ID:       282b4f28-e449-46e2-bc15-6b4718b90674
:END:
2013 年的 Kaggle 貓狗辨識大賽，最佳 model 即是使用 CNN，當時準確率達 95%，2013 年後的準確率已提高至 98%。本案例之資料來源：[[https://www.kaggle.com/c/dogs-vs-cats/data]]，由於原始圖片尺寸未做修改，大小各異，故需先額外處理。

最終希望生成的資料夾架構如下:

#+CAPTION: 原生資料、訓練用資料架構
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 200
[[file:images/實作4:_真實世界圖片辨識/2024-02-29_12-54-38_2024-02-29_12-52-33.png]]

複製圖片到訓練、驗證和測試目錄的程式碼如下：
#+begin_src python -r -n :async :results output :exports both :session QQQ
import os

# 專案的根目錄路徑
ROOT_DIR = os.getcwd()

# 置放coco圖像資料與標註資料的目錄
DATA_PATH = os.path.join(ROOT_DIR, "/Volumes/LaCie/data")

import os, shutil

# 原始數據集的路徑
original_dataset_dir = os.path.join(DATA_PATH, "raw")

# 存儲小數據集的目錄
base_dir = os.path.join(DATA_PATH, "cats_and_dogs_small")
if not os.path.exists(base_dir):
    os.mkdir(base_dir)

# 我們的訓練資料的目錄
train_dir = os.path.join(base_dir, 'train')
if not os.path.exists(train_dir):
    os.mkdir(train_dir)

# 我們的驗證資料的目錄
validation_dir = os.path.join(base_dir, 'validation')
if not os.path.exists(validation_dir):
    os.mkdir(validation_dir)

# 我們的測試資料的目錄
test_dir = os.path.join(base_dir, 'test')
if not os.path.exists(test_dir):
    os.mkdir(test_dir)

# 貓的圖片的訓練資料目錄
train_cats_dir = os.path.join(train_dir, 'cats')
if not os.path.exists(train_cats_dir):
    os.mkdir(train_cats_dir)

# 狗的圖片的訓練資料目錄
train_dogs_dir = os.path.join(train_dir, 'dogs')
if not os.path.exists(train_dogs_dir):
    os.mkdir(train_dogs_dir)

# 貓的圖片的驗證資料目錄
validation_cats_dir = os.path.join(validation_dir, 'cats')
if not os.path.exists(validation_cats_dir):
    os.mkdir(validation_cats_dir)

# 狗的圖片的驗證資料目錄
validation_dogs_dir = os.path.join(validation_dir, 'dogs')
if not os.path.exists(validation_dogs_dir):
    os.mkdir(validation_dogs_dir)

# 貓的圖片的測試資料目錄
test_cats_dir = os.path.join(test_dir, 'cats')
if not os.path.exists(test_cats_dir):
    os.mkdir(test_cats_dir)

# 狗的圖片的測試資料目錄
test_dogs_dir = os.path.join(test_dir, 'dogs')
if not os.path.exists(test_dogs_dir):
    os.mkdir(test_dogs_dir)
#+end_src
*** 分割資料集
以下程式會產生三組資料集：訓練集狗貓各 1000、驗證集各 500、測試集各 500，可再以下列程式驗證：
#+begin_src python -r -n :async :results output :exports both :session QQQ
# 複製前1000個貓的圖片到train_cats_dir
fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dst = os.path.join(train_cats_dir, fname)
    if not os.path.exists(dst):
        shutil.copyfile(src, dst)

print('Copy first 1000 cat images to train_cats_dir complete!')

# 複製下500個貓的圖片到validation_cats_dir
fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dst = os.path.join(validation_cats_dir, fname)
    if not os.path.exists(dst):
        shutil.copyfile(src, dst)

print('Copy next 500 cat images to validation_cats_dir complete!')

# 複製下500個貓的圖片到test_cats_dir
fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dst = os.path.join(test_cats_dir, fname)
    if not os.path.exists(dst):
        shutil.copyfile(src, dst)

print('Copy next 500 cat images to test_cats_dir complete!')

# 複製前1000個狗的圖片到train_dogs_dir
fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dst = os.path.join(train_dogs_dir, fname)
    if not os.path.exists(dst):
        shutil.copyfile(src, dst)

print('Copy first 1000 dog images to train_dogs_dir complete!')

# 複製下500個狗的圖片到validation_dogs_dir
fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dst = os.path.join(validation_dogs_dir, fname)
    if not os.path.exists(dst):
        shutil.copyfile(src, dst)

print('Copy next 500 dog images to validation_dogs_dir complete!')

# C複製下500個狗的圖片到test_dogs_dir
fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dst = os.path.join(test_dogs_dir, fname)
    if not os.path.exists(dst):
        shutil.copyfile(src, dst)

print('Copy next 500 dog images to test_dogs_dir complete!')
#+end_src

#+RESULTS:
: Copy first 1000 cat images to train_cats_dir complete!
: Copy next 500 cat images to validation_cats_dir complete!
: Copy next 500 cat images to test_cats_dir complete!
: Copy first 1000 dog images to train_dogs_dir complete!
: Copy next 500 dog images to validation_dogs_dir complete!
: Copy next 500 dog images to test_dogs_dir complete!
*** 刪除隱藏檔
只有macos需要
#+begin_src python -r -n :async :results output :exports both :session QQQ
import os

# 定義函式來刪除指定目錄下所有以 . 開頭的隱藏檔案
def delete_hidden_files(directory):
    # 遍歷指定目錄下的所有檔案和子目錄
    for root, dirs, files in os.walk(directory):
        for file in files:
            # 如果檔案名稱以 . 開頭，則刪除它
            if file.startswith('.'):
                file_path = os.path.join(root, file)
                os.remove(file_path)

# 呼叫函式來刪除隱藏檔案
delete_hidden_files("/Volumes/LaCie/data/raw")
delete_hidden_files("/Volumes/LaCie/data/cats_and_dogs_small")
#+end_src
*** 複檢檔案個數
#+begin_src python -r -n :async :results output :exports both :session QQQ
print('total training cat images:', len(os.listdir(train_cats_dir)))
print('total training dog images:', len(os.listdir(train_dogs_dir)))
print('total validation cat images:', len(os.listdir(validation_cats_dir)))
print('total validation dog images:', len(os.listdir(validation_dogs_dir)))
print('total test cat images:', len(os.listdir(test_cats_dir)))
print('total test dog images:', len(os.listdir(test_dogs_dir)))
#+end_src

#+RESULTS:
: total training cat images: 1000
: total training dog images: 1000
: total validation cat images: 500
: total validation dog images: 500
: total test cat images: 500
: total test dog images: 500
*** 資料預處理
資料在送入神經網路前應先將 JPEG 檔案格式化成適當的浮點數張量，其步驟如下：
1. 讀取影像檔
1. 將 JPEG 內容解碼為 RGB 的像素
1. 將 RGB 像素轉為浮點數張量
1. 將像素值(0~255)壓縮到[0,1]區間

上述過程可以用 Keras 的 keras.preprocessing.image 模組來處理，它包含 ImageDataGenerator 類別，過程如下：
#+NAME: 使用 ImageDataGenerator 產生器從目錄中讀取影像
#+begin_src python -r -n :async :results output :exports both :session QQQ
from keras.preprocessing.image import ImageDataGenerator

# 所有的圖像將重新被進行歸一化處理 Rescaled by 1./255
train_datagen = ImageDataGenerator(rescale=1./255) #設定訓練、測試資料的 Python 產生器，並將圖片像素值依 1/255 比例重新壓縮到 [0, 1] (ref:ImageDataGenerator)
test_datagen = ImageDataGenerator(rescale=1./255)

# 直接從檔案目錄讀取圖像檔資料
train_generator = train_datagen.flow_from_directory(
        # 這是圖像資料的目錄
        train_dir,
        # 所有的圖像大小會被轉換成150x150
        target_size=(150, 150),
        # 每次產生20圖像的批次資料
        batch_size=20,
        # 由於這是一個二元分類問題, y的lable值也會被轉換成二元的標籤
        class_mode='binary')

# 直接從檔案目錄讀取圖像檔資料
validation_generator = test_datagen.flow_from_directory(
        validation_dir,
        target_size=(150, 150),
        batch_size=20,
        class_mode='binary')
#+end_src

#+RESULTS:
: Found 2000 images belonging to 2 classes.
: Found 1000 images belonging to 2 classes.
查看照片
#+begin_src python -r -n :async :results output :exports both :session QQQ
for data_batch, labels_batch in train_generator:
    print('data batch shape:', data_batch.shape)
    print('labels batch shape:', labels_batch.shape)
    break
#+end_src

#+RESULTS:
: data batch shape: (20, 150, 150, 3)
: labels batch shape: (20,)

結果顯示每批次產生出的資料為 20 張 150\times150 的 RGB 影像以及 20 個 label(即答案)，需留意的是此處的 generator 會無 止盡的生成批次量樣本，也就會不停的持續循環產生影像到目標目錄中，所以要放 break。而上述程式中的 ImageDataGenerator(第[[(ImageDataGenerator)]]行)是一種產生器(Generator)，在 Python 中是一個持續迭代運作的物件，是一個可以與 for...in 一起使用的物件，產生器是使用 yield 建構的。典型的產生器範例如下：
#+NAME: python generator的DEMO
#+BEGIN_SRC python -r -n :results output
def generator():
    i = 0
    while True:
        i += 1
        yield i

for item in generator():
    print(item)
    if item > 3:
        break
#+END_SRC

#+RESULTS:
: 1
: 2
: 3
: 4
*** 建立神經網路
#+begin_src python -r -n :async :results output :exports both :session QQQ
from keras import layers
from keras import models
model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu',
                        input_shape=(150, 150, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.summary()  # 查看模型摘要
#+END_SRC

#+RESULTS:
#+begin_example
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 conv2d (Conv2D)             (None, 148, 148, 32)      896

 max_pooling2d (MaxPooling2  (None, 74, 74, 32)        0
 D)

 conv2d_1 (Conv2D)           (None, 72, 72, 64)        18496

 max_pooling2d_1 (MaxPoolin  (None, 36, 36, 64)        0
 g2D)

 conv2d_2 (Conv2D)           (None, 34, 34, 128)       73856

 max_pooling2d_2 (MaxPoolin  (None, 17, 17, 128)       0
 g2D)

 conv2d_3 (Conv2D)           (None, 15, 15, 128)       147584

 max_pooling2d_3 (MaxPoolin  (None, 7, 7, 128)         0
 g2D)

 flatten (Flatten)           (None, 6272)              0

 dense (Dense)               (None, 512)               3211776

 dense_1 (Dense)             (None, 1)                 513

=================================================================
Total params: 3453121 (13.17 MB)
Trainable params: 3453121 (13.17 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
#+end_example
*** 訓練模型
建構好 model、整理完資料，接下來就可以調整 model 來搭配產生器所產生的資料，我們可以應用 model 的 fit_generator 方法，這個方法的第 1 個參數即是一個 Python 的產生器，然而由於資料是無止盡地產生，所以在宣告訓練時期之前，Keras model 需要知道從產生器抽取多少樣本，這就是 steps_per_epoch 參數的功能，它指定了從產生器取得的批次量，也就是說，model 在運行了 steps_per_epoch 次的梯度下降步驟後，訓練過程將進入下一個訓練週期(epochs)。在以下的例子中，每個批次量包含 20 個樣本，而目標樣本有 2000 個，所以就需要有 100 個批次量。
#+begin_src python -r -n :async :results output :exports both :session QQQ
# 配置 model 以進行訓練
from keras import optimizers

model.compile(loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(learning_rate=1e-4),
              metrics=['acc'])

history = model.fit(
    train_generator,   #設定產生器
    steps_per_epoch=100,   #設定從產生器抽取100個批次量
    epochs=30, verbose=1, #verbose=1, 不顯示訓練過程
    validation_data=validation_generator,
    validation_steps=50)

model.save('cats_and_dogs_small_i.h5')
#+end_src

#+RESULTS:
#+begin_example
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.
Epoch 1/30
100/100 [==============================] - 19s 183ms/step - loss: 0.6925 - acc: 0.5325 - val_loss: 0.6839 - val_acc: 0.5910
Epoch 2/30
100/100 [==============================] - 19s 186ms/step - loss: 0.6794 - acc: 0.5655 - val_loss: 0.6703 - val_acc: 0.5570
Epoch 3/30
100/100 [==============================] - 20s 195ms/step - loss: 0.6570 - acc: 0.6160 - val_loss: 0.6497 - val_acc: 0.6100
Epoch 4/30
100/100 [==============================] - 20s 205ms/step - loss: 0.6314 - acc: 0.6425 - val_loss: 0.6204 - val_acc: 0.6510
Epoch 5/30
100/100 [==============================] - 20s 202ms/step - loss: 0.5973 - acc: 0.6805 - val_loss: 0.6142 - val_acc: 0.6550
Epoch 6/30
100/100 [==============================] - 21s 205ms/step - loss: 0.5708 - acc: 0.7005 - val_loss: 0.6006 - val_acc: 0.6720
Epoch 7/30
100/100 [==============================] - 21s 206ms/step - loss: 0.5496 - acc: 0.7200 - val_loss: 0.5847 - val_acc: 0.6830
Epoch 8/30
100/100 [==============================] - 21s 207ms/step - loss: 0.5374 - acc: 0.7305 - val_loss: 0.5858 - val_acc: 0.6810
Epoch 9/30
100/100 [==============================] - 21s 207ms/step - loss: 0.5207 - acc: 0.7360 - val_loss: 0.5729 - val_acc: 0.6890
Epoch 10/30
100/100 [==============================] - 21s 209ms/step - loss: 0.4929 - acc: 0.7475 - val_loss: 0.6003 - val_acc: 0.6790
Epoch 11/30
100/100 [==============================] - 21s 209ms/step - loss: 0.4752 - acc: 0.7720 - val_loss: 0.5594 - val_acc: 0.7030
Epoch 12/30
100/100 [==============================] - 21s 210ms/step - loss: 0.4528 - acc: 0.7920 - val_loss: 0.6129 - val_acc: 0.6820
Epoch 13/30
100/100 [==============================] - 21s 210ms/step - loss: 0.4314 - acc: 0.7945 - val_loss: 0.6598 - val_acc: 0.6560
Epoch 14/30
100/100 [==============================] - 21s 210ms/step - loss: 0.4089 - acc: 0.8115 - val_loss: 0.5471 - val_acc: 0.7180
Epoch 15/30
100/100 [==============================] - 21s 212ms/step - loss: 0.3870 - acc: 0.8290 - val_loss: 0.5317 - val_acc: 0.7360
Epoch 16/30
100/100 [==============================] - 21s 213ms/step - loss: 0.3602 - acc: 0.8460 - val_loss: 0.5530 - val_acc: 0.7200
Epoch 17/30
100/100 [==============================] - 21s 213ms/step - loss: 0.3459 - acc: 0.8545 - val_loss: 0.6352 - val_acc: 0.7050
Epoch 18/30
100/100 [==============================] - 21s 213ms/step - loss: 0.3171 - acc: 0.8785 - val_loss: 0.5586 - val_acc: 0.7340
Epoch 19/30
100/100 [==============================] - 22s 215ms/step - loss: 0.3034 - acc: 0.8735 - val_loss: 0.5616 - val_acc: 0.7380
Epoch 20/30
100/100 [==============================] - 23s 231ms/step - loss: 0.2810 - acc: 0.8795 - val_loss: 0.6145 - val_acc: 0.7240
Epoch 21/30
100/100 [==============================] - 22s 223ms/step - loss: 0.2573 - acc: 0.9030 - val_loss: 0.5821 - val_acc: 0.7300
Epoch 22/30
100/100 [==============================] - 23s 225ms/step - loss: 0.2400 - acc: 0.9080 - val_loss: 0.6027 - val_acc: 0.7320
Epoch 23/30
100/100 [==============================] - 22s 219ms/step - loss: 0.2166 - acc: 0.9195 - val_loss: 0.6185 - val_acc: 0.7360
Epoch 24/30
100/100 [==============================] - 23s 225ms/step - loss: 0.2017 - acc: 0.9250 - val_loss: 0.6237 - val_acc: 0.7390
Epoch 25/30
100/100 [==============================] - 22s 219ms/step - loss: 0.1797 - acc: 0.9350 - val_loss: 0.7119 - val_acc: 0.7260
Epoch 26/30
100/100 [==============================] - 22s 217ms/step - loss: 0.1644 - acc: 0.9385 - val_loss: 0.7026 - val_acc: 0.7290
Epoch 27/30
100/100 [==============================] - 22s 225ms/step - loss: 0.1365 - acc: 0.9590 - val_loss: 0.6876 - val_acc: 0.7400
Epoch 28/30
100/100 [==============================] - 22s 219ms/step - loss: 0.1319 - acc: 0.9570 - val_loss: 0.6853 - val_acc: 0.7420
Epoch 29/30
100/100 [==============================] - 22s 223ms/step - loss: 0.1172 - acc: 0.9630 - val_loss: 0.7569 - val_acc: 0.7270
Epoch 30/30
100/100 [==============================] - 22s 221ms/step - loss: 0.0980 - acc: 0.9705 - val_loss: 0.7442 - val_acc: 0.7420
/Users/letranger/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(
#+end_example

使用上述 fit_generator 時，還可以傳遞 validation_data 參數，此參數可以接收一個資料產生器，也可以接收 Numpy 陣列，如果接收的資料來自產生器，則還要指定 validation_steps 參數，告訴程式要從產生器中抽取多少次批量進行評估。在完成訓練後把 model 存起來，並繪製訓練週期與驗證週期的 model 損失值與準確度。
*** 評估模型
#+begin_src python -r -n :async :results output :exports both :session QQQ
import matplotlib.pyplot as plt

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.clf()
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.plot()
plt.savefig("images/cats-and-dogs-accuracy-v1.png")
plt.figure()

plt.clf()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.plot()
plt.savefig("images/cats-and-dogs-loss-v1.png")
#+end_src

#+RESULTS: 完整的初步cats and dogs model
#+CAPTION: Cats and Dogs Accuracy V1
#+LABEL:fig:Cat-Dog-Acc-V1
#+name: fig:Cat-Dog-Acc-V1
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/cats-and-dogs-accuracy-v1.png]]

#+CAPTION: Cats and Dogs Loss V1
#+LABEL:fig:Cat-Dog-Loss-V1
#+name: fig:Cat-Dog-Loss-V1
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/cats-and-dogs-loss-v1.png]]

由圖[[fig:Cat-Dog-Acc-V1]]看出訓練準確度成線性成長直到逼近 100%，但驗證準確度則在第三個訓練週期後就停留在 70%；訓練損失分數也呈線性下降，但驗證損失分數則約在第 12 週期後達到最低點。這些都是明顯的 overfitting 訊號。

由於訓練樣本數(2000)相對較少，overfitting 將成為訓練 model 的首要顧慮因素，幾種緩解 overfitting 的技術有：
- dropout
- 權重調整(L2 regularization)
- 資料擴增法(data augmentation)

** 改善#1: 資料擴增
Overfitting 的部份成因是由於樣本太少導致無法訓練出具備普適性、可套用到新資料的 model，想像一下如果有無限量的資料，則 model 將會因應用手邊資料的各種可能面向，也就不致於 overfitting。資料擴增(Data Augmentation)就是由現有訓練樣本生成更多訓練資料的方法，主要是透過隨機變換原始資料，以產生相似的影像，進而增加訓練樣本數。最終目標是在訓練時，model 不會看到兩次完全相同的影像。

在 Keras 中，我們可以藉由設定 ImageDataGenerator，在讀取影像時執行隨機變換(random transformation)來達到資料擴增，至於變換的方向則可以在 ImageDataGenerator 的參數中進一步指定。以下例來看：

#+begin_src python -r -n :async :results output :exports both :session QQQ
datagen = ImageDataGenerator(
    rotation_range=40,       #旋轉角度值(0~180)
    width_shift_range=0.2,   #水平隨機平移(圖片寬度之百分比)
    height_shift_range=0.2,  #垂直隨機平移(圖片高度之百分比)
    shear_range=0.2,         #隨機傾斜(順時鐘傾斜角度)
    zoom_range=0.2,          #隨機縮放(縮放百分比)
    horizontal_flip=True,    #隨機水平翻轉(影像非左右對稱才有效)
    fill_mode='nearest')     #新建影像填補像素方法
#+END_SRC

#+RESULTS:

上述程式之 fill__mode 共提供四種像素填補方法：
- constant: 依照輸入的 cval(浮點數或整數)將影像邊界之外都以該值填補，例如 cval=k，則影像填補為 kkkkkkkk|abcd|kkkkkkkk
- nearest: 以最接近的像素值填補，如：aaaaaaaa|abcd|dddddddd
- reflect: 以影像重複填補(影像以一正一反方向)，如 abcddcba|abcd|dcbaabcd
- wrap: 以影像重複填補，如：abcdabcd|abcd|abcdabcd

以下為實際運作的示範：

#+begin_src python -r -n :async :results output :exports both :session QQQ
import matplotlib
import platform
if platform.system() == 'Darwin':
    matplotlib.use('MacOSX')
else:
    matplotlib.use('TkAgg')

from keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rotation_range=40,       #旋轉角度值(0~180)
    width_shift_range=0.2,   #水平隨機平移(圖片寬度之百分比)
    height_shift_range=0.2,  #垂直隨機平移(圖片高度之百分比)
    shear_range=0.2,         #隨機傾斜(順時鐘傾斜角度)
    zoom_range=0.2,          #隨機縮放(縮放百分比)
    horizontal_flip=True,    #隨機水平翻轉(影像非左右對稱才有效)
    fill_mode='nearest')     #新建影像填補像素方法

import os, shutil

# 解壓縮資料夾所在的目錄路徑(以訓練集中的貓為例)
train_cats_dir = os.path.join(train_dir, 'cats')

from keras.preprocessing import image
import numpy as np

fnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)]

img_path = fnames[3] #選一張影像來擴充
print(img_path)

#讀取影像、調整大小
img = image.load_img(img_path, target_size=(150, 150))
#將其調整為shape=(150, 150, 3)
x = image.img_to_array(img)
#調整shape為(1, 150, 150, 3)
x = x.reshape((1, ) + x.shape)
print(x.shape)

i = 0
# 繪製model的損失率與精確率
import matplotlib.pyplot as plt

for batch in datagen.flow(x, batch_size=1):
    plt.figure(i)
    #imgplot = plt.imshow(image.array_to_img(batch[0]))
    plt.imshow(image.array_to_img(batch[0]))
    #plt.clf()
    plt.plot()
    plt.savefig("CatsAugmentation"+str(i)+".png")
    i += 1
    if i % 4 == 0:
        break
#plt.show()
#plt.savefig("CatsAugmentation.png")
#+END_SRC

#+RESULTS:
: /Volumes/LaCie/data/cats_and_dogs_small/train/cats/cat.3.jpg
: (1, 150, 150, 3)

#+CAPTION: Cats image augmentation
#+LABEL:fig:CatAugmentation
#+name: fig:CatAugmentation
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 400
[[file:images/CatsAugmentation0.png]]

#+CAPTION: Cats image augmentation
#+LABEL:fig:CatAugmentation
#+name: fig:CatAugmentation
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 400
[[file:images/CatsAugmentation1.png]]

#+CAPTION: Cats image augmentation
#+LABEL:fig:CatAugmentation
#+name: fig:CatAugmentation
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 400
[[file:images/CatsAugmentation2.png]]

#+CAPTION: Cats image augmentation
#+LABEL:fig:CatAugmentation
#+name: fig:CatAugmentation
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 400
[[file:images/CatsAugmentation3.png]]

雖然資料擴增能擴充來自少量的原始圖片，但終究無法自行產生資訊，只能重新混合現有資訊，影像間仍是高度相關，仍不足以完全擺脫 overfitting 問題，所以進一步在密集連接的分類器前，在 model 中增加 Dropout 層(Fatten 層後)。
#+begin_src python -r -n :async :results output :exports both :session QQQ
import matplotlib
import platform
if platform.system() == 'Darwin':
    matplotlib.use('MacOSX')
else:
    matplotlib.use('TkAgg')

from keras.preprocessing.image import ImageDataGenerator

import os, shutil

# 解壓縮資料夾所在的目錄路徑
# 參考原程式的路徑

# 分拆成訓練、驗證與測試目錄位置
train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')
test_dir = os.path.join(base_dir, 'test')
train_cats_dir = os.path.join(train_dir, 'cats')
train_dogs_dir = os.path.join(train_dir, 'dogs')
validation_cats_dir = os.path.join(validation_dir, 'cats')
validation_dogs_dir = os.path.join(validation_dir, 'dogs')
test_cats_dir = os.path.join(test_dir, 'cats')
test_dogs_dir = os.path.join(test_dir, 'dogs')

# 建立模組
from keras import layers
from keras import models
from keras import regularizers
model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu',
                        input_shape=(150, 150, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dropout(0.5))
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.summary()  # 查看模型摘要

# 配置 model 以進行訓練
from keras import optimizers

model.compile(loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(learning_rate=1e-4),
              metrics=['acc'])

#資料擴增
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True, )

test_datagen = ImageDataGenerator(rescale=1./255) # 請注意！驗證資料不應該擴充!!!

train_generator = train_datagen.flow_from_directory(
  train_dir,    # 目標目錄
  target_size=(150, 150), # 所有圖像大小調整成 150×150
  batch_size=32,
  class_mode='binary') # 因為使用二元交叉熵 binary_crossentropy 作為損失，所以需要二元標籤

validation_generator = test_datagen.flow_from_directory(
  validation_dir,
  target_size=(150, 150),
  batch_size=32,
  class_mode='binary')

# 訓練
history = model.fit_generator(
  train_generator,
  epochs=50, verbose=1,
  validation_data=validation_generator,
  validation_steps=1)

model.save('cats_and_dogs_small_data_augmentation.h5')

# 繪製model的損失率與精確率
import matplotlib.pyplot as plt

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)
plt.clf()
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.plot()
plt.savefig("CatsDogsDataAugmentation-acc.png")
plt.figure()

plt.clf()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.plot()
plt.savefig("CatsDogsDataAugmentation-loss.png")
#+END_SRC

#+RESULTS:
#+begin_example
Model: "sequential_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 conv2d_12 (Conv2D)          (None, 148, 148, 32)      896

 max_pooling2d_12 (MaxPooli  (None, 74, 74, 32)        0
 ng2D)

 conv2d_13 (Conv2D)          (None, 72, 72, 64)        18496

 max_pooling2d_13 (MaxPooli  (None, 36, 36, 64)        0
 ng2D)

 conv2d_14 (Conv2D)          (None, 34, 34, 128)       73856

 max_pooling2d_14 (MaxPooli  (None, 17, 17, 128)       0
 ng2D)

 conv2d_15 (Conv2D)          (None, 15, 15, 128)       147584

 max_pooling2d_15 (MaxPooli  (None, 7, 7, 128)         0
 ng2D)

 flatten_3 (Flatten)         (None, 6272)              0

 dropout_2 (Dropout)         (None, 6272)              0

 dense_6 (Dense)             (None, 512)               3211776

 dense_7 (Dense)             (None, 1)                 513

=================================================================
Total params: 3453121 (13.17 MB)
Trainable params: 3453121 (13.17 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.
Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
Epoch 1/50
63/63 [==============================] - 19s 302ms/step - loss: 0.6926 - acc: 0.5030 - val_loss: 0.6853 - val_acc: 0.5312
Epoch 2/50
63/63 [==============================] - 19s 296ms/step - loss: 0.6916 - acc: 0.5195 - val_loss: 0.6850 - val_acc: 0.5625
...略...
Epoch 49/50
63/63 [==============================] - 30s 469ms/step - loss: 0.5058 - acc: 0.7555 - val_loss: 0.4970 - val_acc: 0.7188
Epoch 50/50
63/63 [==============================] - 29s 463ms/step - loss: 0.5037 - acc: 0.7535 - val_loss: 0.5148 - val_acc: 0.7188
#+end_example

#+CAPTION: Cats and Dogs Data Augmentation - Loss
#+LABEL:fig:Cat-Dog-Data-Augmentation-Loss
#+name: fig:Cat-Dog-Data-Augmentation-Loss
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/CatsDogsDataAugmentation-loss.png]]

#+CAPTION: Cats and Dogs Data Augmentation - Accuracy
#+LABEL:fig:Cat-Dog-Data-Augmentation-Acc
#+name: fig:Cat-Dog-Data-Augmentation-Acc
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/CatsDogsDataAugmentation-acc.png]]

由圖[[fig:Cat-Dog-Data-Augmentation-Acc]]和[[fig:Cat-Dog-Data-Augmentation-Loss]]可以發現，在加入了 data augmentation 和 dropout 後，訓練曲線與驗證曲線漸趨一致，不再 overfitting，model 的準確度也達到 84%。但值的一提的是: 同樣的資料集與演算法，在 Google colab 上以 GPU 執行的結果(下圖)與在本機執行(上圖)時並不相同。

#+CAPTION: Cats and Dogs Data Augmentation on Google colab - Accuracy
#+LABEL:fig:Cat-Dog-Data-Augmentation-Acc-colab
#+name: fig:Cat-Dog-Data-Augmentation-Acc-colab
#+ATTR_LATEX: :width 300
#+ATTR_HTML: width 400
#+ATTR_ORG: :width 300
[[file:images/Cat-Dog-Data-Augmentation-Acc-colab.png]]

#+Caption: Cats and Dogs Data Augmentation on Google colab - Loss
#+LABEL:fig:Cat-Dog-Data-Augmentation-Loss-colab
#+name: fig:Cat-Dog-Data-Augmentation-Loss-colab
#+ATTR_LATEX: :width 300
#+ATTR_HTML: width 400
#+ATTR_ORG: :width 300
[[file:images/Cat-Dog-Data-Augmentation-loss-colab.png]]

在透過進一步 regularization 技術的使用，以及調整神經網路參數(如每個卷積層的過濾器數量、神經網路中的層數)，我們就能獲得更高的準確度(86%或 87%)，但在資料不及的情況下(如本例)，我們仍很難進一步提升準確度，此時，就要使用預先訓練 model。

** 改善 2: Pretrained network
Pretrained network，以簡單的話來說，就是「站在巨人的肩膀」[fn:9]，所謂「巨人」，就是別人已經用 ImageNet 訓練好的模型，例如 Google 的 Inception Model、Microsoft 的 Resnet Model 等等，把它當作 Pre-trained Model，幫助我們提取出照片的特徵(feature)。順帶一提，所謂的 Transfer Learning 就是把 Pre-trained Model 最後一層拔掉 (註：最後一層是用來分類的)，加入新的一層，然後用新資料訓練新層的參數。

能夠用來被當成 pretrained netwrok 的 model 通常是擁有大量資料集的大規模圖片分類模型，如果這個原始資料集足夠大量且具通用性，那麼 pretrained network 學習的空間層次特徵(spartial hierarchy features)就足以充當視覺世界的通用 model，其特徵對於許多不同的電腦視覺問題都同樣有效，即便是要辨識與原始任務完全不同的類別也能通用。

例如，以 ImageNet 先訓練出一個神經網路(其辨識項目為日常生活用品)，然後重新訓練這個已訓練完成的神經網路，去識別和原始樣本天差地別的家具產品等。和許多淺層的神經網路相較，深度學習的關鍵優勢在於學習到的特徵可移植到不同問題上。

以下由 Karen Simonyan 和 Andrew Zisserman 於 2014 年開發的 VGG16 架構。使用 pretrain network 有兩種方式：特徵萃取(feature extraction)和徵調(fine-tuning)。

*** 特徵萃取
Feature extraction 是使用 pretrained network 學習到的表示法，以這些表示法從新樣本中萃取有趣的特徵，然後將這些特徵輸入到從頭訓練的新分類器中進行處理。用於影像分類的 CNN 分為以下兩部份：以一系列的卷積層和池化層開始，以密集連接的分類器結束。第一部分稱為 model 的 convolutional base (卷積基底)，在 CNN 的情況下，特徵萃取以一個 pretrained network 做為 convolutional base，透過 convolutional base 處理新資料，

為何不連分類器也預先訓練？原因是 CNN 的特徵圖是來自影像上通用 pattern 的概念，因此無論面臨何種電腦視覺問題，都能通用；而分類器學習到的表示法可能只適用於 model 所訓練的類別，僅關於整個影像中該類別相關的機率。此外，卷積特徵圖仍會描述物件出現的位置，但全連接層並沒有空間的概念，全連接層學習到的表示法不再包含物件在輸入影像中位罝的任何訊息，所以只要是和物件出現位置相關的問題，全連接層產生的特徵絕大多數是沒有用的。

特定卷積層所萃取出來的表示法，其普適程度取於該層的深度，model 中較早出現的層會萃取局部、高度通用的特徵圖（例如可視邊緣、顏色或紋理），而較深入的層則會萃取更抽象的概念（如貓耳朵、狗眼），如果新的資料集與訓練原始 model 的資料集有很大的差別，最好使用 model 的前幾層來進行特徵萃取，而不是使用整個 convolutional base。以下以 ImageNet 訓練的 VGG16 所產生的 convolutional base 來實作，類似 pretrained 的影像分類 model 還有 Xception、Inception V3、ResNet50、VGG19、MobileNet，均已收錄於 keras.applications。

**** 初始化 model
要使用這個 pretrained model，還需要傳三個參數給 VGG16 建構式：
- weights: 用於初始化 model 的權重檢查點
- include_top: 指在神經網路頂部有沒有包含密集連接的分類器。預設情況下，密集連接分類器對應於 ImageNet 的 1000 個類別。然而，我們實際想分類的可能沒這麼多層，所以這裡不一定要包含預設分類器。
- input_shape: qpaqamo 供給神經網路的影像張量 shape。這個參數為 optional，如果不傳，則神經網路能處理任何 shape 的輸入張量。

這裡會從網路下載VGG16模型以及其參數權重，會花一點時間....

#+NAME: VGG16 pretrained model架構細節
#+begin_src python -r -n :async :results output :exports both :session QQQ
from keras.applications import VGG16

conv_base = VGG16(weights='imagenet',
                  include_top=False,
                  input_shape=(150, 150, 3))
conv_base.summary()
#+END_SRC

#+RESULTS: VGG16 pretrained model架構細節
#+begin_example
Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5
58889256/58889256 [==============================] - 98s 2us/step
Model: "vgg16"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 input_1 (InputLayer)        [(None, 150, 150, 3)]     0

 block1_conv1 (Conv2D)       (None, 150, 150, 64)      1792

 block1_conv2 (Conv2D)       (None, 150, 150, 64)      36928

 block1_pool (MaxPooling2D)  (None, 75, 75, 64)        0

 block2_conv1 (Conv2D)       (None, 75, 75, 128)       73856

 block2_conv2 (Conv2D)       (None, 75, 75, 128)       147584

 block2_pool (MaxPooling2D)  (None, 37, 37, 128)       0

 block3_conv1 (Conv2D)       (None, 37, 37, 256)       295168

 block3_conv2 (Conv2D)       (None, 37, 37, 256)       590080

 block3_conv3 (Conv2D)       (None, 37, 37, 256)       590080

 block3_pool (MaxPooling2D)  (None, 18, 18, 256)       0

 block4_conv1 (Conv2D)       (None, 18, 18, 512)       1180160

 block4_conv2 (Conv2D)       (None, 18, 18, 512)       2359808

 block4_conv3 (Conv2D)       (None, 18, 18, 512)       2359808

 block4_pool (MaxPooling2D)  (None, 9, 9, 512)         0

 block5_conv1 (Conv2D)       (None, 9, 9, 512)         2359808

 block5_conv2 (Conv2D)       (None, 9, 9, 512)         2359808

 block5_conv3 (Conv2D)       (None, 9, 9, 512)         2359808

 block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0

=================================================================
Total params: 14714688 (56.13 MB)
Trainable params: 14714688 (56.13 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
#+end_example

由上述輸出觀察，最終特徵圖的 shape 為(4, 4, 512)，這算是神經網路的 top 層特徵，這個預訓練的 model 共有 13 層 Conv2D 層，最後要再接上密集連接分類器。做法有二：

1. 在資料集上執行 convolutional base，將輸出記錄到硬碟上的 Numpy 陣列，然後再輸入到獨立的密集分類層。這種解決方案只需要為每個輪入影像執行一次 convolutional base，而 convolutional base 是處理過程中成本最高的部份，所以這種做法速度快成本低。但也因如此，這種做法不允許使用資料擴增法。
1. 在頂部(最後端)增加 Dnese 層來擴展 model (conv_base)，並從輸入資料開始，從頭到尾執行整個處理過程。這種方式允許資料擴增技術，因為每次輸入影像在執行 convolutional base 時都會在 model 處理到。但這種方式的成本較高。

**** 快速特徵萃取
先執行 ImageDataGenerator，將影像轉換為 Numpy 陣列及其 label 向量，然後呼叫 conv_base model 的 predict 方法從這些影像中萃取特徵。

#+begin_src python -r -n :async :results output :exports both :session QQQ
from keras.applications import VGG16

conv_base = VGG16(weights='imagenet',
                  include_top=False,
                  input_shape=(150, 150, 3))

import os
import numpy as np
from keras.preprocessing.image import ImageDataGenerator

base_dir = r'/Volumes/LaCie/data/cats_and_dogs_small'
train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')
test_dir = os.path.join(base_dir, 'test')

datagen = ImageDataGenerator(rescale=1./255)
batch_size = 20

def extract_features(directory, sample_count):
    features = np.zeros(shape=(sample_count, 4, 4, 512))
    labels = np.zeros(shape=(sample_count))
    generator = datagen.flow_from_directory(directory,
                                            target_size=(150, 150),
                                            batch_size=batch_size,
                                            class_mode='binary')
    i = 0
    for inputs_batch, labels_batch in generator:
        features_batch = conv_base.predict(inputs_batch)
        features[i * batch_size : (i + 1) * batch_size] = features_batch
        labels[i * batch_size : (i + 1) * batch_size] = labels_batch
        i += 1
        print(i, end=' ') # 由於萃取需要較長的時間，我們印出 i 來檢視進度
        if i * batch_size >= sample_count:
            break
    return features, labels

train_features, train_labels = extract_features(train_dir, 2000)
validation_features, validation_labels = extract_features(validation_dir, 1000)
test_features, test_labels = extract_features(test_dir, 1000)
#+END_SRC

#+RESULTS:
#+begin_example
Found 2000 images belonging to 2 classes.
1/1 [==============================] - 1s 899ms/step
1/1 [==============================] - 1s 768ms/step
...略...
1/1 [==============================] - 1s 1s/step
100 Found 1000 images belonging to 2 classes.
1/1 [==============================] - 1s 1s/step
...略...
1/1 [==============================] - 1s 1s/step
50 Found 1000 images belonging to 2 classes.
1/1 [==============================] - 1s 1s/step
...略...
1/1 [==============================] - 1s 1s/step
50
#+end_example
**** 攤平資料
由於目前的萃取特徵 shape = (樣本數, 4, 4, 512)，為了要提供給全連接層分類器，必須將資料攤平為(樣本數, 8192)。

#+begin_src python -r -n :async :results output :exports both :session QQQ
train_features = np.reshape(train_features, (2000, 4 * 4 * 512))
validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))
test_features = np.reshape(test_features, (1000, 4 * 4 * 512))
#+END_SRC

#+RESULTS:
**** 訓練
接下來就可以建立我們的密集分類層（使用 dropout 和 regularization)在剛剛萃取的資料和標籤上進行訓練。因為只有兩個全連接層，所以訓練的速度會很快。

#+begin_src python -r -n :async :results output :exports both :session QQQ
from keras import models
from keras import layers
from keras import optimizers

model = models.Sequential()
model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))
model.add(layers.Dropout(0.5))  # 丟棄法
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer=optimizers.RMSprop(lr=2e-5),
              loss='binary_crossentropy',
              metrics=['acc'])

history = model.fit(train_features,
                    train_labels,epochs=30,
                    batch_size=20,
                    validation_data=(validation_features, validation_labels))
#+END_SRC

#+RESULTS:
#+begin_example
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.
WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.
Epoch 1/30
100/100 [==============================] - 1s 9ms/step - loss: 0.7346 - acc: 0.7575 - val_loss: 0.4003 - val_acc: 0.8000
...略...
Epoch 30/30
100/100 [==============================] - 1s 6ms/step - loss: 0.0202 - acc: 0.9920 - val_loss: 0.6260 - val_acc: 0.9030
#+end_example
**** 繪圖
#+begin_src python -r -n :async :results output :exports both :session QQQ
import matplotlib.pyplot as plt

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.cla()
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend(loc="lower right")
plt.savefig('images/pretrained-catdog-acc.png', dpi=300)

plt.figure()
plt.cla()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend(loc="upper left")
plt.savefig('images/pretrained-catdog-loss.png', dpi=300)
#+END_SRC

#+RESULTS:
#+CAPTION: Caption
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/pretrained-catdog-acc.png]]

#+CAPTION: Caption
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/pretrained-catdog-loss.png]]
**** 6. 實際執行結果 :noexport:
#+begin_src python -r -n :async :results output :exports both :session QQQ
  #####
  from keras.applications import VGG16

  conv_base = VGG16(weights='imagenet',
                    include_top=False,
                    input_shape=(150, 150, 3))

  import os
  import numpy as np
  from keras.preprocessing.image import ImageDataGenerator

  base_dir = r'/Volumes/dogs-vs-cats/small'
  train_dir = os.path.join(base_dir, 'train')
  validation_dir = os.path.join(base_dir, 'validation')
  test_dir = os.path.join(base_dir, 'test')

  datagen = ImageDataGenerator(rescale=1./255)
  batch_size = 20

  def extract_features(directory, sample_count):
      features = np.zeros(shape=(sample_count, 4, 4, 512))
      labels = np.zeros(shape=(sample_count))
      generator = datagen.flow_from_directory(directory,
                                              target_size=(150, 150),
                                              batch_size=batch_size,
                                              class_mode='binary')
      i = 0
      for inputs_batch, labels_batch in generator:
          features_batch = conv_base.predict(inputs_batch)
          features[i * batch_size : (i + 1) * batch_size] = features_batch
          labels[i * batch_size : (i + 1) * batch_size] = labels_batch
          i += 1
          print(i, end=' ') # 由於萃取需要較長的時間，我們印出 i 來檢視進度
          if i * batch_size >= sample_count:
              break
      return features, labels

  train_features, train_labels = extract_features(train_dir, 2000)
  validation_features, validation_labels = extract_features(validation_dir, 1000)
  test_features, test_labels = extract_features(test_dir, 1000)

  #####
  train_features = np.reshape(train_features, (2000, 4 * 4 * 512))
  validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))
  test_features = np.reshape(test_features, (1000, 4 * 4 * 512))


  #####
  from keras import models
  from keras import layers
  from keras import optimizers

  model = models.Sequential()
  model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))
  model.add(layers.Dropout(0.5))  # 丟棄法
  model.add(layers.Dense(1, activation='sigmoid'))

  model.compile(optimizer=optimizers.RMSprop(lr=2e-5),
                loss='binary_crossentropy',
                metrics=['acc'])

  history = model.fit(train_features,
                      train_labels,epochs=30,
                      batch_size=20,
                      validation_data=(validation_features, validation_labels))


  #####
  import matplotlib.pyplot as plt

  acc = history.history['acc']
  val_acc = history.history['val_acc']
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  epochs = range(1, len(acc) + 1)
  plt.clf()
  plt.plot(epochs, acc, 'bo', label='Training acc')
  plt.plot(epochs, val_acc, 'b', label='Validation acc')
  plt.title('Training and validation accuracy')
  plt.legend()
  plt.plot()
  plt.savefig("Pretrained-VGG16-1-acc.png")
  plt.figure()

  plt.clf()
  plt.plot(epochs, loss, 'bo', label='Training loss')
  plt.plot(epochs, val_loss, 'b', label='Validation loss')
  plt.title('Training and validation loss')
  plt.legend()
  plt.plot()
  plt.savefig("Pretrained-VGG16-1-loss.png")

#+END_SRC

#+RESULTS:
#+begin_example
Found 2000 images belonging to 2 classes.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 Found 1000 images belonging to 2 classes.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 Found 1000 images belonging to 2 classes.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 Train on 2000 samples, validate on 1000 samples
Epoch 30/30
1980/2000 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9722
2000/2000 [==============================] - 3s 2ms/step - loss: 0.0905 - acc: 0.9725 - val_loss: 0.2450 - val_acc: 0.9020
#+end_example

#+CAPTION: 簡單特徵萃取的訓練和驗證準確度
#+LABEL:fig:Pretrained-VGG16-1-Acc
#+name: fig:Pretrained-VGG16-1-Acc
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
file:images/Pretrained-VGG16-1-acc.png

#+CAPTION: 簡單特徵萃取的訓練和驗證損失
#+LABEL:fig:Pretrained-VGG16-1-loss
#+name: fig:Pretrained-VGG16-1-loss
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/Pretrained-VGG16-1-loss.png]]

圖中顯示可以達到 90%的驗證準確度，比較原來的 model 成效，雖然準確度有提高，但仍可看到 overfitting 的情況，即便 model 裡已套用了 dropout，也許是因為無法使用資料擴增法，對 overfitting 的防治仍然有限。
**** 加入資料擴增的特徵萃取

將資料擴增加入特徵萃取的作法是擴展 conv_base model 並從輸入資料開始，從頭到尾執行整個處理過程，這種做法的運算成本非常昂貴，只能在 GPU 上執行，在 CPU 上絕對難以處理。由於 model 的行為與 layer 類似，因此可以將 model(如 conv\uunder{}base)視為 layer，增加到 Sequential model 中，就如同增加神經網路的 layer 一樣。其作法如下：

#+NAME: 在 convolutional base 卷積基底上增加全連接層分類器
#+begin_src python -r -n :async :results output :exports both :session QQQ
from keras import models
from keras import layers
from keras.applications import VGG16

conv_base = VGG16(weights='imagenet',   # 卷積基底
                  include_top=False,  # 只需要卷積基底的權重模型資訊
                  input_shape=(150, 150, 3))

model = models.Sequential()
model.add(conv_base)        # 將卷積基底視為層加入 Sequential 模型中
model.add(layers.Flatten()) # 攤平
model.add(layers.Dense(256, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid')) # 增加全連接層分類器
model.summary() # 查看模型摘要
#+END_SRC

#+RESULTS: 在 convolutional base 卷積基底上增加全連接層分類器
#+begin_example
Model: "sequential_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 vgg16 (Functional)          (None, 4, 4, 512)         14714688

 flatten_4 (Flatten)         (None, 8192)              0

 dense_10 (Dense)            (None, 256)               2097408

 dense_11 (Dense)            (None, 1)                 257

=================================================================
Total params: 16812353 (64.13 MB)
Trainable params: 16812353 (64.13 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
#+end_example

如上圖，VGG16 的 convolutional base 有 14714688 個參數，在頂部(後端)增加的分類器有 200 多萬個參數。在加入資料擴增之前，凍結 convolutional base 是非常重要的，凍結(freeze)表示在訓練期間禁止更新權重，如果不這樣做，則 convolutional base 先前學習到的表示法就會在訓練期間被修改掉，因為頂部的 Dense 層是隨機初始化的，所以非常大量的權重更新將透過神經網路傳播，會導致先前學習到的表示法被破壞掉。

在 Keras 中，可以透過設定模型的 trainable 屬性為 False 來凍結 convolutional base 神經網路：
#+NAME: 凍結卷積基底神經網路
#+begin_src python -r -n :async :results output :exports both :session QQQ
# freeze convolutional base
print('This is the number of trainable weights '
'before freezing the conv base:', len(model.trainable_weights))
conv_base.trainable = False  # 凍結權重
print('This is the number of trainable weights '
'after freezing the conv base:', len(model.trainable_weights))
#+END_SRC

#+RESULTS: 凍結卷積基底神經網路
: This is the number of trainable weights before freezing the conv base: 30
: This is the number of trainable weights after freezing the conv base: 4

由於 conv_base 被凍結更新權重，所以 model 只會訓練增力的兩個 Dense 層權重，每層有兩個參數要更新(主要權重矩陣和偏差向量)，所以一共剩 4 個 trainable weights，原本的 pretrained model 有 13 層 Conv2D，共 26 個 trainable weights。

接下來就可以使用資料擴增來訓練 model:

#+NAME: 以凍結的 convolutional base 卷積基底進行從頭到尾完整的 model 訓練
#+begin_src python -r -n :async :results output :exports both :session QQQ
from keras import models
from keras import layers
from keras.applications import VGG16

conv_base = VGG16(weights='imagenet',   # 卷積基底
                  include_top=False,
                  input_shape=(150, 150, 3))

import os
import numpy as np
from keras.preprocessing.image import ImageDataGenerator

base_dir = r'/Volumes/LaCie/data/cats_and_dogs_small'
train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')
test_dir = os.path.join(base_dir, 'test')

model = models.Sequential()
model.add(conv_base)        # 將卷積基底視為層加入 Sequential 模型中
model.add(layers.Flatten()) # 攤平
model.add(layers.Dense(256, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid')) # 增加全連接層分類器

conv_base.trainable = False  # 凍結權重

# data augmentation
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers

train_datagen = ImageDataGenerator( # 擴充訓練資料
  rescale=1./255,
  rotation_range=40,
  width_shift_range=0.2,
  height_shift_range=0.2,
  shear_range=0.2,
  zoom_range=0.2,
  horizontal_flip=True,
  fill_mode='nearest')

test_datagen = ImageDataGenerator(rescale=1./255) # 請注意驗證資料不應該擴充

train_generator = train_datagen.flow_from_directory(
  train_dir, # 目標目錄路徑
  target_size=(150, 150), # 調整所有圖像大小成 150×150
  batch_size=20,
  class_mode='binary') # 因為使用二元交叉熵 binary_crossentropy 作為損失分數，所						以需要二元標籤

validation_generator = test_datagen.flow_from_directory(
  validation_dir,
  target_size=(150, 150),
  batch_size=20,
  class_mode='binary')

model.compile( loss='binary_crossentropy',
       optimizer=optimizers.RMSprop(lr=2e-5),
       metrics=['acc'])

history = model.fit_generator(
  train_generator,
  steps_per_epoch=100,
  epochs=30,
  validation_data=validation_generator,
  validation_steps=50)

# 繪製model的損失率與精確率
import matplotlib.pyplot as plt

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)
plt.clf()
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.plot()
plt.savefig("images/CatsDogsDataAugmentationPretrained-acc.png")
plt.figure()

plt.clf()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.plot()
plt.savefig("images/CatsDogsDataAugmentationPretrained-loss.png")
#+END_SRC

#+RESULTS: 以凍結的 convolutional base 卷積基底進行從頭到尾完整的 model 訓練
#+begin_example
Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.
WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.
/var/folders/6z/55c40g5s2qz2rh1t3s5zwtzw0000gn/T/pycu86w4:60: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.
Epoch 1/30
100/100 [==============================] - 146s 1s/step - loss: 0.7296 - acc: 0.6900 - val_loss: 0.3320 - val_acc: 0.8540
Epoch 2/30
100/100 [==============================] - 175s 2s/step - loss: 0.4518 - acc: 0.7910 - val_loss: 0.2749 - val_acc: 0.8850
...略...
Epoch 29/30
100/100 [==============================] - 242s 2s/step - loss: 0.2696 - acc: 0.8820 - val_loss: 0.2479 - val_acc: 0.9060
Epoch 30/30
100/100 [==============================] - 245s 2s/step - loss: 0.2693 - acc: 0.8760 - val_loss: 0.3019 - val_acc: 0.8830
#+end_example

#+CAPTION: Cats and Dogs Data Augmentation / Pretrained- Accuracy
#+LABEL:fig:Cat-Dog-Data-AugmentationPretrained-Acc
#+name: fig:Cat-Dog-Data-AugmentationPretrained-Acc
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/CatsDogsDataAugmentationPretrained-acc.png]]

#+CAPTION: Cats and Dogs Data Augmentation - Loss
#+LABEL:fig:Cat-Dog-Data-AugmentationPretrained-Loss
#+name: fig:Cat-Dog-Data-AugmentationPretrained-Loss
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/CatsDogsDataAugmentationPretrained-loss.png]]

實作結果，驗證準確率達 90%，優於從頭訓練小型神經網路（結果與原書中達 96%有所出入）。

*** 微調
微調(fine-tuning)為另一種廣泛使用的 model reuse 技術，本質上是特徵萃取的變化版，其做法是在特徵萃取的過程中不凍結整個 convolutional base，而是解凍 convolutional base 頂部的某些層以用於特徵萃取，並對於新增加於 model 的部份(如全連接層分類器)與被解凍的部份層一起進行聯合訓練。

微調神經網路的步驟如下：
1. 在已訓練過的基礎神經網路(即 convolutional base)上增加自定義神經網路
2. 凍結 convolutional base
3. 訓練步驟 1 增加的部份(即最頂端的分類器)
4. 解凍 convolutional base 的某幾層
5. 共同訓練解凍層和分類器

以 VGG16 的模組架構為例，其分層架構如下：

#+NAME: VGG16-arch
#+BEGIN_SRC python -r -n :results output :exports both
from keras.applications import VGG16

conv_base = VGG16(weights='imagenet',
                  include_top=False,
                  input_shape=(150, 150, 3))
conv_base.summary()
#+END_SRC

#+RESULTS: VGG16-arch
#+begin_example
Model: "vgg16"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 150, 150, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0
=================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
_________________________________________________________________
#+end_example

我們可以調整這個 convolutional base 的最頂層(block5)三層的卷積層，即 block5_conv1、block5_conv2、block5_conv3 三層，然後凍結 block4_pool 以下的所有層。之所以選擇只解凍 convolutional base 的最頂層，幾個考量原因如下：

 - 相對於 convolutional base 中的低層主要是對更通用、可重複使用的特徵進行編碼；更高層則是對更特定的特徵進行編碼，所以這些特徵需要重新調整才能適用於新的問題。如果是對低層進行微調，則會出現反效果。
- 訓練的參數越多，就越可能 overfitting。convolutional base 有近 1500 萬個參數，因此在少量資料集上訓練會有風險。

解凍部份 convolutional base 的方式如下：

#+NAME: 解凍部份convolutional base
#+BEGIN_SRC python -r -n :results output :exports both
from keras.applications import VGG16

conv_base = VGG16(weights='imagenet',
                  include_top=False,
                  input_shape=(150, 150, 3))
conv_base.summary()

conv_base.trainable = True #先設定所有layer都可訓練?
set_trainable = False #預設為凍結

for layer in conv_base.layers: #由低到高
    if layer.name == 'block5_conv1': #直到出現block5_conv1這層後開始解凍
        set_trainable = True
    if set_trainable:
        layer.trainable = True
    else:
        layer.trainable = False
#+END_SRC

解凍完部份 layer 後即可開始徵調神經網路，這裡使用 RMSProp 優化器以非常低的學習率來微調，降低學習率的目的在減小 3 個解凍層的修改幅度，以免因為過大的修改損害到這些表示法。

#+NAME: 微調
#+begin_src python -r -n :async :results output :exports both :session QQQ
# 編譯模型
model.compile(
    loss='binary_crossentropy',
    optimizer=optimizers.RMSprop(lr=1e-5),
    metrics=['acc'])

# 訓練模型
history = model.fit_generator(
    train_generator,
    steps_per_epoch=100,
    epochs=100,
    validation_data=validation_generator,
    validation_steps=50)
#+END_SRC

#+NAME: 完整徵調過程
#+Begin_src python -r -n :results output :exports both
import os
import numpy as np
from keras.preprocessing.image import ImageDataGenerator

base_dir = r'/Volumes/LaCie/data/cats_and_dogs_small'
train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')
test_dir = os.path.join(base_dir, 'test')

# 部份凍結
from keras.applications import VGG16

conv_base = VGG16(weights='imagenet',
                  include_top=False,
                  input_shape=(150, 150, 3))
conv_base.summary()

conv_base.trainable = True #先設定所有layer都可訓練?
set_trainable = False #預設為凍結

for layer in conv_base.layers: #由低到高
    if layer.name == 'block5_conv1': #直到出現block5_conv1這層後開始解凍
        set_trainable = True
    if set_trainable:
        layer.trainable = True
    else:
        layer.trainable = False

# data augmentation
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers

train_datagen = ImageDataGenerator( # 擴充訓練資料
  rescale=1./255,
  rotation_range=40,
  width_shift_range=0.2,
  height_shift_range=0.2,
  shear_range=0.2,
  zoom_range=0.2,
  horizontal_flip=True,
  fill_mode='nearest')

test_datagen = ImageDataGenerator(rescale=1./255) # 請注意驗證資料不應該擴充

train_generator = train_datagen.flow_from_directory(
  train_dir, # 目標目錄路徑
  target_size=(150, 150), # 調整所有圖像大小成 150×150
  batch_size=20,
  class_mode='binary') # 因為使用二元交叉熵 binary_crossentropy 作為損失分數，所						以需要二元標籤

validation_generator = test_datagen.flow_from_directory(
  validation_dir,
  target_size=(150, 150),
  batch_size=20,
  class_mode='binary')

from keras import models
from keras import layers
from keras import optimizers

# model還是要加後面的layer?
model = models.Sequential()
model.add(conv_base)        # 將卷積基底視為層加入 Sequential 模型中
model.add(layers.Flatten()) # 攤平
model.add(layers.Dense(256, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid')) # 增加全連接層分類器

# 微調
# 編譯模型
model.compile(
    loss='binary_crossentropy',
    optimizer=optimizers.RMSprop(lr=1e-5),
    metrics=['acc'])

# 訓練模型
history = model.fit_generator(
    train_generator,
    steps_per_epoch=100,
    epochs=100,
    validation_data=validation_generator,
    validation_steps=50)

# 繪製model的損失率與精確率
import matplotlib.pyplot as plt

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)
plt.clf()
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.plot()
plt.savefig("images/FineTune-acc-1.png")
plt.figure()

plt.clf()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.plot()
plt.savefig("images/FineTune-loss-1.png")
#+END_SRC

#+RESULTS: 完整徵調過程
#+begin_example
Model: "vgg16"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 150, 150, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0
=================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
_________________________________________________________________
Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
100/100 [==============================] - 602s 6s/step - loss: 0.0200 - acc: 0.9935 - val_loss: 0.0559 - val_acc: 0.9380
#+end_example

#+CAPTION: VGG16 Fine Tune Acc
#+LABEL:fig:VGG16-Fine-Tune-Acc
#+name: fig:VGG16-Fine-Tune-Acc
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/FineTune-acc-1.png]]

#+CAPTION: VGG16 Fine Tune Loss
#+LABEL:fig:VGG16-Fine-Tune Loss
#+name: fig:VGG16-Fine-Tune-Loss
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/FineTune-loss-1.png]]

微調的訓練準確率來到 99%，驗證準確率也有 94%，這是使用 2000 個訓練樣本就達到的結果。

* 視覺化呈現 CNN 的學習內容

CNN 學習的表示法非常適合以視覺化呈現，因為它們大部份就是視覺概念的表示法(represnetations of visual concepts)，幾種常用的視得化技術如下：
- 視覺化中間層 convnet 的輸出(中間啟動函數)：有助於理解 convnet 是如何一層一層的轉化資料，以及對過濾器(filter)的含義。
- 視覺化 CNN 過濾器：用於準確理解 CNN 中每個過濾器所要接受的視覺 patter 或概念中
- 視覺化類別激活熱圖(heatmaps of class activation): 有助於了解影像的哪些部份被識別為某個類別，藉以定位影像中的物件。

** 中間層輸出視覺化
以下的工作主要是在給定輸入影像後，以圖形化的方式顯示卷積神經網路中各個卷積層和池化層輸出的特徵圖。讓我們能看到在CNN的學習過程中，輸入資料是如何經由逐層分解到不同的過濾器。雖然輸入資料為三個維度(width, height, channel)，但其實每個 channel 會針對相對獨立的特徵值(features)進行編碼，所以此處是將每個 channel 的內容獨立繪製成 2D 圖形秀出。

先載入之前儲存好的 model，取一張測試集中的照片(未經訓練過)，秀出原始內容，然後萃取出特徵圖，因為在這裡只是想看一張圖，所以要建新一個新的 Keras model。

#+NAME: 使用既有model
#+Begin_src python -r -n :async :results output :exports both :session view
from keras.models import load_model

# 加載保存的模型：
model = load_model('cats_and_dogs_small_i.h5')
model.summary()  # 打印模型
#+end_src

#+RESULTS: 使用既有model
#+begin_example
WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 conv2d (Conv2D)             (None, 148, 148, 32)      896

 max_pooling2d (MaxPooling2  (None, 74, 74, 32)        0
 D)

 conv2d_1 (Conv2D)           (None, 72, 72, 64)        18496

 max_pooling2d_1 (MaxPoolin  (None, 36, 36, 64)        0
 g2D)

 conv2d_2 (Conv2D)           (None, 34, 34, 128)       73856

 max_pooling2d_2 (MaxPoolin  (None, 17, 17, 128)       0
 g2D)

 conv2d_3 (Conv2D)           (None, 15, 15, 128)       147584

 max_pooling2d_3 (MaxPoolin  (None, 7, 7, 128)         0
 g2D)

 flatten (Flatten)           (None, 6272)              0

 dense (Dense)               (None, 512)               3211776

 dense_1 (Dense)             (None, 1)                 513

=================================================================
Total params: 3453121 (13.17 MB)
Trainable params: 3453121 (13.17 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
#+end_example
留意一下上述的summary()結果，第一層卷積層(conv2d)的輸出為32個channel所生成的148×148的特徵圖，等一下我們就從中間挑出一張來瞧瞧成果。
** 查看新圖
使用一個未被模型看過的圖像作為輸入(一個貓的圖片，它不是訓練集的一部分)：
#+Begin_src python -r -n :async :results output :exports both :session view
img_path = '/Volumes/LaCie/data/cats_and_dogs_small/test/cats/cat.1700.jpg'
# 我們把圖像轉換成網絡要求的張量shape (4D 張量)
# (樣本數, 圖像高度, 圖像寬度, 圖像通道)
from keras.preprocessing import image
import numpy as np

img = image.load_img(img_path, target_size=(150, 150)) # 載入圖像並轉換大小為150x150
img_tensor = image.img_to_array(img) # 把影像物件轉換成 numpy ndarray物件

print("Origin img_tensor shape: ", img_tensor.shape)

img_tensor = np.expand_dims(img_tensor, axis=0) # 多增加一個維度來符合Keras Conv2D的要求

print("After reshape img_tensor shape: ", img_tensor.shape)

# 這個模型的輸入是有經過歸一化的前處理
# 所以我們也要進行相同的前處理
img_tensor /= 255. # 進行資料尺度(scale)的轉換
#+end_src

#+RESULTS:
: Origin img_tensor shape:  (150, 150, 3)
: After reshape img_tensor shape:  (1, 150, 150, 3)

這張測試集照片的原圖如下:
#+Begin_src python -r -n :async :results output :exports none :session view
import matplotlib.pyplot as plt

plt.imshow(img_tensor[0])
#plt.show()
plt.savefig('images/cat.1700.png', dpi=300)
#+end_src
#+CAPTION: 測試集原圖
#+LABEL:fig:Labl
#+name: fig:test-orig-cat
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/cat.1700.png]]

因為原始模型的目的並不是要讓使用者查看每一層的輸出結果，為了讓我們能強力介入每一層去看出各層的輸出，我們要建立一個新的模型，只以一張圖(圖[[fig:test-orig-cat]])做為輸入，並輸出所有經過卷積層和池化層的激勵輸出結果。

這個模型需要兩個參數：輸入張量（或輸入張量列表）以及輸出張量（或輸出張量列表）。所得到的類別是一個Keras模型(model)物件，就如同我們之前所建立的Sequential模型，但是這個Model類別允許具有多個輸出的模型，這是與之前我們建立的Sequential最大的不同。

#+Begin_src python -r -n :async :results output :exports both :session view
from keras import models

# 創建一個list來儲存前8層處理後的outputs
layer_outputs = [layer.output for layer in model.layers[:8]]

# 產生一個model物件, 它的input是原先模型的input, 而它的output則是前8層處理後的outputs
activation_model = models.Model(inputs=model.input, outputs=layer_outputs)

print(layer_outputs) # 看一下物件的結構
#+end_src

#+RESULTS:
: [<KerasTensor: shape=(None, 148, 148, 32) dtype=float32 (created by layer 'conv2d')>,
: <KerasTensor: shape=(None, 74, 74, 32) dtype=float32 (created by layer 'max_pooling2d')>,
: <KerasTensor: shape=(None, 72, 72, 64) dtype=float32 (created by layer 'conv2d_1')>,
: <KerasTensor: shape=(None, 36, 36, 64) dtype=float32 (created by layer 'max_pooling2d_1')>,
: <KerasTensor: shape=(None, 34, 34, 128) dtype=float32 (created by layer 'conv2d_2')>,
: <KerasTensor: shape=(None, 17, 17, 128) dtype=float32 (created by layer 'max_pooling2d_2')>,
: <KerasTensor: shape=(None, 15, 15, 128) dtype=float32 (created by layer 'conv2d_3')>,
: <KerasTensor: shape=(None, 7, 7, 128) dtype=float32 (created by layer 'max_pooling2d_3')>]


當我們將圖像餵進這個模型時，此模型會傳回原始模型中特定層被激勵函數處理過後的值，這個模型只有1個輸入、8個輸出，每一層的激勵函數輸出結果變成一個輸出。

#+Begin_src python -r -n :async :results output :exports both :session view
# 透過model.predict()的處理, model將會回傳一個有8個神經層處理後的output的列表:
activations = activation_model.predict(img_tensor)

print(len(activations)) # 讓我們確認一下產生的數量
#+end_src

#+RESULTS:
: 1/1 [==============================] - 0s 17ms/step
: 8

** 第一層卷積層效果
以下是我們的貓圖(如圖[[fig:test-orig-cat]])經過CNN後的第一個卷積層在經過的激勵函數轉換
後的輸出：
#+Begin_src python -r -n :async :results output :exports both :session view
first_layer_activation = activations[0]
print(first_layer_activation.shape) # 看一下第一層產生的shape
#+end_src

#+RESULTS:
: (1, 148, 148, 32)

這是一個具有32個channel的148x148特徵圖(feature map)。
*** Channel 3
其中第3個channel的圖像為：
#+Begin_src python -r -n :async :results output :exports both :session view
import matplotlib.pyplot as plt
# 秀出第3個頻導的特徵圖
plt.matshow(first_layer_activation[0, :, :, 3], cmap='viridis')
#plt.show()
plt.savefig('images/cat.1700.channel3.png', dpi=300)
#+end_src
#+CAPTION: Caption
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/cat.1700.channel3.png]]

這張圖看起來似乎可以看成對圖片對角邊緣的編碼檢測器。讓我們嘗試第30個頻道(P.S. 這裡每個人的執行結果可能會有所差異，因為卷積層學習的特定濾鏡不是確定性的, not deterministic)。
*** Channel 6
其中第6個channel的圖像為：
#+Begin_src python -r -n :async :results output :exports both :session view
import matplotlib.pyplot as plt
# 秀出第3個頻導的特徵圖
plt.matshow(first_layer_activation[0, :, :, 6], cmap='viridis')
#plt.show()
plt.savefig('images/cat.1700.channel6.png', dpi=300)
#+end_src
#+RESULTS:
#+CAPTION: Channel 6
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/cat.1700.channel6.png]]
*** Channel 24
其中第24個channel的圖像為：
#+Begin_src python -r -n :async :results output :exports both :session view
import matplotlib.pyplot as plt
# 秀出第3個頻導的特徵圖
plt.matshow(first_layer_activation[0, :, :, 24], cmap='viridis')
#plt.show()
plt.savefig('images/cat.1700.channel24.png', dpi=300)
#+end_src
#+RESULTS:
#+CAPTION: Channel 24
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/cat.1700.channel24.png]]
*** Channel 27
其中第27個channel的圖像為：
#+Begin_src python -r -n :async :results output :exports both :session view
import matplotlib.pyplot as plt
# 秀出第3個頻導的特徵圖
plt.matshow(first_layer_activation[0, :, :, 27], cmap='viridis')
#plt.show()
plt.savefig('images/cat.1700.channel27.png', dpi=300)
#+end_src
#+RESULTS:
#+CAPTION: Channel 27
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
#+ATTR_HTML: :width 300
[[file:images/cat.1700.channel27.png]]
*** 依序觀察每一層的結果
#+Begin_src python -r -n :async :results output :exports both :session view
# 秀出第30個頻導的特徵圖
for i in range(32):
    plt.matshow(first_layer_activation[0, :, :, i], cmap='viridis')
    plt.title(f'channel: {i}')
    plt.show()
#plt.savefig('images/cat.1700.channel10.png', dpi=300)
#+end_src

** 各層的輸出結果
現在，我們來輸出這個CNN模型中每一層輪出的完整樣貌，我們從中提取、繪製每8個特徵圖(feature maps)中的通道，並將結果以類似matplotlib的subplot()呈現出來。

#+Begin_src python -r -n :async :results output :exports both :session view
import keras

# 輸出每一層的神經層名稱
layer_names = []
for layer in model.layers[:8]:
    layer_names.append(layer.name)

images_per_row = 16 # 每一排輸出16個特徵圖

# 每一層特徵圖的Shapes
# layer#01 -> (148, 148, 32)
# layer#02 -> (74, 74, 32)
# layer#03 -> (72, 72, 64)
# layer#04 -> (36, 36, 64)
# layer#05 -> (34, 34, 128)
# layer#06 -> (17, 17, 128)
# layer#07 -> (15, 15, 128)
# layer#08 -> (7, 7, 128)

# 輸出特徵圖
for layer_name, layer_activation in zip(layer_names, activations): # 我們有8層的神經元的輸出
    n_features = layer_activation.shape[-1] # 取得每一層特徵圖的數量
    size = layer_activation.shape[1] # 取得每一個特徵圖的寬與高 (1, size, size, n_features)

    # 我們會把多個特徵圖串接在一個比較大的矩陣
    n_cols = n_features // images_per_row # 算一下這個大矩陣的會有幾欄
    display_grid = np.zeros((size * n_cols, images_per_row * size)) # 產生一個大矩陣

    # 模擬matpolotlib subplot()效果
    for col in range(n_cols):
        for row in range(images_per_row):
            channel_image = layer_activation[0, :, :, col * images_per_row + row]
            # Post-process the feature to make it visually palatable
            channel_image -= channel_image.mean()
            channel_image /= channel_image.std()
            channel_image *= 64
            channel_image += 128
            channel_image = np.clip(channel_image, 0, 255).astype('uint8')
            display_grid[col * size : (col + 1) * size,
                         row * size : (row + 1) * size] = channel_image

    # 展示拼貼出來的結果
    scale = 1. / size
    plt.figure(figsize=(scale * display_grid.shape[1],
                        scale * display_grid.shape[0]))
    plt.title(layer_name)
    plt.grid(False)
    fn = layer_name+".png"
    plt.imshow(display_grid, aspect='auto', cmap='viridis')
    plt.savefig('images/'+fn, dpi=300)
#plt.show()
#+end_src
#+CAPTION: 經過第一個卷積層
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 800
[[file:images/conv2d.png]]
#+CAPTION: max pooling轉換後
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 800
[[file:images/max_pooling2d.png]]
#+CAPTION: 經過第二個卷積層
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 800
[[file:images/conv2d_1.png]]
#+CAPTION: 經過第二次max pooling
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 800
[[file:images/max_pooling2d_1.png]]
#+CAPTION: 經過第三個卷積層
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 800
[[file:images/conv2d_2.png]]
#+CAPTION: 第三個max pooling
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 800
[[file:images/max_pooling2d_2.png]]
#+CAPTION: 第四個卷積層
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 800
[[file:images/conv2d_3.png]]
#+CAPTION: 第四個max pooling
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 800
[[file:images/max_pooling2d_3.png]]

由上圖可知，隨層數越來越高，啟動函數的輸出變得越來越抽象，視覺上也越來越難解釋，model 開始編碼出更高階的概念。此外，啟動函數輸出的稀疏性也隨著層數的深度而增加，在第一層中，所有的過濾器都被輸入影響所驅動(都有值)，但接下來就有越來越多的 filter 的值是空的(全黑)，這表示在這些層的輸入影像中已經找不到過濾器要編碼的圖案 pattern 了。

上述示例也證明了深度神經網路所學習到的表示法有一個重要特性：各層萃取的特徵隨著層的𣶶度而變的越來越抽象，越高階的啟動函數越不會帶有關於特定輸入的資訊，卻具備更多關於目標的資訊（此例中指的是貓或狗）。這和人或動物感知世界的方式很像：在觀察一個場景幾秒中後閉眼，我們可以記得場景中有哪些抽象事物，但不會記得每個物體的特殊外觀，因為大腦也會將事物抽象化。

** CMA :noexport:
CMA(class activation map)可以用來理解影像中的哪些部份會讓 convnet 做出最終分類的決策，這有助於 convnet 決策過程的偵錯。CAM 主要是針對輸入影像產生類別激活熱圖(heatmap of class)，這是一個 2D 的分數網格圖，針對輸入影像的每個位置(網格)進行計算，然後指出每個位置相對於目前類別的重要性。我們可以使用"Graid-CAM: Visual Explanations from Deep Networks via Gradient-based Localization"這篇論文提到的方法，即，給定影像，取得卷積層的輸出特徵，以"這個類別對每個 channel 的梯度值"對這個特徵圖中的每個 channel 做加權。進而產生「某張圖片激活某類別的強度」的 2D 分數網格圖。

做法如下(從Google下載tnesorflow的VGG16會花 *一點* 時間)：
#+begin_src shell -r -n :results output :exports both
conda install -c conda-forge grad-cam
#+end_src

#+RESULTS:

#+Begin_src python -r -n :async :results output :exports both :session viewVGG16
from keras.applications.vgg16 import VGG16

model = VGG16(weights='imagenet')  # 請注意, 在頂部包含了密集連接的分類器 (預設 include_top=True)

# 預先處理 VGG16 的輸入影像
from keras.preprocessing import image
from keras.applications.vgg16 import preprocess_input, decode_predictions
import numpy as np

img_path = r'images/african_elephants.jpg'

img = image.load_img(img_path, target_size=(224, 224))
print(type(img))  # 目前圖片為 <class 'PIL.Image.Image'> 物件
print(img.size)  # 可以用 size 屬性查看尺寸 -> (224, 224)

x = image.img_to_array(img) 	# 將 PIL 物件轉為 float32 的 Numpy 陣列
print(x.shape) 				# shape=(224, 224, 3)

# 將 x 陣列 (可視為張量) 增加一個批次軸, shape=(1, 224, 224, 3)
x = np.expand_dims(x, axis=0)
print(x.shape)

x = preprocess_input(x) # 預處理批次量 (這會對每一 channel 做顏色值正規化)

#使用 VGG 神經網路預測圖片類別
preds = model.predict(x)
print('預測結果:', decode_predictions(preds, top=3)[0])

np.argmax(preds[0])

#設定 Gard-CAM 演算法
from keras import backend as K

african_elephant_output = model.output[:, 386] # ← 預測向量中的 "非洲象" 項目

last_conv_layer = model.get_layer('block5_conv3') # block5_conv3 層的輸出特徵圖, 其為 VGG16 中的最後一個卷積層

grads = K.gradients(african_elephant_output, last_conv_layer.output)[0] #  block5_conv3 的輸出特徵圖中關於 "非洲象" 類別的梯度

pooled_grads = K.mean(grads, axis=(0, 1, 2)) #  轉換成向量 shape = (512, ), 其中每個項目是特定特徵圖 channel 的梯度平均強度(值)

#  給定輸入影像的條件下, 讓我們可以存取剛剛定義的數值：pooled_grads 和 block5_conv3 的輸出特徵圖
iterate = K.function([model.input],
                     [pooled_grads, last_conv_layer.output[0]])

#  對於給定的兩隻大象樣本影像, 產生這兩個量值, 以 Numpy 陣列呈現
pooled_grads_value, conv_layer_output_value = iterate([x])

# 將特徵圖陣列中的每個 channel 與 "大象" 類別相關的 "此 channel 的重要程度" 相乘
for i in range(512):
    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]

# 特徵圖的跨 channel 平均值是類別激活函數輸出的熱圖
heatmap = np.mean(conv_layer_output_value, axis=-1)

#熱圖後期處理
import matplotlib.pyplot as plt
import numpy as np

heatmap = np.maximum(heatmap, 0)
heatmap /= np.max(heatmap)
plt.matshow(heatmap)
plt.plot()
plot.savefig("images/heapmapOfClassActivation.png")

# 將熱圖與原始影像疊加在一起
import cv2

img = cv2.imread(img_path)

heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))

heatmap = np.uint8(255 * heatmap)

heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

superimposed_img = heatmap * 0.4 + img # 這裡 0.4 是熱圖強度因子

print('是否儲存成功:', cv2.imwrite('images/elephant_cam.jpg', superimposed_img))

#+END_SRC

#+RESULTS:
: 1994708b-f258-4b4c-8229-69d443c84517

#+CAPTION: Heapmap of class activation
#+LABEL:fig:heapmapOfClassActivation
#+name: fig:heapmapOfClassActivation
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 300
[[file:images/heapmapOfClassActivation.png]]

#+CAPTION: 將激活熱圖與影像叠加
#+LABEL:fig:heapmapOfClassActivationCombination
#+name: fig:heapmapOfClassActivationCombination
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 300
[[file:images/elephant_cam-1.jpg]]

上圖的視覺化技術回答了兩個重要問題：
- 為什麼神經網路認為這個影像裡有非洲象?
- 非洲象位於影像中的哪個位置?

* Read this :noexport:
- [[https://towardsdatascience.com/marathon-bib-identification-and-recognition-25ee7e08d118][Marathon Bib Identification and Recognition]]: number detection project
- [[https://www.youtube.com/watch?v=D641Ucd_xuw][从“卷积”、到“图像卷积操作”、再到“卷积神经网络”，“卷积”意义的3次改变]]
- [[https://ml4a.github.io/demos/][https://ml4a.github.io/demos/]]
- [[https://www.youtube.com/watch?v=z6k_RMKExlQ][ml4a @ itp nyu :: 03 convolutional neural networks]]
- [[https://ml4a.net/][https://ml4a.net/]]
- [[https://www.youtube.com/watch?v=XTlwfDlyMAY][itp S19 02]]
- [[https://openframeworks.cc/setup/emscripten/][emscripten setup]]
- [[https://github.com/ml4a/ml4a-ofx][https://github.com/ml4a/ml4a-ofx]]
- [[https://app.runwayml.com/video-tools/motion-tracking][runwayml]]
- [[https://cycling74.com/forums/max-and-cnmat-osc-help-please][Max]]
- [[https://openframeworks.cc/download/][OF]]
- [[https://www.youtube.com/watch?v=ARnf4ilr9Hc][Intorduction to Runway]]
- 研究OSC: 一種application間傳送message的protocol
- [[https://www.youtube.com/c/GeneKogan/videos][Gene Kogan]]
- [[https://affinelayer.com/pixsrv/][GAN DEMO: Image-to-Image Demo]]
- [[https://hackmd.io/@tcirc39th/SkRgTXY15?print-pdf#/][AI人工智慧初探]]
- [[https://stackoverflow.com/questions/2480650/what-is-the-role-of-the-bias-in-neural-networks][Why activation function?]]
- [[https://hackmd.io/@joanne8826/B1zgGT2Fd][人工神經網路 Artificial Neural Network]] :神經元那張圖可以參考
- [[https://zhuanlan.zhihu.com/p/95984057][Deep Learning A-Z™：人工神经网络实践-筆記2-Section3(ANN)]]
- [[https://www.youtube.com/watch?v=fApFKmXcp2Y&t=19s][池化的影片]]
- [[https://poloclub.github.io/cnn-explainer/][CNN Explainer]]
- [[https://ml-playground.com/#][https://ml-playground.com/#]]
- [[https://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html][ConvNetJS demo]]
- [[https://www.youtube.com/watch?v=sWz4e-DM4JU][ Meta 語音對語音翻譯技術背後的黑科技 (在繪圖 AI 中也有用上喔!) ]]
- [[https://github.com/lutzroeder/netron/releases/tag/v6.1.3][畫CNN圖的工具]]
- [[https://github.com/ashishpatel26/Tools-to-Design-or-Visualize-Architecture-of-Neural-Network][ Tools-to-Design-or-Visualize-Architecture-of-Neural-Network]]
- [[https://neptune.ai/blog/the-best-tools-for-machine-learning-model-visualization][https://neptune.ai/blog/the-best-tools-for-machine-learning-model-visualization]]
- [[https://www.youtube.com/watch?v=Obbr5TdD3Bo][GNN]]: Graph Neural Network Tutorial: 給10000篇paper及cited references, 進行分類
- [[https://www.youtube.com/watch?v=OP5HcXJg2Aw&t=78s][李宏毅CNN]]
- [[https://www.youtube.com/watch?v=V9ZYDCnItr0][為什麼kernel size多是3*3]]
- [[https://www.youtube.com/watch?v=pnkCEzwiSog][解釋的很清楚:神經網路講解40分鐘]]
- [[https://www.youtube.com/watch?v=pjfnMgAnnIk][ “损失函数”是如何设计出来的？直观理解“最小二乘法”和“极大似然估计法” ]]
- [[https://www.youtube.com/watch?v=pnkCEzwiSog][学习分享一年，对神经网络的理解全都在这40分钟里了 ]]

* Next Subject :noexport:
Kaggle: https://www.kaggle.com/ryanholbrook/overfitting-and-underfitting

* Footnotes

[fn:1] [[https://ithelp.ithome.com.tw/articles/10191820][Day 06：處理影像的利器 -- 卷積神經網路(Convolutional Neural Network)]]

[fn:2] [[https://medium.com/@syshen/%E5%85%A5%E9%96%80%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-2-d694cad7d1e5][入門深度學習 — 2]]

[fn:3] [[https://medium.com/chiukevin0321/cnn-%E5%82%B3%E7%B5%B1nn-comparison-9b0a6a9b1e2d][DNN & CNN comparison]]

[fn:4] [[https://medium.com/@chih.sheng.huang821/卷積神經網路-convolutional-neural-network-cnn-卷積計算的倒傳遞推導與稀疏矩陣觀點來看卷積計算-e82ac16e510f][卷積神經網路(Convolutional neural network, CNN):卷積計算的倒傳遞推導與稀疏矩陣觀點來看卷積計算]]

[fn:5] [[https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/][An Intuitive Explanation of Convolutional Neural Networks]]

[fn:6] [[https://www.books.com.tw/products/0010822932][Deep learning 深度學習必讀：Keras 大神帶你用 Python 實作]]

[fn:7] [[https://blog.yeshuanova.com/2018/10/dataset-iris/][機器學習資料集 - Iris dataset ]]

[fn:8] [[https://aifreeblog.herokuapp.com/posts/54/data_science_203/][資料的正規化(Normalization)及標準化(Standardization)]]

[fn:9] [[https://medium.com/yiyi-network/transfer-learning-1f87d4f1886f][Kaggle Learn | Deep Learning 深度學習 | 學習資源介紹 (Part 2)]]
