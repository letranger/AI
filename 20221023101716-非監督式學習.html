<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-02-12 Mon 12:15 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>非監督式學習</title>
<meta name="author" content="Yung-Chin Yen" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="../css/muse.css" />
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">非監督式學習</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org2f28f58">1. 非監督式學習</a>
<ul>
<li><a href="#org22ab1af">1.1. 目的</a></li>
<li><a href="#org5359726">1.2. 非監督式學習的任務</a></li>
</ul>
</li>
<li><a href="#org347d238">2. 非監督式學習的常見演算法</a>
<ul>
<li><a href="#org29db800">2.1. 分群(clustering)</a></li>
<li><a href="#orgbfece29">2.2. 異常檢測與新穎檢測</a></li>
<li><a href="#org53c02b7">2.3. 視覺化與降維</a></li>
<li><a href="#orga2064aa">2.4. 降維</a></li>
<li><a href="#orge0bd8d9">2.5. 關聯規則學習</a></li>
<li><a href="#org2275245">2.6. 自動編碼器</a></li>
</ul>
</li>
<li><a href="#org0563c4d">3. 聚類(集群)</a>
<ul>
<li><a href="#org4d2eef9">3.1. K-Means</a></li>
<li><a href="#NS-Hie-cluster">3.2. Hierarchical clustering</a></li>
<li><a href="#org4c19059">3.3. DBSCAN</a></li>
</ul>
</li>
<li><a href="#org7e9e2c6">4. 異常偵測</a>
<ul>
<li><a href="#org09f62a6">4.1. 準備資料</a></li>
<li><a href="#org7099af4">4.2. 定義異常評分函數</a></li>
<li><a href="#orga4c8a72">4.3. 評估指標：畫圖</a></li>
<li><a href="#org086aec8">4.4. PCA異常偵測</a></li>
<li><a href="#orgb127d26">4.5. Sparse PCA異常偵測</a></li>
<li><a href="#orgb7235d5">4.6. Kernel PCA異常偵測</a></li>
<li><a href="#org17eeae6">4.7. 稀疏隨機投影異常偵測</a></li>
<li><a href="#org6a8b3aa">4.8. 字典學習異常偵測</a></li>
<li><a href="#org4a3f5de">4.9. ICA異常偵測</a></li>
<li><a href="#org72022a1">4.10. 結論</a></li>
</ul>
</li>
<li><a href="#org8bf0165">5. 降維</a>
<ul>
<li><a href="#org3eb15ca">5.1. 讀入資料</a></li>
<li><a href="#orgcfa4e82">5.2. 主成分分析</a></li>
<li><a href="#orge7ac210">5.3. 奇異值分解(Singular Value Decomposition，SVD)</a></li>
<li><a href="#org34898f3">5.4. Isomap</a></li>
<li><a href="#org9212fb1">5.5. 局部線性嵌入法(Locally Linear Embedding)</a></li>
<li><a href="#orge17af58">5.6. t-Distributed Stochastic Neighbor Embedding</a></li>
<li><a href="#orgf87a617">5.7. 字典學習</a></li>
<li><a href="#orgb4db16d">5.8. 獨立成份分析</a></li>
</ul>
</li>
</ul>
</div>
</div>
<a href="https://letranger.github.io/AI/20221023101716-非監督式學習.html"><img align="right" alt="Hits" src="https://hits.sh/letranger.github.io/AI/20221023101716-非監督式學習.html.svg"/></a>
<div id="outline-container-org2f28f58" class="outline-2">
<h2 id="org2f28f58"><span class="section-number-2">1.</span> 非監督式學習</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org22ab1af" class="outline-3">
<h3 id="org22ab1af"><span class="section-number-3">1.1.</span> 目的</h3>
<div class="outline-text-3" id="text-1-1">
<p>
非監督式學習演算法嚐試學習 <b>資料的基礎結構</b> ，而非 <b>做出預測</b> 。例如：
</p>
<ul class="org-ul">
<li>將網站訪客進行分類: 40%為男性、愛看漫畫、通常晚上造訪網站&#x2026;.</li>
<li><p>
將一堆圖片分類：cat、automobile、truck、frog、ship&#x2026;
</p>

<div id="org4742325" class="figure">
<p><img src="images/2022-04-30_10-57-36.jpg" alt="2022-04-30_10-57-36.jpg" width="500" />
</p>
<p><span class="figure-number">Figure 1: </span>Caption</p>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org5359726" class="outline-3">
<h3 id="org5359726"><span class="section-number-3">1.2.</span> 非監督式學習的任務</h3>
<div class="outline-text-3" id="text-1-2">
<p>
如何協助儘量保留非監督式學習資料的重要特徵以利將來進行有效辨識<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>：
</p>
<ul class="org-ul">
<li>降維(dimensionality reduction): 目的是在不損失過多資訊的前提下簡化資料，將多個特徵合併成一個。例如，汽車的里程數與車齡就有合併的依據。</li>
<li>異常檢測(Anamaly Detection): 例如，找出不尋常的信用卡交易以防止詐騙、找出製程中有缺陷的產品、將資料組中的離群值挑出來再傳給另一個演算法</li>
<li><p>
新穎檢測(Novelty Detection): 檢測與訓練組中所有實例(instance)看起來不一樣的新實例。前提是訓練組的數據非常乾淨（clean）。例如，有1000張狗的照片，裡面只有10張吉娃娃，則Novelty就 <b>不應該</b> 把這10張標為novelty，而是找出裡面不小心誤放的貓的照片。
</p>

<div id="orge5deb39" class="figure">
<p><img src="images/2022-04-30_11-35-44.jpg" alt="2022-04-30_11-35-44.jpg" width="500" />
</p>
<p><span class="figure-number">Figure 2: </span>Caption</p>
</div></li>
<li>關聯規則學習(association rule learning): 超市中售出貨物間的關連，可以將常常一起買的物品擺近一點</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org347d238" class="outline-2">
<h2 id="org347d238"><span class="section-number-2">2.</span> 非監督式學習的常見演算法</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org29db800" class="outline-3">
<h3 id="org29db800"><span class="section-number-3">2.1.</span> 分群(clustering)</h3>
<div class="outline-text-3" id="text-2-1">
<p>
聚類(集群)
</p>
<ul class="org-ul">
<li>K-Means</li>
<li>DBSCAN</li>
<li>階層式分群分析(Hierarchical Cluster Analysis, HCA)</li>
</ul>
</div>
</div>
<div id="outline-container-orgbfece29" class="outline-3">
<h3 id="orgbfece29"><span class="section-number-3">2.2.</span> 異常檢測與新穎檢測</h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li>One-class SVM</li>
<li>孤立森林(Isolation Forest)</li>
</ul>
</div>
</div>
<div id="outline-container-org53c02b7" class="outline-3">
<h3 id="org53c02b7"><span class="section-number-3">2.3.</span> 視覺化與降維</h3>
<div class="outline-text-3" id="text-2-3">
<ul class="org-ul">
<li>主成分分析(Principal Component Analysis, PCA)</li>
<li>kernel PCA</li>
<li>局部線性嵌入法(Locally Linear Embedding, LLE)</li>
<li>t-隨機隣近嵌入法(t-Distributed Stochastic Neighbor Emgedding, t-SNE)</li>
</ul>
</div>
</div>
<div id="outline-container-orga2064aa" class="outline-3">
<h3 id="orga2064aa"><span class="section-number-3">2.4.</span> 降維</h3>
<div class="outline-text-3" id="text-2-4">
<p>
There are two major branches of dimensionality reduction. The first is known as linear projection, which involves linearly projecting data from a high- dimensional space to a low-dimensional space. This includes techniques such as<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>:
</p>
<ul class="org-ul">
<li>principal component analysis</li>
<li>singular value decomposition</li>
<li>random projection.</li>
</ul>

<p>
The second is known as manifold learning, which is also referred to as nonlinear dimensionality reduction. This involves techniques such as<sup><a id="fnr.2.100" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>:
</p>
<ul class="org-ul">
<li>isomap: which learns the curved distance (also called the geodesic distance) between points rather than the Euclidean distance</li>
<li>multidimensional scaling (MDS)</li>
<li>locally linear embedding (LLE)</li>
<li>t-distributed stochastic neighbor embedding (t-SNE)</li>
<li>dictionary learning</li>
<li>random trees embedding</li>
<li>independent component analysis</li>
</ul>

<p>
維度縮減演算法(dimensionality reduction algorithm)將原始的高維度輸入資料映射到一個低維度空間，同時過濾掉與整體資料不那麼相關的特徵，並儘可能保留資料中令人感興趣的特徵。主要有兩個分支：線性投影、非線性投影:
</p>
</div>
<div id="outline-container-orgff12869" class="outline-4">
<h4 id="orgff12869"><span class="section-number-4">2.4.1.</span> 線性投影</h4>
<div class="outline-text-4" id="text-2-4-1">
</div>
<ol class="org-ol">
<li><a id="orgbb0f78c"></a>主成分分析(Principal component analysis, PCA)<br />
<div class="outline-text-5" id="text-2-4-1-1">
<p>
PCA有數種變形：mini-batch變形式PCA(incremental PCA)、非線性變形(kernel PCA)、稀疏變形(sparse PCA)
</p>
</div>
</li>
<li><a id="org34104f1"></a>奇異值分解(Singular value decomposition, SVD)<br />
<div class="outline-text-5" id="text-2-4-1-2">
<p>
降低原來特徵所組成的矩陣的秩（rank)，使得原來的矩陣可以使用擁有較小的秩的矩陣所組成的線性組合來表示。
</p>
</div>
</li>
<li><a id="org08fcb57"></a>隨機投影(Random projection)<br />
<div class="outline-text-5" id="text-2-4-1-3">
<p>
由高維投影至低維空間，但同時保留點與點間的矩離，可以使用隨機高斯矩陣（random Gaussian matrix)或隨機稀疏矩陣(random sparse matrix)來實現。
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-org0e53d18" class="outline-4">
<h4 id="org0e53d18"><span class="section-number-4">2.4.2.</span> 流形學習(Manifold learning)</h4>
<div class="outline-text-4" id="text-2-4-2">
</div>
<ol class="org-ol">
<li><a id="orgc9d9b49"></a>Isomap<br />
<div class="outline-text-5" id="text-2-4-2-1">
<p>
透過估算點與粌近點的捷線(geodesic)或曲線距離(curved distance)，而非使用歐式距離(Euclidean distance)來學習資料流形的內蘊幾何。
</p>
</div>
</li>
<li><a id="org80bcdda"></a>t-distributed stochastic neighbor embedding(t-SNE)<br />
<div class="outline-text-5" id="text-2-4-2-2">
<p>
將高維度空間的資料嵌入至二維或三維的空間
</p>
</div>
</li>
</ol>
</div>
</div>
<div id="outline-container-orge0bd8d9" class="outline-3">
<h3 id="orge0bd8d9"><span class="section-number-3">2.5.</span> 關聯規則學習</h3>
<div class="outline-text-3" id="text-2-5">
<ul class="org-ul">
<li>先驗</li>
<li>Eclat</li>
</ul>
</div>
</div>
<div id="outline-container-org2275245" class="outline-3">
<h3 id="org2275245"><span class="section-number-3">2.6.</span> 自動編碼器</h3>
<div class="outline-text-3" id="text-2-6">
<ul class="org-ul">
<li><a href="https://ithelp.ithome.com.tw/articles/10307959">https://ithelp.ithome.com.tw/articles/10307959</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org0563c4d" class="outline-2">
<h2 id="org0563c4d"><span class="section-number-2">3.</span> 聚類(集群)</h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li>任務: grouping objects together based on similarity.</li>
<li>應用:
<ul class="org-ul">
<li>在信用卡詐欺偵測中，聚類可以將詐欺交易分組在一起，將其與正常交易分開​​。</li>
<li>如果我們的資料集中的觀測值只有幾個標籤，我們可以先使用聚類對觀測值進行分組（不使用標籤）。 然後，我們可以將少數標記觀測值的標籤轉移到同一組內的其餘觀測值。 這是遷移學習的一種形式，也是機器學習中一個快速發展的領域。</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org4d2eef9" class="outline-3">
<h3 id="org4d2eef9"><span class="section-number-3">3.1.</span> K-Means</h3>
<div class="outline-text-3" id="text-3-1">
<p>
將n個點劃分到K個聚落中，如此一來每個點都屬於離其最近的聚落中心所對應之聚落，以之作為分群的標準。例如：
</p>

<div id="org9de9af4" class="figure">
<p><img src="images/blobsScatter.png" alt="blobsScatter.png" width="500" />
</p>
<p><span class="figure-number">Figure 3: </span>scikit-learn blobs</p>
</div>
</div>
<div id="outline-container-org2699301" class="outline-4">
<h4 id="org2699301"><span class="section-number-4">3.1.1.</span> K-Means原理</h4>
<div class="outline-text-4" id="text-3-1-1">
<p>
八張未標註動物名稱(標籤)的照片，每張照片有兩個特徵值
</p>

<div id="orgdf1baed" class="figure">
<p><img src="images/聚類(集群)/2024-02-10_20-19-55_2024-02-10_20-19-45.png" alt="2024-02-10_20-19-55_2024-02-10_20-19-45.png" width="500" />
</p>
<p><span class="figure-number">Figure 4: </span>資料庫樣本</p>
</div>

<p>
八張照片的特徵分佈如下
</p>

<div id="orgee9e2a2" class="figure">
<p><img src="images/kms-1.png" alt="kms-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 5: </span>待處理資料</p>
</div>

<p>
K-means 演算法執行步驟如下:
</p>
</div>
<ol class="org-ol">
<li><a id="org4c07bad"></a>決定K值<br />
<div class="outline-text-5" id="text-3-1-1-1">
<p>
K 值指的是現有訓練資料(八張 照片)要分成的群數，此處 K 值 為 2。
</p>
</div>
</li>
<li><a id="org70d5c2f"></a>選定K個中心點<br />
<div class="outline-text-5" id="text-3-1-1-2">
<p>
任意選定 K 個(K=2)中心點，在實際的程式實作可以亂數隨機 產生這 K 個資料點。如圖<a href="#orgc10e91c">6</a>所示，隨機指定的兩群資料點的中心點為(5，5)、(6，9)。
</p>

<div id="orgc10e91c" class="figure">
<p><img src="images/kms-2.png" alt="kms-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 6: </span>標題</p>
</div>
</div>
</li>
<li><a id="orga073d66"></a>將資料點分群<br />
<div class="outline-text-5" id="text-3-1-1-3">
<p>
接下來為所有資料點計算各自與中心點的「歐幾里德距離」，決定該資料點要被歸入哪一個資料群，計算過程及結果如圖<a href="#org6068f1f">7</a>、<a href="#orgcfe0b1c">8</a>所示。
</p>

<div id="org6068f1f" class="figure">
<p><img src="images/km-3.png" alt="km-3.png" width="500" />
</p>
<p><span class="figure-number">Figure 7: </span>標題</p>
</div>

<div id="orgcfe0b1c" class="figure">
<p><img src="images/km-4.png" alt="km-4.png" width="500" />
</p>
<p><span class="figure-number">Figure 8: </span>標題</p>
</div>

<p>
最後將計算結果記錄如下圖，進行第一輸分群：
</p>



<div id="org77055ba" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_15-59-29_2024-02-10_21-14-09.png" alt="2024-02-11_15-59-29_2024-02-10_21-14-09.png" width="500" />
</p>
<p><span class="figure-number">Figure 9: </span>標題</p>
</div>
</div>
</li>
<li><a id="org0efb73a"></a>為 K 個群裡的資料點找出新中心點<br />
<div class="outline-text-5" id="text-3-1-1-4">
<p>
依前一步驟的分類，此 8 張資料點已分為兩群，接下來就為這兩群資料點找出各自的新中心點，計算方式如下:
</p>
<ul class="org-ul">
<li>新<span style="color:green;">★X</span>值: 2+3+4+6+7+9 =5.17</li>
<li>新<span style="color:green;">★Y</span>值: 6+5+8+3+6+4 =5.33</li>
<li>新<span style="color:orange;">★X</span>值:1+8=4.50</li>
<li>新<span style="color:orange;">★Y</span>值:9+8=8.50</li>
</ul>

<div id="org85548af" class="figure">
<p><img src="images/km5.png" alt="km5.png" width="500" />
</p>
<p><span class="figure-number">Figure 10: </span>標題</p>
</div>
</div>
</li>
<li><a id="orgab693f4"></a>重覆步驟 (3)、(4) 進行下一輪的分群，直到分群結果不再變化<br />
<div class="outline-text-5" id="text-3-1-1-5">
<p>
接下來就繼續計算各點到新中心點<span style="color:green;">★</span>(5.17, 5.33)、<span style="color:orange;">★</span>(4.50, 8.50)的距離、依新 的距離重新對資料點進行分群(即步驟 3)，再求出新的中心點(即步驟 4)，如此重覆不斷進行，直到分群結果不再變動即告完成。
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-org8ebe563" class="outline-4">
<h4 id="org8ebe563"><span class="section-number-4">3.1.2.</span> K-Means實作:隨機數字&#xa0;&#xa0;&#xa0;<span class="tag"><span class="sklearn">sklearn</span></span></h4>
<div class="outline-text-4" id="text-3-1-2">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">&#38568;&#27231;&#29983;&#25104;100&#20491;(x, y)</span>
<span style="color: #339CDB;">import</span> pandas <span style="color: #339CDB;">as</span> pd
<span style="color: #85DDFF;">data</span> = {
    <span style="color: #DB8E73;">'x'</span>: [<span style="color: #B5CEA8; font-weight: bold;">25</span>, <span style="color: #B5CEA8; font-weight: bold;">34</span>, <span style="color: #B5CEA8; font-weight: bold;">22</span>, <span style="color: #B5CEA8; font-weight: bold;">27</span>, <span style="color: #B5CEA8; font-weight: bold;">33</span>, <span style="color: #B5CEA8; font-weight: bold;">33</span>, <span style="color: #B5CEA8; font-weight: bold;">31</span>, <span style="color: #B5CEA8; font-weight: bold;">22</span>, <span style="color: #B5CEA8; font-weight: bold;">35</span>, <span style="color: #B5CEA8; font-weight: bold;">34</span>, <span style="color: #B5CEA8; font-weight: bold;">67</span>, <span style="color: #B5CEA8; font-weight: bold;">54</span>, <span style="color: #B5CEA8; font-weight: bold;">57</span>, <span style="color: #B5CEA8; font-weight: bold;">43</span>, <span style="color: #B5CEA8; font-weight: bold;">50</span>, <span style="color: #B5CEA8; font-weight: bold;">57</span>, <span style="color: #B5CEA8; font-weight: bold;">59</span>, <span style="color: #B5CEA8; font-weight: bold;">52</span>, <span style="color: #B5CEA8; font-weight: bold;">65</span>, <span style="color: #B5CEA8; font-weight: bold;">47</span>, <span style="color: #B5CEA8; font-weight: bold;">49</span>, <span style="color: #B5CEA8; font-weight: bold;">48</span>, <span style="color: #B5CEA8; font-weight: bold;">35</span>, <span style="color: #B5CEA8; font-weight: bold;">33</span>, <span style="color: #B5CEA8; font-weight: bold;">44</span>, <span style="color: #B5CEA8; font-weight: bold;">45</span>, <span style="color: #B5CEA8; font-weight: bold;">38</span>,
          <span style="color: #B5CEA8; font-weight: bold;">43</span>, <span style="color: #B5CEA8; font-weight: bold;">51</span>, <span style="color: #B5CEA8; font-weight: bold;">46</span>],
    <span style="color: #DB8E73;">'y'</span>: [<span style="color: #B5CEA8; font-weight: bold;">79</span>, <span style="color: #B5CEA8; font-weight: bold;">51</span>, <span style="color: #B5CEA8; font-weight: bold;">53</span>, <span style="color: #B5CEA8; font-weight: bold;">78</span>, <span style="color: #B5CEA8; font-weight: bold;">59</span>, <span style="color: #B5CEA8; font-weight: bold;">74</span>, <span style="color: #B5CEA8; font-weight: bold;">73</span>, <span style="color: #B5CEA8; font-weight: bold;">57</span>, <span style="color: #B5CEA8; font-weight: bold;">69</span>, <span style="color: #B5CEA8; font-weight: bold;">75</span>, <span style="color: #B5CEA8; font-weight: bold;">51</span>, <span style="color: #B5CEA8; font-weight: bold;">32</span>, <span style="color: #B5CEA8; font-weight: bold;">40</span>, <span style="color: #B5CEA8; font-weight: bold;">47</span>, <span style="color: #B5CEA8; font-weight: bold;">53</span>, <span style="color: #B5CEA8; font-weight: bold;">36</span>, <span style="color: #B5CEA8; font-weight: bold;">35</span>, <span style="color: #B5CEA8; font-weight: bold;">58</span>, <span style="color: #B5CEA8; font-weight: bold;">59</span>, <span style="color: #B5CEA8; font-weight: bold;">50</span>, <span style="color: #B5CEA8; font-weight: bold;">25</span>, <span style="color: #B5CEA8; font-weight: bold;">20</span>, <span style="color: #B5CEA8; font-weight: bold;">14</span>, <span style="color: #B5CEA8; font-weight: bold;">12</span>, <span style="color: #B5CEA8; font-weight: bold;">20</span>, <span style="color: #B5CEA8; font-weight: bold;">5</span>, <span style="color: #B5CEA8; font-weight: bold;">29</span>, <span style="color: #B5CEA8; font-weight: bold;">27</span>,
          <span style="color: #B5CEA8; font-weight: bold;">8</span>, <span style="color: #B5CEA8; font-weight: bold;">7</span>]
    }
<span style="color: #85DDFF;">samples</span> = pd.DataFrame(data)

<span style="color: #339CDB;">import</span> matplotlib.pyplot <span style="color: #339CDB;">as</span> plt
<span style="color: #339CDB;">from</span> sklearn.cluster <span style="color: #339CDB;">import</span> KMeans

<span style="color: #85DDFF;">kmeans</span> = KMeans(n_clusters=<span style="color: #B5CEA8; font-weight: bold;">3</span>) <span style="color: #579C4C;">#</span><span style="color: #579C4C;">&#38928;&#35336;&#20998;&#28858;&#19977;&#32676;&#65292;&#36845;&#20195;&#27425;&#25976;&#30001;&#27169;&#22411;&#33258;&#34892;&#23450;&#32681;</span>
kmeans.fit(samples)
<span style="color: #85DDFF;">cluster</span> = kmeans.predict(samples)

plt.scatter(samples[<span style="color: #DB8E73;">'x'</span>], samples[<span style="color: #DB8E73;">'y'</span>], c=cluster, cmap=plt.cm.Set1)
plt.savefig(<span style="color: #DB8E73;">"images/kmeansScatter.png"</span>, dpi=<span style="color: #B5CEA8; font-weight: bold;">300</span>)
<span style="color: #579C4C;">#</span><span style="color: #579C4C;">plt.show()</span>
</pre>
</div>


<div id="orgf02cb30" class="figure">
<p><img src="images/kmeansScatter.png" alt="kmeansScatter.png" width="500" />
</p>
<p><span class="figure-number">Figure 11: </span>scikit-KMeans</p>
</div>
</div>
</div>
<div id="outline-container-NS-KM-Image" class="outline-4">
<h4 id="NS-KM-Image"><span class="section-number-4">3.1.3.</span> K-Means應用: 壓縮影像</h4>
<div class="outline-text-4" id="text-NS-KM-Image">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #339CDB;">import</span> numpy <span style="color: #339CDB;">as</span> np
<span class="linenr"> 2: </span><span style="color: #339CDB;">import</span> matplotlib.pyplot <span style="color: #339CDB;">as</span> plt <span style="color: #579C4C;"># </span><span style="color: #579C4C;">&#38656;&#23433;&#35037; pillow &#25165;&#33021;&#35712; JPEG</span>
<span class="linenr"> 3: </span><span style="color: #339CDB;">from</span> matplotlib <span style="color: #339CDB;">import</span> image
<span class="linenr"> 4: </span><span style="color: #339CDB;">from</span> sklearn.cluster <span style="color: #339CDB;">import</span> MiniBatchKMeans
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #579C4C;"># </span><span style="color: #579C4C;">K &#20540; (&#35201;&#20445;&#30041;&#30340;&#38991;&#33394;&#25976;&#37327;)</span>
<span class="linenr"> 7: </span><span style="color: #85DDFF;">K</span> = <span style="color: #B5CEA8; font-weight: bold;">4</span>
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span><span style="color: #579C4C;"># </span><span style="color: #579C4C;">&#35712;&#21462;&#22294;&#29255;</span>
<span class="linenr">10: </span><span style="color: #85DDFF;">image</span> = image.imread(r<span style="color: #DB8E73;">'./images/Photo42.jpg'</span>)
<span class="linenr">11: </span><span style="color: #85DDFF;">w</span>, <span style="color: #85DDFF;">h</span>, <span style="color: #85DDFF;">d</span> = <span style="color: #C586C0;">tuple</span>(image.shape)
<span class="linenr">12: </span><span style="color: #C586C0;">print</span>(w,h,d)
<span class="linenr">13: </span><span style="color: #85DDFF;">image_data</span> = np.reshape(image, (w * h, d))/ <span style="color: #B5CEA8; font-weight: bold;">255</span>
<span class="linenr">14: </span><span style="color: #C586C0;">print</span>(image_data.shape)
<span class="linenr">15: </span><span style="color: #C586C0;">print</span>(image_data[<span style="color: #B5CEA8; font-weight: bold;">0</span>])
<span class="linenr">16: </span><span style="color: #C586C0;">print</span>(image_data[<span style="color: #B5CEA8; font-weight: bold;">1</span>])
<span class="linenr">17: </span><span style="color: #579C4C;"># </span><span style="color: #579C4C;">&#23559;&#38991;&#33394;&#20998;&#39006;&#28858; K &#31278;</span>
<span class="linenr">18: </span><span style="color: #85DDFF;">kmeans</span> = MiniBatchKMeans(n_clusters=K, batch_size=<span style="color: #B5CEA8; font-weight: bold;">10</span>)
<span class="linenr">19: </span><span style="color: #85DDFF;">labels</span> = kmeans.fit_predict(image_data)
<span class="linenr">20: </span><span style="color: #C586C0;">print</span>(labels[:<span style="color: #B5CEA8; font-weight: bold;">10</span>])
<span class="linenr">21: </span><span style="color: #85DDFF;">centers</span> = kmeans.cluster_centers_
<span class="linenr">22: </span><span style="color: #C586C0;">print</span>(centers[:<span style="color: #B5CEA8; font-weight: bold;">10</span>])
<span class="linenr">23: </span><span style="color: #579C4C;"># </span><span style="color: #579C4C;">&#26681;&#25818;&#20998;&#39006;&#23559;&#38991;&#33394;&#23531;&#20837;&#26032;&#30340;&#24433;&#20687;&#38499;&#21015;</span>
<span class="linenr">24: </span><span style="color: #85DDFF;">image_compressed</span> = np.zeros(image.shape)
<span class="linenr">25: </span><span style="color: #85DDFF;">label_idx</span> = <span style="color: #B5CEA8; font-weight: bold;">0</span>
<span class="linenr">26: </span><span style="color: #339CDB;">for</span> i <span style="color: #339CDB;">in</span> <span style="color: #C586C0;">range</span>(w):
<span class="linenr">27: </span>  <span style="color: #339CDB;">for</span> j <span style="color: #339CDB;">in</span> <span style="color: #C586C0;">range</span>(h):
<span class="linenr">28: </span>    image_compressed[i][j] = centers[labels[label_idx]]
<span class="linenr">29: </span>    <span style="color: #85DDFF;">label_idx</span> += <span style="color: #B5CEA8; font-weight: bold;">1</span>
<span class="linenr">30: </span>
<span class="linenr">31: </span>plt.imsave(r<span style="color: #DB8E73;">'images/compressTest.jpg'</span>, image_compressed)
</pre>
</div>

<pre class="example">
480 640 3
(307200, 3)
[0.20784314 0.16078431 0.23921569]
[0.21960784 0.17254902 0.25098039]
[0 0 0 0 0 0 0 0 2 2]
[[0.1535014  0.10980392 0.17348273]
 [0.59200603 0.36930618 0.34788839]
 [0.39191176 0.24676471 0.26073529]
 [0.87828054 0.70392157 0.77662142]]
</pre>


<div id="org016b3c7" class="figure">
<p><img src="images/compressTest.jpg" alt="compressTest.jpg" width="500" />
</p>
<p><span class="figure-number">Figure 12: </span>以KMeans壓縮圖片色彩</p>
</div>
</div>
</div>
</div>
<div id="outline-container-NS-Hie-cluster" class="outline-3">
<h3 id="NS-Hie-cluster"><span class="section-number-3">3.2.</span> Hierarchical clustering</h3>
<div class="outline-text-3" id="text-NS-Hie-cluster">
<p>
階層式分群法(Hierarchical Clustering)透過一種階層架構的方式，將資料層層反覆地進行分裂或聚合，以產生最後的樹狀結構，常見的方式有兩種：
</p>
<ul class="org-ul">
<li><p>
聚合式階層分群法(Agglomerative Clustering): 是一種“bottom-up”的方法，也就是先準備好解決問題可能所需的基本元件或方案，再將這些基本元件組合起來，由小而大最後得到整體。因此在階層式分群法中，就是將每個資料點都視為一個個體，再一一聚合<sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup>，如圖<a href="#org5584ca9">13</a><sup><a id="fnr.4" class="footref" href="#fn.4" role="doc-backlink">4</a></sup>。
</p>

<div id="org5584ca9" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_21-29-19_2024-02-11_21-28-47.png" alt="2024-02-11_21-29-19_2024-02-11_21-28-47.png" width="500" />
</p>
<p><span class="figure-number">Figure 13: </span>Buttom-up</p>
</div></li>
<li><p>
分裂式階層分群法(Divisive Clustering): 是一種“top-down”的方法，先對問題有整體的概念，然後再逐步加上細節，最後讓整體的輪廓越來越清楚。而此法在階層式分群法中，先將整個資料集視為一體，再一一的分裂<sup><a id="fnr.3.100" class="footref" href="#fn.3" role="doc-backlink">3</a></sup>，如圖<a href="#org2840d3f">14</a><sup><a id="fnr.4.100" class="footref" href="#fn.4" role="doc-backlink">4</a></sup>。
</p>

<div id="org2840d3f" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_21-30-15_2024-02-11_21-30-06.png" alt="2024-02-11_21-30-15_2024-02-11_21-30-06.png" width="500" />
</p>
<p><span class="figure-number">Figure 14: </span>Top-down</p>
</div></li>
</ul>
</div>
<div id="outline-container-orgf7e3d25" class="outline-4">
<h4 id="orgf7e3d25"><span class="section-number-4">3.2.1.</span> 聚合式階層分群法(Agglomerative)</h4>
<div class="outline-text-4" id="text-3-2-1">
<p>
如果採用聚合的方式，階層式分群法可由樹狀結構的底部開始，將資料或群聚逐次合併。
</p>

<p>
聚合式階層分群步驟：
</p>
<ol class="org-ol">
<li>將各個資料點先視為個別的「群」。</li>
<li>比較各個群之間的距離，找出距離最短的兩個群。</li>
<li>將其合併變成一個新群。</li>
<li>不斷重複直到群的數量符合所要求的數目。</li>
</ol>
</div>
<ol class="org-ol">
<li><a id="org0fea6da"></a>手動分群<br />
<div class="outline-text-5" id="text-3-2-1-1">
<ol class="org-ol">
<li><p>
假設現在有6筆資料，分別標記A、B、C、D、E及F，且每筆資料都是一個群。
</p>

<div id="org755593c" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_09-04-56_2024-02-11_09-04-42.png" alt="2024-02-11_09-04-56_2024-02-11_09-04-42.png" width="500" />
</p>
<p><span class="figure-number">Figure 15: </span>hierar-1</p>
</div></li>
<li><p>
首先找距離最近的兩個群，在此例為A、B。將A與B結合為新的一群G1，就將這些點分成五群了，其中有四群還是單獨的點。
</p>

<div id="org92e5b12" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_09-06-09_2024-02-11_09-06-00.png" alt="2024-02-11_09-06-09_2024-02-11_09-06-00.png" width="500" />
</p>
<p><span class="figure-number">Figure 16: </span>標題</p>
</div></li>
<li><p>
接著，再繼續找距離最近的兩個群，依此範例應為D與E，結合為新的一群G2。
</p>

<div id="org554b059" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_09-06-59_2024-02-11_09-06-54.png" alt="2024-02-11_09-06-59_2024-02-11_09-06-54.png" width="500" />
</p>
<p><span class="figure-number">Figure 17: </span>標題</p>
</div></li>
<li><p>
將F與G2合而為新的群G3，這時，這些資料已經被分為三群了。
</p>

<div id="orgcdf2922" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_09-18-01_2024-02-11_09-07-48.png" alt="2024-02-11_09-18-01_2024-02-11_09-07-48.png" width="500" />
</p>
<p><span class="figure-number">Figure 18: </span>標題</p>
</div></li>
</ol>
</div>
</li>
<li><a id="org2a4cca1"></a>如何定義兩個群聚之間的距離<br />
<ol class="org-ol">
<li><a id="org40aa435"></a>單一連結聚合<br />
<div class="outline-text-6" id="text-3-2-1-2-1">
<p>
Single-linkage agglomerative algorithm, 群聚與群聚間的距離可以定義為不同群聚中最接近兩點間的距離。
</p>

<p>
在分屬不同的兩群中，選擇最接近的兩點之距離，即代表兩群間的距離。因此在群與群間進行聚合時，依據此值最小者做為選取下一步結合之對象。
</p>


<div id="org7d0afcc" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_10-13-45_2024-02-11_09-23-14.png" alt="2024-02-11_10-13-45_2024-02-11_09-23-14.png" width="300" />
</p>
<p><span class="figure-number">Figure 19: </span>標題</p>
</div>

<p>
公式: \( d(G1, G2)=\min\limits_{ A \in G1, B \in G2 }  d(A,B)\)
</p>

<p>
G1、G3與C之間如何聚合？
</p>
<ul class="org-ul">
<li>G1與C之間的距離d(G1,C)＝d(B,C)</li>
<li>G3與C之間的距離d(G3,C)＝d(F,C)</li>
<li>G1與G3之間的距離d(G1,G3)＝d(B,D)</li>
</ul>

<p>
計算完各群間的距離後，可知d(G3,C)為最短距離，因此G3將與C聚合，成為新群G4。
</p>


<div id="org3dfb911" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_10-16-41_2024-02-11_10-16-28.png" alt="2024-02-11_10-16-41_2024-02-11_10-16-28.png" width="500" />
</p>
<p><span class="figure-number">Figure 20: </span>標題</p>
</div>

<p>
倘若要再聚合，由於剩下G1與G4，可聚合成為G5。
</p>


<div id="org53bcdb1" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_10-17-37_2024-02-11_10-17-28.png" alt="2024-02-11_10-17-37_2024-02-11_10-17-28.png" width="500" />
</p>
<p><span class="figure-number">Figure 21: </span>標題</p>
</div>
</div>
</li>
<li><a id="orgcf481dd"></a>完整連結聚合<br />
<div class="outline-text-6" id="text-3-2-1-2-2">
<p>
Complete-linkage agglomerative algorithm, 群聚間的距離定義為不同群聚中最遠兩點間的距離，這樣可以保證這兩個集合合併後, 任何一對的距離不會大於 d。
</p>

<p>
在分屬不同的兩群中，選擇最遠的兩點之距離，即代表兩群間的距離。因此在群與群間進行聚合時，依據此值最小者做為選取下一步結合之對象。
</p>

<p>
公式: \(d(G1,G2)=\max\limits_{A \in G1, B \in G2}d(A,B)\)
</p>


<div id="orga6cbde6" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_10-18-31_2024-02-11_10-18-24.png" alt="2024-02-11_10-18-31_2024-02-11_10-18-24.png" width="300" />
</p>
<p><span class="figure-number">Figure 22: </span>標題</p>
</div>

<p>
G1、G3與C之間如何聚合？
</p>
<ul class="org-ul">
<li>G1與C之間的距離d(G1,C)＝d(A,C)</li>
<li>G3與C之間的距離d(G3,C)＝d(E,C)</li>
<li>G1與G3之間的距離d(G1,G3)＝d(A,E)</li>
</ul>


<div id="org7c491db" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_10-20-11_2024-02-11_10-20-03.png" alt="2024-02-11_10-20-11_2024-02-11_10-20-03.png" width="500" />
</p>
<p><span class="figure-number">Figure 23: </span>標題</p>
</div>


<p>
計算完各群間的距離後，可知d(G1,C)為最短距離，因此G1將與C聚合，成為新群G4。
</p>

<p>
倘若要再聚合，由於剩下G3與G4，可聚合成為G5。
</p>


<div id="org1a83e96" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_10-21-08_2024-02-11_10-21-02.png" alt="2024-02-11_10-21-08_2024-02-11_10-21-02.png" width="500" />
</p>
<p><span class="figure-number">Figure 24: </span>標題</p>
</div>
</div>
</li>
<li><a id="org6d9d418"></a>平均連結聚合<br />
<div class="outline-text-6" id="text-3-2-1-2-3">
<p>
Average-linkage agglomerative algorithm, 群聚間的距離定義為不同群聚間各點與各點間距離總和的平均。沃德法（Ward&rsquo;s method）：群聚間的距離定義為在將兩群合併後，各點到合併後的群中心的距離平方和。
</p>

<p>
在分屬不同的兩群中，各點之距離的平均，即代表兩群間的距離。因此在群與群間進行聚合時，依據此值最小者做為選取下一步結合之對象。
G1、G3與C之間如何聚合？
</p>


<div id="orgd90ca05" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_10-24-58_2024-02-11_10-24-49.png" alt="2024-02-11_10-24-58_2024-02-11_10-24-49.png" width="300" />
</p>
<p><span class="figure-number">Figure 25: </span>標題</p>
</div>

<p>
公式: \(d(G1,G2)=\frac{\sum_{A \in G1, B \in G2}d(A,B)}{|G1|\times|G2|}\)
</p>

<ul class="org-ul">
<li>\( d(G1, C)=\frac{d(A,C)+d(B,C)}{2\times1}\)</li>
<li>\( d(G3, C)=\frac{d(D,C)+d(E,C)+d(F,C)}{3\times1}\)</li>
<li>\( d(G1, G3)=\frac{d(A,D)+d(A,E)+d(A,F)+d(B,D)+d(B,E)+d(B,F)}{2\times3}\)</li>
</ul>
</div>
</li>
</ol>
</li>
<li><a id="org671afb0"></a>決定群數<br />
<div class="outline-text-5" id="text-3-2-1-3">
<p>
可以依照使用者的群數需求或相似度要求，來決定要在哪一層時停止聚合資料。若以完整連結的群間距離計算方式為例，圖上的虛線代表不同的群數，端看使用者需求來決定。
</p>

<div id="org1affc73" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_10-47-21_2024-02-11_10-47-15.png" alt="2024-02-11_10-47-21_2024-02-11_10-47-15.png" width="500" />
</p>
<p><span class="figure-number">Figure 26: </span>標題</p>
</div>
</div>
</li>
<li><a id="org3390bc4"></a>[課堂任務]聚合式階層分群&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span><br />
<ol class="org-ol">
<li><a id="org4ef76b0"></a>資料<br />
<div class="outline-text-6" id="text-3-2-1-4-1">
<p>
在此給定資料並以數值化座標平面表示，其中包含A、B、C、D、E、F、G及H共8個點。假設B與C點合併為G1；G與H點合併為G2，而G2加入F點後形成G3。
</p>

<div id="org40652ac" class="figure">
<p><img src="images/聚類(集群)/2024-02-11_10-49-46_2024-02-11_10-49-38.png" alt="2024-02-11_10-49-46_2024-02-11_10-49-38.png" width="500" />
</p>
<p><span class="figure-number">Figure 27: </span>標題</p>
</div>
</div>
</li>
<li><a id="org24a6289"></a>任務<br />
<div class="outline-text-6" id="text-3-2-1-4-2">
<ol class="org-ol">
<li>請利用「單一連結」的群間距離計算方式完成聚合式階層式分群。</li>
<li>請利用「完整連結」的群間距離計算方式完成聚合式階層式分群。</li>
<li>請利用「平均連結」的群間距離計算方式完成聚合式階層式分群。</li>
</ol>
</div>
</li>
</ol>
</li>
<li><a id="org8340326"></a>分群實作<br />
<ol class="org-ol">
<li><a id="org3c87a41"></a>Agglomerative Clustering Sample<br />
<ul class="org-ul">
<li><a id="orgd52744d"></a>分兩群<br />
<div class="outline-text-7" id="text-orgd52744d">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #339CDB;">from</span> sklearn.cluster <span style="color: #339CDB;">import</span> AgglomerativeClustering
<span style="color: #339CDB;">import</span> matplotlib.pyplot <span style="color: #339CDB;">as</span> plt
<span style="color: #339CDB;">import</span> numpy <span style="color: #339CDB;">as</span> np

<span style="color: #579C4C;"># </span><span style="color: #579C4C;">randomly chosen dataset</span>
<span style="color: #85DDFF;">X</span> = np.array([[<span style="color: #B5CEA8; font-weight: bold;">1</span>, <span style="color: #B5CEA8; font-weight: bold;">2</span>], [<span style="color: #B5CEA8; font-weight: bold;">1</span>, <span style="color: #B5CEA8; font-weight: bold;">4</span>], [<span style="color: #B5CEA8; font-weight: bold;">1</span>, <span style="color: #B5CEA8; font-weight: bold;">0</span>], [<span style="color: #B5CEA8; font-weight: bold;">2</span>, <span style="color: #B5CEA8; font-weight: bold;">1</span>], [<span style="color: #B5CEA8; font-weight: bold;">2</span>, <span style="color: #B5CEA8; font-weight: bold;">3</span>], [<span style="color: #B5CEA8; font-weight: bold;">2</span>, <span style="color: #B5CEA8; font-weight: bold;">4</span>],
              [<span style="color: #B5CEA8; font-weight: bold;">3</span>, <span style="color: #B5CEA8; font-weight: bold;">1</span>], [<span style="color: #B5CEA8; font-weight: bold;">3</span>, <span style="color: #B5CEA8; font-weight: bold;">3</span>], [<span style="color: #B5CEA8; font-weight: bold;">3</span>, <span style="color: #B5CEA8; font-weight: bold;">4</span>], [<span style="color: #B5CEA8; font-weight: bold;">4</span>, <span style="color: #B5CEA8; font-weight: bold;">2</span>], [<span style="color: #B5CEA8; font-weight: bold;">4</span>, <span style="color: #B5CEA8; font-weight: bold;">4</span>], [<span style="color: #B5CEA8; font-weight: bold;">4</span>, <span style="color: #B5CEA8; font-weight: bold;">0</span>]])
<span style="color: #85DDFF;">clustering</span> = AgglomerativeClustering(n_clusters = <span style="color: #B5CEA8; font-weight: bold;">2</span>).fit(X)
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">'&#20998;&#20841;&#32676;:'</span>,clustering.labels_)
</pre>
</div>

<pre class="example">
分兩群: [0 1 0 0 1 1 0 1 1 0 1 0]
</pre>



<div id="orgdd4062c" class="figure">
<p><img src="images/aggclu-1.png" alt="aggclu-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 28: </span>分成兩組</p>
</div>
</div>
</li>
<li><a id="orgb51fb38"></a>分三群<br />
<div class="outline-text-7" id="text-orgb51fb38">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #85DDFF;">clustering</span> = AgglomerativeClustering(n_clusters = <span style="color: #B5CEA8; font-weight: bold;">3</span>).fit(X)
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">'&#20998;&#19977;&#32676;:'</span>,clustering.labels_)
</pre>
</div>

<pre class="example">
分三群: [1 0 1 1 0 0 1 0 0 2 0 2]
</pre>



<div id="orgfac8a49" class="figure">
<p><img src="images/aggclu-2.png" alt="aggclu-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 29: </span>分成三群</p>
</div>
</div>
</li>
<li><a id="org1637f07"></a>分四群<br />
<div class="outline-text-7" id="text-org1637f07">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #85DDFF;">clustering</span> = AgglomerativeClustering(n_clusters = <span style="color: #B5CEA8; font-weight: bold;">4</span>).fit(X)
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">'&#20998;&#22235;&#32676;:'</span>,clustering.labels_)
</pre>
</div>

<pre class="example">
分四群: [0 3 0 0 3 3 0 1 1 2 1 2]
</pre>



<div id="org782115e" class="figure">
<p><img src="images/aggclu-3.png" alt="aggclu-3.png" width="500" />
</p>
<p><span class="figure-number">Figure 30: </span>分成四群</p>
</div>
</div>
</li>
</ul>
</li>
<li><a id="org06a7a40"></a>scipy.cluster.hierarchy[一次分完]<br />
<div class="outline-text-6" id="text-3-2-1-5-2">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #339CDB;">import</span> numpy <span style="color: #339CDB;">as</span> np
<span style="color: #339CDB;">import</span> matplotlib.pyplot <span style="color: #339CDB;">as</span> plt
<span style="color: #339CDB;">import</span> scipy.cluster.hierarchy <span style="color: #339CDB;">as</span> sch

<span style="color: #579C4C;"># </span><span style="color: #579C4C;">randomly chosen dataset</span>
<span style="color: #85DDFF;">X</span> = np.array([[<span style="color: #B5CEA8; font-weight: bold;">1</span>, <span style="color: #B5CEA8; font-weight: bold;">2</span>], [<span style="color: #B5CEA8; font-weight: bold;">1</span>, <span style="color: #B5CEA8; font-weight: bold;">4</span>], [<span style="color: #B5CEA8; font-weight: bold;">1</span>, <span style="color: #B5CEA8; font-weight: bold;">0</span>], [<span style="color: #B5CEA8; font-weight: bold;">2</span>, <span style="color: #B5CEA8; font-weight: bold;">1</span>], [<span style="color: #B5CEA8; font-weight: bold;">2</span>, <span style="color: #B5CEA8; font-weight: bold;">3</span>], [<span style="color: #B5CEA8; font-weight: bold;">2</span>, <span style="color: #B5CEA8; font-weight: bold;">4</span>],
              [<span style="color: #B5CEA8; font-weight: bold;">3</span>, <span style="color: #B5CEA8; font-weight: bold;">1</span>], [<span style="color: #B5CEA8; font-weight: bold;">3</span>, <span style="color: #B5CEA8; font-weight: bold;">3</span>], [<span style="color: #B5CEA8; font-weight: bold;">3</span>, <span style="color: #B5CEA8; font-weight: bold;">4</span>], [<span style="color: #B5CEA8; font-weight: bold;">4</span>, <span style="color: #B5CEA8; font-weight: bold;">2</span>], [<span style="color: #B5CEA8; font-weight: bold;">4</span>, <span style="color: #B5CEA8; font-weight: bold;">4</span>], [<span style="color: #B5CEA8; font-weight: bold;">4</span>, <span style="color: #B5CEA8; font-weight: bold;">0</span>]])
<span style="color: #85DDFF;">y</span> = np.array([<span style="color: #DB8E73;">'A'</span>, <span style="color: #DB8E73;">'B'</span>, <span style="color: #DB8E73;">'C'</span>, <span style="color: #DB8E73;">'D'</span>, <span style="color: #DB8E73;">'E'</span>, <span style="color: #DB8E73;">'F'</span>, <span style="color: #DB8E73;">'G'</span>, <span style="color: #DB8E73;">'H'</span>, <span style="color: #DB8E73;">'I'</span>, <span style="color: #DB8E73;">'J'</span>, <span style="color: #DB8E73;">'K'</span>, <span style="color: #DB8E73;">'L'</span>])

<span style="color: #85DDFF;">dis</span>=sch.linkage(X,metric=<span style="color: #DB8E73;">'euclidean'</span>, method=<span style="color: #DB8E73;">'ward'</span>)
<span style="color: #579C4C;">#</span><span style="color: #579C4C;">metric: &#36317;&#38626;&#30340;&#35336;&#31639;&#26041;&#24335;</span>
<span style="color: #579C4C;">#</span><span style="color: #579C4C;">method: &#32676;&#33287;&#32676;&#20043;&#38291;&#30340;&#35336;&#31639;&#26041;&#24335;&#65292;&#8221;single&#8221;, &#8220;complete&#8221;, &#8220;average&#8221;, &#8220;weighted&#8221;, &#8220;centroid&#8221;, &#8220;median&#8221;, &#8220;ward&#8221;</span>

sch.dendrogram(dis, labels = y)

plt.title(<span style="color: #DB8E73;">'Hierarchical Clustering'</span>)
plt.xticks(rotation=<span style="color: #B5CEA8; font-weight: bold;">30</span>)
plt.savefig(<span style="color: #DB8E73;">"images/hierarCluster-1.png"</span>, dpi=<span style="color: #B5CEA8; font-weight: bold;">300</span>)
<span style="color: #579C4C;">#</span><span style="color: #579C4C;">plt.show()</span>
</pre>
</div>

<div id="orgce51284" class="figure">
<p><img src="images/hierarCluster-1.png" alt="hierarCluster-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 31: </span>Hierarchical Clustering</p>
</div>
</div>
</li>
<li><a id="orga975aea"></a>scipy.cluster.hierarchy[逐步分群]]<br />
<div class="outline-text-6" id="text-3-2-1-5-3">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #339CDB;">import</span> numpy <span style="color: #339CDB;">as</span> np
<span style="color: #339CDB;">import</span> matplotlib.pyplot <span style="color: #339CDB;">as</span> plt
<span style="color: #339CDB;">import</span> scipy.cluster.hierarchy <span style="color: #339CDB;">as</span> sch

<span style="color: #579C4C;"># </span><span style="color: #579C4C;">randomly chosen dataset</span>
<span style="color: #85DDFF;">X</span> = np.array([[<span style="color: #B5CEA8; font-weight: bold;">1</span>, <span style="color: #B5CEA8; font-weight: bold;">2</span>], [<span style="color: #B5CEA8; font-weight: bold;">1</span>, <span style="color: #B5CEA8; font-weight: bold;">4</span>], [<span style="color: #B5CEA8; font-weight: bold;">1</span>, <span style="color: #B5CEA8; font-weight: bold;">0</span>], [<span style="color: #B5CEA8; font-weight: bold;">2</span>, <span style="color: #B5CEA8; font-weight: bold;">1</span>], [<span style="color: #B5CEA8; font-weight: bold;">2</span>, <span style="color: #B5CEA8; font-weight: bold;">3</span>], [<span style="color: #B5CEA8; font-weight: bold;">2</span>, <span style="color: #B5CEA8; font-weight: bold;">4</span>],
              [<span style="color: #B5CEA8; font-weight: bold;">3</span>, <span style="color: #B5CEA8; font-weight: bold;">1</span>], [<span style="color: #B5CEA8; font-weight: bold;">3</span>, <span style="color: #B5CEA8; font-weight: bold;">3</span>], [<span style="color: #B5CEA8; font-weight: bold;">3</span>, <span style="color: #B5CEA8; font-weight: bold;">4</span>], [<span style="color: #B5CEA8; font-weight: bold;">4</span>, <span style="color: #B5CEA8; font-weight: bold;">2</span>], [<span style="color: #B5CEA8; font-weight: bold;">4</span>, <span style="color: #B5CEA8; font-weight: bold;">4</span>], [<span style="color: #B5CEA8; font-weight: bold;">4</span>, <span style="color: #B5CEA8; font-weight: bold;">0</span>]])
<span style="color: #85DDFF;">y</span> = np.array([<span style="color: #DB8E73;">'A'</span>, <span style="color: #DB8E73;">'B'</span>, <span style="color: #DB8E73;">'C'</span>, <span style="color: #DB8E73;">'D'</span>, <span style="color: #DB8E73;">'E'</span>, <span style="color: #DB8E73;">'F'</span>, <span style="color: #DB8E73;">'G'</span>, <span style="color: #DB8E73;">'H'</span>, <span style="color: #DB8E73;">'I'</span>, <span style="color: #DB8E73;">'J'</span>, <span style="color: #DB8E73;">'K'</span>, <span style="color: #DB8E73;">'L'</span>])

<span style="color: #579C4C;">#</span><span style="color: #579C4C;">metric: &#36317;&#38626;&#30340;&#35336;&#31639;&#26041;&#24335;</span>
<span style="color: #579C4C;">#</span><span style="color: #579C4C;">method: &#32676;&#33287;&#32676;&#20043;&#38291;&#30340;&#35336;&#31639;&#26041;&#24335;&#65292;&#8221;single&#8221;, &#8220;complete&#8221;, &#8220;average&#8221;, &#8220;weighted&#8221;, &#8220;centroid&#8221;, &#8220;median&#8221;, &#8220;ward&#8221;</span>

plt.cla()
<span style="color: #579C4C;"># </span><span style="color: #579C4C;">Setting the truncate_mode to 'lastp' to see incremental clustering</span>
plt.figure(figsize=(<span style="color: #B5CEA8; font-weight: bold;">10</span>, <span style="color: #B5CEA8; font-weight: bold;">20</span>))
<span style="color: #339CDB;">for</span> i <span style="color: #339CDB;">in</span> <span style="color: #C586C0;">range</span>(<span style="color: #B5CEA8; font-weight: bold;">2</span>, <span style="color: #C586C0;">len</span>(y) + <span style="color: #B5CEA8; font-weight: bold;">1</span>):
    plt.subplot( <span style="color: #B5CEA8; font-weight: bold;">6</span>, <span style="color: #B5CEA8; font-weight: bold;">2</span>, i - <span style="color: #B5CEA8; font-weight: bold;">1</span>)
    <span style="color: #85DDFF;">labels</span> = y[:i]  <span style="color: #579C4C;"># </span><span style="color: #579C4C;">Adjusting labels for each step</span>
    <span style="color: #85DDFF;">x_step</span> = X[:i]
    <span style="color: #85DDFF;">dis</span>=sch.linkage(x_step, metric=<span style="color: #DB8E73;">'euclidean'</span>, method=<span style="color: #DB8E73;">'ward'</span>)
    sch.dendrogram(dis, labels=labels, truncate_mode=<span style="color: #DB8E73;">'lastp'</span>, p=i)
    plt.title(f<span style="color: #DB8E73;">'Step </span>{i}<span style="color: #DB8E73;">'</span>)

plt.suptitle(<span style="color: #DB8E73;">'Hierarchical Clustering Steps'</span>)
plt.tight_layout(rect=[<span style="color: #B5CEA8; font-weight: bold;">0</span>, <span style="color: #B5CEA8; font-weight: bold;">0.03</span>, <span style="color: #B5CEA8; font-weight: bold;">1</span>, <span style="color: #B5CEA8; font-weight: bold;">0.95</span>])

plt.title(<span style="color: #DB8E73;">'Hierarchical Clustering'</span>)
plt.xticks(rotation=<span style="color: #B5CEA8; font-weight: bold;">30</span>)
plt.savefig(<span style="color: #DB8E73;">"images/hierarCluster-2.png"</span>, dpi=<span style="color: #B5CEA8; font-weight: bold;">300</span>)
<span style="color: #579C4C;">#</span><span style="color: #579C4C;">plt.show()</span>
</pre>
</div>

<div id="orgc24eeb6" class="figure">
<p><img src="images/hierarCluster-2.png" alt="hierarCluster-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 32: </span>Hierarchical Clustering</p>
</div>
</div>
</li>
<li><a id="org51d0bdb"></a>利用距離決定群數，或直接給定群數。<br />
<div class="outline-text-6" id="text-3-2-1-5-4">
<p>
建構好聚落樹狀圖後，我們可以依照距離的切割來進行分類，也可以直接給定想要分類的群數，讓系統自動切割到相對應的距離。
</p>
<ul class="org-ul">
<li>距離切割
所給出的樹狀圖，y軸代表距離，我們可以用特徵之間的距離進行分群的切割。</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #85DDFF;">max_dis</span>=<span style="color: #B5CEA8; font-weight: bold;">5</span>
<span style="color: #85DDFF;">clusters</span>=sch.fcluster(dis,max_dis,criterion=<span style="color: #DB8E73;">'distance'</span>)
<span style="color: #339CDB;">import</span> matplotlib.pyplot <span style="color: #339CDB;">as</span> plt
plt.figure()
plt.scatter(X[:,<span style="color: #B5CEA8; font-weight: bold;">0</span>], X[:,<span style="color: #B5CEA8; font-weight: bold;">1</span>], c=clusters, cmap=plt.cm.Set1)
plt.savefig(<span style="color: #DB8E73;">"images/clusterScatter.png"</span>, dpi=<span style="color: #B5CEA8; font-weight: bold;">300</span>)
</pre>
</div>


<div id="org6fb047e" class="figure">
<p><img src="images/clusterScatter.png" alt="clusterScatter.png" width="500" />
</p>
<p><span class="figure-number">Figure 33: </span>Caption</p>
</div>
<ul class="org-ul">
<li>直接給定群數
同時，我們也可以像sklearn一樣，直接給定我們所想要分出的群數。</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #85DDFF;">k</span>=<span style="color: #B5CEA8; font-weight: bold;">4</span>
<span style="color: #85DDFF;">clusters</span>=sch.fcluster(dis,k,criterion=<span style="color: #DB8E73;">'maxclust'</span>)

<span style="color: #339CDB;">import</span> matplotlib.pyplot <span style="color: #339CDB;">as</span> plt
plt.figure()
plt.scatter(X[:,<span style="color: #B5CEA8; font-weight: bold;">0</span>], X[:,<span style="color: #B5CEA8; font-weight: bold;">1</span>], c=clusters, cmap=plt.cm.Set1)
plt.savefig(<span style="color: #DB8E73;">"images/clusterScatter-1.png"</span>, dpi=<span style="color: #B5CEA8; font-weight: bold;">300</span>)
</pre>
</div>


<div id="orgb3e8b8d" class="figure">
<p><img src="images/clusterScatter-1.png" alt="clusterScatter-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 34: </span>Caption</p>
</div>
</div>
</li>
</ol>
</li>
<li><a id="org5df164e"></a>如何評估最佳分群數:K<br />
<div class="outline-text-5" id="text-3-2-1-6">
<ul class="org-ul">
<li><a href="https://jimmy-huang.medium.com/kmeans%E5%88%86%E7%BE%A4%E6%BC%94%E7%AE%97%E6%B3%95-%E8%88%87-silhouette-%E8%BC%AA%E5%BB%93%E5%88%86%E6%9E%90-8be17e634589">Kmeans分群演算法 與 Silhouette 輪廓分析</a></li>
<li><a href="https://www.geeksforgeeks.org/implementing-agglomerative-clustering-using-sklearn/">Implementing Agglomerative Clustering using Sklearn</a></li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org177733e" class="outline-4">
<h4 id="org177733e"><span class="section-number-4">3.2.2.</span> 分裂式階層分群法(Divisive Clustering)</h4>
<div class="outline-text-4" id="text-3-2-2">
<p>
如果採用分裂的方式，則由樹狀結構的頂端開始，將群聚逐次分裂。
Divisive clustering : Also known as top-down approach. This algorithm also does not require to prespecify the number of clusters. Top-down clustering requires a method for splitting a cluster that contains the whole data and proceeds by splitting clusters recursively until individual data have been splitted into singleton cluster.
</p>
</div>
<ol class="org-ol">
<li><a id="org2e03b52"></a>分群實作<br />
<div class="outline-text-5" id="text-3-2-2-1">

<div id="orgc5ceefa" class="figure">
<p><img src="images/topdown.png" alt="topdown.png" width="600" />
</p>
<p><span class="figure-number">Figure 35: </span>Caption</p>
</div>
</div>
</li>
</ol>
</div>
<div id="outline-container-org1aaadbe" class="outline-4">
<h4 id="org1aaadbe"><span class="section-number-4">3.2.3.</span> TNFSH作業: 聚合式分群作業&#xa0;&#xa0;&#xa0;<span class="tag"><span class="TNFSH">TNFSH</span></span></h4>
<div class="outline-text-4" id="text-3-2-3">
<p>
請利用聚合式分群的模型幫 <b>鳶尾花</b> 分類，
</p>
<ol class="org-ol">
<li>將階層圖畫出來</li>
<li>將K值設為3，輸出分群結果</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org4c19059" class="outline-3">
<h3 id="org4c19059"><span class="section-number-3">3.3.</span> DBSCAN</h3>
<div class="outline-text-3" id="text-3-3">
<p>
DBSCAN will group together closely packed points, where close together is defined as a minimum number of points that must exist within a certain distance. If the point is within a certain distance of multiple clusters, it will be grouped with the cluster to which it is most densely located. Any instance that is not within this certain distance of another cluster is labeled an outlier.
</p>

<p>
In k-means and hierarchical clustering, all points had to be clustered, and outliers were poorly dealt with. In DBSCAN, we can explicitly label points as outliers and avoid having to cluster them. This is powerful. Compared to the other clustering algorithms, DBSCAN is much less prone to the distortion typically caused by outliers in the data. Also, like hierarchical clustering—and unlike k-means—we do not need to prespecify the number of clusters.
</p>
</div>
<div id="outline-container-orgfbdc08d" class="outline-4">
<h4 id="orgfbdc08d"><span class="section-number-4">3.3.1.</span> 實作</h4>
<div class="outline-text-4" id="text-3-3-1">
</div>
<ol class="org-ol">
<li><a id="org01ed61c"></a>讀入資料<br />
<div class="outline-text-5" id="text-3-3-1-1">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #777778; font-style: italic;">'''Main'''</span>
<span style="color: #339CDB;">import</span> numpy <span style="color: #339CDB;">as</span> np
<span style="color: #339CDB;">import</span> pandas <span style="color: #339CDB;">as</span> pd
<span style="color: #339CDB;">import</span> os, time, pickle, gzip
<span style="color: #339CDB;">import</span> datetime

<span style="color: #DB8E73;">'''Data Prep'''</span>
<span style="color: #339CDB;">from</span> sklearn <span style="color: #339CDB;">import</span> preprocessing <span style="color: #339CDB;">as</span> pp

<span style="color: #DB8E73;">'''Data Viz'''</span>
<span style="color: #339CDB;">import</span> matplotlib.pyplot <span style="color: #339CDB;">as</span> plt
<span style="color: #339CDB;">import</span> matplotlib <span style="color: #339CDB;">as</span> mpl
<span style="color: #339CDB;">import</span> seaborn <span style="color: #339CDB;">as</span> sns
<span style="color: #85DDFF;">color</span> = sns.color_palette()
<span style="color: #579C4C;"># </span><span style="color: #579C4C;">Load the datasets</span>
<span style="color: #85DDFF;">current_path</span> = os.getcwd()
<span style="color: #C586C0;">file</span> = os.path.sep.join([<span style="color: #DB8E73;">''</span>, <span style="color: #DB8E73;">'datasets'</span>, <span style="color: #DB8E73;">'mnist.pkl.gz'</span>])

<span style="color: #85DDFF;">f</span> = gzip.<span style="color: #C586C0;">open</span>(current_path+<span style="color: #C586C0;">file</span>, <span style="color: #DB8E73;">'rb'</span>)
<span style="color: #85DDFF;">train_set</span>, <span style="color: #85DDFF;">validation_set</span>, <span style="color: #85DDFF;">test_set</span> = pickle.load(f, encoding=<span style="color: #DB8E73;">'latin1'</span>)
f.close()

<span style="color: #85DDFF;">X_train</span>, <span style="color: #85DDFF;">y_train</span> = train_set[<span style="color: #B5CEA8; font-weight: bold;">0</span>], train_set[<span style="color: #B5CEA8; font-weight: bold;">1</span>]
<span style="color: #85DDFF;">X_validation</span>, <span style="color: #85DDFF;">y_validation</span> = validation_set[<span style="color: #B5CEA8; font-weight: bold;">0</span>], validation_set[<span style="color: #B5CEA8; font-weight: bold;">1</span>]
<span style="color: #85DDFF;">X_test</span>, <span style="color: #85DDFF;">y_test</span> = test_set[<span style="color: #B5CEA8; font-weight: bold;">0</span>], test_set[<span style="color: #B5CEA8; font-weight: bold;">1</span>]

<span style="color: #579C4C;"># </span><span style="color: #579C4C;">Create Pandas DataFrames from the datasets</span>
<span style="color: #85DDFF;">train_index</span> = <span style="color: #C586C0;">range</span>(<span style="color: #B5CEA8; font-weight: bold;">0</span>,<span style="color: #C586C0;">len</span>(X_train))
<span style="color: #85DDFF;">validation_index</span> = <span style="color: #C586C0;">range</span>(<span style="color: #C586C0;">len</span>(X_train), <span style="color: #C586C0;">len</span>(X_train)+<span style="color: #C586C0;">len</span>(X_validation))
<span style="color: #85DDFF;">test_index</span> = <span style="color: #C586C0;">range</span>(<span style="color: #C586C0;">len</span>(X_train)+<span style="color: #C586C0;">len</span>(X_validation), \
                   <span style="color: #C586C0;">len</span>(X_train)+<span style="color: #C586C0;">len</span>(X_validation)+<span style="color: #C586C0;">len</span>(X_test))

<span style="color: #85DDFF;">X_train</span> = pd.DataFrame(data=X_train,index=train_index)
<span style="color: #85DDFF;">y_train</span> = pd.Series(data=y_train,index=train_index)

<span style="color: #85DDFF;">X_validation</span> = pd.DataFrame(data=X_validation,index=validation_index)
<span style="color: #85DDFF;">y_validation</span> = pd.Series(data=y_validation,index=validation_index)

<span style="color: #85DDFF;">X_test</span> = pd.DataFrame(data=X_test,index=test_index)
<span style="color: #85DDFF;">y_test</span> = pd.Series(data=y_test,index=test_index)

</pre>
</div>
</div>
</li>
<li><a id="orgcd59b61"></a>降維<br />
<div class="outline-text-5" id="text-3-3-1-2">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Principal Component Analysis</span>
<span style="color: #339CDB;">from</span> sklearn.decomposition <span style="color: #339CDB;">import</span> PCA

<span style="color: #85DDFF;">n_components</span> = <span style="color: #B5CEA8; font-weight: bold;">784</span>
<span style="color: #85DDFF;">whiten</span> = <span style="color: #339CDB;">False</span>
<span style="color: #85DDFF;">random_state</span> = <span style="color: #B5CEA8; font-weight: bold;">2018</span>

<span style="color: #85DDFF;">pca</span> = PCA(n_components=n_components, whiten=whiten, \
          random_state=random_state)

<span style="color: #85DDFF;">X_train_PCA</span> = pca.fit_transform(X_train)
<span style="color: #85DDFF;">X_train_PCA</span> = pd.DataFrame(data=X_train_PCA, index=train_index)

 <span style="color: #579C4C;"># </span><span style="color: #579C4C;">Log data</span>
<span style="color: #85DDFF;">cwd</span> = os.getcwd()
<span style="color: #85DDFF;">log_dir</span> = cwd+<span style="color: #DB8E73;">"/datasets/"</span>
y_train[<span style="color: #B5CEA8; font-weight: bold;">0</span>:<span style="color: #B5CEA8; font-weight: bold;">2000</span>].to_csv(log_dir+<span style="color: #DB8E73;">'labels.tsv'</span>, sep = <span style="color: #DB8E73;">'</span><span style="color: #339CDB;">\t</span><span style="color: #DB8E73;">'</span>, index=<span style="color: #339CDB;">False</span>, header=<span style="color: #339CDB;">False</span>)

<span style="color: #579C4C;"># </span><span style="color: #579C4C;">Write dimensions to CSV</span>
X_train_PCA.iloc[<span style="color: #B5CEA8; font-weight: bold;">0</span>:<span style="color: #B5CEA8; font-weight: bold;">2000</span>,<span style="color: #B5CEA8; font-weight: bold;">0</span>:<span style="color: #B5CEA8; font-weight: bold;">3</span>].to_csv(log_dir+<span style="color: #DB8E73;">'pca_data.tsv'</span>, sep = <span style="color: #DB8E73;">'</span><span style="color: #339CDB;">\t</span><span style="color: #DB8E73;">'</span>, index=<span style="color: #339CDB;">False</span>, header=<span style="color: #339CDB;">False</span>)
</pre>
</div>
</div>
</li>
<li><a id="orgc9de70e"></a>DBSCAN<br />
<div class="outline-text-5" id="text-3-3-1-3">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Perform DBSCAN</span>
<span style="color: #339CDB;">def</span> <span style="color: #D9DAA2;">analyzeCluster</span>(clusterDF, labelsDF):
    <span style="color: #85DDFF;">countByCluster</span> = pd.DataFrame(data=clusterDF[<span style="color: #DB8E73;">'cluster'</span>].value_counts())
    countByCluster.reset_index(inplace=<span style="color: #339CDB;">True</span>,drop=<span style="color: #339CDB;">False</span>)
    countByCluster.<span style="color: #85DDFF;">columns</span> = [<span style="color: #DB8E73;">'cluster'</span>,<span style="color: #DB8E73;">'clusterCount'</span>]

    <span style="color: #85DDFF;">preds</span> = pd.concat([labelsDF,clusterDF], axis=<span style="color: #B5CEA8; font-weight: bold;">1</span>)
    preds.<span style="color: #85DDFF;">columns</span> = [<span style="color: #DB8E73;">'trueLabel'</span>,<span style="color: #DB8E73;">'cluster'</span>]

    <span style="color: #85DDFF;">countByLabel</span> = pd.DataFrame(data=preds.groupby(<span style="color: #DB8E73;">'trueLabel'</span>).count())

    <span style="color: #85DDFF;">countMostFreq</span> = \
        pd.DataFrame(data=preds.groupby(<span style="color: #DB8E73;">'cluster'</span>).agg( \
                        <span style="color: #339CDB;">lambda</span> x:x.value_counts().iloc[<span style="color: #B5CEA8; font-weight: bold;">0</span>]))
    countMostFreq.reset_index(inplace=<span style="color: #339CDB;">True</span>,drop=<span style="color: #339CDB;">False</span>)
    countMostFreq.<span style="color: #85DDFF;">columns</span> = [<span style="color: #DB8E73;">'cluster'</span>,<span style="color: #DB8E73;">'countMostFrequent'</span>]

    <span style="color: #85DDFF;">accuracyDF</span> = countMostFreq.merge(countByCluster, \
                        left_on=<span style="color: #DB8E73;">"cluster"</span>,right_on=<span style="color: #DB8E73;">"cluster"</span>)
    <span style="color: #85DDFF;">overallAccuracy</span> = accuracyDF.countMostFrequent.<span style="color: #C586C0;">sum</span>()/ \
                        accuracyDF.clusterCount.<span style="color: #C586C0;">sum</span>()

    <span style="color: #85DDFF;">accuracyByLabel</span> = accuracyDF.countMostFrequent/ \
                        accuracyDF.clusterCount

    <span style="color: #339CDB;">return</span> countByCluster, countByLabel, countMostFreq, \
            accuracyDF, overallAccuracy, accuracyByLabel

<span style="color: #339CDB;">from</span> sklearn.cluster <span style="color: #339CDB;">import</span> DBSCAN

<span style="color: #85DDFF;">eps</span> = <span style="color: #B5CEA8; font-weight: bold;">3</span>
<span style="color: #85DDFF;">min_samples</span> = <span style="color: #B5CEA8; font-weight: bold;">5</span>
<span style="color: #85DDFF;">leaf_size</span> = <span style="color: #B5CEA8; font-weight: bold;">30</span>
<span style="color: #85DDFF;">n_jobs</span> = <span style="color: #B5CEA8; font-weight: bold;">4</span>

<span style="color: #85DDFF;">db</span> = DBSCAN(eps=eps, min_samples=min_samples, leaf_size=leaf_size,
            n_jobs=n_jobs)

<span style="color: #85DDFF;">cutoff</span> = <span style="color: #B5CEA8; font-weight: bold;">99</span>
<span style="color: #85DDFF;">X_train_PCA_dbscanClustered</span> = db.fit_predict(X_train_PCA.loc[:,<span style="color: #B5CEA8; font-weight: bold;">0</span>:cutoff])
<span style="color: #85DDFF;">X_train_PCA_dbscanClustered</span> = \
    pd.DataFrame(data=X_train_PCA_dbscanClustered, index=X_train.index, \
                 columns=[<span style="color: #DB8E73;">'cluster'</span>])

countByCluster_dbscan, countByLabel_dbscan, countMostFreq_dbscan, \
    accuracyDF_dbscan, overallAccuracy_dbscan, accuracyByLabel_dbscan \
    = analyzeCluster(X_train_PCA_dbscanClustered, y_train)

overallAccuracy_dbscan
<span style="color: #579C4C;"># </span><span style="color: #579C4C;">Print overall accuracy</span>
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">"Overall accuracy from DBSCAN: "</span>,overallAccuracy_dbscan)

<span style="color: #579C4C;"># </span><span style="color: #579C4C;">Show cluster results</span>
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">"Cluster results for DBSCAN"</span>)
countByCluster_dbscan

</pre>
</div>

<pre class="example">
Overall accuracy from DBSCAN:  0.242
Cluster results for DBSCAN
</pre>
</div>
</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org7e9e2c6" class="outline-2">
<h2 id="org7e9e2c6"><span class="section-number-2">4.</span> 異常偵測</h2>
<div class="outline-text-2" id="text-4">
<p>
在現實的狀況下，詐欺的樣式會隨時間改變，如果只依賴訓練集的label來判斷，時間一久效能就會下降。故需要非監督式學習的詐欺偵測系統來協助。
</p>
</div>
<div id="outline-container-org09f62a6" class="outline-3">
<h3 id="org09f62a6"><span class="section-number-3">4.1.</span> 準備資料</h3>
<div class="outline-text-3" id="text-4-1">
<p>
共有284807筆信用卡交易、其中有492筆詐欺交易(class=1)
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Import libraries</span>
<span style="color: #777778; font-style: italic;">'''Main'''</span>
<span style="color: #339CDB;">import</span> numpy <span style="color: #339CDB;">as</span> np
<span style="color: #339CDB;">import</span> pandas <span style="color: #339CDB;">as</span> pd
<span style="color: #339CDB;">import</span> os, time
<span style="color: #339CDB;">import</span> pickle, gzip

<span style="color: #DB8E73;">'''Data Viz'''</span>
<span style="color: #339CDB;">import</span> matplotlib.pyplot <span style="color: #339CDB;">as</span> plt
<span style="color: #339CDB;">import</span> seaborn <span style="color: #339CDB;">as</span> sns
<span style="color: #85DDFF;">color</span> = sns.color_palette()
<span style="color: #339CDB;">import</span> matplotlib <span style="color: #339CDB;">as</span> mpl

<span style="color: #DB8E73;">'''Data Prep and Model Evaluation'''</span>
<span style="color: #339CDB;">from</span> sklearn <span style="color: #339CDB;">import</span> preprocessing <span style="color: #339CDB;">as</span> pp
<span style="color: #339CDB;">from</span> sklearn.model_selection <span style="color: #339CDB;">import</span> train_test_split
<span style="color: #339CDB;">from</span> sklearn.metrics <span style="color: #339CDB;">import</span> precision_recall_curve, average_precision_score
<span style="color: #339CDB;">from</span> sklearn.metrics <span style="color: #339CDB;">import</span> roc_curve, auc, roc_auc_score

<span style="color: #339CDB;">import</span> numpy <span style="color: #339CDB;">as</span> np
<span style="color: #339CDB;">import</span> pandas <span style="color: #339CDB;">as</span> pd
<span style="color: #85DDFF;">data</span> = pd.read_csv(<span style="color: #DB8E73;">"https://media.githubusercontent.com/media/francis-kang/handson-unsupervised-learning/master/datasets/credit_card_data/credit_card.csv"</span>)
<span style="color: #579C4C;">#</span><span style="color: #579C4C;">data = pd.read_csv("datasets/credit_card.csv")</span>
<span style="color: #579C4C;"># </span><span style="color: #579C4C;">Count total fraud</span>
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">"Number of fraudulent transactions:"</span>, data[<span style="color: #DB8E73;">'Class'</span>].<span style="color: #C586C0;">sum</span>())

<span style="color: #579C4C;"># </span><span style="color: #579C4C;">Suppress warnings</span>
pd.set_option(<span style="color: #DB8E73;">'mode.chained_assignment'</span>, <span style="color: #339CDB;">None</span>)

<span style="color: #579C4C;"># </span><span style="color: #579C4C;">Split to train and test and scale features</span>
<span style="color: #85DDFF;">dataX</span> = data.drop([<span style="color: #DB8E73;">'Class'</span>],axis=<span style="color: #B5CEA8; font-weight: bold;">1</span>)
<span style="color: #85DDFF;">dataY</span> = data.loc[:,<span style="color: #DB8E73;">'Class'</span>].copy()

<span style="color: #85DDFF;">X_train</span>, <span style="color: #85DDFF;">X_test</span>, <span style="color: #85DDFF;">y_train</span>, <span style="color: #85DDFF;">y_test</span> = \
    train_test_split(dataX, dataY, test_size=<span style="color: #B5CEA8; font-weight: bold;">0.33</span>, \
                    random_state=<span style="color: #B5CEA8; font-weight: bold;">2018</span>, stratify=dataY)

<span style="color: #85DDFF;">featuresToScale</span> = X_train.columns
<span style="color: #85DDFF;">sX</span> = pp.StandardScaler(copy=<span style="color: #339CDB;">True</span>)
X_train.<span style="color: #85DDFF;">loc</span>[:,featuresToScale] = sX.fit_transform(X_train.loc[:,featuresToScale])
X_test.<span style="color: #85DDFF;">loc</span>[:,featuresToScale] = sX.transform(X_test.loc[:,featuresToScale])
</pre>
</div>

<pre class="example">
Number of fraudulent transactions: 492
</pre>
</div>
</div>
<div id="outline-container-org7099af4" class="outline-3">
<h3 id="org7099af4"><span class="section-number-3">4.2.</span> 定義異常評分函數</h3>
<div class="outline-text-3" id="text-4-2">
<p>
降維演算法在縮減維度時，會試圖將重建誤差最小化；對於信用卡交易資料來說，那些難以被塑模的交易會產生最大的重建誤差。
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Calculate reconstruction error</span>
<span style="color: #339CDB;">def</span> <span style="color: #D9DAA2;">anomalyScores</span>(originalDF, reducedDF):
    <span style="color: #85DDFF;">loss</span> = np.<span style="color: #C586C0;">sum</span>((np.array(originalDF)-np.array(reducedDF))**<span style="color: #B5CEA8; font-weight: bold;">2</span>, axis=<span style="color: #B5CEA8; font-weight: bold;">1</span>)
    <span style="color: #85DDFF;">loss</span> = pd.Series(data=loss,index=originalDF.index)
    <span style="color: #85DDFF;">loss</span> = (loss-np.<span style="color: #C586C0;">min</span>(loss))/(np.<span style="color: #C586C0;">max</span>(loss)-np.<span style="color: #C586C0;">min</span>(loss))
    <span style="color: #339CDB;">return</span> loss
</pre>
</div>
</div>
</div>
<div id="outline-container-orga4c8a72" class="outline-3">
<h3 id="orga4c8a72"><span class="section-number-3">4.3.</span> 評估指標：畫圖</h3>
<div class="outline-text-3" id="text-4-3">
<p>
使用precision-recall曲線、average precision和auROC做為評估指標。
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Plot results</span>
<span style="color: #339CDB;">def</span> <span style="color: #D9DAA2;">setPlot</span>():
    <span style="color: #339CDB;">import</span> matplotlib.pyplot <span style="color: #339CDB;">as</span> plt
    <span style="color: #339CDB;">from</span> matplotlib <span style="color: #339CDB;">import</span> rcParams
    rcParams.update({<span style="color: #DB8E73;">'figure.autolayout'</span>: <span style="color: #339CDB;">True</span>})
    plt.<span style="color: #85DDFF;">rcParams</span>[<span style="color: #DB8E73;">'font.sans-serif'</span>] = [<span style="color: #DB8E73;">'Arial Unicode MS'</span>]
    plt.<span style="color: #85DDFF;">rcParams</span>[<span style="color: #DB8E73;">'axes.unicode_minus'</span>] = <span style="color: #339CDB;">False</span>

<span style="color: #339CDB;">def</span> <span style="color: #D9DAA2;">plotResults</span>(trueLabels, anomalyScores, returnPreds = <span style="color: #339CDB;">False</span>, imgName=<span style="color: #DB8E73;">''</span>):
    plt.cla()
    setPlot()
    <span style="color: #85DDFF;">preds</span> = pd.concat([trueLabels, anomalyScores], axis=<span style="color: #B5CEA8; font-weight: bold;">1</span>)
    preds.<span style="color: #85DDFF;">columns</span> = [<span style="color: #DB8E73;">'trueLabel'</span>, <span style="color: #DB8E73;">'anomalyScore'</span>]
    <span style="color: #85DDFF;">precision</span>, <span style="color: #85DDFF;">recall</span>, <span style="color: #85DDFF;">thresholds</span> = \
        precision_recall_curve(preds[<span style="color: #DB8E73;">'trueLabel'</span>],preds[<span style="color: #DB8E73;">'anomalyScore'</span>])
    <span style="color: #85DDFF;">average_precision</span> = \
        average_precision_score(preds[<span style="color: #DB8E73;">'trueLabel'</span>],preds[<span style="color: #DB8E73;">'anomalyScore'</span>])

    plt.step(recall, precision, color=<span style="color: #DB8E73;">'k'</span>, alpha=<span style="color: #B5CEA8; font-weight: bold;">0.7</span>, where=<span style="color: #DB8E73;">'post'</span>)
    plt.fill_between(recall, precision, step=<span style="color: #DB8E73;">'post'</span>, alpha=<span style="color: #B5CEA8; font-weight: bold;">0.3</span>, color=<span style="color: #DB8E73;">'k'</span>)

    plt.xlabel(<span style="color: #DB8E73;">'Recall'</span>)
    plt.ylabel(<span style="color: #DB8E73;">'Precision'</span>)
    plt.ylim([<span style="color: #B5CEA8; font-weight: bold;">0.0</span>, <span style="color: #B5CEA8; font-weight: bold;">1.05</span>])
    plt.xlim([<span style="color: #B5CEA8; font-weight: bold;">0.0</span>, <span style="color: #B5CEA8; font-weight: bold;">1.0</span>])

    plt.title(<span style="color: #DB8E73;">'Precision-Recall curve: &#24179;&#22343;&#31934;&#30906;&#29575;:{0:0.2f}'</span>.<span style="color: #C586C0;">format</span>(average_precision))

    plt.savefig(<span style="color: #DB8E73;">'images/'</span>+imgName+<span style="color: #DB8E73;">'-1.png'</span>, dpi=<span style="color: #B5CEA8; font-weight: bold;">300</span>, bbox_inches=<span style="color: #DB8E73;">'tight'</span>)

    <span style="color: #85DDFF;">fpr</span>, <span style="color: #85DDFF;">tpr</span>, <span style="color: #85DDFF;">thresholds</span> = roc_curve(preds[<span style="color: #DB8E73;">'trueLabel'</span>], \
                                     preds[<span style="color: #DB8E73;">'anomalyScore'</span>])
    <span style="color: #85DDFF;">areaUnderROC</span> = auc(fpr, tpr)
    plt.cla()
    setPlot()
    plt.plot(fpr, tpr, color=<span style="color: #DB8E73;">'r'</span>, lw=<span style="color: #B5CEA8; font-weight: bold;">2</span>, label=<span style="color: #DB8E73;">'ROC curve'</span>)
    plt.plot([<span style="color: #B5CEA8; font-weight: bold;">0</span>, <span style="color: #B5CEA8; font-weight: bold;">1</span>], [<span style="color: #B5CEA8; font-weight: bold;">0</span>, <span style="color: #B5CEA8; font-weight: bold;">1</span>], color=<span style="color: #DB8E73;">'k'</span>, lw=<span style="color: #B5CEA8; font-weight: bold;">2</span>, linestyle=<span style="color: #DB8E73;">'--'</span>)
    plt.xlim([<span style="color: #B5CEA8; font-weight: bold;">0.0</span>, <span style="color: #B5CEA8; font-weight: bold;">1.0</span>])
    plt.ylim([<span style="color: #B5CEA8; font-weight: bold;">0.0</span>, <span style="color: #B5CEA8; font-weight: bold;">1.05</span>])
    plt.xlabel(<span style="color: #DB8E73;">'False Positive Rate'</span>)
    plt.ylabel(<span style="color: #DB8E73;">'True Positive Rate'</span>)
    plt.title(<span style="color: #DB8E73;">'Receiver operating characteristic: &#26354;&#32218;&#20197;&#19979;&#38754;&#31309;:{0:0.2f}'</span>.<span style="color: #C586C0;">format</span>(areaUnderROC))
    plt.legend(loc=<span style="color: #DB8E73;">"lower right"</span>)
    plt.savefig(<span style="color: #DB8E73;">'images/'</span>+imgName+<span style="color: #DB8E73;">'-2.png'</span>, dpi=<span style="color: #B5CEA8; font-weight: bold;">300</span>, bbox_inches=<span style="color: #DB8E73;">'tight'</span>)
    <span style="color: #339CDB;">if</span> returnPreds==<span style="color: #339CDB;">True</span>:
        <span style="color: #339CDB;">return</span> preds


<span style="color: #579C4C;"># </span><span style="color: #579C4C;">View scatterplot</span>
<span style="color: #339CDB;">def</span> <span style="color: #D9DAA2;">scatterPlot</span>(xDF, yDF, algoName, imgName=<span style="color: #DB8E73;">''</span>):
    plt.cla()
    setPlot()
    <span style="color: #85DDFF;">tempDF</span> = pd.DataFrame(data=xDF.loc[:,<span style="color: #B5CEA8; font-weight: bold;">0</span>:<span style="color: #B5CEA8; font-weight: bold;">1</span>], index=xDF.index)
    <span style="color: #85DDFF;">tempDF</span> = pd.concat((tempDF,yDF), axis=<span style="color: #B5CEA8; font-weight: bold;">1</span>, join=<span style="color: #DB8E73;">"inner"</span>)
    tempDF.<span style="color: #85DDFF;">columns</span> = [<span style="color: #DB8E73;">"First Vector"</span>, <span style="color: #DB8E73;">"Second Vector"</span>, <span style="color: #DB8E73;">"Label"</span>]
    sns.lmplot(x=<span style="color: #DB8E73;">"First Vector"</span>, y=<span style="color: #DB8E73;">"Second Vector"</span>, hue=<span style="color: #DB8E73;">"Label"</span>, \
               data=tempDF, fit_reg=<span style="color: #339CDB;">False</span>)
    <span style="color: #85DDFF;">ax</span> = plt.gca()
    ax.set_title(<span style="color: #DB8E73;">"&#28436;&#31639;&#27861;:"</span>+algoName)
    plt.savefig(<span style="color: #DB8E73;">'images/'</span>+imgName+<span style="color: #DB8E73;">'.png'</span>, dpi=<span style="color: #B5CEA8; font-weight: bold;">300</span>, bbox_inches=<span style="color: #DB8E73;">'tight'</span>)
</pre>
</div>
</div>
</div>
<div id="outline-container-org086aec8" class="outline-3">
<h3 id="org086aec8"><span class="section-number-3">4.4.</span> PCA異常偵測</h3>
<div class="outline-text-3" id="text-4-4">
<p>
使用PCA模型來重建信用卡交易、計算重交的交易與原始交易的差異，那些PCA重建的較差的交易就是異常(可能為詐欺)。對於PCA來說，保留越多主成分、越有助於PCA學習到原始交易的資料結構，但若保留太多主成分，PCA可能太容易重建原始交易，反而讓所有的重建誤差都變小。
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">30 principal components</span>
<span style="color: #339CDB;">from</span> sklearn.decomposition <span style="color: #339CDB;">import</span> PCA

<span style="color: #85DDFF;">n_components</span> = <span style="color: #B5CEA8; font-weight: bold;">30</span> <span style="color: #579C4C;">#</span><span style="color: #579C4C;">&#20445;&#30041;30o&#22266;&#20027;&#25104;&#20998;</span>
<span style="color: #85DDFF;">whiten</span> = <span style="color: #339CDB;">False</span>
<span style="color: #85DDFF;">random_state</span> = <span style="color: #B5CEA8; font-weight: bold;">2018</span>

<span style="color: #85DDFF;">pca</span> = PCA(n_components=n_components, whiten=whiten, \
          random_state=random_state)

<span style="color: #85DDFF;">X_train_PCA</span> = pca.fit_transform(X_train)
<span style="color: #85DDFF;">X_train_PCA</span> = pd.DataFrame(data=X_train_PCA, index=X_train.index)

<span style="color: #85DDFF;">X_train_PCA_inverse</span> = pca.inverse_transform(X_train_PCA)
<span style="color: #85DDFF;">X_train_PCA_inverse</span> = pd.DataFrame(data=X_train_PCA_inverse, index=X_train.index)

scatterPlot(X_train_PCA, y_train, <span style="color: #DB8E73;">'AD-PCA'</span>, <span style="color: #DB8E73;">'AD-PCA'</span>)
<span style="color: #85DDFF;">anomalyScoresPCA</span> = anomalyScores(X_train, X_train_PCA_inverse)
<span style="color: #85DDFF;">preds</span> = plotResults(y_train, anomalyScoresPCA, <span style="color: #339CDB;">True</span>, <span style="color: #DB8E73;">'AD-PCA'</span>)

</pre>
</div>


<div id="orgc860de0" class="figure">
<p><img src="images/AD-PCA.png" alt="AD-PCA.png" width="500" />
</p>
<p><span class="figure-number">Figure 36: </span>PCA異常偵測/30</p>
</div>


<div id="orgd300330" class="figure">
<p><img src="images/AD-PCA-1.png" alt="AD-PCA-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 37: </span>PCA異常偵測/30</p>
</div>


<div id="orgc2c491e" class="figure">
<p><img src="images/AD-PCA-2.png" alt="AD-PCA-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 38: </span>PCA異常偵測/30</p>
</div>

<p>
平均精確率不到1%，太差，必須不斷實驗找出最佳的PCA成份(<a href="http://bit.ly/2Gd4v7e">http://bit.ly/2Gd4v7e</a>)
</p>
</div>
<div id="outline-container-org0c89551" class="outline-4">
<h4 id="org0c89551"><span class="section-number-4">4.4.1.</span> 最後找出27個</h4>
<div class="outline-text-4" id="text-4-4-1">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">27 principal components</span>
<span style="color: #339CDB;">from</span> sklearn.decomposition <span style="color: #339CDB;">import</span> PCA

<span style="color: #85DDFF;">n_components</span> = <span style="color: #B5CEA8; font-weight: bold;">27</span>
<span style="color: #85DDFF;">whiten</span> = <span style="color: #339CDB;">False</span>
<span style="color: #85DDFF;">random_state</span> = <span style="color: #B5CEA8; font-weight: bold;">2018</span>

<span style="color: #85DDFF;">pca</span> = PCA(n_components=n_components, whiten=whiten, random_state=random_state)

<span style="color: #85DDFF;">X_train_PCA</span> = pca.fit_transform(X_train)
<span style="color: #85DDFF;">X_train_PCA</span> = pd.DataFrame(data=X_train_PCA, index=X_train.index)

<span style="color: #85DDFF;">X_train_PCA_inverse</span> = pca.inverse_transform(X_train_PCA)
<span style="color: #85DDFF;">X_train_PCA_inverse</span> = pd.DataFrame(data=X_train_PCA_inverse, index=X_train.index)

scatterPlot(X_train_PCA, y_train, <span style="color: #DB8E73;">'AD-PCA'</span>, <span style="color: #DB8E73;">'AD-PCA1'</span>)
<span style="color: #579C4C;"># </span><span style="color: #579C4C;">View plot</span>
<span style="color: #85DDFF;">anomalyScoresPCA</span> = anomalyScores(X_train, X_train_PCA_inverse)
<span style="color: #85DDFF;">preds</span> = plotResults(y_train, anomalyScoresPCA, <span style="color: #339CDB;">True</span>, <span style="color: #DB8E73;">'AD-PCA1'</span>)
</pre>
</div>

<div id="orgbe395bc" class="figure">
<p><img src="images/AD-PCA1.png" alt="AD-PCA1.png" width="500" />
</p>
<p><span class="figure-number">Figure 39: </span>PCA異常偵測/27</p>
</div>


<div id="org8315c76" class="figure">
<p><img src="images/AD-PCA1-1.png" alt="AD-PCA1-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 40: </span>PCA異常偵測/27</p>
</div>

<div id="orgf9894df" class="figure">
<p><img src="images/AD-PCA1-2.png" alt="AD-PCA1-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 41: </span>PCA異常偵測/27</p>
</div>
</div>
</div>
<div id="outline-container-org335105f" class="outline-4">
<h4 id="org335105f"><span class="section-number-4">4.4.2.</span> 分析結果</h4>
<div class="outline-text-4" id="text-4-4-2">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Analyze results</span>
preds.sort_values(by=<span style="color: #DB8E73;">"anomalyScore"</span>,ascending=<span style="color: #339CDB;">False</span>,inplace=<span style="color: #339CDB;">True</span>)
<span style="color: #85DDFF;">cutoff</span> = <span style="color: #B5CEA8; font-weight: bold;">350</span>
<span style="color: #85DDFF;">predsTop</span> = preds[:cutoff]
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">"Precision: "</span>,np.<span style="color: #C586C0;">round</span>(predsTop. \
            anomalyScore[predsTop.trueLabel==<span style="color: #B5CEA8; font-weight: bold;">1</span>].count()/cutoff,<span style="color: #B5CEA8; font-weight: bold;">2</span>))
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">"Recall: "</span>,np.<span style="color: #C586C0;">round</span>(predsTop. \
            anomalyScore[predsTop.trueLabel==<span style="color: #B5CEA8; font-weight: bold;">1</span>].count()/y_train.<span style="color: #C586C0;">sum</span>(),<span style="color: #B5CEA8; font-weight: bold;">2</span>))
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">"Fraud Caught out of 330 Cases:"</span>, predsTop.trueLabel.<span style="color: #C586C0;">sum</span>())
</pre>
</div>

<pre class="example">
Precision:  0.75
Recall:  0.79
Fraud Caught out of 330 Cases: 261
</pre>
</div>
</div>
</div>
<div id="outline-container-orgb127d26" class="outline-3">
<h3 id="orgb127d26"><span class="section-number-3">4.5.</span> Sparse PCA異常偵測</h3>
<div class="outline-text-3" id="text-4-5">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Sparse PCA</span>
<span style="color: #339CDB;">from</span> sklearn.decomposition <span style="color: #339CDB;">import</span> SparsePCA

<span style="color: #85DDFF;">n_components</span> = <span style="color: #B5CEA8; font-weight: bold;">27</span>
<span style="color: #85DDFF;">alpha</span> = <span style="color: #B5CEA8; font-weight: bold;">0.0001</span>
<span style="color: #85DDFF;">random_state</span> = <span style="color: #B5CEA8; font-weight: bold;">2018</span>
<span style="color: #85DDFF;">n_jobs</span> = -<span style="color: #B5CEA8; font-weight: bold;">1</span>

<span style="color: #85DDFF;">sparsePCA</span> = SparsePCA(n_components=n_components, \
                alpha=alpha, random_state=random_state, n_jobs=n_jobs)

sparsePCA.fit(X_train.loc[:,:])
<span style="color: #85DDFF;">X_train_sparsePCA</span> = sparsePCA.transform(X_train)
<span style="color: #85DDFF;">X_train_sparsePCA</span> = pd.DataFrame(data=X_train_sparsePCA, index=X_train.index)

scatterPlot(X_train_sparsePCA, y_train, <span style="color: #DB8E73;">"Sparse PCA"</span>, <span style="color: #DB8E73;">"AD-SPCA"</span>)
<span style="color: #579C4C;"># </span><span style="color: #579C4C;">View plot</span>
<span style="color: #85DDFF;">X_train_sparsePCA_inverse</span> = np.array(X_train_sparsePCA). \
    dot(sparsePCA.components_) + np.array(X_train.mean(axis=<span style="color: #B5CEA8; font-weight: bold;">0</span>))
<span style="color: #85DDFF;">X_train_sparsePCA_inverse</span> = \
    pd.DataFrame(data=X_train_sparsePCA_inverse, index=X_train.index)

<span style="color: #85DDFF;">anomalyScoresSparsePCA</span> = anomalyScores(X_train, X_train_sparsePCA_inverse)
<span style="color: #85DDFF;">preds</span> = plotResults(y_train, anomalyScoresSparsePCA, <span style="color: #339CDB;">True</span>, <span style="color: #DB8E73;">'AD-SPCA'</span>)
</pre>
</div>


<div id="org1eb37c4" class="figure">
<p><img src="images/AD-SPCA.png" alt="AD-SPCA.png" width="500" />
</p>
<p><span class="figure-number">Figure 42: </span>Sparse PCA異常偵測/27</p>
</div>


<div id="org1dc245e" class="figure">
<p><img src="images/AD-SPCA-1.png" alt="AD-SPCA-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 43: </span>Sparse PCA異常偵測/27</p>
</div>

<div id="orgc1688f3" class="figure">
<p><img src="images/AD-SPCA-2.png" alt="AD-SPCA-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 44: </span>Sparse PCA異常偵測/27</p>
</div>
</div>
<div id="outline-container-orga3b0d15" class="outline-4">
<h4 id="orga3b0d15"><span class="section-number-4">4.5.1.</span> 分析結果</h4>
<div class="outline-text-4" id="text-4-5-1">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Analyze results</span>
preds.sort_values(by=<span style="color: #DB8E73;">"anomalyScore"</span>,ascending=<span style="color: #339CDB;">False</span>,inplace=<span style="color: #339CDB;">True</span>)
<span style="color: #85DDFF;">cutoff</span> = <span style="color: #B5CEA8; font-weight: bold;">350</span>
<span style="color: #85DDFF;">predsTop</span> = preds[:cutoff]
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">"Precision: "</span>,np.<span style="color: #C586C0;">round</span>(predsTop. \
            anomalyScore[predsTop.trueLabel==<span style="color: #B5CEA8; font-weight: bold;">1</span>].count()/cutoff,<span style="color: #B5CEA8; font-weight: bold;">2</span>))
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">"Recall: "</span>,np.<span style="color: #C586C0;">round</span>(predsTop. \
            anomalyScore[predsTop.trueLabel==<span style="color: #B5CEA8; font-weight: bold;">1</span>].count()/y_train.<span style="color: #C586C0;">sum</span>(),<span style="color: #B5CEA8; font-weight: bold;">2</span>))
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">"Fraud Caught out of 330 Cases:"</span>, predsTop.trueLabel.<span style="color: #C586C0;">sum</span>())
</pre>
</div>

<pre class="example">
Precision:  0.75
Recall:  0.79
Fraud Caught out of 330 Cases: 261
</pre>
</div>
</div>
</div>
<div id="outline-container-orgb7235d5" class="outline-3">
<h3 id="orgb7235d5"><span class="section-number-3">4.6.</span> Kernel PCA異常偵測</h3>
<div class="outline-text-3" id="text-4-6">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Kernel PCA</span>
<span style="color: #339CDB;">from</span> sklearn.decomposition <span style="color: #339CDB;">import</span> KernelPCA

<span style="color: #85DDFF;">n_components</span> = <span style="color: #B5CEA8; font-weight: bold;">27</span>
<span style="color: #85DDFF;">kernel</span> = <span style="color: #DB8E73;">'rbf'</span>
<span style="color: #85DDFF;">gamma</span> = <span style="color: #339CDB;">None</span>
<span style="color: #85DDFF;">fit_inverse_transform</span> = <span style="color: #339CDB;">True</span>
<span style="color: #85DDFF;">random_state</span> = <span style="color: #B5CEA8; font-weight: bold;">2018</span>
<span style="color: #85DDFF;">n_jobs</span> = <span style="color: #B5CEA8; font-weight: bold;">1</span>

<span style="color: #85DDFF;">kernelPCA</span> = KernelPCA(n_components=n_components, kernel=kernel, \
                gamma=gamma, fit_inverse_transform= \
                fit_inverse_transform, n_jobs=n_jobs, \
                random_state=random_state)

kernelPCA.fit(X_train.iloc[:<span style="color: #B5CEA8; font-weight: bold;">2000</span>])
<span style="color: #85DDFF;">X_train_kernelPCA</span> = kernelPCA.transform(X_train)
<span style="color: #85DDFF;">X_train_kernelPCA</span> = pd.DataFrame(data=X_train_kernelPCA, \
                                 index=X_train.index)

<span style="color: #85DDFF;">X_train_kernelPCA_inverse</span> = kernelPCA.inverse_transform(X_train_kernelPCA)
<span style="color: #85DDFF;">X_train_kernelPCA_inverse</span> = pd.DataFrame(data=X_train_kernelPCA_inverse, \
                                         index=X_train.index)

scatterPlot(X_train_kernelPCA, y_train, <span style="color: #DB8E73;">"Kernel PCA"</span>, <span style="color: #DB8E73;">"AD-KPCA"</span>)
<span style="color: #579C4C;"># </span><span style="color: #579C4C;">View plot</span>
<span style="color: #579C4C;"># </span><span style="color: #579C4C;">View plot</span>
<span style="color: #85DDFF;">anomalyScoresKernelPCA</span> = anomalyScores(X_train, X_train_kernelPCA_inverse)
<span style="color: #85DDFF;">preds</span> = plotResults(y_train, anomalyScoresKernelPCA, <span style="color: #339CDB;">True</span>, <span style="color: #DB8E73;">'AD-KPCA'</span>)
</pre>
</div>


<div id="org479811a" class="figure">
<p><img src="images/AD-KPCA.png" alt="AD-KPCA.png" width="500" />
</p>
<p><span class="figure-number">Figure 45: </span>Kernel PCA異常偵測/27</p>
</div>


<div id="org921ebe5" class="figure">
<p><img src="images/AD-KPCA-1.png" alt="AD-KPCA-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 46: </span>Kernel PCA異常偵測/27</p>
</div>

<div id="orgc6facb4" class="figure">
<p><img src="images/AD-KPCA-2.png" alt="AD-KPCA-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 47: </span>Kernel PCA異常偵測/27</p>
</div>
</div>
<div id="outline-container-orgcc43705" class="outline-4">
<h4 id="orgcc43705"><span class="section-number-4">4.6.1.</span> 分析結果</h4>
<div class="outline-text-4" id="text-4-6-1">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Analyze results</span>
preds.sort_values(by=<span style="color: #DB8E73;">"anomalyScore"</span>,ascending=<span style="color: #339CDB;">False</span>,inplace=<span style="color: #339CDB;">True</span>)
<span style="color: #85DDFF;">cutoff</span> = <span style="color: #B5CEA8; font-weight: bold;">350</span>
<span style="color: #85DDFF;">predsTop</span> = preds[:cutoff]
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">"Precision: "</span>,np.<span style="color: #C586C0;">round</span>(predsTop. \
            anomalyScore[predsTop.trueLabel==<span style="color: #B5CEA8; font-weight: bold;">1</span>].count()/cutoff,<span style="color: #B5CEA8; font-weight: bold;">2</span>))
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">"Recall: "</span>,np.<span style="color: #C586C0;">round</span>(predsTop. \
            anomalyScore[predsTop.trueLabel==<span style="color: #B5CEA8; font-weight: bold;">1</span>].count()/y_train.<span style="color: #C586C0;">sum</span>(),<span style="color: #B5CEA8; font-weight: bold;">2</span>))
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">"Fraud Caught out of 330 Cases:"</span>, predsTop.trueLabel.<span style="color: #C586C0;">sum</span>())
<span style="color: #579C4C;"># </span><span style="color: #579C4C;">Write dimensions to CSV</span>
X_train_kernelPCA.loc[sample_indices,:].to_csv(<span style="color: #DB8E73;">'kernel_pca_data.tsv'</span>, sep = <span style="color: #DB8E73;">'</span><span style="color: #339CDB;">\t</span><span style="color: #DB8E73;">'</span>, index=<span style="color: #339CDB;">False</span>, header=<span style="color: #339CDB;">False</span>)
</pre>
</div>

<pre class="example">
Precision:  0.22
Recall:  0.23
Fraud Caught out of 330 Cases: 77
</pre>

<p>
結果不如普通的PCA與sparse PCA
</p>
</div>
</div>
</div>
<div id="outline-container-org17eeae6" class="outline-3">
<h3 id="org17eeae6"><span class="section-number-3">4.7.</span> 稀疏隨機投影異常偵測</h3>
<div class="outline-text-3" id="text-4-7">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Sparse Random Projection</span>
<span style="color: #339CDB;">from</span> sklearn.random_projection <span style="color: #339CDB;">import</span> SparseRandomProjection

<span style="color: #85DDFF;">n_components</span> = <span style="color: #B5CEA8; font-weight: bold;">27</span>
<span style="color: #85DDFF;">density</span> = <span style="color: #DB8E73;">'auto'</span>
<span style="color: #85DDFF;">eps</span> = .<span style="color: #B5CEA8; font-weight: bold;">01</span>
<span style="color: #85DDFF;">dense_output</span> = <span style="color: #339CDB;">True</span>
<span style="color: #85DDFF;">random_state</span> = <span style="color: #B5CEA8; font-weight: bold;">2018</span>

<span style="color: #85DDFF;">SRP</span> = SparseRandomProjection(n_components=n_components, \
        density=density, eps=eps, dense_output=dense_output, \
                                random_state=random_state)

<span style="color: #85DDFF;">X_train_SRP</span> = SRP.fit_transform(X_train)
<span style="color: #85DDFF;">X_train_SRP</span> = pd.DataFrame(data=X_train_SRP, index=X_train.index)

scatterPlot(X_train_SRP, y_train, <span style="color: #DB8E73;">"Sparse Random Projection"</span>, <span style="color: #DB8E73;">"AD-SRP"</span>)
<span style="color: #579C4C;"># </span><span style="color: #579C4C;">View plot</span>
<span style="color: #85DDFF;">X_train_SRP_inverse</span> = np.array(X_train_SRP).dot(SRP.components_.todense())
<span style="color: #85DDFF;">X_train_SRP_inverse</span> = pd.DataFrame(data=X_train_SRP_inverse, index=X_train.index)

<span style="color: #85DDFF;">anomalyScoresSRP</span> = anomalyScores(X_train, X_train_SRP_inverse)
<span style="color: #85DDFF;">preds</span> = plotResults(y_train, anomalyScoresSRP, <span style="color: #339CDB;">True</span>, <span style="color: #DB8E73;">"AD-SRP"</span>)
</pre>
</div>


<div id="orgaca1117" class="figure">
<p><img src="images/AD-SRP.png" alt="AD-SRP.png" width="500" />
</p>
<p><span class="figure-number">Figure 48: </span>稀疏隨機投影異常偵測</p>
</div>

<div id="org2083480" class="figure">
<p><img src="images/AD-SRP-1.png" alt="AD-SRP-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 49: </span>稀疏隨機投影異常偵測</p>
</div>

<div id="org7baf76c" class="figure">
<p><img src="images/AD-SRP-2.png" alt="AD-SRP-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 50: </span>稀疏隨機投影異常偵測</p>
</div>
</div>
<div id="outline-container-org60878b7" class="outline-4">
<h4 id="org60878b7"><span class="section-number-4">4.7.1.</span> 分析結果</h4>
<div class="outline-text-4" id="text-4-7-1">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Analyze results</span>
preds.sort_values(by=<span style="color: #DB8E73;">"anomalyScore"</span>,ascending=<span style="color: #339CDB;">False</span>,inplace=<span style="color: #339CDB;">True</span>)
<span style="color: #85DDFF;">cutoff</span> = <span style="color: #B5CEA8; font-weight: bold;">350</span>
<span style="color: #85DDFF;">predsTop</span> = preds[:cutoff]
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">"Precision: "</span>,np.<span style="color: #C586C0;">round</span>(predsTop. \
            anomalyScore[predsTop.trueLabel==<span style="color: #B5CEA8; font-weight: bold;">1</span>].count()/cutoff,<span style="color: #B5CEA8; font-weight: bold;">2</span>))
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">"Recall: "</span>,np.<span style="color: #C586C0;">round</span>(predsTop. \
            anomalyScore[predsTop.trueLabel==<span style="color: #B5CEA8; font-weight: bold;">1</span>].count()/y_train.<span style="color: #C586C0;">sum</span>(),<span style="color: #B5CEA8; font-weight: bold;">2</span>))
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">"Fraud Caught out of 330 Cases:"</span>, predsTop.trueLabel.<span style="color: #C586C0;">sum</span>())
<span style="color: #579C4C;"># </span><span style="color: #579C4C;">Write dimensions to CSV</span>
X_train_SRP.loc[sample_indices,:].to_csv(<span style="color: #DB8E73;">'sparse_random_projection_data.tsv'</span>, sep = <span style="color: #DB8E73;">'</span><span style="color: #339CDB;">\t</span><span style="color: #DB8E73;">'</span>, index=<span style="color: #339CDB;">False</span>, header=<span style="color: #339CDB;">False</span>)
</pre>
</div>

<pre class="example">
Precision:  0.21
Recall:  0.22
Fraud Caught out of 330 Cases: 73
</pre>
</div>
</div>
</div>
<div id="outline-container-org6a8b3aa" class="outline-3">
<h3 id="org6a8b3aa"><span class="section-number-3">4.8.</span> 字典學習異常偵測</h3>
<div class="outline-text-3" id="text-4-8">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Mini-batch dictionary learning</span>
<span style="color: #339CDB;">from</span> sklearn.decomposition <span style="color: #339CDB;">import</span> MiniBatchDictionaryLearning

<span style="color: #85DDFF;">n_components</span> = <span style="color: #B5CEA8; font-weight: bold;">28</span>
<span style="color: #85DDFF;">alpha</span> = <span style="color: #B5CEA8; font-weight: bold;">1</span>
<span style="color: #85DDFF;">batch_size</span> = <span style="color: #B5CEA8; font-weight: bold;">200</span>
<span style="color: #85DDFF;">max_iter</span> = <span style="color: #B5CEA8; font-weight: bold;">200</span>
<span style="color: #85DDFF;">random_state</span> = <span style="color: #B5CEA8; font-weight: bold;">2018</span>

<span style="color: #85DDFF;">miniBatchDictLearning</span> = MiniBatchDictionaryLearning( \
    n_components=n_components, alpha=alpha, batch_size=batch_size, \
    max_iter=max_iter, random_state=random_state)

miniBatchDictLearning.fit(X_train)
<span style="color: #85DDFF;">X_train_miniBatchDictLearning</span> = \
    miniBatchDictLearning.fit_transform(X_train)
<span style="color: #85DDFF;">X_train_miniBatchDictLearning</span> = \
    pd.DataFrame(data=X_train_miniBatchDictLearning, index=X_train.index)

scatterPlot(X_train_miniBatchDictLearning, y_train, \
            <span style="color: #DB8E73;">"Mini-batch Dictionary Learning"</span>, <span style="color: #DB8E73;">"AD-MBDL"</span>)
<span style="color: #579C4C;"># </span><span style="color: #579C4C;">View plot</span>
<span style="color: #85DDFF;">X_train_miniBatchDictLearning_inverse</span> = \
    np.array(X_train_miniBatchDictLearning). \
    dot(miniBatchDictLearning.components_)

<span style="color: #85DDFF;">X_train_miniBatchDictLearning_inverse</span> = \
    pd.DataFrame(data=X_train_miniBatchDictLearning_inverse, \
                 index=X_train.index)

<span style="color: #85DDFF;">anomalyScoresMiniBatchDictLearning</span> = anomalyScores(X_train, \
    X_train_miniBatchDictLearning_inverse)
<span style="color: #85DDFF;">preds</span> = plotResults(y_train, anomalyScoresMiniBatchDictLearning, <span style="color: #339CDB;">True</span>, <span style="color: #DB8E73;">"AD-MBDL"</span>)
</pre>
</div>


<div id="orgef1818c" class="figure">
<p><img src="images/AD-MBDL.png" alt="AD-MBDL.png" width="500" />
</p>
<p><span class="figure-number">Figure 51: </span>字典學習異常偵測</p>
</div>

<div id="org9b08899" class="figure">
<p><img src="images/AD-MBDL-1.png" alt="AD-MBDL-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 52: </span>字典學習異常偵測</p>
</div>

<div id="orgdd60d4d" class="figure">
<p><img src="images/AD-MBDL-2.png" alt="AD-MBDL-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 53: </span>字典學習異常偵測</p>
</div>
</div>
<div id="outline-container-org9815bae" class="outline-4">
<h4 id="org9815bae"><span class="section-number-4">4.8.1.</span> 分析結果</h4>
<div class="outline-text-4" id="text-4-8-1">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Analyze results</span>
preds.sort_values(by=<span style="color: #DB8E73;">"anomalyScore"</span>,ascending=<span style="color: #339CDB;">False</span>,inplace=<span style="color: #339CDB;">True</span>)
<span style="color: #85DDFF;">cutoff</span> = <span style="color: #B5CEA8; font-weight: bold;">350</span>
<span style="color: #85DDFF;">predsTop</span> = preds[:cutoff]
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">"Precision: "</span>,np.<span style="color: #C586C0;">round</span>(predsTop. \
            anomalyScore[predsTop.trueLabel==<span style="color: #B5CEA8; font-weight: bold;">1</span>].count()/cutoff,<span style="color: #B5CEA8; font-weight: bold;">2</span>))
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">"Recall: "</span>,np.<span style="color: #C586C0;">round</span>(predsTop. \
            anomalyScore[predsTop.trueLabel==<span style="color: #B5CEA8; font-weight: bold;">1</span>].count()/y_train.<span style="color: #C586C0;">sum</span>(),<span style="color: #B5CEA8; font-weight: bold;">2</span>))
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">"Fraud Caught out of 330 Cases:"</span>, predsTop.trueLabel.<span style="color: #C586C0;">sum</span>())
<span style="color: #579C4C;"># </span><span style="color: #579C4C;">Write dimensions to CSV</span>
X_train_miniBatchDictLearning.loc[sample_indices,:].to_csv(<span style="color: #DB8E73;">'dictionary_learning_data.tsv'</span>, sep = <span style="color: #DB8E73;">'</span><span style="color: #339CDB;">\t</span><span style="color: #DB8E73;">'</span>, index=<span style="color: #339CDB;">False</span>, header=<span style="color: #339CDB;">False</span>)
</pre>
</div>

<pre class="example">
Precision:  0.43
Recall:  0.46
Fraud Caught out of 330 Cases: 151
</pre>
</div>
</div>
</div>
<div id="outline-container-org4a3f5de" class="outline-3">
<h3 id="org4a3f5de"><span class="section-number-3">4.9.</span> ICA異常偵測</h3>
<div class="outline-text-3" id="text-4-9">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Independent Component Analysis</span>
<span style="color: #339CDB;">from</span> sklearn.decomposition <span style="color: #339CDB;">import</span> FastICA

<span style="color: #85DDFF;">n_components</span> = <span style="color: #B5CEA8; font-weight: bold;">27</span>
<span style="color: #85DDFF;">algorithm</span> = <span style="color: #DB8E73;">'parallel'</span>
<span style="color: #85DDFF;">whiten</span> = <span style="color: #DB8E73;">'arbitrary-variance'</span>
<span style="color: #85DDFF;">max_iter</span> = <span style="color: #B5CEA8; font-weight: bold;">200</span>
<span style="color: #85DDFF;">random_state</span> = <span style="color: #B5CEA8; font-weight: bold;">2018</span>

<span style="color: #85DDFF;">fastICA</span> = FastICA(n_components=n_components, \
    algorithm=algorithm, whiten=whiten, max_iter=max_iter, \
    random_state=random_state)

<span style="color: #85DDFF;">X_train_fastICA</span> = fastICA.fit_transform(X_train)
<span style="color: #85DDFF;">X_train_fastICA</span> = pd.DataFrame(data=X_train_fastICA, index=X_train.index)

<span style="color: #85DDFF;">X_train_fastICA_inverse</span> = fastICA.inverse_transform(X_train_fastICA)
<span style="color: #85DDFF;">X_train_fastICA_inverse</span> = pd.DataFrame(data=X_train_fastICA_inverse, \
                                       index=X_train.index)

scatterPlot(X_train_fastICA, y_train, <span style="color: #DB8E73;">"Independent Component Analysis"</span>, <span style="color: #DB8E73;">"AD-ICA"</span>)
<span style="color: #579C4C;"># </span><span style="color: #579C4C;">View plot</span>
<span style="color: #85DDFF;">anomalyScoresFastICA</span> = anomalyScores(X_train, X_train_fastICA_inverse)
<span style="color: #85DDFF;">preds</span> = plotResults(y_train, anomalyScoresFastICA, <span style="color: #339CDB;">True</span>, <span style="color: #DB8E73;">"AD-ICA"</span>)
</pre>
</div>


<div id="orgd4eb301" class="figure">
<p><img src="images/AD-ICA.png" alt="AD-ICA.png" width="500" />
</p>
<p><span class="figure-number">Figure 54: </span>ICA異常偵測</p>
</div>

<div id="org7a53d9c" class="figure">
<p><img src="images/AD-ICA-1.png" alt="AD-ICA-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 55: </span>ICA異常偵測</p>
</div>

<div id="org216e130" class="figure">
<p><img src="images/AD-ICA-2.png" alt="AD-ICA-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 56: </span>ICA異常偵測</p>
</div>
</div>
<div id="outline-container-org3ddbca5" class="outline-4">
<h4 id="org3ddbca5"><span class="section-number-4">4.9.1.</span> 分析結果</h4>
<div class="outline-text-4" id="text-4-9-1">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Analyze results</span>
preds.sort_values(by=<span style="color: #DB8E73;">"anomalyScore"</span>,ascending=<span style="color: #339CDB;">False</span>,inplace=<span style="color: #339CDB;">True</span>)
<span style="color: #85DDFF;">cutoff</span> = <span style="color: #B5CEA8; font-weight: bold;">350</span>
<span style="color: #85DDFF;">predsTop</span> = preds[:cutoff]
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">"Precision: "</span>,np.<span style="color: #C586C0;">round</span>(predsTop. \
            anomalyScore[predsTop.trueLabel==<span style="color: #B5CEA8; font-weight: bold;">1</span>].count()/cutoff,<span style="color: #B5CEA8; font-weight: bold;">2</span>))
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">"Recall: "</span>,np.<span style="color: #C586C0;">round</span>(predsTop. \
            anomalyScore[predsTop.trueLabel==<span style="color: #B5CEA8; font-weight: bold;">1</span>].count()/y_train.<span style="color: #C586C0;">sum</span>(),<span style="color: #B5CEA8; font-weight: bold;">2</span>))
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">"Fraud Caught out of 330 Cases:"</span>, predsTop.trueLabel.<span style="color: #C586C0;">sum</span>())
<span style="color: #579C4C;"># </span><span style="color: #579C4C;">Write dimensions to CSV</span>
<span style="color: #579C4C;"># </span><span style="color: #579C4C;">Write dimensions to CSV</span>
X_train_fastICA.loc[sample_indices,:].to_csv(<span style="color: #DB8E73;">'independent_component_analysis_data.tsv'</span>, sep = <span style="color: #DB8E73;">'</span><span style="color: #339CDB;">\t</span><span style="color: #DB8E73;">'</span>, index=<span style="color: #339CDB;">False</span>, header=<span style="color: #339CDB;">False</span>)
</pre>
</div>

<pre class="example">
Precision:  0.75
Recall:  0.79
Fraud Caught out of 330 Cases: 261
</pre>
</div>
</div>
</div>
<div id="outline-container-org72022a1" class="outline-3">
<h3 id="org72022a1"><span class="section-number-3">4.10.</span> 結論</h3>
<div class="outline-text-3" id="text-4-10">
<p>
普通PCA與ICA能捕捉到超過80%的已知詐欺，並有80%的精確率，較之監督式學習能捕捉到90%，已十分難得。
</p>
</div>
</div>
</div>
<div id="outline-container-org8bf0165" class="outline-2">
<h2 id="org8bf0165"><span class="section-number-2">5.</span> 降維</h2>
<div class="outline-text-2" id="text-5">
<ul class="org-ul">
<li>本例以Colab為執行平台，透過資料的圖形化分佈觀察不同降維的效果。</li>
<li>於Colab執行時可以先將例中的savefig()註解掉</li>
</ul>
</div>
<div id="outline-container-org3eb15ca" class="outline-3">
<h3 id="org3eb15ca"><span class="section-number-3">5.1.</span> 讀入資料</h3>
<div class="outline-text-3" id="text-5-1">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #777778; font-style: italic;">'''Main'''</span>
<span style="color: #339CDB;">import</span> numpy <span style="color: #339CDB;">as</span> np
<span style="color: #339CDB;">import</span> pandas <span style="color: #339CDB;">as</span> pd
<span style="color: #339CDB;">import</span> os, time, pickle, gzip
<span style="color: #339CDB;">import</span> datetime

<span style="color: #DB8E73;">'''Data Prep'''</span>
<span style="color: #339CDB;">from</span> sklearn <span style="color: #339CDB;">import</span> preprocessing <span style="color: #339CDB;">as</span> pp

<span style="color: #DB8E73;">'''Data Viz'''</span>
<span style="color: #339CDB;">import</span> matplotlib.pyplot <span style="color: #339CDB;">as</span> plt
<span style="color: #339CDB;">import</span> matplotlib <span style="color: #339CDB;">as</span> mpl
<span style="color: #339CDB;">import</span> seaborn <span style="color: #339CDB;">as</span> sns
<span style="color: #85DDFF;">color</span> = sns.color_palette()
<span style="color: #579C4C;"># </span><span style="color: #579C4C;">Load the datasets</span>
<span style="color: #85DDFF;">current_path</span> = os.getcwd()
<span style="color: #C586C0;">file</span> = os.path.sep.join([<span style="color: #DB8E73;">''</span>, <span style="color: #DB8E73;">'datasets'</span>, <span style="color: #DB8E73;">'mnist.pkl.gz'</span>])

<span style="color: #85DDFF;">f</span> = gzip.<span style="color: #C586C0;">open</span>(current_path+<span style="color: #C586C0;">file</span>, <span style="color: #DB8E73;">'rb'</span>)
<span style="color: #85DDFF;">train_set</span>, <span style="color: #85DDFF;">validation_set</span>, <span style="color: #85DDFF;">test_set</span> = pickle.load(f, encoding=<span style="color: #DB8E73;">'latin1'</span>)
f.close()

<span style="color: #85DDFF;">X_train</span>, <span style="color: #85DDFF;">y_train</span> = train_set[<span style="color: #B5CEA8; font-weight: bold;">0</span>], train_set[<span style="color: #B5CEA8; font-weight: bold;">1</span>]
<span style="color: #85DDFF;">X_validation</span>, <span style="color: #85DDFF;">y_validation</span> = validation_set[<span style="color: #B5CEA8; font-weight: bold;">0</span>], validation_set[<span style="color: #B5CEA8; font-weight: bold;">1</span>]
<span style="color: #85DDFF;">X_test</span>, <span style="color: #85DDFF;">y_test</span> = test_set[<span style="color: #B5CEA8; font-weight: bold;">0</span>], test_set[<span style="color: #B5CEA8; font-weight: bold;">1</span>]

<span style="color: #579C4C;"># </span><span style="color: #579C4C;">Create Pandas DataFrames from the datasets</span>
<span style="color: #85DDFF;">train_index</span> = <span style="color: #C586C0;">range</span>(<span style="color: #B5CEA8; font-weight: bold;">0</span>,<span style="color: #C586C0;">len</span>(X_train))
<span style="color: #85DDFF;">validation_index</span> = <span style="color: #C586C0;">range</span>(<span style="color: #C586C0;">len</span>(X_train), <span style="color: #C586C0;">len</span>(X_train)+<span style="color: #C586C0;">len</span>(X_validation))
<span style="color: #85DDFF;">test_index</span> = <span style="color: #C586C0;">range</span>(<span style="color: #C586C0;">len</span>(X_train)+<span style="color: #C586C0;">len</span>(X_validation), \
                   <span style="color: #C586C0;">len</span>(X_train)+<span style="color: #C586C0;">len</span>(X_validation)+<span style="color: #C586C0;">len</span>(X_test))

<span style="color: #85DDFF;">X_train</span> = pd.DataFrame(data=X_train,index=train_index)
<span style="color: #85DDFF;">y_train</span> = pd.Series(data=y_train,index=train_index)

<span style="color: #85DDFF;">X_validation</span> = pd.DataFrame(data=X_validation,index=validation_index)
<span style="color: #85DDFF;">y_validation</span> = pd.Series(data=y_validation,index=validation_index)

<span style="color: #85DDFF;">X_test</span> = pd.DataFrame(data=X_test,index=test_index)
<span style="color: #85DDFF;">y_test</span> = pd.Series(data=y_test,index=test_index)

<span style="color: #339CDB;">def</span> <span style="color: #D9DAA2;">one_hot</span>(series):
    <span style="color: #85DDFF;">label_binarizer</span> = pp.LabelBinarizer()
    label_binarizer.fit(<span style="color: #C586C0;">range</span>(<span style="color: #C586C0;">max</span>(series)+<span style="color: #B5CEA8; font-weight: bold;">1</span>))
    <span style="color: #339CDB;">return</span> label_binarizer.transform(series)
<span style="color: #579C4C;"># </span><span style="color: #579C4C;">Define reversal of one-hot encoder function</span>
<span style="color: #339CDB;">def</span> <span style="color: #D9DAA2;">reverse_one_hot</span>(originalSeries, newSeries):
    <span style="color: #85DDFF;">label_binarizer</span> = pp.LabelBinarizer()
    label_binarizer.fit(<span style="color: #C586C0;">range</span>(<span style="color: #C586C0;">max</span>(originalSeries)+<span style="color: #B5CEA8; font-weight: bold;">1</span>))
    <span style="color: #339CDB;">return</span> label_binarizer.inverse_transform(newSeries)
<span style="color: #579C4C;"># </span><span style="color: #579C4C;">Create one-hot vectors for the labels</span>
<span style="color: #85DDFF;">y_train_oneHot</span> = one_hot(y_train)
<span style="color: #85DDFF;">y_validation_oneHot</span> = one_hot(y_validation)
<span style="color: #85DDFF;">y_test_oneHot</span> = one_hot(y_test)
</pre>
</div>

<pre class="example">
317cc734-822d-4872-ae62-5961ec4613ae
</pre>
</div>
</div>
<div id="outline-container-orgcfa4e82" class="outline-3">
<h3 id="orgcfa4e82"><span class="section-number-3">5.2.</span> 主成分分析</h3>
<div class="outline-text-3" id="text-5-2">
</div>
<div id="outline-container-orgffab07d" class="outline-4">
<h4 id="orgffab07d"><span class="section-number-4">5.2.1.</span> PCA</h4>
<div class="outline-text-4" id="text-5-2-1">
<p>
PCA會找資料在低維度空間的表示方法，同時盡可能保留資料的變異性。
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Principal Component Analysisva</span>
<span style="color: #339CDB;">from</span> sklearn.decomposition <span style="color: #339CDB;">import</span> PCA

<span style="color: #85DDFF;">n_components</span> = <span style="color: #B5CEA8; font-weight: bold;">784</span>
<span style="color: #85DDFF;">whiten</span> = <span style="color: #339CDB;">False</span>
<span style="color: #85DDFF;">random_state</span> = <span style="color: #B5CEA8; font-weight: bold;">2018</span>

<span style="color: #85DDFF;">pca</span> = PCA(n_components=n_components, whiten=whiten, \
          random_state=random_state)

<span style="color: #85DDFF;">X_train_PCA</span> = pca.fit_transform(X_train)
<span style="color: #85DDFF;">X_train_PCA</span> = pd.DataFrame(data=X_train_PCA, index=train_index)
<span style="color: #579C4C;"># </span><span style="color: #579C4C;">Percentage of Variance Captured by 784 principal components</span>
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">"Variance Explained by all 784 principal components: "</span>, \
      <span style="color: #C586C0;">sum</span>(pca.explained_variance_ratio_))
<span style="color: #579C4C;"># </span><span style="color: #579C4C;">Percentage of Variance Captured by X principal components</span>
<span style="color: #85DDFF;">importanceOfPrincipalComponents</span> = \
    pd.DataFrame(data=pca.explained_variance_ratio_)
<span style="color: #85DDFF;">importanceOfPrincipalComponents</span> = importanceOfPrincipalComponents.T

<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">'Variance Captured by First 10 Principal Components: '</span>,
      importanceOfPrincipalComponents.loc[:,<span style="color: #B5CEA8; font-weight: bold;">0</span>:<span style="color: #B5CEA8; font-weight: bold;">9</span>].<span style="color: #C586C0;">sum</span>(axis=<span style="color: #B5CEA8; font-weight: bold;">1</span>).values)
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">'Variance Captured by First 20 Principal Components: '</span>,
      importanceOfPrincipalComponents.loc[:,<span style="color: #B5CEA8; font-weight: bold;">0</span>:<span style="color: #B5CEA8; font-weight: bold;">19</span>].<span style="color: #C586C0;">sum</span>(axis=<span style="color: #B5CEA8; font-weight: bold;">1</span>).values)
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">'Variance Captured by First 50 Principal Components: '</span>,
      importanceOfPrincipalComponents.loc[:,<span style="color: #B5CEA8; font-weight: bold;">0</span>:<span style="color: #B5CEA8; font-weight: bold;">49</span>].<span style="color: #C586C0;">sum</span>(axis=<span style="color: #B5CEA8; font-weight: bold;">1</span>).values)
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">'Variance Captured by First 100 Principal Components: '</span>,
      importanceOfPrincipalComponents.loc[:,<span style="color: #B5CEA8; font-weight: bold;">0</span>:<span style="color: #B5CEA8; font-weight: bold;">99</span>].<span style="color: #C586C0;">sum</span>(axis=<span style="color: #B5CEA8; font-weight: bold;">1</span>).values)
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">'Variance Captured by First 200 Principal Components: '</span>,
      importanceOfPrincipalComponents.loc[:,<span style="color: #B5CEA8; font-weight: bold;">0</span>:<span style="color: #B5CEA8; font-weight: bold;">199</span>].<span style="color: #C586C0;">sum</span>(axis=<span style="color: #B5CEA8; font-weight: bold;">1</span>).values)
<span style="color: #C586C0;">print</span>(<span style="color: #DB8E73;">'Variance Captured by First 300 Principal Components: '</span>,
      importanceOfPrincipalComponents.loc[:,<span style="color: #B5CEA8; font-weight: bold;">0</span>:<span style="color: #B5CEA8; font-weight: bold;">299</span>].<span style="color: #C586C0;">sum</span>(axis=<span style="color: #B5CEA8; font-weight: bold;">1</span>).values)
</pre>
</div>

<pre class="example">
Variance Explained by all 784 principal components:  1.0000000096602508
Variance Captured by First 10 Principal Components:  [0.48876238]
Variance Captured by First 20 Principal Components:  [0.6439802]
Variance Captured by First 50 Principal Components:  [0.82486093]
Variance Captured by First 100 Principal Components:  [0.9146585]
Variance Captured by First 200 Principal Components:  [0.9665007]
Variance Captured by First 300 Principal Components:  [0.98624885]
</pre>

<p>
由結果看，若將MNIST的原始784個特徵值縮減至300個，仍有近99%的解釋力，即，能捕捉到99%的變異量。PCA能讓我們縮減原始資料的維度，同時保持最多的顯著資訊。
</p>
</div>
<ol class="org-ol">
<li><a id="org59a72b9"></a>2個成分<br />
<div class="outline-text-5" id="text-5-2-1-1">
<p>
如果只拿第1、第2個主成分特徵來進行預測，圖示結果如下：
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Define scatterplot function</span>
<span style="color: #339CDB;">def</span> <span style="color: #D9DAA2;">scatterPlot</span>(xDF, yDF, algoName):
    <span style="color: #85DDFF;">tempDF</span> = pd.DataFrame(data=xDF.loc[:,<span style="color: #B5CEA8; font-weight: bold;">0</span>:<span style="color: #B5CEA8; font-weight: bold;">1</span>], index=xDF.index)
    <span style="color: #85DDFF;">tempDF</span> = pd.concat((tempDF,yDF), axis=<span style="color: #B5CEA8; font-weight: bold;">1</span>, join=<span style="color: #DB8E73;">"inner"</span>)
    tempDF.<span style="color: #85DDFF;">columns</span> = [<span style="color: #DB8E73;">"First Vector"</span>, <span style="color: #DB8E73;">"Second Vector"</span>, <span style="color: #DB8E73;">"Label"</span>]
    sns.lmplot(x=<span style="color: #DB8E73;">"First Vector"</span>, y=<span style="color: #DB8E73;">"Second Vector"</span>, hue=<span style="color: #DB8E73;">"Label"</span>, data=tempDF, fit_reg=<span style="color: #339CDB;">False</span>)
    <span style="color: #85DDFF;">ax</span> = plt.gca()
    ax.set_title(<span style="color: #DB8E73;">"Separation of Observations using "</span>+algoName)

<span style="color: #579C4C;"># </span><span style="color: #579C4C;">View scatterplot</span>
scatterPlot(X_train_PCA, y_train, <span style="color: #DB8E73;">"PCA"</span>)
<span style="color: #579C4C;">#</span><span style="color: #579C4C;">plt.show()</span>
plt.savefig(<span style="color: #DB8E73;">'images/PCA-MNIST-1.png'</span>, dpi=<span style="color: #B5CEA8; font-weight: bold;">300</span>, bbox_inches=<span style="color: #DB8E73;">'tight'</span>)
</pre>
</div>


<div id="org9b62619" class="figure">
<p><img src="images/PCA-MNIST-1.png" alt="PCA-MNIST-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 57: </span>PCA降維</p>
</div>

<p>
由上圖可以看出PCA光找出最有價值的兩個特徵值就能對大致區分數0~9的不同類別，這在非監督式學習中是大分有用的。當資料集有數百萬個特徵、數十億筆資籵時，PCA可以大幅減少機器學習的訓練時間。
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-orga42ccd9" class="outline-4">
<h4 id="orga42ccd9"><span class="section-number-4">5.2.2.</span> Incremental PCA</h4>
<div class="outline-text-4" id="text-5-2-2">
<p>
當資枓集大到無法載入記憶體時，可以小批次的遞增使用PCA，將資料集逐批送入記憶體，其結果與PCA相仿。
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Incremental PCA</span>
plt.cla()
<span style="color: #339CDB;">from</span> sklearn.decomposition <span style="color: #339CDB;">import</span> IncrementalPCA

<span style="color: #85DDFF;">n_components</span> = <span style="color: #B5CEA8; font-weight: bold;">784</span>
<span style="color: #85DDFF;">batch_size</span> = <span style="color: #339CDB;">None</span>

<span style="color: #85DDFF;">incrementalPCA</span> = IncrementalPCA(n_components=n_components, batch_size=batch_size)

<span style="color: #85DDFF;">X_train_incrementalPCA</span> = incrementalPCA.fit_transform(X_train)
<span style="color: #85DDFF;">X_train_incrementalPCA</span> = \
    pd.DataFrame(data=X_train_incrementalPCA, index=train_index)

<span style="color: #85DDFF;">X_validation_incrementalPCA</span> = incrementalPCA.transform(X_validation)
<span style="color: #85DDFF;">X_validation_incrementalPCA</span> = \
    pd.DataFrame(data=X_validation_incrementalPCA, index=validation_index)

scatterPlot(X_train_incrementalPCA, y_train, <span style="color: #DB8E73;">"Incremental PCA"</span>)
plt.savefig(<span style="color: #DB8E73;">'images/PCA-MNIST-2.png'</span>, dpi=<span style="color: #B5CEA8; font-weight: bold;">300</span>, bbox_inches=<span style="color: #DB8E73;">'tight'</span>)
</pre>
</div>


<div id="org3b4f25c" class="figure">
<p><img src="images/PCA-MNIST-2.png" alt="PCA-MNIST-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 58: </span>Incremental PCA</p>
</div>
</div>
</div>
<div id="outline-container-orgab62996" class="outline-4">
<h4 id="orgab62996"><span class="section-number-4">5.2.3.</span> Sparse PCA</h4>
<div class="outline-text-4" id="text-5-2-3">
<p>
一般的PCA希望儘量縮小特徵空間，提高空間中資枓點的密度。但有些機器學習可能需要讓資料點的密度更稀疏，此時可使用Sparse PCA，其稀疏程度由aplha控制。
</p>
<ul class="org-ul">
<li>計算速度會較慢，故只取10000個樣本訓練</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Sparse PCA</span>
plt.cla()
<span style="color: #339CDB;">from</span> sklearn.decomposition <span style="color: #339CDB;">import</span> SparsePCA

<span style="color: #85DDFF;">n_components</span> = <span style="color: #B5CEA8; font-weight: bold;">100</span>
<span style="color: #85DDFF;">alpha</span> = <span style="color: #B5CEA8; font-weight: bold;">0.0001</span>
<span style="color: #85DDFF;">random_state</span> = <span style="color: #B5CEA8; font-weight: bold;">2018</span>
<span style="color: #85DDFF;">n_jobs</span> = -<span style="color: #B5CEA8; font-weight: bold;">1</span>

<span style="color: #85DDFF;">sparsePCA</span> = SparsePCA(n_components=n_components, \
                alpha=alpha, random_state=random_state, n_jobs=n_jobs)

sparsePCA.fit(X_train.loc[:<span style="color: #B5CEA8; font-weight: bold;">10000</span>,:])
<span style="color: #85DDFF;">X_train_sparsePCA</span> = sparsePCA.transform(X_train)
<span style="color: #85DDFF;">X_train_sparsePCA</span> = pd.DataFrame(data=X_train_sparsePCA, index=train_index)

<span style="color: #85DDFF;">X_validation_sparsePCA</span> = sparsePCA.transform(X_validation)
<span style="color: #85DDFF;">X_validation_sparsePCA</span> = \
    pd.DataFrame(data=X_validation_sparsePCA, index=validation_index)

scatterPlot(X_train_sparsePCA, y_train, <span style="color: #DB8E73;">"Sparse PCA"</span>)
plt.savefig(<span style="color: #DB8E73;">'images/PCA-MNIST-3.png'</span>, dpi=<span style="color: #B5CEA8; font-weight: bold;">300</span>, bbox_inches=<span style="color: #DB8E73;">'tight'</span>)

</pre>
</div>


<div id="orgbb8bb40" class="figure">
<p><img src="images/PCA-MNIST-3.png" alt="PCA-MNIST-3.png" width="500" />
</p>
<p><span class="figure-number">Figure 59: </span>Sparse PCA</p>
</div>
</div>
</div>
<div id="outline-container-orgd8de717" class="outline-4">
<h4 id="orgd8de717"><span class="section-number-4">5.2.4.</span> Kernel PCA</h4>
<div class="outline-text-4" id="text-5-2-4">
<p>
非線性投影PCA，透過學習相似度函數(kernel function)，kernel PCA找出大多數資枓點聚集的隱含特徵空間，使用kernel PCA需要設定預期的成分數量、kernel的型態、kernel的係數(gamma)，常見的kernel PCA有radial basis function kernel、RBF kernel。
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Kernel PCA</span>
plt.cla()
<span style="color: #339CDB;">from</span> sklearn.decomposition <span style="color: #339CDB;">import</span> KernelPCA

<span style="color: #85DDFF;">n_components</span> = <span style="color: #B5CEA8; font-weight: bold;">100</span>
<span style="color: #85DDFF;">kernel</span> = <span style="color: #DB8E73;">'rbf'</span>
<span style="color: #85DDFF;">gamma</span> = <span style="color: #339CDB;">None</span>
<span style="color: #85DDFF;">random_state</span> = <span style="color: #B5CEA8; font-weight: bold;">2018</span>
<span style="color: #85DDFF;">n_jobs</span> = <span style="color: #B5CEA8; font-weight: bold;">1</span>

<span style="color: #85DDFF;">kernelPCA</span> = KernelPCA(n_components=n_components, kernel=kernel, \
                      gamma=gamma, n_jobs=n_jobs, random_state=random_state)

kernelPCA.fit(X_train.loc[:<span style="color: #B5CEA8; font-weight: bold;">10000</span>,:])
<span style="color: #85DDFF;">X_train_kernelPCA</span> = kernelPCA.transform(X_train)
<span style="color: #85DDFF;">X_train_kernelPCA</span> = pd.DataFrame(data=X_train_kernelPCA,index=train_index)

<span style="color: #85DDFF;">X_validation_kernelPCA</span> = kernelPCA.transform(X_validation)
<span style="color: #85DDFF;">X_validation_kernelPCA</span> = \
    pd.DataFrame(data=X_validation_kernelPCA, index=validation_index)

scatterPlot(X_train_kernelPCA, y_train, <span style="color: #DB8E73;">"Kernel PCA"</span>)
plt.savefig(<span style="color: #DB8E73;">'images/PCA-MNIST-4.png'</span>, dpi=<span style="color: #B5CEA8; font-weight: bold;">300</span>, bbox_inches=<span style="color: #DB8E73;">'tight'</span>)
</pre>
</div>


<div id="orgf0d772e" class="figure">
<p><img src="images/PCA-MNIST-4.png" alt="PCA-MNIST-4.png" width="500" />
</p>
<p><span class="figure-number">Figure 60: </span>Kernel PCA</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orge7ac210" class="outline-3">
<h3 id="orge7ac210"><span class="section-number-3">5.3.</span> 奇異值分解(Singular Value Decomposition，SVD)</h3>
<div class="outline-text-3" id="text-5-3">
<p>
SVD是在機器學習領域廣泛應用的演算法，它不光可以用於降維演算法中的特徵分解，還可以用於推薦系統，以及自然語言處理等領域，目的在減少原始特徵值矩陣的秩，目前幾乎所有封裝好的PCA算法內部採用的都是SVD算法進行特徵值、特徵向量以及K值的求解。
</p>

<p>
SVD是一種線性代數的技術，它將一個矩陣分解為三個矩陣的乘積，包括一個左奇異向量矩陣、一個對角奇異值矩陣和一個右奇異向量矩陣。
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Singular Value Decomposition</span>
plt.cla()
<span style="color: #339CDB;">from</span> sklearn.decomposition <span style="color: #339CDB;">import</span> TruncatedSVD

<span style="color: #85DDFF;">n_components</span> = <span style="color: #B5CEA8; font-weight: bold;">200</span>
<span style="color: #85DDFF;">algorithm</span> = <span style="color: #DB8E73;">'randomized'</span>
<span style="color: #85DDFF;">n_iter</span> = <span style="color: #B5CEA8; font-weight: bold;">5</span>
<span style="color: #85DDFF;">random_state</span> = <span style="color: #B5CEA8; font-weight: bold;">2018</span>

<span style="color: #85DDFF;">svd</span> = TruncatedSVD(n_components=n_components, algorithm=algorithm, \
                   n_iter=n_iter, random_state=random_state)

<span style="color: #85DDFF;">X_train_svd</span> = svd.fit_transform(X_train)
<span style="color: #85DDFF;">X_train_svd</span> = pd.DataFrame(data=X_train_svd, index=train_index)

<span style="color: #85DDFF;">X_validation_svd</span> = svd.transform(X_validation)
<span style="color: #85DDFF;">X_validation_svd</span> = pd.DataFrame(data=X_validation_svd, index=validation_index)

scatterPlot(X_train_svd, y_train, <span style="color: #DB8E73;">"Singular Value Decomposition"</span>)
plt.savefig(<span style="color: #DB8E73;">'images/SVD-MNIST.png'</span>, dpi=<span style="color: #B5CEA8; font-weight: bold;">300</span>, bbox_inches=<span style="color: #DB8E73;">'tight'</span>)
</pre>
</div>


<div id="orgdb2efae" class="figure">
<p><img src="images/SVD-MNIST.png" alt="SVD-MNIST.png" width="500" />
</p>
<p><span class="figure-number">Figure 61: </span>Caption</p>
</div>
</div>
</div>
<div id="outline-container-org34898f3" class="outline-3">
<h3 id="org34898f3"><span class="section-number-3">5.4.</span> Isomap</h3>
<div class="outline-text-3" id="text-5-4">
<p>
非線性投影，基本的流形學習方法為isometric mapping，簡稱isomap。Isomap透過計算點與點間的成對距離（曲線距離或捷線距離，而非歐幾里德距離）來學習能代表原始特徵集的一個新低維度embedding。
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Isomap</span>
plt.cla()
<span style="color: #339CDB;">from</span> sklearn.manifold <span style="color: #339CDB;">import</span> Isomap

<span style="color: #85DDFF;">n_neighbors</span> = <span style="color: #B5CEA8; font-weight: bold;">5</span>
<span style="color: #85DDFF;">n_components</span> = <span style="color: #B5CEA8; font-weight: bold;">10</span>
<span style="color: #85DDFF;">n_jobs</span> = <span style="color: #B5CEA8; font-weight: bold;">4</span>

<span style="color: #85DDFF;">isomap</span> = Isomap(n_neighbors=n_neighbors, \
                n_components=n_components, n_jobs=n_jobs)

isomap.fit(X_train.loc[<span style="color: #B5CEA8; font-weight: bold;">0</span>:<span style="color: #B5CEA8; font-weight: bold;">5000</span>,:])
<span style="color: #85DDFF;">X_train_isomap</span> = isomap.transform(X_train)
<span style="color: #85DDFF;">X_train_isomap</span> = pd.DataFrame(data=X_train_isomap, index=train_index)

<span style="color: #85DDFF;">X_validation_isomap</span> = isomap.transform(X_validation)
<span style="color: #85DDFF;">X_validation_isomap</span> = pd.DataFrame(data=X_validation_isomap, \
                                   index=validation_index)

scatterPlot(X_train_isomap, y_train, <span style="color: #DB8E73;">"Isomap"</span>)
plt.savefig(<span style="color: #DB8E73;">'images/ISOMAP-MNIST.png'</span>, dpi=<span style="color: #B5CEA8; font-weight: bold;">300</span>, bbox_inches=<span style="color: #DB8E73;">'tight'</span>)
</pre>
</div>


<div id="orgd3bdf61" class="figure">
<p><img src="images/ISOMAP-MNIST.png" alt="ISOMAP-MNIST.png" width="500" />
</p>
<p><span class="figure-number">Figure 62: </span>Isomap</p>
</div>
</div>
</div>
<div id="outline-container-org9212fb1" class="outline-3">
<h3 id="org9212fb1"><span class="section-number-3">5.5.</span> 局部線性嵌入法(Locally Linear Embedding)</h3>
<div class="outline-text-3" id="text-5-5">
<p>
LLE透過以下方式來找出高維資枓中的非線性結構
</p>
<ul class="org-ul">
<li>分割資料成為較小的子集（包含數個點的鄰近區域）</li>
<li>將每個子集塑模成一個線性的embedding</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Locally Linear Embedding (LLE)</span>

<span style="color: #339CDB;">from</span> sklearn.manifold <span style="color: #339CDB;">import</span> LocallyLinearEmbedding
<span style="color: #339CDB;">import</span> matplotlib.pyplot <span style="color: #339CDB;">as</span> plt
plt.cla()
<span style="color: #85DDFF;">n_neighbors</span> = <span style="color: #B5CEA8; font-weight: bold;">10</span>
<span style="color: #85DDFF;">n_components</span> = <span style="color: #B5CEA8; font-weight: bold;">2</span>
<span style="color: #85DDFF;">method</span> = <span style="color: #DB8E73;">'modified'</span>
<span style="color: #85DDFF;">n_jobs</span> = <span style="color: #B5CEA8; font-weight: bold;">4</span>
<span style="color: #85DDFF;">random_state</span> = <span style="color: #B5CEA8; font-weight: bold;">2018</span>

<span style="color: #85DDFF;">lle</span> = LocallyLinearEmbedding(n_neighbors=n_neighbors, \
        n_components=n_components, method=method, \
        random_state=random_state, n_jobs=n_jobs)

lle.fit(X_train.loc[<span style="color: #B5CEA8; font-weight: bold;">0</span>:<span style="color: #B5CEA8; font-weight: bold;">5000</span>,:])
<span style="color: #85DDFF;">X_train_lle</span> = lle.transform(X_train)
<span style="color: #85DDFF;">X_train_lle</span> = pd.DataFrame(data=X_train_lle, index=train_index)

<span style="color: #85DDFF;">X_validation_lle</span> = lle.transform(X_validation)
<span style="color: #85DDFF;">X_validation_lle</span> = pd.DataFrame(data=X_validation_lle, index=validation_index)

scatterPlot(X_train_lle, y_train, <span style="color: #DB8E73;">"Locally Linear Embedding"</span>)
plt.savefig(<span style="color: #DB8E73;">'images/LLE-MNIST.png'</span>, dpi=<span style="color: #B5CEA8; font-weight: bold;">300</span>, bbox_inches=<span style="color: #DB8E73;">'tight'</span>)
</pre>
</div>

<pre class="example">
c257da09-7359-4991-bef2-19ed630c5ce1
</pre>



<div id="org4d7d477" class="figure">
<p><img src="images/LLE-MNIST.png" alt="LLE-MNIST.png" width="500" />
</p>
<p><span class="figure-number">Figure 63: </span>局部線性嵌入法</p>
</div>
</div>
</div>
<div id="outline-container-orge17af58" class="outline-3">
<h3 id="orge17af58"><span class="section-number-3">5.6.</span> t-Distributed Stochastic Neighbor Embedding</h3>
<div class="outline-text-3" id="text-5-6">
<p>
t-SNE建立兩個機率分佈來將高維資料點塑模至二維或三維空間，並使在此空間中彼此相似的點靠近、不相似的點疏遠。
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">t-SNE</span>
plt.cla()
<span style="color: #339CDB;">from</span> sklearn.manifold <span style="color: #339CDB;">import</span> TSNE

<span style="color: #85DDFF;">n_components</span> = <span style="color: #B5CEA8; font-weight: bold;">2</span>
<span style="color: #85DDFF;">learning_rate</span> = <span style="color: #B5CEA8; font-weight: bold;">300</span>
<span style="color: #85DDFF;">perplexity</span> = <span style="color: #B5CEA8; font-weight: bold;">30</span>
<span style="color: #85DDFF;">early_exaggeration</span> = <span style="color: #B5CEA8; font-weight: bold;">12</span>
<span style="color: #85DDFF;">init</span> = <span style="color: #DB8E73;">'random'</span>
<span style="color: #85DDFF;">random_state</span> = <span style="color: #B5CEA8; font-weight: bold;">2018</span>

<span style="color: #85DDFF;">tSNE</span> = TSNE(n_components=n_components, learning_rate=learning_rate, \
            perplexity=perplexity, early_exaggeration=early_exaggeration, \
            init=init, random_state=random_state)

<span style="color: #85DDFF;">X_train_tSNE</span> = tSNE.fit_transform(X_train_PCA.loc[:<span style="color: #B5CEA8; font-weight: bold;">5000</span>,:<span style="color: #B5CEA8; font-weight: bold;">9</span>])
<span style="color: #85DDFF;">X_train_tSNE</span> = pd.DataFrame(data=X_train_tSNE, index=train_index[:<span style="color: #B5CEA8; font-weight: bold;">5001</span>])

scatterPlot(X_train_tSNE, y_train, <span style="color: #DB8E73;">"t-SNE"</span>)
plt.savefig(<span style="color: #DB8E73;">'images/tSNE-MNIST.png'</span>, dpi=<span style="color: #B5CEA8; font-weight: bold;">300</span>, bbox_inches=<span style="color: #DB8E73;">'tight'</span>)
</pre>
</div>


<div id="orgfddb19d" class="figure">
<p><img src="images/tSNE-MNIST.png" alt="tSNE-MNIST.png" width="500" />
</p>
<p><span class="figure-number">Figure 64: </span>t-Distributed Stochastic Neighbor Embedding</p>
</div>
</div>
</div>
<div id="outline-container-orgf87a617" class="outline-3">
<h3 id="orgf87a617"><span class="section-number-3">5.7.</span> 字典學習</h3>
<div class="outline-text-3" id="text-5-7">
<p>
不依賴幾何指標或距離指標，當資料量很大時，嚴格鑽析每個樣本就會消耗大量時間，MiniBatch方過降低計算精度來換取執行時間，但仍能藉由龐大的資料量來取得合理的效能。
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Mini-batch dictionary learning</span>
plt.cla()
<span style="color: #339CDB;">from</span> sklearn.decomposition <span style="color: #339CDB;">import</span> MiniBatchDictionaryLearning

<span style="color: #85DDFF;">n_components</span> = <span style="color: #B5CEA8; font-weight: bold;">50</span>
<span style="color: #85DDFF;">alpha</span> = <span style="color: #B5CEA8; font-weight: bold;">1</span>
<span style="color: #85DDFF;">batch_size</span> = <span style="color: #B5CEA8; font-weight: bold;">200</span>
<span style="color: #85DDFF;">max_iter</span> = <span style="color: #B5CEA8; font-weight: bold;">1000</span>
<span style="color: #85DDFF;">random_state</span> = <span style="color: #B5CEA8; font-weight: bold;">2018</span>

<span style="color: #85DDFF;">miniBatchDictLearning</span> = MiniBatchDictionaryLearning( \
                        n_components=n_components, alpha=alpha, \
                        batch_size=batch_size, max_iter = max_iter, \
                        random_state=random_state)

miniBatchDictLearning.fit(X_train.loc[:,:<span style="color: #B5CEA8; font-weight: bold;">10000</span>])
<span style="color: #85DDFF;">X_train_miniBatchDictLearning</span> = miniBatchDictLearning.fit_transform(X_train)
<span style="color: #85DDFF;">X_train_miniBatchDictLearning</span> = pd.DataFrame( \
    data=X_train_miniBatchDictLearning, index=train_index)

<span style="color: #85DDFF;">X_validation_miniBatchDictLearning</span> = \
    miniBatchDictLearning.transform(X_validation)
<span style="color: #85DDFF;">X_validation_miniBatchDictLearning</span> = \
    pd.DataFrame(data=X_validation_miniBatchDictLearning, \
    index=validation_index)

scatterPlot(X_train_miniBatchDictLearning, y_train, \
            <span style="color: #DB8E73;">"Mini-batch Dictionary Learning"</span>)
plt.savefig(<span style="color: #DB8E73;">'images/DIC-MNIST.png'</span>, dpi=<span style="color: #B5CEA8; font-weight: bold;">300</span>, bbox_inches=<span style="color: #DB8E73;">'tight'</span>)
</pre>
</div>

<pre class="example">
/usr/local/lib/python3.11/site-packages/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
  new_code = orthogonal_mp_gram(
/usr/local/lib/python3.11/site-packages/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
  new_code = orthogonal_mp_gram(
</pre>



<div id="org9f99021" class="figure">
<p><img src="images/DIC-MNIST.png" alt="DIC-MNIST.png" width="500" />
</p>
<p><span class="figure-number">Figure 65: </span>字典學習</p>
</div>
</div>
</div>
<div id="outline-container-orgb4db16d" class="outline-3">
<h3 id="orgb4db16d"><span class="section-number-3">5.8.</span> 獨立成份分析</h3>
<div class="outline-text-3" id="text-5-8">
<p>
Independent component analysis
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #579C4C;"># </span><span style="color: #579C4C;">Independent Component Analysis</span>
plt.cla()
<span style="color: #339CDB;">from</span> sklearn.decomposition <span style="color: #339CDB;">import</span> FastICA

<span style="color: #85DDFF;">n_components</span> = <span style="color: #B5CEA8; font-weight: bold;">25</span>
<span style="color: #85DDFF;">algorithm</span> = <span style="color: #DB8E73;">'parallel'</span>
<span style="color: #85DDFF;">whiten</span>=<span style="color: #DB8E73;">'arbitrary-variance'</span>
<span style="color: #85DDFF;">max_iter</span> = <span style="color: #B5CEA8; font-weight: bold;">1000</span>
<span style="color: #85DDFF;">random_state</span> = <span style="color: #B5CEA8; font-weight: bold;">2018</span>

<span style="color: #85DDFF;">fastICA</span> = FastICA(n_components=n_components, algorithm=algorithm, \
                  whiten=whiten, max_iter=max_iter, random_state=random_state)

<span style="color: #85DDFF;">X_train_fastICA</span> = fastICA.fit_transform(X_train)
<span style="color: #85DDFF;">X_train_fastICA</span> = pd.DataFrame(data=X_train_fastICA, index=train_index)

<span style="color: #85DDFF;">X_validation_fastICA</span> = fastICA.transform(X_validation)
<span style="color: #85DDFF;">X_validation_fastICA</span> = pd.DataFrame(data=X_validation_fastICA, \
                                    index=validation_index)

scatterPlot(X_train_fastICA, y_train, <span style="color: #DB8E73;">"Independent Component Analysis"</span>)
plt.savefig(<span style="color: #DB8E73;">'images/ICA-MNIST.png'</span>, dpi=<span style="color: #B5CEA8; font-weight: bold;">300</span>, bbox_inches=<span style="color: #DB8E73;">'tight'</span>)
</pre>
</div>

<pre class="example">
/usr/local/lib/python3.11/site-packages/sklearn/decomposition/_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.
  warnings.warn(
</pre>



<div id="org92b7308" class="figure">
<p><img src="images/ICA-MNIST.png" alt="ICA-MNIST.png" width="500" />
</p>
<p><span class="figure-number">Figure 66: </span>獨立成份分析</p>
</div>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Hands-On Machine Learning with Scikit-Learn: Aurelien Geron
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://www.oreilly.com/library/view/hands-on-unsupervised-learning/9781492035633/">Hands-On Unsupervised Learning Using Python</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://medium.com/ai-academy-taiwan/clustering-method-4-ed927a5b4377">Clustering method 4</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4" role="doc-backlink">4</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://builtin.com/machine-learning/agglomerative-clustering">Hierarchical Clustering: Agglomerative + Divisive Clustering</a>
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Yung-Chin Yen</p>
<p class="date">Created: 2024-02-12 Mon 12:15</p>
</div>
</body>
</html>
